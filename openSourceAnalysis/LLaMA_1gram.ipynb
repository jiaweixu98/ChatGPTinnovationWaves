{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:21<00:00,  2.80it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# we can not find 'PanGU-Σ', 'Luminous' in close Source index, cannot find 'Galatica', 'YaLM', 'PanGu-α' in open source list , so delete them.\n",
    "openSourceList = ['T5', 'mT5', 'CPM-2','T0','GPT-NeoX-20B','CodeGen','Tk-Instruct','UL2','OPT','NLLB','BLOOM','GLM','Flan-T5','mT0','BLOOMZ','OPT-IML','Pythia','LLaMA','Vicuna','ChatGLM','CodeGeeX','Koala','GPT-2','GPT2','GPT 2']\n",
    "closeSourceList = ['GShard','GPT-3','GPT 3', 'GPT3','LaMDA','HyperCLOVA','Codex','ERNIE 3','Jurassic-1','FLAN','MT-NLG','Yuan 1.0','Anthropic','WebGPT','Gopher','ERNIE 3.0 Titan','GLaM','InstructGPT','ChatGPT','AlphaCode','Chinchilla','PaLM','Cohere','AlexaTM','Sparrow','WeLM','U-PaLM','Flan-PaLM','Flan-U-PaLM','Alpaca','GPT-4' ,'GPT 4', 'GPT4','Claude']\n",
    "\n",
    "readfile = \"/data/jx4237data/DataForChatGPTinnovationWaves/\"\n",
    "df = pd.read_csv(readfile + 'LLM09enhanced.csv',dtype=object)\n",
    "# some paper we need to delete:\n",
    "# “fake” palm paper 1909.02134\n",
    "# fake openai 1506.04006\n",
    "delete_list = ['1506.04006','1909.02134']\n",
    "df = df[~df['id'].isin(delete_list)]\n",
    "# model2paper\n",
    "model2paper = {}\n",
    "for model in tqdm(openSourceList+closeSourceList):\n",
    "    if model in model2paper:\n",
    "        print('error')\n",
    "    model2paper[model] = set()\n",
    "    for paper in df[df['abstract'].str.contains(\"\\\\b%s\\\\b\"%model) | df['abstract'].str.lower().str.contains(\"\\\\b%s\\\\b\"%model.lower())  | df['title'].str.contains(\"\\\\b%s\\\\b\"%model) | df['title'].str.lower().str.contains(\"\\\\b%s\\\\b\"%model.lower())]['id']  :\n",
    "        model2paper[model].add(paper)\n",
    "\n",
    "for i in model2paper['GPT 4']:\n",
    "    model2paper['GPT-4'].add(i)\n",
    "for i in model2paper['GPT4']:\n",
    "    model2paper['GPT-4'].add(i)\n",
    "for i in model2paper['GPT 3']:\n",
    "    model2paper['GPT-3'].add(i)\n",
    "for i in model2paper['GPT3']:\n",
    "    model2paper['GPT-3'].add(i)\n",
    "for i in model2paper['GPT2']:\n",
    "    model2paper['GPT-2'].add(i)\n",
    "for i in model2paper['GPT 2']:\n",
    "    model2paper['GPT-2'].add(i)\n",
    "del model2paper['GPT 4']\n",
    "del model2paper['GPT4']\n",
    "del model2paper['GPT3']\n",
    "del model2paper['GPT 3']\n",
    "del model2paper['GPT2']\n",
    "del model2paper['GPT 2']\n",
    "\n",
    "# paper2model\n",
    "paper2model = {}\n",
    "for model, paperSet in model2paper.items():\n",
    "    for paper in paperSet:\n",
    "        if paper not in paper2model:\n",
    "            paper2model[paper] =[model]\n",
    "        else:\n",
    "            paper2model[paper].append(model)\n",
    "\n",
    "\n",
    "openPaperSet = set()\n",
    "closePaperSet = set()\n",
    "for k,v in model2paper.items():\n",
    "    if k in closeSourceList:\n",
    "        for paper in v:\n",
    "            closePaperSet.add(paper)\n",
    "    if k in openSourceList:\n",
    "        for paper in v:\n",
    "            openPaperSet.add(paper)\n",
    "pureOpen = openPaperSet - closePaperSet\n",
    "pureClose = closePaperSet - openPaperSet\n",
    "mixed = openPaperSet & closePaperSet\n",
    "all = openPaperSet | closePaperSet\n",
    "\n",
    "paper2pubdate = {}\n",
    "for i in range(len(df)):\n",
    "    paper2pubdate[list(df['id'])[i]] = list(df['update_date'])[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jx4237/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')  # Download the WordNet dataset if you haven't already\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.txt') as f:\n",
    "    stopwordslist = f.read().splitlines()\n",
    "stopwordslist.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genrawdata(modelname):\n",
    "    c = Counter()\n",
    "    for i in df[df['id'].isin(model2paper[modelname])]['abstract'] + df[df['id'].isin(model2paper[modelname])]['title']:\n",
    "        for word in re.split('[, \\.!?:()]+', i.strip()):\n",
    "            if lemmatizer.lemmatize(word.lower()) in stopwordslist:\n",
    "                continue\n",
    "            else:\n",
    "                c.update([lemmatizer.lemmatize(word.lower())])\n",
    "    totalunique = len(c)\n",
    "    sumcounts = sum(c.values())\n",
    "    model_list = []\n",
    "    for k,v in sorted(c.items(), key= lambda x:-x[1]):\n",
    "        model_list.append({\n",
    "            'types':k,\n",
    "            'counts':v,\n",
    "            'totalunique':totalunique,\n",
    "            'probs':v/sumcounts,\n",
    "        })\n",
    "    return {modelname:model_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genrawdataByname(paperset,name):\n",
    "    c = Counter()\n",
    "    for i in df[df['id'].isin(paperset)]['abstract'] + df[df['id'].isin(paperset)]['title']:\n",
    "        for word in re.split('[, \\.!?:()]+', i.strip()):\n",
    "            if lemmatizer.lemmatize(word.lower()) in stopwordslist:\n",
    "                continue\n",
    "            else:\n",
    "                c.update([lemmatizer.lemmatize(word.lower())])\n",
    "    totalunique = len(c)\n",
    "    sumcounts = sum(c.values())\n",
    "    model_list = []\n",
    "    for k,v in sorted(c.items(), key= lambda x:-x[1]):\n",
    "        model_list.append({\n",
    "            'types':k,\n",
    "            'counts':v,\n",
    "            'totalunique':totalunique,\n",
    "            'probs':v/sumcounts,\n",
    "        })\n",
    "    return {name:model_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8185701"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "openSourceList = ['T5', 'mT5', 'CPM-2','T0','GPT-NeoX-20B','CodeGen','Tk-Instruct','UL2','OPT','NLLB','BLOOM','GLM','Flan-T5','mT0','BLOOMZ','OPT-IML','Pythia','LLaMA','Vicuna','ChatGLM','CodeGeeX','Koala','GPT-2']\n",
    "closeSourceList = ['GShard','GPT-3','LaMDA','HyperCLOVA','Codex','ERNIE 3','Jurassic-1','FLAN','MT-NLG','Yuan 1.0','Anthropic','WebGPT','Gopher','ERNIE 3.0 Titan','GLaM','InstructGPT','ChatGPT','AlphaCode','Chinchilla','PaLM','Cohere','AlexaTM','Sparrow','WeLM','U-PaLM','Flan-PaLM','Flan-U-PaLM','Alpaca','GPT-4' ,'Claude']\n",
    "\n",
    "opensorcePaper = set()\n",
    "closedsorcePaper = set()\n",
    "\n",
    "for i in openSourceList:\n",
    "    opensorcePaper = opensorcePaper | model2paper[i]\n",
    "for i in closeSourceList:\n",
    "    closedsorcePaper = closedsorcePaper | model2paper[i]\n",
    "\n",
    "\n",
    "\n",
    "s = json.dumps({**genrawdataByname(opensorcePaper,'open sorce'), **genrawdataByname(closedsorcePaper,'closed sorce'),**genrawdata('LLaMA'),**genrawdata('GPT-4'),**genrawdata('GPT-3'),**genrawdata('T5'), **genrawdata('GPT-2'),**genrawdata('PaLM'), **genrawdata('ChatGPT'), })\n",
    "open(\"LLM09enhanced.json\",\"w\").write(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

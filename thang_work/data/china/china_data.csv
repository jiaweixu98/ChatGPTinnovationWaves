id_x,submitter,authors_x,title,comments,journal-ref,doi,report-no,categories,license,abstract,versions,update_date,authors_parsed,ContainChatGPT,ContainGPT,publish_date_v1,versionsNumber,authorNumber,categoryNumber,modelNumber,OpenModelNumber,CloseModelNumber,id_y,corpusId,url,venue,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy,s2FieldsOfStudy,authors_y,Institute,Country,month_year
1904.09636,Ming Gong,"Ze Yang, Linjun Shou, Ming Gong, Wutao Lin, Daxin Jiang",Model Compression with Multi-Task Knowledge Distillation for Web-scale Question Answering System,"9 pages, 2 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep pre-training and fine-tuning models (like BERT, OpenAI GPT) have demonstrated excellent results in question answering areas. However, due to the sheer amount of model parameters, the inference speed of these models is very slow. How to apply these complex models to real business scenarios becomes a challenging but practical problem. Previous works often leverage model compression approaches to resolve this problem. However, these methods usually induce information loss during the model compression procedure, leading to incomparable results between compressed model and the original model. To tackle this challenge, we propose a Multi-task Knowledge Distillation Model (MKDM for short) for web-scale Question Answering system, by distilling knowledge from multiple teacher models to a light-weight student model. In this way, more generalized knowledge can be transferred. The experiment results show that our method can significantly outperform the baseline methods and even achieve comparable results with the original teacher models, along with significant speedup of model inference. ","[{'version': 'v1', 'created': 'Sun, 21 Apr 2019 17:46:24 GMT'}]",2019-04-23,"[['Yang', 'Ze', ''], ['Shou', 'Linjun', ''], ['Gong', 'Ming', ''], ['Lin', 'Wutao', ''], ['Jiang', 'Daxin', '']]",0,1,2019-04-21,1,5,1,0,0,0,5fd20d2fd9a5408221a76c37675c329a9310963d,127983002.0,https://www.semanticscholar.org/paper/5fd20d2fd9a5408221a76c37675c329a9310963d,arXiv.org,2019.0,27.0,16.0,4.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1971985', 'name': 'Ze Yang'}, {'authorId': '24962156', 'name': 'Linjun Shou'}, {'authorId': '50175330', 'name': 'Ming Gong'}, {'authorId': '5617558', 'name': 'Wutao Lin'}, {'authorId': '71790825', 'name': 'Daxin Jiang'}]","['Beijing University of Posts and Telecommunications', 'Microsoft']",['China'],2019-04
1905.07504,Zhongyang Li,"Zhongyang Li, Xiao Ding and Ting Liu",Story Ending Prediction by Transferable BERT,Accepted and to appear in IJCAI 2019,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advances, such as GPT and BERT, have shown success in incorporating a pre-trained transformer language model and fine-tuning operation to improve downstream NLP systems. However, this framework still has some fundamental problems in effectively incorporating supervised knowledge from other related tasks. In this study, we investigate a transferable BERT (TransBERT) training framework, which can transfer not only general language knowledge from large-scale unlabeled data but also specific kinds of knowledge from various semantically related supervised tasks, for a target task. Particularly, we propose utilizing three kinds of transfer tasks, including natural language inference, sentiment classification, and next action prediction, to further train BERT based on a pre-trained model. This enables the model to get a better initialization for the target task. We take story ending prediction as the target task to conduct experiments. The final result, an accuracy of 91.8%, dramatically outperforms previous state-of-the-art baseline methods. Several comparative experiments give some helpful suggestions on how to select transfer tasks. Error analysis shows what are the strength and weakness of BERT-based models for story ending prediction. ","[{'version': 'v1', 'created': 'Fri, 17 May 2019 23:52:08 GMT'}, {'version': 'v2', 'created': 'Tue, 21 May 2019 02:37:11 GMT'}]",2019-05-22,"[['Li', 'Zhongyang', ''], ['Ding', 'Xiao', ''], ['Liu', 'Ting', '']]",0,1,2019-05-17,2,3,2,0,0,0,07c53193b50aa0b108b14b9edfbef64ea1e9119b,159040000.0,https://www.semanticscholar.org/paper/07c53193b50aa0b108b14b9edfbef64ea1e9119b,International Joint Conference on Artificial Intelligence,2019.0,36.0,48.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '3239620', 'name': 'Zhongyang Li'}, {'authorId': '2117434160', 'name': 'Xiao Ding'}, {'authorId': '40282288', 'name': 'Ting Liu'}]",['Harbin Institute of Technology'],['China'],2019-05
1906.09072,Ruijia Yang,"YiGui Luo, RuiJia Yang, Wei Sha, WeiYi Ding, YouTeng Sun, YiSi Wang",Evolution Attack On Neural Networks,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many studies have been done to prove the vulnerability of neural networks to adversarial example. A trained and well-behaved model can be fooled by a visually imperceptible perturbation, i.e., an originally correctly classified image could be misclassified after a slight perturbation. In this paper, we propose a black-box strategy to attack such networks using an evolution algorithm. First, we formalize the generation of an adversarial example into the optimization problem of perturbations that represent the noise added to an original image at each pixel. To solve this optimization problem in a black-box way, we find that an evolution algorithm perfectly meets our requirement since it can work without any gradient information. Therefore, we test various evolution algorithms, including a simple genetic algorithm, a parameter-exploring policy gradient, an OpenAI evolution strategy, and a covariance matrix adaptive evolution strategy. Experimental results show that a covariance matrix adaptive evolution Strategy performs best in this optimization problem. Additionally, we also perform several experiments to explore the effect of different regularizations on improving the quality of an adversarial example. ","[{'version': 'v1', 'created': 'Fri, 21 Jun 2019 11:22:03 GMT'}]",2019-06-24,"[['Luo', 'YiGui', ''], ['Yang', 'RuiJia', ''], ['Sha', 'Wei', ''], ['Ding', 'WeiYi', ''], ['Sun', 'YouTeng', ''], ['Wang', 'YiSi', '']]",0,0,2019-06-21,1,6,1,0,0,0,4d8c6ca1cb8de00e854950ca3029ab2c6e7e9e8c,195316591.0,https://www.semanticscholar.org/paper/4d8c6ca1cb8de00e854950ca3029ab2c6e7e9e8c,arXiv.org,2019.0,25.0,0.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2118264343', 'name': 'Yigui Luo'}, {'authorId': '2115427816', 'name': 'Ruijia Yang'}, {'authorId': '2093764603', 'name': 'Wei Sha'}, {'authorId': '104514106', 'name': 'Weiyi Ding'}, {'authorId': '150352117', 'name': 'YouTeng Sun'}, {'authorId': '2115869699', 'name': 'Yisi Wang'}]","['Software Engineering Institute', 'Tongji University']","['China', 'United States']",2019-06
1907.00151,Yi Liao,"Yi Liao, Yasheng Wang, Qun Liu, Xin Jiang",GPT-based Generation for Classical Chinese Poetry,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a simple yet effective method for generating high quality classical Chinese poetry with Generative Pre-trained Language Model (GPT). The method adopts a simple GPT model, without using any human crafted rules or features, or designing any additional neural components. While the proposed model learns to generate various forms of classical Chinese poems, including Jueju, L\""{u}shi, various Cipai and Couples, the generated poems are of very high quality. We also propose and implement a method to fine-tune the model to generate acrostic poetry. To the best of our knowledge, this is the first to employ GPT in developing a poetry generation system. We have released an online mini demonstration program on Wechat to show the generation capability of the proposed method for classical Chinese poetry. ","[{'version': 'v1', 'created': 'Sat, 29 Jun 2019 06:04:48 GMT'}, {'version': 'v2', 'created': 'Tue, 2 Jul 2019 14:18:55 GMT'}, {'version': 'v3', 'created': 'Wed, 3 Jul 2019 12:55:06 GMT'}, {'version': 'v4', 'created': 'Fri, 12 Jul 2019 05:44:58 GMT'}, {'version': 'v5', 'created': 'Thu, 5 Sep 2019 02:34:36 GMT'}]",2019-09-06,"[['Liao', 'Yi', ''], ['Wang', 'Yasheng', ''], ['Liu', 'Qun', ''], ['Jiang', 'Xin', '']]",0,1,2019-06-29,5,4,2,0,0,0,83b56c3c7a61767bd88d85796aa5dbc4976912c3,195767456.0,https://www.semanticscholar.org/paper/83b56c3c7a61767bd88d85796aa5dbc4976912c3,arXiv.org,2019.0,15.0,32.0,3.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2048004675', 'name': 'Yi Liao'}, {'authorId': '2108738457', 'name': 'Yasheng Wang'}, {'authorId': '30738758', 'name': 'Qun Liu'}, {'authorId': '145820291', 'name': 'Xin Jiang'}]",['Huawei Technologies (China)'],['China'],2019-06
1908.05672,Lei Li,"Jiacheng Yang, Mingxuan Wang, Hao Zhou, Chengqi Zhao, Yong Yu, Weinan
  Zhang, Lei Li",Towards Making the Most of BERT in Neural Machine Translation,"10pages. the same as AAAI 2020 version, reformated with additional
  link to github repository",,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  GPT-2 and BERT demonstrate the effectiveness of using pre-trained language models (LMs) on various natural language processing tasks. However, LM fine-tuning often suffers from catastrophic forgetting when applied to resource-rich tasks. In this work, we introduce a concerted training framework (CTNMT) that is the key to integrate the pre-trained LMs to neural machine translation (NMT). Our proposed CTNMT consists of three techniques: a) asymptotic distillation to ensure that the NMT model can retain the previous pre-trained knowledge; b) a dynamic switching gate to avoid catastrophic forgetting of pre-trained knowledge; and c) a strategy to adjust the learning paces according to a scheduled policy. Our experiments in machine translation show CTNMT gains of up to 3 BLEU score on the WMT14 English-German language pair which even surpasses the previous state-of-the-art pre-training aided NMT by 1.4 BLEU score. While for the large WMT14 English-French task with 40 millions of sentence-pairs, our base model still significantly improves upon the state-of-the-art Transformer big model by more than 1 BLEU score. The code and model can be downloaded from https://github.com/bytedance/neurst/ tree/master/examples/ctnmt. ","[{'version': 'v1', 'created': 'Thu, 15 Aug 2019 03:33:50 GMT'}, {'version': 'v2', 'created': 'Mon, 19 Aug 2019 04:36:18 GMT'}, {'version': 'v3', 'created': 'Fri, 30 Aug 2019 11:26:20 GMT'}, {'version': 'v4', 'created': 'Thu, 26 Mar 2020 12:12:56 GMT'}, {'version': 'v5', 'created': 'Mon, 20 Jun 2022 02:58:06 GMT'}]",2022-06-22,"[['Yang', 'Jiacheng', ''], ['Wang', 'Mingxuan', ''], ['Zhou', 'Hao', ''], ['Zhao', 'Chengqi', ''], ['Yu', 'Yong', ''], ['Zhang', 'Weinan', ''], ['Li', 'Lei', '']]",0,1,2019-08-15,5,7,2,1,1,0,7a09101ac03b74db501648597fa54e992a0fc84f,201058686.0,https://www.semanticscholar.org/paper/7a09101ac03b74db501648597fa54e992a0fc84f,AAAI Conference on Artificial Intelligence,2019.0,30.0,109.0,17.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2141054362', 'name': 'Jiacheng Yang'}, {'authorId': '50468534', 'name': 'Mingxuan Wang'}, {'authorId': '2111824520', 'name': 'Hao Zhou'}, {'authorId': '144562857', 'name': 'Chengqi Zhao'}, {'authorId': '1811427', 'name': 'Yong Yu'}, {'authorId': '2108309275', 'name': 'Weinan Zhang'}, {'authorId': '143900005', 'name': 'Lei Li'}]","['ByteDance', 'Shanghai Jiao Tong University']",['China'],2019-08
1908.08206,Liang Wang,"Liang Wang, Wei Zhao, Ruoyu Jia, Sujian Li, Jingming Liu",Denoising based Sequence-to-Sequence Pre-training for Text Generation,Accepted to EMNLP 2019,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  This paper presents a new sequence-to-sequence (seq2seq) pre-training method PoDA (Pre-training of Denoising Autoencoders), which learns representations suitable for text generation tasks. Unlike encoder-only (e.g., BERT) or decoder-only (e.g., OpenAI GPT) pre-training approaches, PoDA jointly pre-trains both the encoder and decoder by denoising the noise-corrupted text, and it also has the advantage of keeping the network architecture unchanged in the subsequent fine-tuning stage. Meanwhile, we design a hybrid model of Transformer and pointer-generator networks as the backbone architecture for PoDA. We conduct experiments on two text generation tasks: abstractive summarization, and grammatical error correction. Results on four datasets show that PoDA can improve model performance over strong baselines without using any task-specific techniques and significantly speed up convergence. ","[{'version': 'v1', 'created': 'Thu, 22 Aug 2019 05:26:25 GMT'}]",2019-08-23,"[['Wang', 'Liang', ''], ['Zhao', 'Wei', ''], ['Jia', 'Ruoyu', ''], ['Li', 'Sujian', ''], ['Liu', 'Jingming', '']]",0,1,2019-08-22,1,5,1,0,0,0,4f4de89e3adb6742433a8edc667fe07422082a15,201306636.0,https://www.semanticscholar.org/paper/4f4de89e3adb6742433a8edc667fe07422082a15,Conference on Empirical Methods in Natural Language Processing,2019.0,66.0,33.0,6.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '145769448', 'name': 'Liang Wang'}, {'authorId': '38695118', 'name': 'Wei Zhao'}, {'authorId': '2053920033', 'name': 'Ruoyu Jia'}, {'authorId': '48831399', 'name': 'Sujian Li'}, {'authorId': '49721664', 'name': 'Jingming Liu'}]","['Yuanfudao AI Lab, Beijing, China', 'Peking University']",['China'],2019-08
1909.02209,Zhuosheng Zhang,"Zhuosheng Zhang, Yuwei Wu, Hai Zhao, Zuchao Li, Shuailiang Zhang, Xi
  Zhou, Xiang Zhou",Semantics-aware BERT for Language Understanding,Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-2020),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The latest work on language representations carefully integrates contextualized features into language model training, which enables a series of success especially in various machine reading comprehension and natural language inference tasks. However, the existing language representation models including ELMo, GPT and BERT only exploit plain context-sensitive features such as character or word embeddings. They rarely consider incorporating structured semantic information which can provide rich semantics for language representation. To promote natural language understanding, we propose to incorporate explicit contextual semantics from pre-trained semantic role labeling, and introduce an improved language representation model, Semantics-aware BERT (SemBERT), which is capable of explicitly absorbing contextual semantics over a BERT backbone. SemBERT keeps the convenient usability of its BERT precursor in a light fine-tuning way without substantial task-specific modifications. Compared with BERT, semantics-aware BERT is as simple in concept but more powerful. It obtains new state-of-the-art or substantially improves results on ten reading comprehension and language inference tasks. ","[{'version': 'v1', 'created': 'Thu, 5 Sep 2019 04:47:10 GMT'}, {'version': 'v2', 'created': 'Wed, 20 Nov 2019 03:38:35 GMT'}, {'version': 'v3', 'created': 'Tue, 4 Feb 2020 09:43:22 GMT'}]",2020-02-05,"[['Zhang', 'Zhuosheng', ''], ['Wu', 'Yuwei', ''], ['Zhao', 'Hai', ''], ['Li', 'Zuchao', ''], ['Zhang', 'Shuailiang', ''], ['Zhou', 'Xi', ''], ['Zhou', 'Xiang', '']]",0,1,2019-09-05,3,7,1,0,0,0,5744f56d3253bd7c4341d36de40a93fceaa266b3,202539891.0,https://www.semanticscholar.org/paper/5744f56d3253bd7c4341d36de40a93fceaa266b3,AAAI Conference on Artificial Intelligence,2019.0,46.0,290.0,40.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '3322871', 'name': 'Zhuosheng Zhang'}, {'authorId': '2145022662', 'name': 'Yuwei Wu'}, {'authorId': '36225434', 'name': 'Zhao Hai'}, {'authorId': '30658665', 'name': 'Z. Li'}, {'authorId': '66686199', 'name': 'Shuailiang Zhang'}, {'authorId': '2144243659', 'name': 'Xi Zhou'}, {'authorId': '2118828391', 'name': 'Xiang Zhou'}]","['Shanghai Jiao Tong University', 'CloudWalk Technology, Shanghai, China']",['China'],2019-09
1910.03756,Qingyang Wu,"Qingyang Wu, Yichi Zhang, Yu Li, Zhou Yu",Alternating Recurrent Dialog Model with Large-scale Pre-trained Language Models,EACL 2021 (oral),,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing dialog system models require extensive human annotations and are difficult to generalize to different tasks. The recent success of large pre-trained language models such as BERT and GPT-2 (Devlin et al., 2019; Radford et al., 2019) have suggested the effectiveness of incorporating language priors in down-stream NLP tasks. However, how much pre-trained language models can help dialog response generation is still under exploration. In this paper, we propose a simple, general, and effective framework: Alternating Roles Dialog Model (ARDM). ARDM models each speaker separately and takes advantage of the large pre-trained language model. It requires no supervision from human annotations such as belief states or dialog acts to achieve effective conversations. ARDM outperforms or is on par with state-of-the-art methods on two popular task-oriented dialog datasets: CamRest676 and MultiWOZ. Moreover, we can generalize ARDM to more challenging, non-collaborative tasks such as persuasion. In persuasion tasks, ARDM is capable of generating human-like responses to persuade people to donate to a charity. ","[{'version': 'v1', 'created': 'Wed, 9 Oct 2019 02:31:37 GMT'}, {'version': 'v2', 'created': 'Sun, 10 Nov 2019 02:01:13 GMT'}, {'version': 'v3', 'created': 'Mon, 26 Apr 2021 19:48:38 GMT'}]",2021-04-28,"[['Wu', 'Qingyang', ''], ['Zhang', 'Yichi', ''], ['Li', 'Yu', ''], ['Yu', 'Zhou', '']]",0,1,2019-10-09,3,4,2,1,1,0,ebb1f10d878e5720ac01796bfb6769290a71149b,203952980.0,https://www.semanticscholar.org/paper/ebb1f10d878e5720ac01796bfb6769290a71149b,Conference of the European Chapter of the Association for Computational Linguistics,2019.0,29.0,54.0,8.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '31060482', 'name': 'Qingyang Wu'}, {'authorId': '46868553', 'name': 'Yichi Zhang'}, {'authorId': '40058381', 'name': 'Yu Li'}, {'authorId': '1564034697', 'name': 'Zhou Yu'}]","['Tsinghua University', 'University of California, Davis']","['China', 'United States']",2019-10
1911.02707,Houyu Zhang,"Houyu Zhang, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu",Grounded Conversation Generation as Guided Traverses in Commonsense Knowledge Graphs,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Human conversations naturally evolve around related concepts and scatter to multi-hop concepts. This paper presents a new conversation generation model, ConceptFlow, which leverages commonsense knowledge graphs to explicitly model conversation flows. By grounding conversations to the concept space, ConceptFlow represents the potential conversation flow as traverses in the concept space along commonsense relations. The traverse is guided by graph attentions in the concept graph, moving towards more meaningful directions in the concept space, in order to generate more semantic and informative responses. Experiments on Reddit conversations demonstrate ConceptFlow's effectiveness over previous knowledge-aware conversation models and GPT-2 based models while using 70% fewer parameters, confirming the advantage of explicit modeling conversation structures. All source codes of this work are available at https://github.com/thunlp/ConceptFlow. ","[{'version': 'v1', 'created': 'Thu, 7 Nov 2019 01:40:39 GMT'}, {'version': 'v2', 'created': 'Mon, 6 Apr 2020 04:02:51 GMT'}, {'version': 'v3', 'created': 'Tue, 5 May 2020 18:12:41 GMT'}]",2020-05-07,"[['Zhang', 'Houyu', ''], ['Liu', 'Zhenghao', ''], ['Xiong', 'Chenyan', ''], ['Liu', 'Zhiyuan', '']]",0,1,2019-11-07,3,4,2,1,1,0,04ef00b711990004b9c4440b1e3aa59b28701cfc,214802401.0,https://www.semanticscholar.org/paper/04ef00b711990004b9c4440b1e3aa59b28701cfc,Annual Meeting of the Association for Computational Linguistics,2019.0,50.0,109.0,15.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '92156482', 'name': 'Houyu Zhang'}, {'authorId': '49047064', 'name': 'Zhenghao Liu'}, {'authorId': '144628574', 'name': 'Chenyan Xiong'}, {'authorId': '49293587', 'name': 'Zhiyuan Liu'}]","['Tsinghua University', 'Microsoft', 'Brown University']","['China', 'United States']",2019-11
1911.03597,Yinpeng Guo,"Yinpeng Guo, Yi Liao, Xin Jiang, Qing Zhang, Yibo Zhang, Qun Liu",Zero-Shot Paraphrase Generation with Multilingual Language Models,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Leveraging multilingual parallel texts to automatically generate paraphrases has drawn much attention as size of high-quality paraphrase corpus is limited. Round-trip translation, also known as the pivoting method, is a typical approach to this end. However, we notice that the pivoting process involves multiple machine translation models and is likely to incur semantic drift during the two-step translations. In this paper, inspired by the Transformer-based language models, we propose a simple and unified paraphrasing model, which is purely trained on multilingual parallel data and can conduct zero-shot paraphrase generation in one step. Compared with the pivoting approach, paraphrases generated by our model is more semantically similar to the input sentence. Moreover, since our model shares the same architecture as GPT (Radford et al., 2018), we are able to pre-train the model on large-scale unparallel corpus, which further improves the fluency of the output sentences. In addition, we introduce the mechanism of denoising auto-encoder (DAE) to improve diversity and robustness of the model. Experimental results show that our model surpasses the pivoting method in terms of relevance, diversity, fluency and efficiency. ","[{'version': 'v1', 'created': 'Sat, 9 Nov 2019 02:49:31 GMT'}]",2019-11-12,"[['Guo', 'Yinpeng', ''], ['Liao', 'Yi', ''], ['Jiang', 'Xin', ''], ['Zhang', 'Qing', ''], ['Zhang', 'Yibo', ''], ['Liu', 'Qun', '']]",0,1,2019-11-09,1,6,1,0,0,0,68d8eced6b881e408a61bd2dd642c5a11ebb9192,207852917.0,https://www.semanticscholar.org/paper/68d8eced6b881e408a61bd2dd642c5a11ebb9192,arXiv.org,2019.0,33.0,20.0,2.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '121083081', 'name': 'Yinpeng Guo'}, {'authorId': '2048004675', 'name': 'Yi Liao'}, {'authorId': '145820291', 'name': 'Xin Jiang'}, {'authorId': '2112724233', 'name': 'Qing Zhang'}, {'authorId': '2107986486', 'name': 'Yibo Zhang'}, {'authorId': '1688015', 'name': 'Qun Liu'}]",['Huawei Technologies (China)'],['China'],2019-11
1911.11931,Xuhui Zhou,"Xuhui Zhou, Yue Zhang, Leyang Cui, Dandan Huang",Evaluating Commonsense in Pre-trained Language Models,AAAI 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Contextualized representations trained over large raw text data have given remarkable improvements for NLP tasks including question answering and reading comprehension. There have been works showing that syntactic, semantic and word sense knowledge are contained in such representations, which explains why they benefit such tasks. However, relatively little work has been done investigating commonsense knowledge contained in contextualized representations, which is crucial for human question answering and reading comprehension. We study the commonsense ability of GPT, BERT, XLNet, and RoBERTa by testing them on seven challenging benchmarks, finding that language modeling and its variants are effective objectives for promoting models' commonsense ability while bi-directional context and larger training set are bonuses. We additionally find that current models do poorly on tasks require more necessary inference steps. Finally, we test the robustness of models by making dual test cases, which are correlated so that the correct prediction of one sample should lead to correct prediction of the other. Interestingly, the models show confusion on these test cases, which suggests that they learn commonsense at the surface rather than the deep level. We release a test set, named CATs publicly, for future research. ","[{'version': 'v1', 'created': 'Wed, 27 Nov 2019 03:22:40 GMT'}, {'version': 'v2', 'created': 'Thu, 11 Feb 2021 05:14:52 GMT'}]",2021-02-12,"[['Zhou', 'Xuhui', ''], ['Zhang', 'Yue', ''], ['Cui', 'Leyang', ''], ['Huang', 'Dandan', '']]",0,1,2019-11-27,2,4,2,0,0,0,01f2b214962997260020279bd1fd1f8f372249d4,208310123.0,https://www.semanticscholar.org/paper/01f2b214962997260020279bd1fd1f8f372249d4,AAAI Conference on Artificial Intelligence,2019.0,23.0,130.0,8.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '144101734', 'name': 'Xuhui Zhou'}, {'authorId': '1591125925', 'name': 'Yue Zhang'}, {'authorId': '152496687', 'name': 'Leyang Cui'}, {'authorId': '2110409012', 'name': 'Dandan Huang'}]","['University of Washington', 'Westlake University', 'Zhejiang University']","['China', 'United States']",2019-11
1912.06719,Susan Zhang,"Jonathan Raiman, Susan Zhang, Christy Dennison",Neural Network Surgery with Sets,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The cost to train machine learning models has been increasing exponentially, making exploration and research into the correct features and architecture a costly or intractable endeavor at scale. However, using a technique named ""surgery"" OpenAI Five was continuously trained to play the game DotA 2 over the course of 10 months through 20 major changes in features and architecture. Surgery transfers trained weights from one network to another after a selection process to determine which sections of the model are unchanged and which must be re-initialized. In the past, the selection process relied on heuristics, manual labor, or pre-existing boundaries in the structure of the model, limiting the ability to salvage experiments after modifications of the feature set or input reorderings.   We propose a solution to automatically determine which components of a neural network model should be salvaged and which require retraining. We achieve this by allowing the model to operate over discrete sets of features and use set-based operations to determine the exact relationship between inputs and outputs, and how they change across tweaks in model architecture. In this paper, we introduce the methodology for enabling neural networks to operate on sets, derive two methods for detecting feature-parameter interaction maps, and show their equivalence. We empirically validate that we can surgery weights across feature and architecture changes to the OpenAI Five model. ","[{'version': 'v1', 'created': 'Fri, 13 Dec 2019 21:41:39 GMT'}, {'version': 'v2', 'created': 'Thu, 19 Mar 2020 17:16:00 GMT'}]",2020-03-20,"[['Raiman', 'Jonathan', ''], ['Zhang', 'Susan', ''], ['Dennison', 'Christy', '']]",0,0,2019-12-13,2,3,2,0,0,0,3c0e8216dff22126c89d2d26e8cfdf4924331eaa,209376217.0,https://www.semanticscholar.org/paper/3c0e8216dff22126c89d2d26e8cfdf4924331eaa,arXiv.org,2019.0,11.0,4.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '34042420', 'name': 'Jonathan Raiman'}, {'authorId': '2108244542', 'name': 'Susan Zhang'}, {'authorId': '1468636850', 'name': 'Christy Dennison'}]",['Dali University'],['China'],2019-12
1912.06721,Susan Zhang,"Jonathan Raiman, Susan Zhang, Filip Wolski",Long-Term Planning and Situational Awareness in OpenAI Five,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Understanding how knowledge about the world is represented within model-free deep reinforcement learning methods is a major challenge given the black box nature of its learning process within high-dimensional observation and action spaces. AlphaStar and OpenAI Five have shown that agents can be trained without any explicit hierarchical macro-actions to reach superhuman skill in games that require taking thousands of actions before reaching the final goal. Assessing the agent's plans and game understanding becomes challenging given the lack of hierarchy or explicit representations of macro-actions in these models, coupled with the incomprehensible nature of the internal representations.   In this paper, we study the distributed representations learned by OpenAI Five to investigate how game knowledge is gradually obtained over the course of training. We also introduce a general technique for learning a model from the agent's hidden states to identify the formation of plans and subgoals. We show that the agent can learn situational similarity across actions, and find evidence of planning towards accomplishing subgoals minutes before they are executed. We perform a qualitative analysis of these predictions during the games against the DotA 2 world champions OG in April 2019. ","[{'version': 'v1', 'created': 'Fri, 13 Dec 2019 21:49:30 GMT'}]",2019-12-17,"[['Raiman', 'Jonathan', ''], ['Zhang', 'Susan', ''], ['Wolski', 'Filip', '']]",0,0,2019-12-13,1,3,2,0,0,0,e4d3b4de9a43d9abefbba6174897e82f77c74faf,209376876.0,https://www.semanticscholar.org/paper/e4d3b4de9a43d9abefbba6174897e82f77c74faf,arXiv.org,2019.0,15.0,6.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '34042420', 'name': 'Jonathan Raiman'}, {'authorId': '2108244542', 'name': 'Susan Zhang'}, {'authorId': '143909660', 'name': 'F. Wolski'}]",['Dali University'],['China'],2019-12
2001.05139,Jian Guan,"Jian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, Minlie Huang",A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation,"Accept at Transactions of the Association for Computational
  Linguistics 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Story generation, namely generating a reasonable story from a leading context, is an important but challenging task. In spite of the success in modeling fluency and local coherence, existing neural language generation models (e.g., GPT-2) still suffer from repetition, logic conflicts, and lack of long-range coherence in generated stories. We conjecture that this is because of the difficulty of associating relevant commonsense knowledge, understanding the causal relationships, and planning entities and events with proper temporal order. In this paper, we devise a knowledge-enhanced pretraining model for commonsense story generation. We propose to utilize commonsense knowledge from external knowledge bases to generate reasonable stories. To further capture the causal and temporal dependencies between the sentences in a reasonable story, we employ multi-task learning which combines a discriminative objective to distinguish true and fake stories during fine-tuning. Automatic and manual evaluation shows that our model can generate more reasonable stories than state-of-the-art baselines, particularly in terms of logic and global coherence. ","[{'version': 'v1', 'created': 'Wed, 15 Jan 2020 05:42:27 GMT'}]",2020-01-16,"[['Guan', 'Jian', ''], ['Huang', 'Fei', ''], ['Zhao', 'Zhihao', ''], ['Zhu', 'Xiaoyan', ''], ['Huang', 'Minlie', '']]",0,1,2020-01-15,1,5,1,1,1,0,c6a84615bc36486cd0170f8a3e1b7e5ec8f5344e,210701352.0,https://www.semanticscholar.org/paper/c6a84615bc36486cd0170f8a3e1b7e5ec8f5344e,Transactions of the Association for Computational Linguistics,2020.0,59.0,192.0,31.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '145902734', 'name': 'Jian Guan'}, {'authorId': '152159016', 'name': 'Fei Huang'}, {'authorId': '1491087002', 'name': 'Zhihao Zhao'}, {'authorId': '145213540', 'name': 'Xiaoyan Zhu'}, {'authorId': '1730108', 'name': 'Minlie Huang'}]","['Beihang University', 'Tsinghua University', 'Beijing Information Science & Technology University', 'Artificial Intelligence Research Institute']","['China', 'Spain']",2020-01
2003.01473,Qiaolin Xia,"Qiaolin Xia, Haoyang Huang, Nan Duan, Dongdong Zhang, Lei Ji, Zhifang
  Sui, Edward Cui, Taroon Bharti, Xin Liu, Ming Zhou",XGPT: Cross-modal Generative Pre-Training for Image Captioning,"12 pages, 3 figures, 7 tables",,,,cs.CL cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While many BERT-based cross-modal pre-trained models produce excellent results on downstream understanding tasks like image-text retrieval and VQA, they cannot be applied to generation tasks directly. In this paper, we propose XGPT, a new method of Cross-modal Generative Pre-Training for Image Captioning that is designed to pre-train text-to-image caption generators through three novel generation tasks, including Image-conditioned Masked Language Modeling (IMLM), Image-conditioned Denoising Autoencoding (IDA), and Text-conditioned Image Feature Generation (TIFG). As a result, the pre-trained XGPT can be fine-tuned without any task-specific architecture modifications to create state-of-the-art models for image captioning. Experiments show that XGPT obtains new state-of-the-art results on the benchmark datasets, including COCO Captions and Flickr30k Captions. We also use XGPT to generate new image captions as data augmentation for the image retrieval task and achieve significant improvement on all recall metrics. ","[{'version': 'v1', 'created': 'Tue, 3 Mar 2020 12:13:06 GMT'}, {'version': 'v2', 'created': 'Wed, 4 Mar 2020 07:56:09 GMT'}]",2020-03-05,"[['Xia', 'Qiaolin', ''], ['Huang', 'Haoyang', ''], ['Duan', 'Nan', ''], ['Zhang', 'Dongdong', ''], ['Ji', 'Lei', ''], ['Sui', 'Zhifang', ''], ['Cui', 'Edward', ''], ['Bharti', 'Taroon', ''], ['Liu', 'Xin', ''], ['Zhou', 'Ming', '']]",0,1,2020-03-03,2,10,3,0,0,0,1d0d9550ecd2bece6a34fe1ffd12fb7504e7aaa0,211817758.0,https://www.semanticscholar.org/paper/1d0d9550ecd2bece6a34fe1ffd12fb7504e7aaa0,Natural Language Processing and Chinese Computing,2020.0,38.0,57.0,7.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '10007273', 'name': 'Qiaolin Xia'}, {'authorId': '15086992', 'name': 'Haoyang Huang'}, {'authorId': '46429989', 'name': 'Nan Duan'}, {'authorId': '40232931', 'name': 'Dongdong Zhang'}, {'authorId': '144906579', 'name': 'Lei Ji'}, {'authorId': '3335836', 'name': 'Zhifang Sui'}, {'authorId': '144530394', 'name': 'Edward Cui'}, {'authorId': '1490606819', 'name': 'Taroon Bharti'}, {'authorId': '2146075914', 'name': 'Xin Liu'}, {'authorId': '92660691', 'name': 'Ming Zhou'}]",['Peking University'],['China'],2020-03
2003.11528,Jinyi Hu,"Jinyi Hu, Maosong Sun",Generating Major Types of Chinese Classical Poetry in a Uniformed Framework,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Poetry generation is an interesting research topic in the field of text generation. As one of the most valuable literary and cultural heritages of China, Chinese classical poetry is very familiar and loved by Chinese people from generation to generation. It has many particular characteristics in its language structure, ranging from form, sound to meaning, thus is regarded as an ideal testing task for text generation. In this paper, we propose a GPT-2 based uniformed framework for generating major types of Chinese classical poems. We define a unified format for formulating all types of training samples by integrating detailed form information, then present a simple form-stressed weighting method in GPT-2 to strengthen the control to the form of the generated poems, with special emphasis on those forms with longer body length. Preliminary experimental results show this enhanced model can generate Chinese classical poems of major types with high quality in both form and content, validating the effectiveness of the proposed strategy. The model has been incorporated into Jiuge, the most influential Chinese classical poetry generation system developed by Tsinghua University (Guo et al., 2019). ","[{'version': 'v1', 'created': 'Fri, 13 Mar 2020 14:16:25 GMT'}]",2020-03-26,"[['Hu', 'Jinyi', ''], ['Sun', 'Maosong', '']]",0,1,2020-03-13,1,2,1,1,1,0,ca449d734701b0fb85ab5faccf7ca0a98c378c71,214641015.0,https://www.semanticscholar.org/paper/ca449d734701b0fb85ab5faccf7ca0a98c378c71,International Conference on Language Resources and Evaluation,2020.0,15.0,9.0,1.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Education', 'source': 's2-fos-model'}]","[{'authorId': '92837695', 'name': 'Jinyi Hu'}, {'authorId': '1753344', 'name': 'Maosong Sun'}]",['Tsinghua University'],['China'],2020-03
2004.02251,He Bai,"He Bai, Peng Shi, Jimmy Lin, Luchen Tan, Kun Xiong, Wen Gao, Jie Liu,
  Ming Li",Semantics of the Unwritten: The Effect of End of Paragraph and Sequence Tokens on Text Generation with GPT2,Accepted by ACL-IJCNLP SRW 2021,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The semantics of a text is manifested not only by what is read, but also by what is not read. In this article, we will study how the implicit ""not read"" information such as end-of-paragraph (\eop) and end-of-sequence (\eos) affect the quality of text generation. Specifically, we find that the pre-trained language model GPT2 can generate better continuations by learning to generate the \eop in the fine-tuning stage. Experimental results on English story generation show that \eop can lead to higher BLEU score and lower \eos perplexity. We also conduct experiments on a self-collected Chinese essay dataset with Chinese-GPT2, a character level LM without \eop or \eos during pre-training. Experimental results show that the Chinese GPT2 can generate better essay endings with \eop. ","[{'version': 'v1', 'created': 'Sun, 5 Apr 2020 16:55:09 GMT'}, {'version': 'v2', 'created': 'Mon, 7 Jun 2021 18:26:58 GMT'}]",2021-06-09,"[['Bai', 'He', ''], ['Shi', 'Peng', ''], ['Lin', 'Jimmy', ''], ['Tan', 'Luchen', ''], ['Xiong', 'Kun', ''], ['Gao', 'Wen', ''], ['Liu', 'Jie', ''], ['Li', 'Ming', '']]",0,1,2020-04-05,2,8,1,1,1,0,188c4b9a13bf98e0c8818d43baf75b1930342912,235368160.0,https://www.semanticscholar.org/paper/188c4b9a13bf98e0c8818d43baf75b1930342912,Annual Meeting of the Association for Computational Linguistics,2020.0,18.0,4.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '37374479', 'name': 'He Bai'}, {'authorId': '2055357849', 'name': 'Peng Shi'}, {'authorId': '145580839', 'name': 'Jimmy J. Lin'}, {'authorId': '40379164', 'name': 'Luchen Tan'}, {'authorId': '50033756', 'name': 'Kun Xiong'}, {'authorId': '2153578299', 'name': 'Wen Gao'}, {'authorId': '2146651412', 'name': 'Jie Liu'}, {'authorId': '2150652992', 'name': 'Ming Li'}]","['Peking University', 'Capital Normal University', 'University of Waterloo']","['Canada', 'China']",2020-04
2004.11579,Yi Liao,"Yi Liao, Xin Jiang, Qun Liu",Probabilistically Masked Language Model Capable of Autoregressive Generation in Arbitrary Word Order,Accepted by ACL 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Masked language model and autoregressive language model are two types of language models. While pretrained masked language models such as BERT overwhelm the line of natural language understanding (NLU) tasks, autoregressive language models such as GPT are especially capable in natural language generation (NLG). In this paper, we propose a probabilistic masking scheme for the masked language model, which we call probabilistically masked language model (PMLM). We implement a specific PMLM with a uniform prior distribution on the masking ratio named u-PMLM. We prove that u-PMLM is equivalent to an autoregressive permutated language model. One main advantage of the model is that it supports text generation in arbitrary order with surprisingly good quality, which could potentially enable new applications over traditional unidirectional generation. Besides, the pretrained u-PMLM also outperforms BERT on a set of downstream NLU tasks. ","[{'version': 'v1', 'created': 'Fri, 24 Apr 2020 07:38:19 GMT'}]",2020-04-27,"[['Liao', 'Yi', ''], ['Jiang', 'Xin', ''], ['Liu', 'Qun', '']]",0,1,2020-04-24,1,3,1,0,0,0,f02bb4bc0b711fd90d32bd7dadd505465d6b45e1,216144416.0,https://www.semanticscholar.org/paper/f02bb4bc0b711fd90d32bd7dadd505465d6b45e1,Annual Meeting of the Association for Computational Linguistics,2020.0,34.0,27.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2048004675', 'name': 'Yi Liao'}, {'authorId': '2110310493', 'name': 'Xin Jiang'}, {'authorId': '1688015', 'name': 'Qun Liu'}]",['Huawei Technologies (China)'],['China'],2020-04
2004.12303,Xinyue Zheng,"Xinyue Zheng, Peng Wang, Qigang Wang, Zhongchao Shi",Challenge Closed-book Science Exam: A Meta-learning Based Question Answering System,,,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Prior work in standardized science exams requires support from large text corpus, such as targeted science corpus fromWikipedia or SimpleWikipedia. However, retrieving knowledge from the large corpus is time-consuming and questions embedded in complex semantic representation may interfere with retrieval. Inspired by the dual process theory in cognitive science, we propose a MetaQA framework, where system 1 is an intuitive meta-classifier and system 2 is a reasoning module. Specifically, our method based on meta-learning method and large language model BERT, which can efficiently solve science problems by learning from related example questions without relying on external knowledge bases. We evaluate our method on AI2 Reasoning Challenge (ARC), and the experimental results show that meta-classifier yields considerable classification performance on emerging question types. The information provided by meta-classifier significantly improves the accuracy of reasoning module from 46.6% to 64.2%, which has a competitive advantage over retrieval-based QA methods. ","[{'version': 'v1', 'created': 'Sun, 26 Apr 2020 07:43:30 GMT'}]",2020-04-28,"[['Zheng', 'Xinyue', ''], ['Wang', 'Peng', ''], ['Wang', 'Qigang', ''], ['Shi', 'Zhongchao', '']]",0,0,2020-04-26,1,4,2,0,0,0,ac39d602fb265c78994ca6be2f6596977f50f694,216553690.0,https://www.semanticscholar.org/paper/ac39d602fb265c78994ca6be2f6596977f50f694,Pacific Rim Knowledge Acquisition Workshop,2020.0,32.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '151482815', 'name': 'Xinyue Zheng'}, {'authorId': '2155300440', 'name': 'Peng Wang'}, {'authorId': '2145778553', 'name': 'Qigang Wang'}, {'authorId': '2558130', 'name': 'Zhongchao Shi'}]","['AI Lab, Lenovo Research, Beijing, 100089, China']",['China'],2020-04
2004.12817,Kaitao Song,"Kaitao Song, Hao Sun, Xu Tan, Tao Qin, Jianfeng Lu, Hongzhi Liu and
  Tie-Yan Liu",LightPAFF: A Two-Stage Distillation Framework for Pre-training and Fine-tuning,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While pre-training and fine-tuning, e.g., BERT~\citep{devlin2018bert}, GPT-2~\citep{radford2019language}, have achieved great success in language understanding and generation tasks, the pre-trained models are usually too big for online deployment in terms of both memory cost and inference speed, which hinders them from practical online usage. In this paper, we propose LightPAFF, a Lightweight Pre-training And Fine-tuning Framework that leverages two-stage knowledge distillation to transfer knowledge from a big teacher model to a lightweight student model in both pre-training and fine-tuning stages. In this way the lightweight model can achieve similar accuracy as the big teacher model, but with much fewer parameters and thus faster online inference speed. LightPAFF can support different pre-training methods (such as BERT, GPT-2 and MASS~\citep{song2019mass}) and be applied to many downstream tasks. Experiments on three language understanding tasks, three language modeling tasks and three sequence to sequence generation tasks demonstrate that while achieving similar accuracy with the big BERT, GPT-2 and MASS models, LightPAFF reduces the model size by nearly 5x and improves online inference speed by 5x-7x. ","[{'version': 'v1', 'created': 'Mon, 27 Apr 2020 14:00:09 GMT'}]",2020-04-28,"[['Song', 'Kaitao', ''], ['Sun', 'Hao', ''], ['Tan', 'Xu', ''], ['Qin', 'Tao', ''], ['Lu', 'Jianfeng', ''], ['Liu', 'Hongzhi', ''], ['Liu', 'Tie-Yan', '']]",0,1,2020-04-27,1,7,2,1,1,0,20db5ac6e88e2457c82856354a2e5d521482f360,214187404.0,https://www.semanticscholar.org/paper/20db5ac6e88e2457c82856354a2e5d521482f360,arXiv.org,2020.0,25.0,12.0,2.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '50982078', 'name': 'Kaitao Song'}, {'authorId': None, 'name': 'Hao Sun'}, {'authorId': '2112780268', 'name': 'Xu Tan'}, {'authorId': '143826491', 'name': 'Tao Qin'}, {'authorId': '2152960206', 'name': 'Jianfeng Lu'}, {'authorId': '2109502875', 'name': 'Hongzhi Liu'}, {'authorId': '2110264337', 'name': 'Tie-Yan Liu'}]","['Peking University', 'Nanjing University of Science and Technology', 'Microsoft']",['China'],2020-04
2006.05009,Jiahua Liu,"Shi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong, Paul Bennett,
  Jianfeng Gao, Zhiyuan Liu",Few-Shot Generative Conversational Query Rewriting,Accepted by SIGIR 2020,,,,cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Conversational query rewriting aims to reformulate a concise conversational query to a fully specified, context-independent query that can be effectively handled by existing information retrieval systems. This paper presents a few-shot generative approach to conversational query rewriting. We develop two methods, based on rules and self-supervised learning, to generate weak supervision data using large amounts of ad hoc search sessions, and to fine-tune GPT-2 to rewrite conversational queries. On the TREC Conversational Assistance Track, our weakly supervised GPT-2 rewriter improves the state-of-the-art ranking accuracy by 12%, only using very limited amounts of manual query rewrites. In the zero-shot learning setting, the rewriter still gives a comparable result to previous state-of-the-art systems. Our analyses reveal that GPT-2 effectively picks up the task syntax and learns to capture context dependencies, even for hard cases that involve group references and long-turn dependencies. ","[{'version': 'v1', 'created': 'Tue, 9 Jun 2020 01:47:58 GMT'}]",2020-06-11,"[['Yu', 'Shi', ''], ['Liu', 'Jiahua', ''], ['Yang', 'Jingqin', ''], ['Xiong', 'Chenyan', ''], ['Bennett', 'Paul', ''], ['Gao', 'Jianfeng', ''], ['Liu', 'Zhiyuan', '']]",0,1,2020-06-09,1,7,1,1,1,0,ba960d5b53f3795be5d9600da2adea63754bfc9f,219559295.0,https://www.semanticscholar.org/paper/ba960d5b53f3795be5d9600da2adea63754bfc9f,Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,2020.0,5.0,96.0,15.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '150311558', 'name': 'S. Yu'}, {'authorId': '46701066', 'name': 'Jiahua Liu'}, {'authorId': '2121269197', 'name': 'Jingqin Yang'}, {'authorId': '144628574', 'name': 'Chenyan Xiong'}, {'authorId': '144609235', 'name': 'Paul N. Bennett'}, {'authorId': '1800422', 'name': 'Jianfeng Gao'}, {'authorId': '49293587', 'name': 'Zhiyuan Liu'}]","['Tsinghua University', 'Microsoft']","['China', 'United States']",2020-06
2008.00312,Xinyang Zhang,"Xinyang Zhang, Zheng Zhang, Shouling Ji and Ting Wang",Trojaning Language Models for Fun and Profit,"Additional experiments and text editing; To appear in 2021 6th IEEE
  European Symposium on Security and Privacy",,,,cs.CR cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent years have witnessed the emergence of a new paradigm of building natural language processing (NLP) systems: general-purpose, pre-trained language models (LMs) are composed with simple downstream models and fine-tuned for a variety of NLP tasks. This paradigm shift significantly simplifies the system development cycles. However, as many LMs are provided by untrusted third parties, their lack of standardization or regulation entails profound security implications, which are largely unexplored.   To bridge this gap, this work studies the security threats posed by malicious LMs to NLP systems. Specifically, we present TROJAN-LM, a new class of trojaning attacks in which maliciously crafted LMs trigger host NLP systems to malfunction in a highly predictable manner. By empirically studying three state-of-the-art LMs (BERT, GPT-2, XLNet) in a range of security-critical NLP tasks (toxic comment detection, question answering, text completion) as well as user studies on crowdsourcing platforms, we demonstrate that TROJAN-LM possesses the following properties: (i) flexibility - the adversary is able to flexibly dene logical combinations (e.g., 'and', 'or', 'xor') of arbitrary words as triggers, (ii) efficacy - the host systems misbehave as desired by the adversary with high probability when trigger-embedded inputs are present, (iii) specificity - the trojan LMs function indistinguishably from their benign counterparts on clean inputs, and (iv) fluency - the trigger-embedded inputs appear as fluent natural language and highly relevant to their surrounding contexts. We provide analytical justification for the practicality of TROJAN-LM, and further discuss potential countermeasures and their challenges, which lead to several promising research directions. ","[{'version': 'v1', 'created': 'Sat, 1 Aug 2020 18:22:38 GMT'}, {'version': 'v2', 'created': 'Wed, 10 Mar 2021 21:52:58 GMT'}]",2021-03-12,"[['Zhang', 'Xinyang', ''], ['Zhang', 'Zheng', ''], ['Ji', 'Shouling', ''], ['Wang', 'Ting', '']]",0,1,2020-08-01,2,4,3,1,1,0,11fe33206746251656698bf5188fc622aea7fc21,220936152.0,https://www.semanticscholar.org/paper/11fe33206746251656698bf5188fc622aea7fc21,European Symposium on Security and Privacy,2020.0,78.0,76.0,8.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': None, 'name': 'Xinyang Zhang'}, {'authorId': '1852415', 'name': 'Zheng Zhang'}, {'authorId': '2155389584', 'name': 'Ting Wang'}]","['Pennsylvania State University', 'Zhejiang University']","['China', 'United States']",2020-08
2008.06239,Andrea Madotto Mr,"Andrea Madotto, Zihan Liu, Zhaojiang Lin, Pascale Fung",Language Models as Few-Shot Learner for Task-Oriented Dialogue Systems,"Blog (https://andreamad8.github.io/few-shot-gpt/), Medium
  (https://medium.com/@madottoandrea/language-model-as-few-shot-learner-for-task-oriented-dialogue-systems-db4765796744)
  and Code (https://github.com/andreamad8/TASK-ORIENTED-LM-FEWSHOT)",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Task-oriented dialogue systems use four connected modules, namely, Natural Language Understanding (NLU), a Dialogue State Tracking (DST), Dialogue Policy (DP) and Natural Language Generation (NLG). A research challenge is to learn each module with the least amount of samples (i.e., few-shots) given the high cost related to the data collection. The most common and effective technique to solve this problem is transfer learning, where large language models, either pre-trained on text or task-specific data, are fine-tuned on the few samples. These methods require fine-tuning steps and a set of parameters for each task. Differently, language models, such as GPT-2 (Radford et al., 2019) and GPT-3 (Brown et al., 2020), allow few-shot learning by priming the model with few examples. In this paper, we evaluate the priming few-shot ability of language models in the NLU, DST, DP and NLG tasks. Importantly, we highlight the current limitations of this approach, and we discuss the possible implication for future work. ","[{'version': 'v1', 'created': 'Fri, 14 Aug 2020 08:23:21 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Aug 2020 10:56:47 GMT'}]",2020-08-21,"[['Madotto', 'Andrea', ''], ['Liu', 'Zihan', ''], ['Lin', 'Zhaojiang', ''], ['Fung', 'Pascale', '']]",0,1,2020-08-14,2,4,2,2,1,1,5d639fa3df4f3a7bacd24f2651daf0b97d2b75a3,221135886.0,https://www.semanticscholar.org/paper/5d639fa3df4f3a7bacd24f2651daf0b97d2b75a3,arXiv.org,2020.0,25.0,47.0,3.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '3064807', 'name': 'Andrea Madotto'}, {'authorId': '152613855', 'name': 'Zihan Liu'}]",['Hong Kong University of Science and Technology'],['China'],2020-08
2008.12579,Andrea Madotto Mr,"Andrea Madotto, Zhaojiang Lin, Yejin Bang, Pascale Fung",The Adapter-Bot: All-In-One Controllable Conversational Model,"Andrea Madotto and Zhaojiang Lin contributed equally to this work.
  Video demo: https://www.youtube.com/watch?v=Jz8KWE_gKH0&feature=youtu.be",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Considerable progress has been made towards conversational models that generate coherent and fluent responses by training large language models on large dialogue datasets. These models have little or no control of the generated responses and miss two important features: continuous dialogue skills integration and seamlessly leveraging diverse knowledge sources. In this paper, we propose the Adapter-Bot, a dialogue model that uses a fixed backbone conversational model such as DialGPT (Zhang et al., 2019) and triggers on-demand dialogue skills (e.g., emphatic response, weather information, movie recommendation) via different adapters (Houlsby et al., 2019). Each adapter can be trained independently, thus allowing a continual integration of skills without retraining the entire model. Depending on the skills, the model is able to process multiple knowledge types, such as text, tables, and graphs, in a seamless manner. The dialogue skills can be triggered automatically via a dialogue manager, or manually, thus allowing high-level control of the generated responses. At the current stage, we have implemented 12 response styles (e.g., positive, negative etc.), 8 goal-oriented skills (e.g. weather information, movie recommendation, etc.), and personalized and emphatic responses. We evaluate our model using automatic evaluation by comparing it with existing state-of-the-art conversational models, and we have released an interactive system at adapter.bot.ust.hk. ","[{'version': 'v1', 'created': 'Fri, 28 Aug 2020 10:59:31 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Oct 2020 02:44:30 GMT'}]",2020-10-22,"[['Madotto', 'Andrea', ''], ['Lin', 'Zhaojiang', ''], ['Bang', 'Yejin', ''], ['Fung', 'Pascale', '']]",0,1,2020-08-28,2,4,2,0,0,0,fa544c82344a556b69316f05f2ea6f51fa202139,221370525.0,https://www.semanticscholar.org/paper/fa544c82344a556b69316f05f2ea6f51fa202139,AAAI Conference on Artificial Intelligence,2020.0,77.0,49.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '3064807', 'name': 'Andrea Madotto'}, {'authorId': '100466830', 'name': 'Zhaojiang Lin'}, {'authorId': '23672613', 'name': 'Yejin Bang'}, {'authorId': '40539650', 'name': 'Pascale Fung'}]",['Hong Kong University of Science and Technology'],['China'],2020-08
2009.05866,Nicholas Capel,"Nicholas Capel, Naifu Zhang",Extended Radial Basis Function Controller for Reinforcement Learning,,,,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There have been attempts in reinforcement learning to exploit a priori knowledge about the structure of the system. This paper proposes a hybrid reinforcement learning controller which dynamically interpolates a model-based linear controller and an arbitrary differentiable policy. The linear controller is designed based on local linearised model knowledge, and stabilises the system in a neighbourhood about an operating point. The coefficients of interpolation between the two controllers are determined by a scaled distance function measuring the distance between the current state and the operating point. The overall hybrid controller is proven to maintain the stability guarantee around the neighborhood of the operating point and still possess the universal function approximation property of the arbitrary non-linear policy. Learning has been done on both model-based (PILCO) and model-free (DDPG) frameworks. Simulation experiments performed in OpenAI gym demonstrate stability and robustness of the proposed hybrid controller. This paper thus introduces a principled method allowing for the direct importing of control methodology into reinforcement learning. ","[{'version': 'v1', 'created': 'Sat, 12 Sep 2020 20:56:48 GMT'}, {'version': 'v2', 'created': 'Wed, 9 Dec 2020 06:44:17 GMT'}]",2020-12-10,"[['Capel', 'Nicholas', ''], ['Zhang', 'Naifu', '']]",0,0,2020-09-12,2,2,2,0,0,0,44d95c9fb06aa06d0c0dbddcd6057e7f659197e3,221655595.0,https://www.semanticscholar.org/paper/44d95c9fb06aa06d0c0dbddcd6057e7f659197e3,arXiv.org,2020.0,37.0,1.0,0.0,False,"['Computer Science', 'Mathematics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Mathematics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '3486120', 'name': 'Nicholas Capel'}, {'authorId': '31513628', 'name': 'Naifu Zhang'}]","['Tsinghua University', 'Amazon']","['China', 'United States']",2020-09
2009.13282,Liang Zhao,"Liang Zhao, Jingjing Xu, Junyang Lin, Yichang Zhang, Hongxia Yang, Xu
  Sun",Graph-based Multi-hop Reasoning for Long Text Generation,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Long text generation is an important but challenging task.The main problem lies in learning sentence-level semantic dependencies which traditional generative models often suffer from. To address this problem, we propose a Multi-hop Reasoning Generation (MRG) approach that incorporates multi-hop reasoning over a knowledge graph to learn semantic dependencies among sentences. MRG consists of twoparts, a graph-based multi-hop reasoning module and a path-aware sentence realization module. The reasoning module is responsible for searching skeleton paths from a knowledge graph to imitate the imagination process in the human writing for semantic transfer. Based on the inferred paths, the sentence realization module then generates a complete sentence. Unlike previous black-box models, MRG explicitly infers the skeleton path, which provides explanatory views tounderstand how the proposed model works. We conduct experiments on three representative tasks, including story generation, review generation, and product description generation. Automatic and manual evaluation show that our proposed method can generate more informative and coherentlong text than strong baselines, such as pre-trained models(e.g. GPT-2) and knowledge-enhanced models. ","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 12:47:59 GMT'}]",2020-09-29,"[['Zhao', 'Liang', ''], ['Xu', 'Jingjing', ''], ['Lin', 'Junyang', ''], ['Zhang', 'Yichang', ''], ['Yang', 'Hongxia', ''], ['Sun', 'Xu', '']]",0,1,2020-09-28,1,6,1,1,1,0,2873dfc8f7bcfa80bb77e366e124428ed42bab3b,221970966.0,https://www.semanticscholar.org/paper/2873dfc8f7bcfa80bb77e366e124428ed42bab3b,arXiv.org,2020.0,50.0,9.0,2.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2116734918', 'name': 'Liang Zhao'}, {'authorId': '47883405', 'name': 'Jingjing Xu'}, {'authorId': '35996608', 'name': 'Junyang Lin'}, {'authorId': '29343468', 'name': 'Yichang Zhang'}, {'authorId': '38385080', 'name': 'Hongxia Yang'}, {'authorId': '11774802', 'name': 'Xu Sun'}]","['Alibaba', 'Peking University', 'Beijing Institute of Big Data Research']",['China'],2020-09
2009.13984,Raymond Lee,"Nuobei Shi, Qin Zeng and Raymond Lee",The design and implementation of Language Learning Chatbot with XAI using Ontology and Transfer Learning,"19 pages, 20 figures, published paper in International Conference on
  NLP & Big Data (NLPD 2020)","Dhinaharan Nagamalai et al. (Eds): CSEIT, WiMoNe, NCS, CIoT, CMLA,
  DMSE, NLPD - 2020 pp. 305-323, 2020. CS & IT - CSCP 2020",,,cs.AI cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we proposed a transfer learning-based English language learning chatbot, whose output generated by GPT-2 can be explained by corresponding ontology graph rooted by fine-tuning dataset. We design three levels for systematically English learning, including phonetics level for speech recognition and pronunciation correction, semantic level for specific domain conversation, and the simulation of free-style conversation in English - the highest level of language chatbot communication as free-style conversation agent. For academic contribution, we implement the ontology graph to explain the performance of free-style conversation, following the concept of XAI (Explainable Artificial Intelligence) to visualize the connections of neural network in bionics, and explain the output sentence from language model. From implementation perspective, our Language Learning agent integrated the mini-program in WeChat as front-end, and fine-tuned GPT-2 model of transfer learning as back-end to interpret the responses by ontology graph. ","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 13:11:40 GMT'}]",2020-09-30,"[['Shi', 'Nuobei', ''], ['Zeng', 'Qin', ''], ['Lee', 'Raymond', '']]",0,1,2020-09-29,1,3,3,1,1,0,901ce6d0ea070663b58f65b91c309ae14cb9b06b,221996039.0,https://www.semanticscholar.org/paper/901ce6d0ea070663b58f65b91c309ae14cb9b06b,Computer Science & Information Technology (CS & IT),2020.0,17.0,8.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1974482689', 'name': 'Nuobei Shi'}, {'authorId': '1807173782', 'name': 'Qin Zeng'}, {'authorId': '2110703210', 'name': 'Raymond S. T. Lee'}]",['United International College'],['China'],2020-09
2010.02569,Ze Yang,"Ze Yang, Wei Wu, Can Xu, Xinnian Liang, Jiaqi Bai, Liran Wang, Wei
  Wang, Zhoujun Li",StyleDGPT: Stylized Response Generation with Pre-trained Language Models,Findings of EMNLP2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generating responses following a desired style has great potentials to extend applications of open-domain dialogue systems, yet is refrained by lacking of parallel data for training. In this work, we explore the challenging task with pre-trained language models that have brought breakthrough to various natural language tasks. To this end, we introduce a KL loss and a style classifier to the fine-tuning step in order to steer response generation towards the target style in both a word-level and a sentence-level. Comprehensive empirical studies with two public datasets indicate that our model can significantly outperform state-of-the-art methods in terms of both style consistency and contextual coherence. ","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 09:29:50 GMT'}]",2020-10-07,"[['Yang', 'Ze', ''], ['Wu', 'Wei', ''], ['Xu', 'Can', ''], ['Liang', 'Xinnian', ''], ['Bai', 'Jiaqi', ''], ['Wang', 'Liran', ''], ['Wang', 'Wei', ''], ['Li', 'Zhoujun', '']]",0,1,2020-10-06,1,8,1,0,0,0,d5ad08322a1ad86902c39024b5be764dd92b1988,222140894.0,https://www.semanticscholar.org/paper/d5ad08322a1ad86902c39024b5be764dd92b1988,Findings,2020.0,60.0,17.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1390867959', 'name': 'Ze Yang'}, {'authorId': '145717888', 'name': 'Wei Wu'}, {'authorId': '46747953', 'name': 'Can Xu'}, {'authorId': '120436437', 'name': 'Xinnian Liang'}, {'authorId': '2107018151', 'name': 'Jiaqi Bai'}, {'authorId': '46659660', 'name': 'Liran Wang'}, {'authorId': '2158628501', 'name': 'Wei Wang'}, {'authorId': '1707275', 'name': 'Zhoujun Li'}]","['China Resources (China)', 'Beihang University', 'Microsoft', 'Meituan']",['China'],2020-10
2010.04344,Andrea Madotto Mr,"Andrea Madotto, Etsuko Ishii, Zhaojiang Lin, Sumanth Dathathri,
  Pascale Fung",Plug-and-Play Conversational Models,"Accepted in EMNLP findings, and code available at
  https://github.com/andreamad8/PPCM",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There has been considerable progress made towards conversational models that generate coherent and fluent responses; however, this often involves training large language models on large dialogue datasets, such as Reddit. These large conversational models provide little control over the generated responses, and this control is further limited in the absence of annotated conversational datasets for attribute specific generation that can be used for fine-tuning the model. In this paper, we first propose and evaluate plug-and-play methods for controllable response generation, which does not require dialogue specific datasets and does not rely on fine-tuning a large model. While effective, the decoding procedure induces considerable computational overhead, rendering the conversational model unsuitable for interactive usage. To overcome this, we introduce an approach that does not require further computation at decoding time, while also does not require any fine-tuning of a large language model. We demonstrate, through extensive automatic and human evaluation, a high degree of control over the generated conversational responses with regard to multiple desired attributes, while being fluent. ","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 03:17:51 GMT'}]",2020-10-12,"[['Madotto', 'Andrea', ''], ['Ishii', 'Etsuko', ''], ['Lin', 'Zhaojiang', ''], ['Dathathri', 'Sumanth', ''], ['Fung', 'Pascale', '']]",0,0,2020-10-09,1,5,2,0,0,0,ef3a96d8f42e8caa1994caba2e53ca98121b4d1f,222272299.0,https://www.semanticscholar.org/paper/ef3a96d8f42e8caa1994caba2e53ca98121b4d1f,Findings,2020.0,66.0,46.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '3064807', 'name': 'Andrea Madotto'}, {'authorId': '38524906', 'name': 'Etsuko Ishii'}, {'authorId': '100466830', 'name': 'Zhaojiang Lin'}, {'authorId': '3491117', 'name': 'Sumanth Dathathri'}, {'authorId': '40539650', 'name': 'Pascale Fung'}]",['Hong Kong University of Science and Technology'],['China'],2020-10
2010.06925,Chuhan Wu,"Chuhan Wu, Fangzhao Wu, Yongfeng Huang",DA-Transformer: Distance-aware Transformer,To appear in NAACL 2021,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transformer has achieved great success in the NLP field by composing various advanced models like BERT and GPT. However, Transformer and its existing variants may not be optimal in capturing token distances because the position or distance embeddings used by these methods usually cannot keep the precise information of real distances, which may not be beneficial for modeling the orders and relations of contexts. In this paper, we propose DA-Transformer, which is a distance-aware Transformer that can exploit the real distance. We propose to incorporate the real distances between tokens to re-scale the raw self-attention weights, which are computed by the relevance between attention query and key. Concretely, in different self-attention heads the relative distance between each pair of tokens is weighted by different learnable parameters, which control the different preferences on long- or short-term information of these heads. Since the raw weighted real distances may not be optimal for adjusting self-attention weights, we propose a learnable sigmoid function to map them into re-scaled coefficients that have proper ranges. We first clip the raw self-attention weights via the ReLU function to keep non-negativity and introduce sparsity, and then multiply them with the re-scaled coefficients to encode real distance information into self-attention. Extensive experiments on five benchmark datasets show that DA-Transformer can effectively improve the performance of many tasks and outperform the vanilla Transformer and its several variants. ","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 10:09:01 GMT'}, {'version': 'v2', 'created': 'Sun, 11 Apr 2021 09:01:02 GMT'}]",2021-04-13,"[['Wu', 'Chuhan', ''], ['Wu', 'Fangzhao', ''], ['Huang', 'Yongfeng', '']]",0,1,2020-10-14,2,3,1,0,0,0,cf469e6fffbc5344572959a3f71d03b8037f284b,222341758.0,https://www.semanticscholar.org/paper/cf469e6fffbc5344572959a3f71d03b8037f284b,North American Chapter of the Association for Computational Linguistics,2020.0,24.0,22.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '15161448', 'name': 'Chuhan Wu'}, {'authorId': '2397264', 'name': 'Fangzhao Wu'}, {'authorId': '1731776', 'name': 'Yongfeng Huang'}]","['Tsinghua University', 'Microsoft']",['China'],2020-10
2010.07576,Yu Cao,"Yu Cao, Wei Bi, Meng Fang, Dacheng Tao",Pretrained Language Models for Dialogue Generation with Multiple Input Sources,"9 pages (containing 4 pages of references and appendix), accepted to
  EMNLP2020-Findings",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large-scale pretrained language models have achieved outstanding performance on natural language understanding tasks. However, it is still under investigating how to apply them to dialogue generation tasks, especially those with responses conditioned on multiple sources. Previous work simply concatenates all input sources or averages information from different input sources. In this work, we study dialogue models with multiple input sources adapted from the pretrained language model GPT2. We explore various methods to fuse multiple separate attention information corresponding to different sources. Our experimental results show that proper fusion methods deliver higher relevance with dialogue history than simple fusion baselines. ","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 07:53:28 GMT'}]",2020-10-16,"[['Cao', 'Yu', ''], ['Bi', 'Wei', ''], ['Fang', 'Meng', ''], ['Tao', 'Dacheng', '']]",0,1,2020-10-15,1,4,1,1,1,0,20ea023f2bcd3b03954a2d436306e8a5ca2bff85,222378565.0,https://www.semanticscholar.org/paper/20ea023f2bcd3b03954a2d436306e8a5ca2bff85,Findings,2020.0,24.0,17.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '143789117', 'name': 'Yu Cao'}, {'authorId': '1844673750', 'name': 'Wei Bi'}, {'authorId': '2055723543', 'name': 'Meng Fang'}, {'authorId': '143719920', 'name': 'D. Tao'}]","['Tencent', 'University of Sydney']","['China', 'Australia']",2020-10
2010.08618,Sudha Rao,"Allison Hegel, Sudha Rao, Asli Celikyilmaz and Bill Dolan",Substance over Style: Document-Level Targeted Content Transfer,This paper has been accepted to be published at EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing language models excel at writing from scratch, but many real-world scenarios require rewriting an existing document to fit a set of constraints. Although sentence-level rewriting has been fairly well-studied, little work has addressed the challenge of rewriting an entire document coherently. In this work, we introduce the task of document-level targeted content transfer and address it in the recipe domain, with a recipe as the document and a dietary restriction (such as vegan or dairy-free) as the targeted constraint. We propose a novel model for this task based on the generative pre-trained language model (GPT-2) and train on a large number of roughly-aligned recipe pairs (https://github.com/microsoft/document-level-targeted-content-transfer). Both automatic and human evaluations show that our model out-performs existing methods by generating coherent and diverse rewrites that obey the constraint while remaining close to the original document. Finally, we analyze our model's rewrites to assess progress toward the goal of making language generation more attuned to constraints that are substantive rather than stylistic. ","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 20:26:10 GMT'}]",2020-10-20,"[['Hegel', 'Allison', ''], ['Rao', 'Sudha', ''], ['Celikyilmaz', 'Asli', ''], ['Dolan', 'Bill', '']]",0,1,2020-10-16,1,4,1,1,1,0,03763454e488be8530468b3f99c93d5211c7748f,224703411.0,https://www.semanticscholar.org/paper/03763454e488be8530468b3f99c93d5211c7748f,Conference on Empirical Methods in Natural Language Processing,2020.0,41.0,1.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '70414766', 'name': 'Allison Hegel'}, {'authorId': '1845230025', 'name': 'Sudha Rao'}, {'authorId': '1709797', 'name': 'Asli Celikyilmaz'}, {'authorId': '66648221', 'name': 'Bill Dolan'}]","['Microsoft', 'Lexion, Seattle, WA, USA']","['China', 'United States']",2020-10
2010.08822,Wei Wang,"Wei Wang, Piji Li, Hai-Tao Zheng",Consistency and Coherency Enhanced Story Generation,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Story generation is a challenging task, which demands to maintain consistency of the plots and characters throughout the story. Previous works have shown that GPT2, a large-scale language model, has achieved good performance on story generation. However, we observe that several serious issues still exist in the stories generated by GPT2 which can be categorized into two folds: consistency and coherency. In terms of consistency, on one hand, GPT2 cannot guarantee the consistency of the plots explicitly. On the other hand, the generated stories usually contain coreference errors. In terms of coherency, GPT2 does not take account of the discourse relations between sentences of stories directly. To enhance the consistency and coherency of the generated stories, we propose a two-stage generation framework, where the first stage is to organize the story outline which depicts the story plots and events, and the second stage is to expand the outline into a complete story. Therefore the plots consistency can be controlled and guaranteed explicitly. In addition, coreference supervision signals are incorporated to reduce coreference errors and improve the coreference consistency. Moreover, we design an auxiliary task of discourse relation modeling to improve the coherency of the generated stories. Experimental results on a story dataset show that our model outperforms the baseline approaches in terms of both automatic metrics and human evaluation. ","[{'version': 'v1', 'created': 'Sat, 17 Oct 2020 16:40:37 GMT'}]",2020-10-20,"[['Wang', 'Wei', ''], ['Li', 'Piji', ''], ['Zheng', 'Hai-Tao', '']]",0,1,2020-10-17,1,3,1,1,1,0,f2f8664380fadae5a58e34a920748f9132c1b6c2,224706317.0,https://www.semanticscholar.org/paper/f2f8664380fadae5a58e34a920748f9132c1b6c2,European Conference on Information Retrieval,2020.0,35.0,10.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '38593127', 'name': 'Wei Wang'}, {'authorId': '2193560', 'name': 'Piji Li'}, {'authorId': '16215052', 'name': 'Haitao Zheng'}]","['Tencent', 'Tsinghua University']",['China'],2020-10
2010.11967,Chenguang Wang,"Chenguang Wang, Xiao Liu, Dawn Song",Language Models are Open Knowledge Graphs,"30 pages, 32 figures, 3 tables",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper shows how to construct knowledge graphs (KGs) from pre-trained language models (e.g., BERT, GPT-2/3), without human supervision. Popular KGs (e.g, Wikidata, NELL) are built in either a supervised or semi-supervised manner, requiring humans to create knowledge. Recent deep language models automatically acquire knowledge from large-scale corpora via pre-training. The stored knowledge has enabled the language models to improve downstream NLP tasks, e.g., answering questions, and writing code and articles. In this paper, we propose an unsupervised method to cast the knowledge contained within language models into KGs. We show that KGs are constructed with a single forward pass of the pre-trained language models (without fine-tuning) over the corpora. We demonstrate the quality of the constructed KGs by comparing to two KGs (Wikidata, TAC KBP) created by humans. Our KGs also provide open factual knowledge that is new in the existing KGs. Our code and KGs will be made publicly available. ","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 18:01:56 GMT'}]",2020-10-26,"[['Wang', 'Chenguang', ''], ['Liu', 'Xiao', ''], ['Song', 'Dawn', '']]",0,1,2020-10-22,1,3,3,1,1,0,3ee955bfb656e30a337b22a1149b5ecc91a91217,225062414.0,https://www.semanticscholar.org/paper/3ee955bfb656e30a337b22a1149b5ecc91a91217,arXiv.org,2020.0,82.0,86.0,7.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '2108755854', 'name': 'Chenguang Wang'}, {'authorId': '2111312892', 'name': 'Xiao Liu'}, {'authorId': '143711382', 'name': 'D. Song'}]",['Tsinghua University'],['China'],2020-10
2011.07956,Dong-Ho Lee,"Wangchunshu Zhou, Dong-Ho Lee, Ravi Kiran Selvam, Seyeon Lee, Bill
  Yuchen Lin, Xiang Ren",Pre-training Text-to-Text Transformers for Concept-centric Common Sense,"15 pages, 4 figures. Code and Data: https://github.com/INK-USC/CALM/",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational commonsense knowledge about everyday concepts, which is crucial to many downstream tasks that need common sense to understand or generate. To augment PTLMs with concept-centric commonsense knowledge, in this paper, we propose both generative and contrastive objectives for learning common sense from the text, and use them as intermediate self-supervised learning tasks for incrementally pre-training PTLMs (before task-specific fine-tuning on downstream datasets). Furthermore, we develop a joint pre-training framework to unify generative and contrastive objectives so that they can mutually reinforce each other. Extensive experimental results show that our method, concept-aware language model (CALM), can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge graphs, yielding better performance on both NLU and NLG tasks. We show that while only incrementally pre-trained on a relatively small corpus for a few steps, CALM outperforms baseline methods by a consistent margin and even comparable with some larger PTLMs, which suggests that CALM can serve as a general, plug-and-play method for improving the commonsense reasoning ability of a PTLM. ","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 07:00:37 GMT'}, {'version': 'v2', 'created': 'Wed, 25 Nov 2020 04:53:38 GMT'}]",2020-11-26,"[['Zhou', 'Wangchunshu', ''], ['Lee', 'Dong-Ho', ''], ['Selvam', 'Ravi Kiran', ''], ['Lee', 'Seyeon', ''], ['Lin', 'Bill Yuchen', ''], ['Ren', 'Xiang', '']]",0,0,2020-10-24,2,6,3,1,1,0,abaadb4c6affc4d874c4f59bfac60686e851cb5e,226964491.0,https://www.semanticscholar.org/paper/abaadb4c6affc4d874c4f59bfac60686e851cb5e,International Conference on Learning Representations,2020.0,45.0,61.0,7.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '150341221', 'name': 'Wangchunshu Zhou'}, {'authorId': '73037511', 'name': 'Dong-Ho Lee'}, {'authorId': '1838125907', 'name': 'Ravi Kiran Selvam'}, {'authorId': '2108701349', 'name': 'Seyeon Lee'}, {'authorId': '51583409', 'name': 'Bill Yuchen Lin'}, {'authorId': '1384550891', 'name': 'Xiang Ren'}]","['University of Southern California', 'Beihang University']","['China', 'United States']",2020-10
2011.12692,Deheng Ye,"Deheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia
  Chen, Zhao Liu, Fuhao Qiu, Hongsheng Yu, Yinyuting Yin, Bei Shi, Liang Wang,
  Tengfei Shi, Qiang Fu, Wei Yang, Lanxiao Huang, Wei Liu",Towards Playing Full MOBA Games with Deep Reinforcement Learning,NeurIPS 2020,,,,cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  MOBA games, e.g., Honor of Kings, League of Legends, and Dota 2, pose grand challenges to AI systems such as multi-agent, enormous state-action space, complex action control, etc. Developing AI for playing MOBA games has raised much attention accordingly. However, existing work falls short in handling the raw game complexity caused by the explosion of agent combinations, i.e., lineups, when expanding the hero pool in case that OpenAI's Dota AI limits the play to a pool of only 17 heroes. As a result, full MOBA games without restrictions are far from being mastered by any existing AI system. In this paper, we propose a MOBA AI learning paradigm that methodologically enables playing full MOBA games with deep reinforcement learning. Specifically, we develop a combination of novel and existing learning techniques, including curriculum self-play learning, policy distillation, off-policy adaption, multi-head value estimation, and Monte-Carlo tree-search, in training and playing a large pool of heroes, meanwhile addressing the scalability issue skillfully. Tested on Honor of Kings, a popular MOBA game, we show how to build superhuman AI agents that can defeat top esports players. The superiority of our AI is demonstrated by the first large-scale performance test of MOBA AI agent in the literature. ","[{'version': 'v1', 'created': 'Wed, 25 Nov 2020 12:52:33 GMT'}, {'version': 'v2', 'created': 'Thu, 26 Nov 2020 03:30:08 GMT'}, {'version': 'v3', 'created': 'Tue, 22 Dec 2020 11:58:54 GMT'}, {'version': 'v4', 'created': 'Thu, 31 Dec 2020 13:25:17 GMT'}]",2021-01-01,"[['Ye', 'Deheng', ''], ['Chen', 'Guibin', ''], ['Zhang', 'Wen', ''], ['Chen', 'Sheng', ''], ['Yuan', 'Bo', ''], ['Liu', 'Bo', ''], ['Chen', 'Jia', ''], ['Liu', 'Zhao', ''], ['Qiu', 'Fuhao', ''], ['Yu', 'Hongsheng', ''], ['Yin', 'Yinyuting', ''], ['Shi', 'Bei', ''], ['Wang', 'Liang', ''], ['Shi', 'Tengfei', ''], ['Fu', 'Qiang', ''], ['Yang', 'Wei', ''], ['Huang', 'Lanxiao', ''], ['Liu', 'Wei', '']]",0,0,2020-11-25,4,18,2,0,0,0,681bbcf763fd7c59869853bedfe6b9f02a3364e6,227162956.0,https://www.semanticscholar.org/paper/681bbcf763fd7c59869853bedfe6b9f02a3364e6,Neural Information Processing Systems,2020.0,39.0,115.0,7.0,False,"['Computer Science', 'Psychology']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Psychology', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2055648566', 'name': 'Deheng Ye'}, {'authorId': '1510409929', 'name': 'Guibin Chen'}, {'authorId': '2108543115', 'name': 'Wen Zhang'}, {'authorId': '2118529933', 'name': 'Sheng Chen'}, {'authorId': '2055907828', 'name': 'Bo Yuan'}, {'authorId': '40107085', 'name': 'Bo Liu'}, {'authorId': '2144157470', 'name': 'Jia Chen'}, {'authorId': '2118395857', 'name': 'Zhao Liu'}, {'authorId': '2072293042', 'name': 'Fuhao Qiu'}, {'authorId': '1417653814', 'name': 'Hongsheng Yu'}, {'authorId': '1470464839', 'name': 'Yinyuting Yin'}, {'authorId': '46318189', 'name': 'Bei Shi'}, {'authorId': '2144696467', 'name': 'Liang Wang'}, {'authorId': '1596814619', 'name': 'Tengfei Shi'}, {'authorId': '2091914469', 'name': 'Qiang Fu'}, {'authorId': '2005150594', 'name': 'Wei Yang'}, {'authorId': '2108955773', 'name': 'Lanxiao Huang'}, {'authorId': '2157221557', 'name': 'Wei Liu'}]",['Tencent'],['China'],2020-11
2012.00364,Hanting Chen,"Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua
  Liu, Siwei Ma, Chunjing Xu, Chao Xu, Wen Gao",Pre-Trained Image Processing Transformer,CVPR 2021,,,,cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As the computing power of modern hardware is increasing strongly, pre-trained deep learning models (e.g., BERT, GPT-3) learned on large-scale datasets have shown their effectiveness over conventional methods. The big progress is mainly contributed to the representation ability of transformer and its variant architectures. In this paper, we study the low-level computer vision task (e.g., denoising, super-resolution and deraining) and develop a new pre-trained model, namely, image processing transformer (IPT). To maximally excavate the capability of transformer, we present to utilize the well-known ImageNet benchmark for generating a large amount of corrupted image pairs. The IPT model is trained on these images with multi-heads and multi-tails. In addition, the contrastive learning is introduced for well adapting to different image processing tasks. The pre-trained model can therefore efficiently employed on desired task after fine-tuning. With only one pre-trained model, IPT outperforms the current state-of-the-art methods on various low-level benchmarks. Code is available at https://github.com/huawei-noah/Pretrained-IPT and https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/IPT ","[{'version': 'v1', 'created': 'Tue, 1 Dec 2020 09:42:46 GMT'}, {'version': 'v2', 'created': 'Thu, 3 Dec 2020 05:02:15 GMT'}, {'version': 'v3', 'created': 'Fri, 28 May 2021 02:41:58 GMT'}, {'version': 'v4', 'created': 'Mon, 8 Nov 2021 07:08:21 GMT'}]",2021-11-09,"[['Chen', 'Hanting', ''], ['Wang', 'Yunhe', ''], ['Guo', 'Tianyu', ''], ['Xu', 'Chang', ''], ['Deng', 'Yiping', ''], ['Liu', 'Zhenhua', ''], ['Ma', 'Siwei', ''], ['Xu', 'Chunjing', ''], ['Xu', 'Chao', ''], ['Gao', 'Wen', '']]",0,1,2020-12-01,4,10,2,1,0,1,43cb4886a8056d5005702edbc51be327542b2124,227239228.0,https://www.semanticscholar.org/paper/43cb4886a8056d5005702edbc51be327542b2124,Computer Vision and Pattern Recognition,2020.0,98.0,953.0,102.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Engineering', 'source': 's2-fos-model'}]","[{'authorId': '2118023932', 'name': 'Hanting Chen'}, {'authorId': '2108702980', 'name': 'Yunhe Wang'}, {'authorId': '50412584', 'name': 'Tianyu Guo'}, {'authorId': None, 'name': 'Chang Xu'}, {'authorId': '2115656092', 'name': 'Yiping Deng'}, {'authorId': '2125024057', 'name': 'Zhenhua Liu'}, {'authorId': '2217676532', 'name': 'Siwei Ma'}, {'authorId': '1691522', 'name': 'Chunjing Xu'}, {'authorId': '2004428678', 'name': 'Chao Xu'}, {'authorId': '2153706048', 'name': 'Wen Gao'}]","['Peking University', 'University of Sydney', 'Peng Cheng Laboratory', 'Huawei Technologies (China)']","['China', 'Australia']",2020-12
2012.00413,Zhengyan Zhang,"Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia
  Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng,
  Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu,
  Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun",CPM: A Large-scale Generative Chinese Pre-trained Language Model,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained Language Models (PLMs) have proven to be beneficial for various downstream NLP tasks. Recently, GPT-3, with 175 billion parameters and 570GB training data, drew a lot of attention due to the capacity of few-shot (even zero-shot) learning. However, applying GPT-3 to address Chinese NLP tasks is still challenging, as the training corpus of GPT-3 is primarily English, and the parameters are not publicly available. In this technical report, we release the Chinese Pre-trained Language Model (CPM) with generative pre-training on large-scale Chinese training data. To the best of our knowledge, CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained language model, which could facilitate several downstream Chinese NLP tasks, such as conversation, essay generation, cloze test, and language understanding. Extensive experiments demonstrate that CPM achieves strong performance on many NLP tasks in the settings of few-shot (even zero-shot) learning. The code and parameters are available at https://github.com/TsinghuaAI/CPM-Generate. ","[{'version': 'v1', 'created': 'Tue, 1 Dec 2020 11:32:56 GMT'}]",2020-12-02,"[['Zhang', 'Zhengyan', ''], ['Han', 'Xu', ''], ['Zhou', 'Hao', ''], ['Ke', 'Pei', ''], ['Gu', 'Yuxian', ''], ['Ye', 'Deming', ''], ['Qin', 'Yujia', ''], ['Su', 'Yusheng', ''], ['Ji', 'Haozhe', ''], ['Guan', 'Jian', ''], ['Qi', 'Fanchao', ''], ['Wang', 'Xiaozhi', ''], ['Zheng', 'Yanan', ''], ['Zeng', 'Guoyang', ''], ['Cao', 'Huanqi', ''], ['Chen', 'Shengqi', ''], ['Li', 'Daixuan', ''], ['Sun', 'Zhenbo', ''], ['Liu', 'Zhiyuan', ''], ['Huang', 'Minlie', ''], ['Han', 'Wentao', ''], ['Tang', 'Jie', ''], ['Li', 'Juanzi', ''], ['Zhu', 'Xiaoyan', ''], ['Sun', 'Maosong', '']]",0,1,2020-12-01,1,25,1,1,0,1,cc50f846ed7222698d130cddbc58ed4d547914ed,227238757.0,https://www.semanticscholar.org/paper/cc50f846ed7222698d130cddbc58ed4d547914ed,AI Open,2020.0,42.0,85.0,9.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2621696', 'name': 'Zhengyan Zhang'}, {'authorId': '48506411', 'name': 'Xu Han'}, {'authorId': '2111824520', 'name': 'Hao Zhou'}, {'authorId': '1886879', 'name': 'Pei Ke'}, {'authorId': '2116405624', 'name': 'Yuxian Gu'}, {'authorId': '50816334', 'name': 'Deming Ye'}, {'authorId': '50625437', 'name': 'Yujia Qin'}, {'authorId': '48576745', 'name': 'Yusheng Su'}, {'authorId': '51111817', 'name': 'Haozhe Ji'}, {'authorId': '145902734', 'name': 'Jian Guan'}, {'authorId': '51466208', 'name': 'Fanchao Qi'}, {'authorId': '48631777', 'name': 'Xiaozhi Wang'}, {'authorId': '2111090780', 'name': 'Yanan Zheng'}, {'authorId': '1398454307', 'name': 'Guoyang Zeng'}, {'authorId': '47709883', 'name': 'Huanqi Cao'}, {'authorId': '2333063', 'name': 'S. Chen'}, {'authorId': '1605885812', 'name': 'Daixuan Li'}, {'authorId': '113907105', 'name': 'Zhenbo Sun'}, {'authorId': '49293587', 'name': 'Zhiyuan Liu'}, {'authorId': '1730108', 'name': 'Minlie Huang'}, {'authorId': '2114924784', 'name': 'Wentao Han'}, {'authorId': '2109541439', 'name': 'Jie Tang'}, {'authorId': '8549842', 'name': 'Juan-Zi Li'}, {'authorId': '145213540', 'name': 'Xiaoyan Zhu'}, {'authorId': '1753344', 'name': 'Maosong Sun'}]",['Tsinghua University'],['China'],2020-12
2012.00744,Harry Wang,"Jinggang Zhuo, Ling Fan, Harry Jiannan Wang",A Framework and Dataset for Abstract Art Generation via CalligraphyGAN,"Accepted by NeurIPS 2020 Workshop on Machine Learning for Creativity
  and Design, Vancouver, Canada",,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  With the advancement of deep learning, artificial intelligence (AI) has made many breakthroughs in recent years and achieved superhuman performance in various tasks such as object detection, reading comprehension, and video games. Generative Modeling, such as various Generative Adversarial Networks (GAN) models, has been applied to generate paintings and music. Research in Natural Language Processing (NLP) also had a leap forward in 2018 since the release of the pre-trained contextual neural language models such as BERT and recently released GPT3. Despite the exciting AI applications aforementioned, AI is still significantly lagging behind humans in creativity, which is often considered the ultimate moonshot for AI. Our work is inspired by Chinese calligraphy, which is a unique form of visual art where the character itself is an aesthetic painting. We also draw inspirations from paintings of the Abstract Expressionist movement in the 1940s and 1950s, such as the work by American painter Franz Kline. In this paper, we present a creative framework based on Conditional Generative Adversarial Networks and Contextual Neural Language Model to generate abstract artworks that have intrinsic meaning and aesthetic value, which is different from the existing work, such as image captioning and text-to-image generation, where the texts are the descriptions of the images. In addition, we have publicly released a Chinese calligraphy image dataset and demonstrate our framework using a prototype system and a user study. ","[{'version': 'v1', 'created': 'Wed, 2 Dec 2020 16:24:20 GMT'}]",2020-12-03,"[['Zhuo', 'Jinggang', ''], ['Fan', 'Ling', ''], ['Wang', 'Harry Jiannan', '']]",0,1,2020-12-02,1,3,1,1,0,1,2adbd8f367c5ab43dcaba87c980aabd3747d4994,227247926.0,https://www.semanticscholar.org/paper/2adbd8f367c5ab43dcaba87c980aabd3747d4994,arXiv.org,2020.0,6.0,1.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Art', 'source': 's2-fos-model'}]","[{'authorId': '2030709785', 'name': 'Jinggang Zhuo'}, {'authorId': '1406286691', 'name': 'Ling Fan'}, {'authorId': '2145329798', 'name': 'Harry J. Wang'}]","['Tongji University', 'University of Delaware']","['China', 'United States']",2020-12
2012.01775,Xiaodong Gu,"Xiaodong Gu, Kang Min Yoo, Jung-Woo Ha",DialogBERT: Discourse-Aware Response Generation via Learning to Recover and Rank Utterances,Published as a conference paper at AAAI 2021,,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Recent advances in pre-trained language models have significantly improved neural response generation. However, existing methods usually view the dialogue context as a linear sequence of tokens and learn to generate the next word through token-level self-attention. Such token-level encoding hinders the exploration of discourse-level coherence among utterances. This paper presents DialogBERT, a novel conversational response generation model that enhances previous PLM-based dialogue models. DialogBERT employs a hierarchical Transformer architecture. To efficiently capture the discourse-level coherence among utterances, we propose two training objectives, including masked utterance regression and distributed utterance order ranking in analogy to the original BERT training. Experiments on three multi-turn conversation datasets show that our approach remarkably outperforms the baselines, such as BART and DialoGPT, in terms of quantitative evaluation. The human evaluation suggests that DialogBERT generates more coherent, informative, and human-like responses than the baselines with significant margins. ","[{'version': 'v1', 'created': 'Thu, 3 Dec 2020 09:06:23 GMT'}, {'version': 'v2', 'created': 'Mon, 13 Dec 2021 06:48:34 GMT'}]",2021-12-14,"[['Gu', 'Xiaodong', ''], ['Yoo', 'Kang Min', ''], ['Ha', 'Jung-Woo', '']]",0,1,2020-12-03,2,3,3,0,0,0,138622fff5cd7c6023a8b874958a0a0c857f9d41,227255094.0,https://www.semanticscholar.org/paper/138622fff5cd7c6023a8b874958a0a0c857f9d41,AAAI Conference on Artificial Intelligence,2020.0,40.0,58.0,12.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2118712301', 'name': 'X. Gu'}, {'authorId': '31760501', 'name': 'Kang Min Yoo'}, {'authorId': '2577039', 'name': 'Jung-Woo Ha'}]","['NAVER', 'Shanghai Jiao Tong University']","['China', 'South Korea']",2020-12
2012.02469,Ju Fan,"Nan Tang, Ju Fan, Fangyi Li, Jianhong Tu, Xiaoyong Du, Guoliang Li,
  Sam Madden, Mourad Ouzzani",RPT: Relational Pre-trained Transformer Is Almost All You Need towards Democratizing Data Preparation,,,,,cs.LG cs.DB,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Can AI help automate human-easy but computer-hard data preparation tasks that burden data scientists, practitioners, and crowd workers? We answer this question by presenting RPT, a denoising auto-encoder for tuple-to-X models (X could be tuple, token, label, JSON, and so on). RPT is pre-trained for a tuple-to-tuple model by corrupting the input tuple and then learning a model to reconstruct the original tuple. It adopts a Transformer-based neural translation architecture that consists of a bidirectional encoder (similar to BERT) and a left-to-right autoregressive decoder (similar to GPT), leading to a generalization of both BERT and GPT. The pre-trained RPT can already support several common data preparation tasks such as data cleaning, auto-completion and schema matching. Better still, RPT can be fine-tuned on a wide range of data preparation tasks, such as value normalization, data transformation, data annotation, etc. To complement RPT, we also discuss several appealing techniques such as collaborative training and few-shot learning for entity resolution, and few-shot learning and NLP question-answering for information extraction. In addition, we identify a series of research opportunities to advance the field of data preparation. ","[{'version': 'v1', 'created': 'Fri, 4 Dec 2020 08:52:05 GMT'}, {'version': 'v2', 'created': 'Wed, 31 Mar 2021 08:28:30 GMT'}]",2021-04-01,"[['Tang', 'Nan', ''], ['Fan', 'Ju', ''], ['Li', 'Fangyi', ''], ['Tu', 'Jianhong', ''], ['Du', 'Xiaoyong', ''], ['Li', 'Guoliang', ''], ['Madden', 'Sam', ''], ['Ouzzani', 'Mourad', '']]",0,1,2020-12-04,2,8,2,0,0,0,bfe6cce534d76df1aa7ebd629c09e30d6b8abdd4,232432173.0,https://www.semanticscholar.org/paper/bfe6cce534d76df1aa7ebd629c09e30d6b8abdd4,Proceedings of the VLDB Endowment,2020.0,65.0,42.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '8669763', 'name': 'N. Tang'}, {'authorId': '1704755170', 'name': 'Ju Fan'}, {'authorId': '2146330398', 'name': 'Fangyi Li'}, {'authorId': '49365463', 'name': 'Jianhong Tu'}, {'authorId': '144589756', 'name': 'Xiaoyong Du'}, {'authorId': '2108491555', 'name': 'Guoliang Li'}, {'authorId': '144478906', 'name': 'S. Madden'}, {'authorId': '2168047', 'name': 'M. Ouzzani'}]","['Massachusetts Institute of Technology', 'Tsinghua University', 'Hamad bin Khalifa University', 'Renmin University of China']","['China', 'United States', 'Qatar']",2020-12
2012.03539,Yunyi Yang,"Yunyi Yang, Yunhao Li, Xiaojun Quan",UBAR: Towards Fully End-to-End Task-Oriented Dialog Systems with GPT-2,Accepted by AAAI 2021,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents our task-oriented dialog system UBAR which models task-oriented dialogs on a dialog session level. Specifically, UBAR is acquired by fine-tuning the large pre-trained unidirectional language model GPT-2 on the sequence of the entire dialog session which is composed of user utterance, belief state, database result, system act, and system response of every dialog turn. Additionally, UBAR is evaluated in a more realistic setting, where its dialog context has access to user utterances and all content it generated such as belief states, system acts, and system responses. Experimental results on the MultiWOZ datasets show that UBAR achieves state-of-the-art performances in multiple settings, improving the combined score of response generation, policy optimization, and end-to-end modeling by 4.7, 3.5, and 9.4 points respectively. Thorough analyses demonstrate that the session-level training sequence formulation and the generated dialog context are essential for UBAR to operate as a fully end-to-end task-oriented dialog system in real life. We also examine the transfer ability of UBAR to new domains with limited data and provide visualization and a case study to illustrate the advantages of UBAR in modeling on a dialog session level. ","[{'version': 'v1', 'created': 'Mon, 7 Dec 2020 09:08:16 GMT'}, {'version': 'v2', 'created': 'Thu, 18 Mar 2021 02:34:26 GMT'}]",2021-03-19,"[['Yang', 'Yunyi', ''], ['Li', 'Yunhao', ''], ['Quan', 'Xiaojun', '']]",0,1,2020-12-07,2,3,1,1,1,0,63169665bd592fb818678c47644b29302877d50e,227334663.0,https://www.semanticscholar.org/paper/63169665bd592fb818678c47644b29302877d50e,AAAI Conference on Artificial Intelligence,2020.0,44.0,131.0,31.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '49307508', 'name': 'Yunyi Yang'}, {'authorId': '2135329729', 'name': 'Yunhao Li'}, {'authorId': '38472218', 'name': 'Xiaojun Quan'}]",['Sun Yat-sen University'],['China'],2020-12
2012.07528,Souheil Fenghour Fenghour,"Souheil Fenghour, Daqing Chen, Kun Guo, Perry Xiao",Disentangling Homophemes in Lip Reading using Perplexity Analysis,17 pages,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The performance of automated lip reading using visemes as a classification schema has achieved less success compared with the use of ASCII characters and words largely due to the problem of different words sharing identical visemes. The Generative Pre-Training transformer is an effective autoregressive language model used for many tasks in Natural Language Processing, including sentence prediction and text classification.   This paper proposes a new application for this model and applies it in the context of lip reading, where it serves as a language model to convert visual speech in the form of visemes, to language in the form of words and sentences. The network uses the search for optimal perplexity to perform the viseme-to-word mapping and is thus a solution to the one-to-many mapping problem that exists whereby various words that sound different when spoken look identical. This paper proposes a method to tackle the one-to-many mapping problem when performing automated lip reading using solely visual cues in two separate scenarios: the first scenario is where the word boundary, that is, the beginning and the ending of a word, is unknown; and the second scenario is where the boundary is known.   Sentences from the benchmark BBC dataset ""Lip Reading Sentences in the Wild""(LRS2), are classified with a character error rate of 10.7% and a word error rate of 18.0%. The main contribution of this paper is to propose a method of predicting words through the use of perplexity analysis when only visual cues are present, using an autoregressive language model. ","[{'version': 'v1', 'created': 'Sat, 28 Nov 2020 12:12:17 GMT'}]",2020-12-15,"[['Fenghour', 'Souheil', ''], ['Chen', 'Daqing', ''], ['Guo', 'Kun', ''], ['Xiao', 'Perry', '']]",0,1,2020-11-28,1,4,1,0,0,0,7247e231b58345cc0040dd87f653814e5243ce7c,229152352.0,https://www.semanticscholar.org/paper/7247e231b58345cc0040dd87f653814e5243ce7c,arXiv.org,2020.0,41.0,2.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '115685706', 'name': 'Souheil Fenghour'}, {'authorId': '47514164', 'name': 'Daqing Chen'}, {'authorId': '2034207177', 'name': 'Kun Guo'}, {'authorId': '30685117', 'name': 'Perry Xiao'}]","[""Xi'an VANXUM Electronics Technology Co. Ltd. Xi'an, China"", 'London South Bank University']","['China', 'United Kingdom']",2020-11
2012.14535,Linfeng Song,"Jie Hao, Linfeng Song, Liwei Wang, Kun Xu, Zhaopeng Tu and Dong Yu",Robust Dialogue Utterance Rewriting as Sequence Tagging,11 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The task of dialogue rewriting aims to reconstruct the latest dialogue utterance by copying the missing content from the dialogue context. Until now, the existing models for this task suffer from the robustness issue, i.e., performances drop dramatically when testing on a different domain. We address this robustness issue by proposing a novel sequence-tagging-based model so that the search space is significantly reduced, yet the core of this task is still well covered. As a common issue of most tagging models for text generation, the model's outputs may lack fluency. To alleviate this issue, we inject the loss signal from BLEU or GPT-2 under a REINFORCE framework. Experiments show huge improvements of our model over the current state-of-the-art systems on domain transfer. ","[{'version': 'v1', 'created': 'Tue, 29 Dec 2020 00:05:35 GMT'}]",2021-01-01,"[['Hao', 'Jie', ''], ['Song', 'Linfeng', ''], ['Wang', 'Liwei', ''], ['Xu', 'Kun', ''], ['Tu', 'Zhaopeng', ''], ['Yu', 'Dong', '']]",0,1,2020-12-29,1,6,1,1,1,0,0e635104e5378bc226b0e07e429fcef44f959fc8,229923054.0,https://www.semanticscholar.org/paper/0e635104e5378bc226b0e07e429fcef44f959fc8,arXiv.org,2020.0,29.0,6.0,2.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '145485960', 'name': 'Jie Hao'}, {'authorId': '1748796', 'name': 'Linfeng Song'}, {'authorId': '39060743', 'name': 'Liwei Wang'}, {'authorId': '151485141', 'name': 'Kun Xu'}, {'authorId': '2909321', 'name': 'Zhaopeng Tu'}, {'authorId': '2111505433', 'name': 'Dong Yu'}]","['Tencent', 'Chinese University of Hong Kong', 'Florida State University']","['China', 'United States']",2020-12
2101.00416,Wangchunshu Zhou,"Wangchunshu Zhou, Tao Ge, Canwen Xu, Ke Xu, Furu Wei",Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we generalize text infilling (e.g., masked language models) by proposing Sequence Span Rewriting (SSR) as a self-supervised sequence-to-sequence (seq2seq) pre-training objective. SSR provides more fine-grained learning signals for text representations by supervising the model to rewrite imperfect spans to ground truth, and it is more consistent than text infilling with many downstream seq2seq tasks that rewrite a source sentences into a target sentence. Our experiments with T5 models on various seq2seq tasks show that SSR can substantially improve seq2seq pre-training. Moreover, we observe SSR is especially helpful to improve pre-training a small-size seq2seq model with a powerful imperfect span generator, which indicates a new perspective of transferring knowledge from a large model to a smaller model for seq2seq pre-training. ","[{'version': 'v1', 'created': 'Sat, 2 Jan 2021 10:27:11 GMT'}, {'version': 'v2', 'created': 'Fri, 24 Sep 2021 07:45:41 GMT'}]",2021-09-27,"[['Zhou', 'Wangchunshu', ''], ['Ge', 'Tao', ''], ['Xu', 'Canwen', ''], ['Xu', 'Ke', ''], ['Wei', 'Furu', '']]",0,0,2021-01-02,2,5,1,1,1,0,124b25a74691a993673359288637716d83899a06,230437660.0,https://www.semanticscholar.org/paper/124b25a74691a993673359288637716d83899a06,Conference on Empirical Methods in Natural Language Processing,2021.0,67.0,14.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '150341221', 'name': 'Wangchunshu Zhou'}, {'authorId': '50251691', 'name': 'Tao Ge'}, {'authorId': '145389711', 'name': 'Ke Xu'}, {'authorId': '49807919', 'name': 'Furu Wei'}]","['Beihang University', 'University of California, San Diego']","['China', 'United States']",2021-01
2101.09914,Lei Huang,"Lei Huang, Jiecong Lin, Xiangtao Li, Linqi Song and Ka-Chun Wong",EGFI: Drug-Drug Interaction Extraction and Generation with Fusion of Enriched Entity and Sentence Information,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The rapid growth in literature accumulates diverse and yet comprehensive biomedical knowledge hidden to be mined such as drug interactions. However, it is difficult to extract the heterogeneous knowledge to retrieve or even discover the latest and novel knowledge in an efficient manner. To address such a problem, we propose EGFI for extracting and consolidating drug interactions from large-scale medical literature text data. Specifically, EGFI consists of two parts: classification and generation. In the classification part, EGFI encompasses the language model BioBERT which has been comprehensively pre-trained on biomedical corpus. In particular, we propose the multi-head attention mechanism and pack BiGRU to fuse multiple semantic information for rigorous context modeling. In the generation part, EGFI utilizes another pre-trained language model BioGPT-2 where the generation sentences are selected based on filtering rules. We evaluated the classification part on ""DDIs 2013"" dataset and ""DTIs"" dataset, achieving the FI score of 0.842 and 0.720 respectively. Moreover, we applied the classification part to distinguish high-quality generated sentences and verified with the exiting growth truth to confirm the filtered sentences. The generated sentences that are not recorded in DrugBank and DDIs 2013 dataset also demonstrate the potential of EGFI to identify novel drug relationships. ","[{'version': 'v1', 'created': 'Mon, 25 Jan 2021 06:52:29 GMT'}]",2021-01-26,"[['Huang', 'Lei', ''], ['Lin', 'Jiecong', ''], ['Li', 'Xiangtao', ''], ['Song', 'Linqi', ''], ['Wong', 'Ka-Chun', '']]",0,1,2021-01-25,1,5,1,0,0,0,f19a2aac35863c5ab1d24ac8963f20b8f086299c,231698741.0,https://www.semanticscholar.org/paper/f19a2aac35863c5ab1d24ac8963f20b8f086299c,Briefings Bioinform.,2021.0,43.0,9.0,0.0,True,"['Computer Science', 'Medicine']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Medicine', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2143627084', 'name': 'Lei Huang'}, {'authorId': '3228565', 'name': 'Jiecong Lin'}, {'authorId': '2144393680', 'name': 'Xiangtao Li'}, {'authorId': '38117987', 'name': 'Linqi Song'}, {'authorId': '37063249', 'name': 'Ka-chun Wong'}]","['City University of Hong Kong', 'Jilin University']",['China'],2021-01
2102.03556,Ernie Chang,"Ernie Chang, Xiaoyu Shen, Dawei Zhu, Vera Demberg, Hui Su",Neural Data-to-Text Generation with LM-based Text Augmentation,Accepted EACL 2021,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  For many new application domains for data-to-text generation, the main obstacle in training neural models consists of a lack of training data. While usually large numbers of instances are available on the data side, often only very few text samples are available. To address this problem, we here propose a novel few-shot approach for this setting. Our approach automatically augments the data available for training by (i) generating new text samples based on replacing specific values by alternative ones from the same category, (ii) generating new text samples based on GPT-2, and (iii) proposing an automatic method for pairing the new text samples with data samples. As the text augmentation can introduce noise to the training data, we use cycle consistency as an objective, in order to make sure that a given data sample can be correctly reconstructed after having been formulated as text (and that text samples can be reconstructed from data). On both the E2E and WebNLG benchmarks, we show that this weakly supervised training paradigm is able to outperform fully supervised seq2seq models with less than 10% annotations. By utilizing all annotated data, our model can boost the performance of a standard seq2seq model by over 5 BLEU points, establishing a new state-of-the-art on both datasets. ","[{'version': 'v1', 'created': 'Sat, 6 Feb 2021 10:21:48 GMT'}]",2021-02-09,"[['Chang', 'Ernie', ''], ['Shen', 'Xiaoyu', ''], ['Zhu', 'Dawei', ''], ['Demberg', 'Vera', ''], ['Su', 'Hui', '']]",0,1,2021-02-06,1,5,1,1,1,0,c7ab9761a25f730de840e9b140bd77551746314a,231847372.0,https://www.semanticscholar.org/paper/c7ab9761a25f730de840e9b140bd77551746314a,Conference of the European Chapter of the Association for Computational Linguistics,2021.0,57.0,37.0,6.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '48025720', 'name': 'Ernie Chang'}, {'authorId': '2562211', 'name': 'Xiaoyu Shen'}, {'authorId': '47770349', 'name': 'D. Zhu'}, {'authorId': '2869436', 'name': 'Vera Demberg'}, {'authorId': '2087042666', 'name': 'Hui Su'}]","['Tencent', 'Saarland University', 'Amazon']","['Germany', 'China']",2021-02
2102.04664,Shuai Lu,"Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy,
  Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li,
  Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou,
  Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, Shujie Liu",CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation,14 pages; Revise CodeBLEU scores for all models on text-to-code task,,,,cs.SE cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Benchmark datasets have a significant impact on accelerating research in programming language tasks. In this paper, we introduce CodeXGLUE, a benchmark dataset to foster machine learning research for program understanding and generation. CodeXGLUE includes a collection of 10 tasks across 14 datasets and a platform for model evaluation and comparison. CodeXGLUE also features three baseline systems, including the BERT-style, GPT-style, and Encoder-Decoder models, to make it easy for researchers to use the platform. The availability of such data and baselines can help the development and validation of new methods that can be applied to various program understanding and generation problems. ","[{'version': 'v1', 'created': 'Tue, 9 Feb 2021 06:16:25 GMT'}, {'version': 'v2', 'created': 'Tue, 16 Mar 2021 08:28:37 GMT'}]",2021-03-17,"[['Lu', 'Shuai', ''], ['Guo', 'Daya', ''], ['Ren', 'Shuo', ''], ['Huang', 'Junjie', ''], ['Svyatkovskiy', 'Alexey', ''], ['Blanco', 'Ambrosio', ''], ['Clement', 'Colin', ''], ['Drain', 'Dawn', ''], ['Jiang', 'Daxin', ''], ['Tang', 'Duyu', ''], ['Li', 'Ge', ''], ['Zhou', 'Lidong', ''], ['Shou', 'Linjun', ''], ['Zhou', 'Long', ''], ['Tufano', 'Michele', ''], ['Gong', 'Ming', ''], ['Zhou', 'Ming', ''], ['Duan', 'Nan', ''], ['Sundaresan', 'Neel', ''], ['Deng', 'Shao Kun', ''], ['Fu', 'Shengyu', ''], ['Liu', 'Shujie', '']]",0,1,2021-02-09,2,22,2,0,0,0,69a72ff5b30642d11c96635e99aadad3140d33a7,231855531.0,https://www.semanticscholar.org/paper/69a72ff5b30642d11c96635e99aadad3140d33a7,NeurIPS Datasets and Benchmarks,2021.0,116.0,543.0,127.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2115338656', 'name': 'Shuai Lu'}, {'authorId': '51223794', 'name': 'Daya Guo'}, {'authorId': '50052368', 'name': 'Shuo Ren'}, {'authorId': '145505727', 'name': 'Junjie Huang'}, {'authorId': '47090739', 'name': 'Alexey Svyatkovskiy'}, {'authorId': '37488446', 'name': 'Ambrosio Blanco'}, {'authorId': '2064509404', 'name': 'Colin B. Clement'}, {'authorId': '1943097969', 'name': 'Dawn Drain'}, {'authorId': '71790825', 'name': 'Daxin Jiang'}, {'authorId': '39483833', 'name': 'Duyu Tang'}, {'authorId': '1410115257', 'name': 'Ge Li'}, {'authorId': '2143359114', 'name': 'Lidong Zhou'}, {'authorId': '24962156', 'name': 'Linjun Shou'}, {'authorId': '2135918679', 'name': 'Long Zhou'}, {'authorId': '40626221', 'name': 'Michele Tufano'}, {'authorId': '50175330', 'name': 'Ming Gong'}, {'authorId': '92660691', 'name': 'Ming Zhou'}, {'authorId': '46429989', 'name': 'Nan Duan'}, {'authorId': '145507437', 'name': 'Neel Sundaresan'}, {'authorId': '1702996983', 'name': 'Shao Kun Deng'}, {'authorId': '2072784644', 'name': 'Shengyu Fu'}, {'authorId': '1803054', 'name': 'Shujie Liu'}]","['Peking University', 'Beihang University', 'Sun Yat-sen University', 'Microsoft']",['China'],2021-02
2103.01403,Qing Li,"Qing Li, Siyuan Huang, Yining Hong, Yixin Zhu, Ying Nian Wu, Song-Chun
  Zhu","A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics",ICLR 2023. website: https://liqing-ustc.github.io/HINT,,,,cs.LG cs.AI cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Inspired by humans' exceptional ability to master arithmetic and generalize to new problems, we present a new dataset, Handwritten arithmetic with INTegers (HINT), to examine machines' capability of learning generalizable concepts at three levels: perception, syntax, and semantics. In HINT, machines are tasked with learning how concepts are perceived from raw signals such as images (i.e., perception), how multiple concepts are structurally combined to form a valid expression (i.e., syntax), and how concepts are realized to afford various reasoning tasks (i.e., semantics), all in a weakly supervised manner. Focusing on systematic generalization, we carefully design a five-fold test set to evaluate both the interpolation and the extrapolation of learned concepts w.r.t. the three levels. Further, we design a few-shot learning split to determine whether or not models can rapidly learn new concepts and generalize them to more complex scenarios. To comprehend existing models' limitations, we undertake extensive experiments with various sequence-to-sequence models, including RNNs, Transformers, and GPT-3 (with the chain of thought prompting). The results indicate that current models struggle to extrapolate to long-range syntactic dependency and semantics. Models exhibit a considerable gap toward human-level generalization when evaluated with new concepts in a few-shot setting. Moreover, we discover that it is infeasible to solve HINT by merely scaling up the dataset and the model size; this strategy contributes little to the extrapolation of syntax and semantics. Finally, in zero-shot GPT-3 experiments, the chain of thought prompting exhibits impressive results and significantly boosts the test accuracy. We believe the HINT dataset and the experimental findings are of great interest to the learning community on systematic generalization. ","[{'version': 'v1', 'created': 'Tue, 2 Mar 2021 01:32:54 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Sep 2022 02:16:59 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Apr 2023 07:54:24 GMT'}]",2023-04-19,"[['Li', 'Qing', ''], ['Huang', 'Siyuan', ''], ['Hong', 'Yining', ''], ['Zhu', 'Yixin', ''], ['Wu', 'Ying Nian', ''], ['Zhu', 'Song-Chun', '']]",0,1,2021-03-02,3,6,3,1,0,1,268fedc5d786fa197b294dccab7eea02dc08038a,252383681.0,https://www.semanticscholar.org/paper/268fedc5d786fa197b294dccab7eea02dc08038a,International Conference on Learning Representations,2021.0,117.0,4.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2117895397', 'name': 'Qing Li'}, {'authorId': '1713084', 'name': 'Siyuan Huang'}, {'authorId': '151261268', 'name': 'Yining Hong'}, {'authorId': '2118318556', 'name': 'Yixin Zhu'}, {'authorId': '39092098', 'name': 'Y. Wu'}, {'authorId': '145380991', 'name': 'Song-Chun Zhu'}]","['Peking University', 'University of California, Los Angeles', 'The University of Tokyo']","['China', 'United States', 'Japan']",2021-03
2103.06561,Zhiwu Lu,"Yuqi Huo, Manli Zhang, Guangzhen Liu, Haoyu Lu, Yizhao Gao, Guoxing
  Yang, Jingyuan Wen, Heng Zhang, Baogui Xu, Weihao Zheng, Zongzheng Xi,
  Yueqian Yang, Anwen Hu, Jinming Zhao, Ruichen Li, Yida Zhao, Liang Zhang,
  Yuqing Song, Xin Hong, Wanqing Cui, Danyang Hou, Yingyan Li, Junyi Li, Peiyu
  Liu, Zheng Gong, Chuhao Jin, Yuchong Sun, Shizhe Chen, Zhiwu Lu, Zhicheng
  Dou, Qin Jin, Yanyan Lan, Wayne Xin Zhao, Ruihua Song, and Ji-Rong Wen",WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training,"This paper is the outcome of the Chinese multi-modal pre-training
  project called 'WenLan'",,,,cs.CV cs.IR,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Multi-modal pre-training models have been intensively explored to bridge vision and language in recent years. However, most of them explicitly model the cross-modal interaction between image-text pairs, by assuming that there exists strong semantic correlation between the text and image modalities. Since this strong assumption is often invalid in real-world scenarios, we choose to implicitly model the cross-modal correlation for large-scale multi-modal pre-training, which is the focus of the Chinese project `WenLan' led by our team. Specifically, with the weak correlation assumption over image-text pairs, we propose a two-tower pre-training model called BriVL within the cross-modal contrastive learning framework. Unlike OpenAI CLIP that adopts a simple contrastive learning method, we devise a more advanced algorithm by adapting the latest method MoCo into the cross-modal scenario. By building a large queue-based dictionary, our BriVL can incorporate more negative samples in limited GPU resources. We further construct a large Chinese multi-source image-text dataset called RUC-CAS-WenLan for pre-training our BriVL model. Extensive experiments demonstrate that the pre-trained BriVL model outperforms both UNITER and OpenAI CLIP on various downstream tasks. ","[{'version': 'v1', 'created': 'Thu, 11 Mar 2021 09:39:49 GMT'}, {'version': 'v2', 'created': 'Sat, 13 Mar 2021 07:52:50 GMT'}, {'version': 'v3', 'created': 'Tue, 16 Mar 2021 14:01:03 GMT'}, {'version': 'v4', 'created': 'Wed, 17 Mar 2021 12:17:02 GMT'}, {'version': 'v5', 'created': 'Fri, 19 Mar 2021 23:30:38 GMT'}, {'version': 'v6', 'created': 'Thu, 8 Jul 2021 13:56:05 GMT'}]",2021-07-09,"[['Huo', 'Yuqi', ''], ['Zhang', 'Manli', ''], ['Liu', 'Guangzhen', ''], ['Lu', 'Haoyu', ''], ['Gao', 'Yizhao', ''], ['Yang', 'Guoxing', ''], ['Wen', 'Jingyuan', ''], ['Zhang', 'Heng', ''], ['Xu', 'Baogui', ''], ['Zheng', 'Weihao', ''], ['Xi', 'Zongzheng', ''], ['Yang', 'Yueqian', ''], ['Hu', 'Anwen', ''], ['Zhao', 'Jinming', ''], ['Li', 'Ruichen', ''], ['Zhao', 'Yida', ''], ['Zhang', 'Liang', ''], ['Song', 'Yuqing', ''], ['Hong', 'Xin', ''], ['Cui', 'Wanqing', ''], ['Hou', 'Danyang', ''], ['Li', 'Yingyan', ''], ['Li', 'Junyi', ''], ['Liu', 'Peiyu', ''], ['Gong', 'Zheng', ''], ['Jin', 'Chuhao', ''], ['Sun', 'Yuchong', ''], ['Chen', 'Shizhe', ''], ['Lu', 'Zhiwu', ''], ['Dou', 'Zhicheng', ''], ['Jin', 'Qin', ''], ['Lan', 'Yanyan', ''], ['Zhao', 'Wayne Xin', ''], ['Song', 'Ruihua', ''], ['Wen', 'Ji-Rong', '']]",0,0,2021-03-11,6,35,2,0,0,0,0b6f13177a90a02d44a41c62659988561f56c168,232185258.0,https://www.semanticscholar.org/paper/0b6f13177a90a02d44a41c62659988561f56c168,arXiv.org,2021.0,43.0,97.0,7.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '4140493', 'name': 'Yuqi Huo'}, {'authorId': '151483168', 'name': 'Manli Zhang'}, {'authorId': '151472765', 'name': 'Guangzhen Liu'}, {'authorId': '2115608152', 'name': 'Haoyu Lu'}, {'authorId': '1939358', 'name': 'Yizhao Gao'}, {'authorId': '2109946595', 'name': 'Guoxing Yang'}, {'authorId': '2113342436', 'name': 'Jing Wen'}, {'authorId': None, 'name': 'Heng Zhang'}, {'authorId': '2171840775', 'name': 'Baogui Xu'}, {'authorId': '2152975905', 'name': 'Weihao Zheng'}, {'authorId': '2056797110', 'name': 'Zongzheng Xi'}, {'authorId': None, 'name': 'Yueqian Yang'}, {'authorId': '120897486', 'name': 'Anwen Hu'}, {'authorId': '2109905684', 'name': 'Jinming Zhao'}, {'authorId': None, 'name': 'Ruichen Li'}, {'authorId': '50976845', 'name': 'Yida Zhao'}, {'authorId': '2146643767', 'name': 'Liang Zhang'}, {'authorId': '2152601849', 'name': 'Yuqing Song'}, {'authorId': '145251362', 'name': 'Xin Hong'}, {'authorId': '84299447', 'name': 'Wanqing Cui'}, {'authorId': '17627246', 'name': 'Danyang Hou'}, {'authorId': '2111233834', 'name': 'Yingyan Li'}, {'authorId': '2138220600', 'name': 'Junyi Li'}, {'authorId': '2108129670', 'name': 'Peiyu Liu'}, {'authorId': '2053583858', 'name': 'Zheng Gong'}, {'authorId': '153823521', 'name': 'Chu Jin'}, {'authorId': '2109029275', 'name': 'Yuchong Sun'}, {'authorId': '3009919', 'name': 'Shizhe Chen'}, {'authorId': '1776220', 'name': 'Zhiwu Lu'}, {'authorId': '1897235', 'name': 'Zhicheng Dou'}, {'authorId': '1721329', 'name': 'Qin Jin'}, {'authorId': '37510256', 'name': 'Yanyan Lan'}, {'authorId': '2542603', 'name': 'Wayne Xin Zhao'}, {'authorId': '35119829', 'name': 'Ruihua Song'}, {'authorId': '153693432', 'name': 'Ji-rong Wen'}]","['Chinese Academy of Sciences', 'Renmin University of China']",['China'],2021-03
2104.10935,Peihua Li,"Jiangtao Xie, Ruiren Zeng, Qilong Wang, Ziqi Zhou, Peihua Li",SoT: Delving Deeper into Classification Head for Transformer,,,,,cs.CV cs.AI cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Transformer models are not only successful in natural language processing (NLP) but also demonstrate high potential in computer vision (CV). Despite great advance, most of works only focus on improvement of architectures but pay little attention to the classification head. For years transformer models base exclusively on classification token to construct the final classifier, without explicitly harnessing high-level word tokens. In this paper, we propose a novel transformer model called second-order transformer (SoT), exploiting simultaneously the classification token and word tokens for the classifier. Specifically, we empirically disclose that high-level word tokens contain rich information, which per se are very competent with the classifier and moreover, are complementary to the classification token. To effectively harness such rich information, we propose multi-headed global cross-covariance pooling with singular value power normalization, which shares similar philosophy and thus is compatible with the transformer block, better than commonly used pooling methods. Then, we study comprehensively how to explicitly combine word tokens with classification token for building the final classification head. For CV tasks, our SoT significantly improves state-of-the-art vision transformers on challenging benchmarks including ImageNet and ImageNet-A. For NLP tasks, through fine-tuning based on pretrained language transformers including GPT and BERT, our SoT greatly boosts the performance on widely used tasks such as CoLA and RTE. Code will be available at https://peihuali.org/SoT ","[{'version': 'v1', 'created': 'Thu, 22 Apr 2021 09:05:09 GMT'}, {'version': 'v2', 'created': 'Sat, 18 Dec 2021 04:28:10 GMT'}]",2021-12-21,"[['Xie', 'Jiangtao', ''], ['Zeng', 'Ruiren', ''], ['Wang', 'Qilong', ''], ['Zhou', 'Ziqi', ''], ['Li', 'Peihua', '']]",0,1,2021-04-22,2,5,3,0,0,0,409e010e6859f764486b6f0fb6f088a817398bdf,245335371.0,https://www.semanticscholar.org/paper/409e010e6859f764486b6f0fb6f088a817398bdf,,2021.0,75.0,4.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '144418234', 'name': 'Jiangtao Xie'}, {'authorId': '47689605', 'name': 'Rui Zeng'}, {'authorId': '2108995796', 'name': 'Qilong Wang'}, {'authorId': '2112252436', 'name': 'Ziqi Zhou'}, {'authorId': '40426020', 'name': 'P. Li'}]","['Dalian University of Technology', 'MEGVII Technology', 'Tianjin University']",['China'],2021-04
2104.12145,Renjie Li,"Renjie Li, Ceyao Zhang, Sixuan Mao, Hai Huang, Mou Zhong, Yiou Cui,
  Xiyuan Zhou, Feng Yin, Zhaoyu Zhang",LLM helps design and optimize photonic crystal surface emitting lasers,"14 pages,",,,,physics.optics,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Conventional design and optimization of Photonic Crystal Surface Emitting Lasers (PCSEL) usually requires expert knowledge in semiconductor physics and optimization algorithms, which is also known as the inverse design problem. However, with the trend towards automation and depersonalization of the entire integrated circuits (IC) industry, the conventional method, with the drawback of being relatively labor-intensive and sub-optimal, warrants further refinement. This technical dilemma remained until the emergence of Large Language Models (LLMs), such as OpenAI's ChatGPT and Google's Bard. This paper explores the possibility of applying LLMs to machine learning-based design and optimization of PCSELs. Specifically, we utilize GPT3.5 and GPT4. By simply having conversations, GPT assisted us with writing Finite Difference Time Domain (FDTD) simulation code and deep reinforcement learning code to acquire the optimized PCSEL solution, spanning from the proposition of ideas to the realization of algorithms. Given that GPT will perform better when given detailed and specific questions, we break down the PCSEL design problem into a series of sub-problems and converse with GPT by posing open-ended heuristic questions rather than definitive commands. This paper shows that LLMs, such as ChatGPT, can guide the nanophotonic design and optimization processes, on both the conceptual and technical level, and we propose new human-AI co-design strategies and show their practical implications. We achieve a significant milestone for the first step towards an automated end to end nanophotonic design and production pipeline. ","[{'version': 'v1', 'created': 'Sun, 25 Apr 2021 12:55:22 GMT'}, {'version': 'v2', 'created': 'Fri, 11 Aug 2023 12:15:46 GMT'}]",2023-08-14,"[['Li', 'Renjie', ''], ['Zhang', 'Ceyao', ''], ['Mao', 'Sixuan', ''], ['Huang', 'Hai', ''], ['Zhong', 'Mou', ''], ['Cui', 'Yiou', ''], ['Zhou', 'Xiyuan', ''], ['Yin', 'Feng', ''], ['Zhang', 'Zhaoyu', '']]",1,1,2021-04-25,2,9,1,3,0,3,620a0d6d62f43d75fc8cb60b98e167cbb62f44ab,233394625.0,https://www.semanticscholar.org/paper/620a0d6d62f43d75fc8cb60b98e167cbb62f44ab,,2021.0,58.0,1.0,0.0,False,['Physics'],"[{'category': 'Physics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2143498404', 'name': 'Renjie Li'}, {'authorId': '2000868582', 'name': 'Ceyao Zhang'}, {'authorId': '2229336933', 'name': 'Sixuan Mao'}, {'authorId': '2231257853', 'name': 'Hai Huang'}, {'authorId': '2228586123', 'name': 'Mou Zhong'}, {'authorId': '2219859403', 'name': 'Yiou Cui'}, {'authorId': '2157629246', 'name': 'Xiyuan Zhou'}, {'authorId': '5951460', 'name': 'F. Yin'}, {'authorId': '48806475', 'name': 'Zhaoyu Zhang'}]","['Chinese University of Hong Kong, Shenzhen', 'Zhaoyu Zhang * ,', 'Future Network of Intelligence Institute, Shenzhen, China.', 'Shenzhen Research Institute of Big Data']",['China'],2021-04
2105.12544,Xiachong Feng,"Xiachong Feng, Xiaocheng Feng, Libo Qin, Bing Qin, Ting Liu",Language Model as an Annotator: Exploring DialoGPT for Dialogue Summarization,ACL 2021,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Current dialogue summarization systems usually encode the text with a number of general semantic features (e.g., keywords and topics) to gain more powerful dialogue modeling capabilities. However, these features are obtained via open-domain toolkits that are dialog-agnostic or heavily relied on human annotations. In this paper, we show how DialoGPT, a pre-trained model for conversational response generation, can be developed as an unsupervised dialogue annotator, which takes advantage of dialogue background knowledge encoded in DialoGPT. We apply DialoGPT to label three types of features on two dialogue summarization datasets, SAMSum and AMI, and employ pre-trained and non pre-trained models as our summarizes. Experimental results show that our proposed method can obtain remarkable improvements on both datasets and achieves new state-of-the-art performance on the SAMSum dataset. ","[{'version': 'v1', 'created': 'Wed, 26 May 2021 13:50:13 GMT'}, {'version': 'v2', 'created': 'Fri, 28 May 2021 01:34:49 GMT'}]",2021-05-31,"[['Feng', 'Xiachong', ''], ['Feng', 'Xiaocheng', ''], ['Qin', 'Libo', ''], ['Qin', 'Bing', ''], ['Liu', 'Ting', '']]",0,1,2021-05-26,2,5,1,0,0,0,f80b837a211f71c6647c5755d4569559f0c2c0f7,235196029.0,https://www.semanticscholar.org/paper/f80b837a211f71c6647c5755d4569559f0c2c0f7,Annual Meeting of the Association for Computational Linguistics,2021.0,50.0,49.0,14.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '51056442', 'name': 'Xiachong Feng'}, {'authorId': '2674998', 'name': 'Xiaocheng Feng'}, {'authorId': None, 'name': 'Libo Qin'}, {'authorId': '152277111', 'name': 'Bing Qin'}, {'authorId': '40282288', 'name': 'Ting Liu'}]","['Harbin Institute of Technology', 'Peng Cheng Laboratory']",['China'],2021-05
2105.14781,Yilin Niu,"Yilin Niu, Fei Huang, Jiaming Liang, Wenkai Chen, Xiaoyan Zhu, Minlie
  Huang",A Semantic-based Method for Unsupervised Commonsense Question Answering,Accepted by ACL 2021 (long paper),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Unsupervised commonsense question answering is appealing since it does not rely on any labeled task data. Among existing work, a popular solution is to use pre-trained language models to score candidate choices directly conditioned on the question or context. However, such scores from language models can be easily affected by irrelevant factors, such as word frequencies, sentence structures, etc. These distracting factors may not only mislead the model to choose a wrong answer but also make it oversensitive to lexical perturbations in candidate answers.   In this paper, we present a novel SEmantic-based Question Answering method (SEQA) for unsupervised commonsense question answering. Instead of directly scoring each answer choice, our method first generates a set of plausible answers with generative models (e.g., GPT-2), and then uses these plausible answers to select the correct choice by considering the semantic similarity between each plausible answer and each choice. We devise a simple, yet sound formalism for this idea and verify its effectiveness and robustness with extensive experiments. We evaluate the proposed method on four benchmark datasets, and our method achieves the best results in unsupervised settings. Moreover, when attacked by TextFooler with synonym replacement, SEQA demonstrates much less performance drops than baselines, thereby indicating stronger robustness. ","[{'version': 'v1', 'created': 'Mon, 31 May 2021 08:21:52 GMT'}]",2021-06-01,"[['Niu', 'Yilin', ''], ['Huang', 'Fei', ''], ['Liang', 'Jiaming', ''], ['Chen', 'Wenkai', ''], ['Zhu', 'Xiaoyan', ''], ['Huang', 'Minlie', '']]",0,1,2021-05-31,1,6,1,1,1,0,8e80a1ee0093771343ed8f0bed6eb6dbe7d4d7d8,235254600.0,https://www.semanticscholar.org/paper/8e80a1ee0093771343ed8f0bed6eb6dbe7d4d7d8,Annual Meeting of the Association for Computational Linguistics,2021.0,34.0,12.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '10680347', 'name': 'Yilin Niu'}, {'authorId': '152159016', 'name': 'Fei Huang'}, {'authorId': '2118673924', 'name': 'Jiaming Liang'}, {'authorId': '2139081042', 'name': 'Wenkai Chen'}, {'authorId': '145213540', 'name': 'Xiaoyan Zhu'}, {'authorId': '1730108', 'name': 'Minlie Huang'}]","['Beijing University of Posts and Telecommunications', 'Artificial Intelligence Research Institute', 'Beijing Information Science & Technology University', 'State Key Lab of Intelligent Technology and Systems;', 'Tsinghua University', 'Delta College of Science and Technology']","['China', 'Spain', 'Sudan']",2021-05
2105.14897,Shuai Bai,"Shuai Bai, Zhedong Zheng, Xiaohan Wang, Junyang Lin, Zhu Zhang, Chang
  Zhou, Yi Yang, Hongxia Yang",Connecting Language and Vision for Natural Language-Based Vehicle Retrieval,"CVPR 2021 AI CITY CHALLENGE Natural Language-Based Vehicle Retrieval
  Top 1",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Vehicle search is one basic task for the efficient traffic management in terms of the AI City. Most existing practices focus on the image-based vehicle matching, including vehicle re-identification and vehicle tracking. In this paper, we apply one new modality, i.e., the language description, to search the vehicle of interest and explore the potential of this task in the real-world scenario. The natural language-based vehicle search poses one new challenge of fine-grained understanding of both vision and language modalities. To connect language and vision, we propose to jointly train the state-of-the-art vision models with the transformer-based language model in an end-to-end manner. Except for the network structure design and the training strategy, several optimization objectives are also re-visited in this work. The qualitative and quantitative experiments verify the effectiveness of the proposed method. Our proposed method has achieved the 1st place on the 5th AI City Challenge, yielding competitive performance 18.69% MRR accuracy on the private test set. We hope this work can pave the way for the future study on using language description effectively and efficiently for real-world vehicle retrieval systems. The code will be available at https://github.com/ShuaiBai623/AIC2021-T5-CLV. ","[{'version': 'v1', 'created': 'Mon, 31 May 2021 11:42:03 GMT'}]",2021-06-01,"[['Bai', 'Shuai', ''], ['Zheng', 'Zhedong', ''], ['Wang', 'Xiaohan', ''], ['Lin', 'Junyang', ''], ['Zhang', 'Zhu', ''], ['Zhou', 'Chang', ''], ['Yang', 'Yi', ''], ['Yang', 'Hongxia', '']]",0,0,2021-05-31,1,8,1,1,1,0,b2de5bd44ed8eb9cfd46d2e7e5a0fff61e68cfee,235254214.0,https://www.semanticscholar.org/paper/b2de5bd44ed8eb9cfd46d2e7e5a0fff61e68cfee,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),2021.0,71.0,22.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '3768186', 'name': 'Shuai Bai'}, {'authorId': '7435343', 'name': 'Zhedong Zheng'}, {'authorId': '2109074276', 'name': 'Xiaohan Wang'}, {'authorId': '35996608', 'name': 'Junyang Lin'}, {'authorId': '2109107985', 'name': 'Zhu Zhang'}, {'authorId': '144161025', 'name': 'Chang Zhou'}, {'authorId': '7179962', 'name': 'Yi Yang'}, {'authorId': '38385080', 'name': 'Hongxia Yang'}]","['University of Technology Sydney', 'Alibaba', 'Zhejiang University']","['China', 'Australia']",2021-05
2106.02227,Zekang Li,"Zekang Li, Jinchao Zhang, Zhengcong Fei, Yang Feng, Jie Zhou",Conversations Are Not Flat: Modeling the Dynamic Information Flow across Dialogue Utterances,ACL2021 main conference (long paper),,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Nowadays, open-domain dialogue models can generate acceptable responses according to the historical context based on the large-scale pre-trained language models. However, they generally concatenate the dialogue history directly as the model input to predict the response, which we named as the flat pattern and ignores the dynamic information flow across dialogue utterances. In this work, we propose the DialoFlow model, in which we introduce a dynamic flow mechanism to model the context flow, and design three training objectives to capture the information dynamics across dialogue utterances by addressing the semantic influence brought about by each utterance in large-scale pre-training. Experiments on the multi-reference Reddit Dataset and DailyDialog Dataset demonstrate that our DialoFlow significantly outperforms the DialoGPT on the dialogue generation task. Besides, we propose the Flow score, an effective automatic metric for evaluating interactive human-bot conversation quality based on the pre-trained DialoFlow, which presents high chatbot-level correlation ($r=0.9$) with human ratings among 11 chatbots. Code and pre-trained models will be public. \footnote{\url{https://github.com/ictnlp/DialoFlow}} ","[{'version': 'v1', 'created': 'Fri, 4 Jun 2021 03:04:06 GMT'}]",2021-06-07,"[['Li', 'Zekang', ''], ['Zhang', 'Jinchao', ''], ['Fei', 'Zhengcong', ''], ['Feng', 'Yang', ''], ['Zhou', 'Jie', '']]",0,1,2021-06-04,1,5,2,0,0,0,1b05155aa1f7532609bf2cf5fea668a745399780,235352646.0,https://www.semanticscholar.org/paper/1b05155aa1f7532609bf2cf5fea668a745399780,Annual Meeting of the Association for Computational Linguistics,2021.0,27.0,44.0,9.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '15401738', 'name': 'Zekang Li'}, {'authorId': '2108970018', 'name': 'Jinchao Zhang'}, {'authorId': '2066415714', 'name': 'Zhengcong Fei'}, {'authorId': '49771779', 'name': 'Yang Feng'}, {'authorId': '49178343', 'name': 'Jie Zhou'}]","['Tencent', 'Joint work with Pattern Recognition Center, WeChat AI,', 'University of Chinese Academy of Sciences', 'Institute of Computing Technology']",['China'],2021-06
2106.03983,Hai Hu,"Hai Hu, He Zhou, Zuoyu Tian, Yiwen Zhang, Yina Ma, Yanting Li, Yixin
  Nie, Kyle Richardson",Investigating Transfer Learning in Multilingual Pre-trained Language Models through Chinese Natural Language Inference,accepted to ACL Findings 2021,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multilingual transformers (XLM, mT5) have been shown to have remarkable transfer skills in zero-shot settings. Most transfer studies, however, rely on automatically translated resources (XNLI, XQuAD), making it hard to discern the particular linguistic knowledge that is being transferred, and the role of expert annotated monolingual datasets when developing task-specific models. We investigate the cross-lingual transfer abilities of XLM-R for Chinese and English natural language inference (NLI), with a focus on the recent large-scale Chinese dataset OCNLI. To better understand linguistic transfer, we created 4 categories of challenge and adversarial tasks (totaling 17 new datasets) for Chinese that build on several well-known resources for English (e.g., HANS, NLI stress-tests). We find that cross-lingual models trained on English NLI do transfer well across our Chinese tasks (e.g., in 3/4 of our challenge categories, they perform as well/better than the best monolingual models, even on 3/5 uniquely Chinese linguistic phenomena such as idioms, pro drop). These results, however, come with important caveats: cross-lingual models often perform best when trained on a mixture of English and high-quality monolingual NLI data (OCNLI), and are often hindered by automatically translated resources (XNLI-zh). For many phenomena, all models continue to struggle, highlighting the need for our new diagnostics to help benchmark Chinese and cross-lingual models. All new datasets/code are released at https://github.com/huhailinguist/ChineseNLIProbing. ","[{'version': 'v1', 'created': 'Mon, 7 Jun 2021 22:00:18 GMT'}]",2021-06-09,"[['Hu', 'Hai', ''], ['Zhou', 'He', ''], ['Tian', 'Zuoyu', ''], ['Zhang', 'Yiwen', ''], ['Ma', 'Yina', ''], ['Li', 'Yanting', ''], ['Nie', 'Yixin', ''], ['Richardson', 'Kyle', '']]",0,0,2021-06-07,1,8,2,1,1,0,1cde1aa4f7bcebc47b35518cec452893ea6b824c,235367897.0,https://www.semanticscholar.org/paper/1cde1aa4f7bcebc47b35518cec452893ea6b824c,Findings,2021.0,58.0,6.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '145309512', 'name': 'Hai Hu'}, {'authorId': '1596810069', 'name': 'He Zhou'}, {'authorId': '105042132', 'name': 'Zuoyu Tian'}, {'authorId': '2108139880', 'name': 'Yiwen Zhang'}, {'authorId': '2146277459', 'name': 'Yina Ma'}, {'authorId': '2155221186', 'name': 'Yanting Li'}, {'authorId': '40383658', 'name': 'Yixin Nie'}, {'authorId': '46666605', 'name': 'Kyle Richardson'}]","['Northwestern University', 'University of North Carolina at Chapel Hill', 'Shanghai Jiao Tong University', 'Allen Institute for Artificial Intelligence', 'Indiana University Bloomington', 'Brigham Young University']","['China', 'United States']",2021-06
2106.07854,Duzhen Zhang,"Duzhen Zhang, Tielin Zhang, Shuncheng Jia, Xiang Cheng and Bo Xu",Population-coding and Dynamic-neurons improved Spiking Actor Network for Reinforcement Learning,,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the Deep Neural Networks (DNNs) as a powerful function approximator, Deep Reinforcement Learning (DRL) has been excellently demonstrated on robotic control tasks. Compared to DNNs with vanilla artificial neurons, the biologically plausible Spiking Neural Network (SNN) contains a diverse population of spiking neurons, making it naturally powerful on state representation with spatial and temporal information. Based on a hybrid learning framework, where a spike actor-network infers actions from states and a deep critic network evaluates the actor, we propose a Population-coding and Dynamic-neurons improved Spiking Actor Network (PDSAN) for efficient state representation from two different scales: input coding and neuronal coding. For input coding, we apply population coding with dynamically receptive fields to directly encode each input state component. For neuronal coding, we propose different types of dynamic-neurons (containing 1st-order and 2nd-order neuronal dynamics) to describe much more complex neuronal dynamics. Finally, the PDSAN is trained in conjunction with deep critic networks using the Twin Delayed Deep Deterministic policy gradient algorithm (TD3-PDSAN). Extensive experimental results show that our TD3-PDSAN model achieves better performance than state-of-the-art models on four OpenAI gym benchmark tasks. It is an important attempt to improve RL with SNN towards the effective computation satisfying biological plausibility. ","[{'version': 'v1', 'created': 'Tue, 15 Jun 2021 03:14:41 GMT'}, {'version': 'v2', 'created': 'Wed, 23 Jun 2021 08:54:31 GMT'}, {'version': 'v3', 'created': 'Thu, 22 Sep 2022 08:49:43 GMT'}]",2022-09-23,"[['Zhang', 'Duzhen', ''], ['Zhang', 'Tielin', ''], ['Jia', 'Shuncheng', ''], ['Cheng', 'Xiang', ''], ['Xu', 'Bo', '']]",0,0,2021-06-15,3,5,1,0,0,0,5d75adf5f262e9757239aca0538f591ee1b39e33,260502782.0,https://www.semanticscholar.org/paper/5d75adf5f262e9757239aca0538f591ee1b39e33,,2021.0,37.0,1.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '70123445', 'name': 'Duzhen Zhang'}, {'authorId': '2952471', 'name': 'Tielin Zhang'}, {'authorId': '1990801834', 'name': 'Shuncheng Jia'}, {'authorId': '2047900811', 'name': 'Xiang Cheng'}, {'authorId': '2109511678', 'name': 'Bo Xu'}]","['Center for Excellence in Brain Science and Intelligence Technology', 'University of Chinese Academy of Sciences', 'Chinese Academy of Sciences']",['China'],2021-06
2106.10715,Zhengyan Zhang,"Zhengyan Zhang, Yuxian Gu, Xu Han, Shengqi Chen, Chaojun Xiao, Zhenbo
  Sun, Yuan Yao, Fanchao Qi, Jian Guan, Pei Ke, Yanzheng Cai, Guoyang Zeng,
  Zhixing Tan, Zhiyuan Liu, Minlie Huang, Wentao Han, Yang Liu, Xiaoyan Zhu,
  Maosong Sun",CPM-2: Large-scale Cost-effective Pre-trained Language Models,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In recent years, the size of pre-trained language models (PLMs) has grown by leaps and bounds. However, efficiency issues of these large-scale PLMs limit their utilization in real-world scenarios. We present a suite of cost-effective techniques for the use of PLMs to deal with the efficiency issues of pre-training, fine-tuning, and inference. (1) We introduce knowledge inheritance to accelerate the pre-training process by exploiting existing PLMs instead of training models from scratch. (2) We explore the best practice of prompt tuning with large-scale PLMs. Compared with conventional fine-tuning, prompt tuning significantly reduces the number of task-specific parameters. (3) We implement a new inference toolkit, namely InfMoE, for using large-scale PLMs with limited computational resources. Based on our cost-effective pipeline, we pre-train two models: an encoder-decoder bilingual model with 11 billion parameters (CPM-2) and its corresponding MoE version with 198 billion parameters. In our experiments, we compare CPM-2 with mT5 on downstream tasks. Experimental results show that CPM-2 has excellent general language intelligence. Moreover, we validate the efficiency of InfMoE when conducting inference of large-scale models having tens of billions of parameters on a single GPU. All source code and model parameters are available at https://github.com/TsinghuaAI/CPM. ","[{'version': 'v1', 'created': 'Sun, 20 Jun 2021 15:43:54 GMT'}, {'version': 'v2', 'created': 'Wed, 23 Jun 2021 07:15:36 GMT'}, {'version': 'v3', 'created': 'Thu, 24 Jun 2021 13:23:42 GMT'}]",2021-06-25,"[['Zhang', 'Zhengyan', ''], ['Gu', 'Yuxian', ''], ['Han', 'Xu', ''], ['Chen', 'Shengqi', ''], ['Xiao', 'Chaojun', ''], ['Sun', 'Zhenbo', ''], ['Yao', 'Yuan', ''], ['Qi', 'Fanchao', ''], ['Guan', 'Jian', ''], ['Ke', 'Pei', ''], ['Cai', 'Yanzheng', ''], ['Zeng', 'Guoyang', ''], ['Tan', 'Zhixing', ''], ['Liu', 'Zhiyuan', ''], ['Huang', 'Minlie', ''], ['Han', 'Wentao', ''], ['Liu', 'Yang', ''], ['Zhu', 'Xiaoyan', ''], ['Sun', 'Maosong', '']]",0,0,2021-06-20,3,19,1,2,2,0,00a95c2e2af1c6ef7ba41fe502a8cc729cdd284d,235490263.0,https://www.semanticscholar.org/paper/00a95c2e2af1c6ef7ba41fe502a8cc729cdd284d,AI Open,2021.0,44.0,59.0,8.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2148904862', 'name': 'Zhengyan Zhang'}, {'authorId': '2116405624', 'name': 'Yuxian Gu'}, {'authorId': '48506411', 'name': 'Xu Han'}, {'authorId': '2118529462', 'name': 'Shengqi Chen'}, {'authorId': '51131083', 'name': 'Chaojun Xiao'}, {'authorId': '113907105', 'name': 'Zhenbo Sun'}, {'authorId': '1390925224', 'name': 'Yuan Yao'}, {'authorId': '51466208', 'name': 'Fanchao Qi'}, {'authorId': '145902734', 'name': 'Jian Guan'}, {'authorId': '1886879', 'name': 'Pei Ke'}, {'authorId': '2111082056', 'name': 'Yanzheng Cai'}, {'authorId': '1398454307', 'name': 'Guoyang Zeng'}, {'authorId': '3468510', 'name': 'Zhixing Tan'}, {'authorId': '49293587', 'name': 'Zhiyuan Liu'}, {'authorId': '1730108', 'name': 'Minlie Huang'}, {'authorId': '2114924784', 'name': 'Wentao Han'}, {'authorId': '2152797839', 'name': 'Yang Liu'}, {'authorId': '145213540', 'name': 'Xiaoyan Zhu'}, {'authorId': '1753344', 'name': 'Maosong Sun'}]",['Tsinghua University'],['China'],2021-06
2106.13928,Rui Huang,"Jingxuan Li, Rui Huang, Wei Li, Kai Yao, Weiguo Tan",Toward Less Hidden Cost of Code Completion with Acceptance and Ranking Models,"10 pages, 7 figures, accepted by ICSME 2021",,,,cs.SE cs.AI cs.PL,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Code completion is widely used by software developers to provide coding suggestions given a partially written code snippet. Apart from the traditional code completion methods, which only support single token completion at minimal positions, recent studies show the ability to provide longer code completion at more flexible positions. However, such frequently triggered and longer completion results reduce the overall precision as they generate more invalid results. Moreover, different studies are mostly incompatible with each other. Thus, it is vital to develop an ensemble framework that can combine results from multiple models to draw merits and offset defects of each model.   This paper conducts a coding simulation to collect data from code context and different code completion models and then apply the data in two tasks. First, we introduce an acceptance model which can dynamically control whether to display completion results to the developer. It uses simulation features to predict whether correct results exist in the output of these models. Our best model reduces the percentage of false-positive completion from 55.09% to 17.44%. Second, we design a fusion ranking scheme that can automatically identify the priority of the completion results and reorder the candidates from multiple code completion models. This scheme is flexible in dealing with various models, regardless of the type or the length of their completion results. We integrate this ranking scheme with two frequency models and a GPT-2 styled language model, along with the acceptance model to yield 27.80% and 37.64% increase in TOP1 and TOP5 accuracy, respectively. In addition, we propose a new code completion evaluation metric, Benefit-Cost Ratio(BCR), taking into account the benefit of keystrokes saving and hidden cost of completion list browsing, which is closer to real coder experience scenario. ","[{'version': 'v1', 'created': 'Sat, 26 Jun 2021 03:02:49 GMT'}]",2021-06-29,"[['Li', 'Jingxuan', ''], ['Huang', 'Rui', ''], ['Li', 'Wei', ''], ['Yao', 'Kai', ''], ['Tan', 'Weiguo', '']]",0,1,2021-06-26,1,5,3,1,1,0,eb41816d63465d8cb6af546ccfa28de029b027ee,235658662.0,https://www.semanticscholar.org/paper/eb41816d63465d8cb6af546ccfa28de029b027ee,IEEE International Conference on Software Maintenance and Evolution,2021.0,41.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2157341042', 'name': 'Jingxuan Li'}, {'authorId': '2140386274', 'name': 'Rui Huang'}, {'authorId': '2157337593', 'name': 'Wei Li'}, {'authorId': '40895035', 'name': 'Kai-Lang Yao'}, {'authorId': '2115474385', 'name': 'Weiguo Tan'}]",['Huawei Technologies (China)'],['China'],2021-06
2106.15332,Jun Wang,"Yixuan Qiao, Hao Chen, Jun Wang, Yihao Chen, Xianbin Ye, Ziliang Li,
  Xianbiao Qi, Peng Gao, Guotong Xie",Winner Team Mia at TextVQA Challenge 2021: Vision-and-Language Representation Learning with Pre-trained Sequence-to-Sequence Model,Winner of TextVQA 2021,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  TextVQA requires models to read and reason about text in images to answer questions about them. Specifically, models need to incorporate a new modality of text present in the images and reason over it to answer TextVQA questions. In this challenge, we use generative model T5 for TextVQA task. Based on pre-trained checkpoint T5-3B from HuggingFace repository, two other pre-training tasks including masked language modeling(MLM) and relative position prediction(RPP) are designed to better align object feature and scene text. In the stage of pre-training, encoder is dedicate to handle the fusion among multiple modalities: question text, object text labels, scene text labels, object visual features, scene visual features. After that decoder generates the text sequence step-by-step, cross entropy loss is required by default. We use a large-scale scene text dataset in pre-training and then fine-tune the T5-3B with the TextVQA dataset only. ","[{'version': 'v1', 'created': 'Thu, 24 Jun 2021 06:39:37 GMT'}]",2021-06-30,"[['Qiao', 'Yixuan', ''], ['Chen', 'Hao', ''], ['Wang', 'Jun', ''], ['Chen', 'Yihao', ''], ['Ye', 'Xianbin', ''], ['Li', 'Ziliang', ''], ['Qi', 'Xianbiao', ''], ['Gao', 'Peng', ''], ['Xie', 'Guotong', '']]",0,0,2021-06-24,1,9,1,1,1,0,9ee87103ccf8ce60c2e049c6411840593cd0af57,235669751.0,https://www.semanticscholar.org/paper/9ee87103ccf8ce60c2e049c6411840593cd0af57,arXiv.org,2021.0,3.0,6.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '148407220', 'name': 'Yixuan Qiao'}, {'authorId': '2149051600', 'name': 'Hao Chen'}, {'authorId': '2152811266', 'name': 'Jun Wang'}, {'authorId': '2116613438', 'name': 'Yihao Chen'}, {'authorId': '2100406437', 'name': 'Xia Ye'}, {'authorId': '2118274894', 'name': 'Ziliang Li'}, {'authorId': '2689287', 'name': 'Xianbiao Qi'}, {'authorId': '2088487395', 'name': 'Peng Gao'}, {'authorId': '2052157466', 'name': 'G. Xie'}]","['Ping An Health Cloud Company Limited., Shenzhen, China.', 'University of Science and Technology Beijing', 'Jinan University', 'Ping An International Smart City Technology Co., Ltd., Shenzhen, China.', 'Central University of Finance and Economics', 'Visual Computing Group, Ping An Property & Casualty Insurance Company, Shenzhen, China.']",['China'],2021-06
2108.01547,Pei Ke,"Hao Zhou, Pei Ke, Zheng Zhang, Yuxian Gu, Yinhe Zheng, Chujie Zheng,
  Yida Wang, Chen Henry Wu, Hao Sun, Xiaocong Yang, Bosi Wen, Xiaoyan Zhu,
  Minlie Huang, Jie Tang",EVA: An Open-Domain Chinese Dialogue System with Large-Scale Generative Pre-Training,"8 pages, 4 figures",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although pre-trained language models have remarkably enhanced the generation ability of dialogue systems, open-domain Chinese dialogue systems are still limited by the dialogue data and the model size compared with English ones. In this paper, we propose EVA, a Chinese dialogue system that contains the largest Chinese pre-trained dialogue model with 2.8B parameters. To build this model, we collect the largest Chinese dialogue dataset named WDC-Dialogue from various public social media. This dataset contains 1.4B context-response pairs and is used as the pre-training corpus of EVA. Extensive experiments on automatic and human evaluation show that EVA outperforms other Chinese pre-trained dialogue models especially in the multi-turn interaction of human-bot conversations. ","[{'version': 'v1', 'created': 'Tue, 3 Aug 2021 14:55:24 GMT'}]",2021-08-04,"[['Zhou', 'Hao', ''], ['Ke', 'Pei', ''], ['Zhang', 'Zheng', ''], ['Gu', 'Yuxian', ''], ['Zheng', 'Yinhe', ''], ['Zheng', 'Chujie', ''], ['Wang', 'Yida', ''], ['Wu', 'Chen Henry', ''], ['Sun', 'Hao', ''], ['Yang', 'Xiaocong', ''], ['Wen', 'Bosi', ''], ['Zhu', 'Xiaoyan', ''], ['Huang', 'Minlie', ''], ['Tang', 'Jie', '']]",0,1,2021-08-03,1,14,2,0,0,0,2d29e1e684f8db8a143b3313cee991c4c786d340,236881307.0,https://www.semanticscholar.org/paper/2d29e1e684f8db8a143b3313cee991c4c786d340,arXiv.org,2021.0,27.0,41.0,8.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '144751955', 'name': 'Hao Zhou'}, {'authorId': '1886879', 'name': 'Pei Ke'}, {'authorId': '2148904901', 'name': 'Zheng Zhang'}, {'authorId': '2116405624', 'name': 'Yuxian Gu'}, {'authorId': '51456789', 'name': 'Yinhe Zheng'}, {'authorId': '146452866', 'name': 'Chujie Zheng'}, {'authorId': '2143476152', 'name': 'Yida Wang'}, {'authorId': '114621402', 'name': 'Chen Henry Wu'}, {'authorId': '144990601', 'name': 'Hao Sun'}, {'authorId': '2124756940', 'name': 'Xiaocong Yang'}, {'authorId': '2122225897', 'name': 'Bosi Wen'}, {'authorId': '145213540', 'name': 'Xiaoyan Zhu'}, {'authorId': '1730108', 'name': 'Minlie Huang'}, {'authorId': '2148911990', 'name': 'Jie Tang'}]",['Tsinghua University'],['China'],2021-08
2108.02984,Wei Wang,"Wei Wang, Piji Li, Hai-Tao Zheng",Sentence Semantic Regression for Text Generation,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recall the classical text generation works, the generation framework can be briefly divided into two phases: \textbf{idea reasoning} and \textbf{surface realization}. The target of idea reasoning is to figure out the main idea which will be presented in the following talking/writing periods. Surface realization aims to arrange the most appropriate sentence to depict and convey the information distilled from the main idea. However, the current popular token-by-token text generation methods ignore this crucial process and suffer from many serious issues, such as idea/topic drift. To tackle the problems and realize this two-phase paradigm, we propose a new framework named Sentence Semantic Regression (\textbf{SSR}) based on sentence-level language modeling. For idea reasoning, two architectures \textbf{SSR-AR} and \textbf{SSR-NonAR} are designed to conduct sentence semantic regression autoregressively (like GPT2/3) and bidirectionally (like BERT). In the phase of surface realization, a mixed-granularity sentence decoder is designed to generate text with better consistency by jointly incorporating the predicted sentence-level main idea as well as the preceding contextual token-level information. We conduct experiments on four tasks of story ending prediction, story ending generation, dialogue generation, and sentence infilling. The results show that SSR can obtain better performance in terms of automatic metrics and human evaluation. ","[{'version': 'v1', 'created': 'Fri, 6 Aug 2021 07:35:59 GMT'}]",2021-08-09,"[['Wang', 'Wei', ''], ['Li', 'Piji', ''], ['Zheng', 'Hai-Tao', '']]",0,1,2021-08-06,1,3,1,1,1,0,157fa5a93a2873e9ba9a76aa7a137f3b94a06f0e,236950731.0,https://www.semanticscholar.org/paper/157fa5a93a2873e9ba9a76aa7a137f3b94a06f0e,arXiv.org,2021.0,48.0,0.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2158624629', 'name': 'Wei Wang'}, {'authorId': '2109069108', 'name': 'Pijian Li'}, {'authorId': '16215052', 'name': 'Haitao Zheng'}]","['Tencent', 'Tsinghua University']",['China'],2021-08
2108.11023,Hongbin Liu,"Hongbin Liu, Jinyuan Jia, Wenjie Qu, Neil Zhenqiang Gong",EncoderMI: Membership Inference against Pre-trained Encoders in Contrastive Learning,"To appear in ACM Conference on Computer and Communications Security
  (CCS), 2021",,,,cs.CR cs.CV cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Given a set of unlabeled images or (image, text) pairs, contrastive learning aims to pre-train an image encoder that can be used as a feature extractor for many downstream tasks. In this work, we propose EncoderMI, the first membership inference method against image encoders pre-trained by contrastive learning. In particular, given an input and a black-box access to an image encoder, EncoderMI aims to infer whether the input is in the training dataset of the image encoder. EncoderMI can be used 1) by a data owner to audit whether its (public) data was used to pre-train an image encoder without its authorization or 2) by an attacker to compromise privacy of the training data when it is private/sensitive. Our EncoderMI exploits the overfitting of the image encoder towards its training data. In particular, an overfitted image encoder is more likely to output more (or less) similar feature vectors for two augmented versions of an input in (or not in) its training dataset. We evaluate EncoderMI on image encoders pre-trained on multiple datasets by ourselves as well as the Contrastive Language-Image Pre-training (CLIP) image encoder, which is pre-trained on 400 million (image, text) pairs collected from the Internet and released by OpenAI. Our results show that EncoderMI can achieve high accuracy, precision, and recall. We also explore a countermeasure against EncoderMI via preventing overfitting through early stopping. Our results show that it achieves trade-offs between accuracy of EncoderMI and utility of the image encoder, i.e., it can reduce the accuracy of EncoderMI, but it also incurs classification accuracy loss of the downstream classifiers built based on the image encoder. ","[{'version': 'v1', 'created': 'Wed, 25 Aug 2021 03:00:45 GMT'}]",2021-08-26,"[['Liu', 'Hongbin', ''], ['Jia', 'Jinyuan', ''], ['Qu', 'Wenjie', ''], ['Gong', 'Neil Zhenqiang', '']]",0,0,2021-08-25,1,4,3,0,0,0,dcf115bd311988638bf4791bf6f88b1aeed0322d,237290083.0,https://www.semanticscholar.org/paper/dcf115bd311988638bf4791bf6f88b1aeed0322d,Conference on Computer and Communications Security,2021.0,51.0,46.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2110279247', 'name': 'Hongbin Liu'}, {'authorId': '143987304', 'name': 'Jinyuan Jia'}, {'authorId': '2124940443', 'name': 'Wenjie Qu'}, {'authorId': '144516687', 'name': 'N. Gong'}]","['Huazhong University of Science and Technology', 'Duke University']","['China', 'United States']",2021-08
2108.12144,Qingyuan Liang,"Qingyuan Liang, Zeyu Sun, Qihao Zhu, Wenjie Zhang, Lian Yu, Yingfei
  Xiong, Lu Zhang",Lyra: A Benchmark for Turducken-Style Code Generation,,,,,cs.SE cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Recently, neural techniques have been used to generate source code automatically. While promising for declarative languages, these approaches achieve much poorer performance on datasets for imperative languages. Since a declarative language is typically embedded in an imperative language (i.e., the turducken-style programming) in real-world software development, the promising results on declarative languages can hardly lead to significant reduction of manual software development efforts. In this paper, we define a new code generation task: given a natural language comment, this task aims to generate a program in a base imperative language with an embedded declarative language. To our knowledge, this is the first turducken-style code generation task. For this task, we present Lyra: a dataset in Python with embedded SQL. This dataset contains 2,000 carefully annotated database manipulation programs from real-world projects. Each program is paired with both a Chinese comment and an English comment. In our experiment, we adopted Transformer, BERT-style, and GPT-style models as baselines. In the best setting, the generation performance of GPT-style models is better than others, where the AST exact matching accuracy is 24% and 25.5% when using Chinese and English comments, respectively. Therefore, we believe that Lyra provides a new challenge for code generation. Yet, overcoming this challenge may significantly boost the applicability of code generation techniques for real-world software development. ","[{'version': 'v1', 'created': 'Fri, 27 Aug 2021 07:22:55 GMT'}, {'version': 'v2', 'created': 'Wed, 4 May 2022 15:59:44 GMT'}, {'version': 'v3', 'created': 'Sun, 24 Jul 2022 04:54:17 GMT'}]",2022-07-26,"[['Liang', 'Qingyuan', ''], ['Sun', 'Zeyu', ''], ['Zhu', 'Qihao', ''], ['Zhang', 'Wenjie', ''], ['Yu', 'Lian', ''], ['Xiong', 'Yingfei', ''], ['Zhang', 'Lu', '']]",0,1,2021-08-27,3,7,2,0,0,0,7ad2b12526e77badd629b10880db147afce4864a,237346909.0,https://www.semanticscholar.org/paper/7ad2b12526e77badd629b10880db147afce4864a,International Joint Conference on Artificial Intelligence,2021.0,45.0,6.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2055746376', 'name': 'Qingyuan Liang'}, {'authorId': '2007961688', 'name': 'Zeyu Sun'}, {'authorId': '2152203384', 'name': 'Qihao Zhu'}, {'authorId': '2108393773', 'name': 'Wenjie Zhang'}, {'authorId': '2148660276', 'name': 'Lian Yu'}, {'authorId': '3096536', 'name': 'Yingfei Xiong'}, {'authorId': '144281339', 'name': 'Lu Zhang'}]",['Peking University'],['China'],2021-08
2108.13679,Weizhi Wang,"Weizhi Wang, Zhirui Zhang, Junliang Guo, Yinpei Dai, Boxing Chen and
  Weihua Luo",Task-Oriented Dialogue System as Natural Language Generation,SIGIR 2022,,10.1145/3477495.3531920,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we propose to formulate the task-oriented dialogue system as the purely natural language generation task, so as to fully leverage the large-scale pre-trained models like GPT-2 and simplify complicated delexicalization prepossessing. However, directly applying this method heavily suffers from the dialogue entity inconsistency caused by the removal of delexicalized tokens, as well as the catastrophic forgetting problem of the pre-trained model during fine-tuning, leading to unsatisfactory performance. To alleviate these problems, we design a novel GPT-Adapter-CopyNet network, which incorporates the lightweight adapter and CopyNet modules into GPT-2 to achieve better performance on transfer learning and dialogue entity generation. Experimental results conducted on the DSTC8 Track 1 benchmark and MultiWOZ dataset demonstrate that our proposed approach significantly outperforms baseline models with a remarkable performance on automatic and human evaluations. ","[{'version': 'v1', 'created': 'Tue, 31 Aug 2021 08:36:42 GMT'}, {'version': 'v2', 'created': 'Wed, 1 Sep 2021 07:33:04 GMT'}, {'version': 'v3', 'created': 'Sun, 24 Apr 2022 12:30:31 GMT'}]",2022-04-26,"[['Wang', 'Weizhi', ''], ['Zhang', 'Zhirui', ''], ['Guo', 'Junliang', ''], ['Dai', 'Yinpei', ''], ['Chen', 'Boxing', ''], ['Luo', 'Weihua', '']]",0,1,2021-08-31,3,6,2,1,1,0,3b0615caf74c7c84a33be78813ec7ebd0f53033d,237363762.0,https://www.semanticscholar.org/paper/3b0615caf74c7c84a33be78813ec7ebd0f53033d,Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,2021.0,39.0,24.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108438929', 'name': 'Weizhi Wang'}, {'authorId': '4947404', 'name': 'Zhirui Zhang'}, {'authorId': '2117224175', 'name': 'Junliang Guo'}, {'authorId': '30087809', 'name': 'Yinpei Dai'}, {'authorId': '2152687324', 'name': 'Boxing Chen'}, {'authorId': '48244817', 'name': 'Weihua Luo'}]","['Alibaba', 'Tencent', 'Microsoft', 'University of California, Santa Barbara']","['China', 'United States']",2021-08
2109.02401,Tiezheng Yu,"Tiezheng Yu, Wenliang Dai, Zihan Liu, Pascale Fung",Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization,Long Paper Accepted in EMNLP 2021,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Multimodal abstractive summarization (MAS) models that summarize videos (vision modality) and their corresponding transcripts (text modality) are able to extract the essential information from massive multimodal data on the Internet. Recently, large-scale generative pre-trained language models (GPLMs) have been shown to be effective in text generation tasks. However, existing MAS models cannot leverage GPLMs' powerful generation ability. To fill this research gap, we aim to study two research questions: 1) how to inject visual information into GPLMs without hurting their generation ability; and 2) where is the optimal place in GPLMs to inject the visual information? In this paper, we present a simple yet effective method to construct vision guided (VG) GPLMs for the MAS task using attention-based add-on layers to incorporate visual information while maintaining their original text generation ability. Results show that our best model significantly surpasses the prior state-of-the-art model by 5.7 ROUGE-1, 5.3 ROUGE-2, and 5.1 ROUGE-L scores on the How2 dataset, and our visual guidance method contributes 83.6% of the overall improvement. Furthermore, we conduct thorough ablation studies to analyze the effectiveness of various modality fusion methods and fusion locations. ","[{'version': 'v1', 'created': 'Mon, 6 Sep 2021 12:31:21 GMT'}, {'version': 'v2', 'created': 'Wed, 8 Sep 2021 02:20:27 GMT'}, {'version': 'v3', 'created': 'Fri, 1 Oct 2021 13:14:00 GMT'}, {'version': 'v4', 'created': 'Sun, 10 Oct 2021 16:15:49 GMT'}]",2021-10-12,"[['Yu', 'Tiezheng', ''], ['Dai', 'Wenliang', ''], ['Liu', 'Zihan', ''], ['Fung', 'Pascale', '']]",0,1,2021-09-06,4,4,1,0,0,0,ec33756aa312a33a5979d4d542da2eeabaa8d1a0,237420398.0,https://www.semanticscholar.org/paper/ec33756aa312a33a5979d4d542da2eeabaa8d1a0,Conference on Empirical Methods in Natural Language Processing,2021.0,65.0,40.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1660855299', 'name': 'Tiezheng Yu'}, {'authorId': '47653392', 'name': 'Wenliang Dai'}, {'authorId': '2117941142', 'name': 'Zihan Liu'}, {'authorId': '2057151752', 'name': 'Pascale Fung'}]",['Hong Kong University of Science and Technology'],['China'],2021-09
2109.03034,Jianhao Shen,"Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang,
  Qun Liu",Generate & Rank: A Multi-task Framework for Math Word Problems,Findings of EMNLP2021,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Math word problem (MWP) is a challenging and critical task in natural language processing. Many recent studies formalize MWP as a generation task and have adopted sequence-to-sequence models to transform problem descriptions to mathematical expressions. However, mathematical expressions are prone to minor mistakes while the generation objective does not explicitly handle such mistakes. To address this limitation, we devise a new ranking task for MWP and propose Generate & Rank, a multi-task framework based on a generative pre-trained language model. By joint training with generation and ranking, the model learns from its own mistakes and is able to distinguish between correct and incorrect expressions. Meanwhile, we perform tree-based disturbance specially designed for MWP and an online update to boost the ranker. We demonstrate the effectiveness of our proposed method on the benchmark and the results show that our method consistently outperforms baselines in all datasets. Particularly, in the classical Math23k, our method is 7% (78.4% $\rightarrow$ 85.4%) higher than the state-of-the-art. ","[{'version': 'v1', 'created': 'Tue, 7 Sep 2021 12:21:49 GMT'}]",2021-09-08,"[['Shen', 'Jianhao', ''], ['Yin', 'Yichun', ''], ['Li', 'Lin', ''], ['Shang', 'Lifeng', ''], ['Jiang', 'Xin', ''], ['Zhang', 'Ming', ''], ['Liu', 'Qun', '']]",0,1,2021-09-07,1,7,2,0,0,0,4698fc4712f0212c8a3810fd67b41ee8b8896aba,237434245.0,https://www.semanticscholar.org/paper/4698fc4712f0212c8a3810fd67b41ee8b8896aba,Conference on Empirical Methods in Natural Language Processing,2021.0,37.0,73.0,12.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Mathematics', 'source': 's2-fos-model'}]","[{'authorId': '2115733983', 'name': 'Jianhao Shen'}, {'authorId': '1384668226', 'name': 'Yichun Yin'}, {'authorId': '2155689998', 'name': 'Lin Li'}, {'authorId': '50812138', 'name': 'Lifeng Shang'}, {'authorId': '2110310493', 'name': 'Xin Jiang'}, {'authorId': '2145178175', 'name': 'Ming Zhang'}, {'authorId': '30738758', 'name': 'Qun Liu'}]","['Peking University', 'Huawei Technologies (China)']",['China'],2021-09
2109.03137,Zhihua Jin,"Zhihua Jin, Xin Jiang, Xingbo Wang, Qun Liu, Yong Wang, Xiaozhe Ren,
  Huamin Qu",NumGPT: Improving Numeracy Ability of Generative Pre-trained Models,"8 pages, 3 figures",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing generative pre-trained language models (e.g., GPT) focus on modeling the language structure and semantics of general texts. However, those models do not consider the numerical properties of numbers and cannot perform robustly on numerical reasoning tasks (e.g., math word problems and measurement estimation). In this paper, we propose NumGPT, a generative pre-trained model that explicitly models the numerical properties of numbers in texts. Specifically, it leverages a prototype-based numeral embedding to encode the mantissa of the number and an individual embedding to encode the exponent of the number. A numeral-aware loss function is designed to integrate numerals into the pre-training objective of NumGPT. We conduct extensive experiments on four different datasets to evaluate the numeracy ability of NumGPT. The experiment results show that NumGPT outperforms baseline models (e.g., GPT and GPT with DICE) on a range of numerical reasoning tasks such as measurement estimation, number comparison, math word problems, and magnitude classification. Ablation studies are also conducted to evaluate the impact of pre-training and model hyperparameters on the performance. ","[{'version': 'v1', 'created': 'Tue, 7 Sep 2021 15:06:12 GMT'}, {'version': 'v2', 'created': 'Wed, 13 Oct 2021 00:46:10 GMT'}]",2021-10-14,"[['Jin', 'Zhihua', ''], ['Jiang', 'Xin', ''], ['Wang', 'Xingbo', ''], ['Liu', 'Qun', ''], ['Wang', 'Yong', ''], ['Ren', 'Xiaozhe', ''], ['Qu', 'Huamin', '']]",0,1,2021-09-07,2,7,2,0,0,0,58c7a31bf47948d936de937b1cf7b49463608557,237431469.0,https://www.semanticscholar.org/paper/58c7a31bf47948d936de937b1cf7b49463608557,arXiv.org,2021.0,30.0,14.0,1.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2111472932', 'name': 'Zhihua Jin'}, {'authorId': '2110310493', 'name': 'Xin Jiang'}, {'authorId': '50141732', 'name': 'Xingbo Wang'}, {'authorId': '30738758', 'name': 'Qun Liu'}, {'authorId': '2153953618', 'name': 'Yong Wang'}, {'authorId': '153457264', 'name': 'Xiaozhe Ren'}, {'authorId': '2064924230', 'name': 'Huamin Qu'}]","['Hong Kong University of Science and Technology', 'Huawei Technologies (China)', 'Singapore Management University']","['China', 'Singapore']",2021-09
2109.04645,Fei Mi,"Fei Mi, Yitong Li, Yasheng Wang, Xin Jiang and Qun Liu",CINS: Comprehensive Instruction for Few-shot Learning in Task-oriented Dialog Systems,Accepted at AAAI2022,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As labeling cost for different modules in task-oriented dialog (ToD) systems is high, a major challenge in practice is to learn different tasks with the least amount of labeled data. Recently, prompting methods over pre-trained language models (PLMs) have shown promising results for few-shot learning in ToD. To better utilize the power of PLMs, this paper proposes Comprehensive Instruction (CINS) that exploits PLMs with extra task-specific instructions. We design a schema (definition, constraint, prompt) of instructions and their customized realizations for three important downstream tasks in ToD, i.e. intent classification, dialog state tracking, and natural language generation. A sequence-to-sequence model (T5) is adopted to solve these three tasks in a unified framework. Extensive experiments are conducted on these ToD tasks in realistic few-shot learning scenarios with small validation data. Empirical results demonstrate that the proposed CINS approach consistently improves techniques that finetune PLMs with raw input or short prompts. ","[{'version': 'v1', 'created': 'Fri, 10 Sep 2021 03:23:06 GMT'}, {'version': 'v2', 'created': 'Tue, 14 Sep 2021 09:35:51 GMT'}, {'version': 'v3', 'created': 'Mon, 13 Dec 2021 11:46:18 GMT'}, {'version': 'v4', 'created': 'Mon, 21 Mar 2022 14:24:12 GMT'}]",2022-03-22,"[['Mi', 'Fei', ''], ['Li', 'Yitong', ''], ['Wang', 'Yasheng', ''], ['Jiang', 'Xin', ''], ['Liu', 'Qun', '']]",0,0,2021-09-10,4,5,2,1,1,0,20da8033ed8b696e2e27ec40b1aa8a0ab82b964c,237485305.0,https://www.semanticscholar.org/paper/20da8033ed8b696e2e27ec40b1aa8a0ab82b964c,AAAI Conference on Artificial Intelligence,2021.0,46.0,30.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '33727421', 'name': 'Fei Mi'}, {'authorId': '50024168', 'name': 'Yitong Li'}, {'authorId': '2136912252', 'name': 'Yasheng Wang'}, {'authorId': '2110310493', 'name': 'Xin Jiang'}, {'authorId': '30738758', 'name': 'Qun Liu'}]",['Huawei Technologies (China)'],['China'],2021-09
2109.04825,Ekaterina Artemova,"Laida Kushnareva, Daniil Cherniavskii, Vladislav Mikhailov, Ekaterina
  Artemova, Serguei Barannikov, Alexander Bernstein, Irina Piontkovskaya,
  Dmitri Piontkovski, Evgeny Burnaev",Artificial Text Detection via Examining the Topology of Attention Maps,Accepted to EMNLP 2021,"Proceedings of the 2021 Conference on Empirical Methods in Natural
  Language Processing, pages 635-649",10.18653/v1/2021.emnlp-main.50,,cs.CL cs.LG math.AT,http://creativecommons.org/licenses/by/4.0/,"  The impressive capabilities of recent generative models to create texts that are challenging to distinguish from the human-written ones can be misused for generating fake news, product reviews, and even abusive content. Despite the prominent performance of existing methods for artificial text detection, they still lack interpretability and robustness towards unseen models. To this end, we propose three novel types of interpretable topological features for this task based on Topological Data Analysis (TDA) which is currently understudied in the field of NLP. We empirically show that the features derived from the BERT model outperform count- and neural-based baselines up to 10\% on three common datasets, and tend to be the most robust towards unseen GPT-style generation models as opposed to existing methods. The probing analysis of the features reveals their sensitivity to the surface and syntactic properties. The results demonstrate that TDA is a promising line with respect to NLP tasks, specifically the ones that incorporate surface and structural information. ","[{'version': 'v1', 'created': 'Fri, 10 Sep 2021 12:13:45 GMT'}, {'version': 'v2', 'created': 'Thu, 28 Apr 2022 15:11:30 GMT'}]",2023-02-09,"[['Kushnareva', 'Laida', ''], ['Cherniavskii', 'Daniil', ''], ['Mikhailov', 'Vladislav', ''], ['Artemova', 'Ekaterina', ''], ['Barannikov', 'Serguei', ''], ['Bernstein', 'Alexander', ''], ['Piontkovskaya', 'Irina', ''], ['Piontkovski', 'Dmitri', ''], ['Burnaev', 'Evgeny', '']]",0,1,2021-09-10,2,9,3,0,0,0,2af03a9bff2e6f14deca8f0b7bb243a56efd182a,237485580.0,https://www.semanticscholar.org/paper/2af03a9bff2e6f14deca8f0b7bb243a56efd182a,Conference on Empirical Methods in Natural Language Processing,2021.0,53.0,35.0,2.0,True,"['Computer Science', 'Mathematics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Mathematics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '147328740', 'name': 'Laida Kushnareva'}, {'authorId': '2037913268', 'name': 'D. Cherniavskii'}, {'authorId': '51259225', 'name': 'V. Mikhailov'}, {'authorId': '13033978', 'name': 'E. Artemova'}, {'authorId': '101098891', 'name': 'S. Barannikov'}, {'authorId': '47065547', 'name': 'A. Bernstein'}, {'authorId': '31430204', 'name': 'Irina Piontkovskaya'}, {'authorId': '1948165', 'name': 'D. Piontkovski'}, {'authorId': '51139941', 'name': 'Evgeny Burnaev'}]","['Skolkovo Institute of Science and Technology', 'Huawei Technologies (China)', 'French National Centre for Scientific Research', 'National Research University Higher School of Economics']","['China', 'Russia', 'France']",2021-09
2109.05179,Fanyi Qu,"Fanyi Qu, Xin Jia, Yunfang Wu",Asking Questions Like Educational Experts: Automatically Generating Question-Answer Pairs on Real-World Examination Data,Accepted as a long paper in the main conference of EMNLP 2021,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Generating high quality question-answer pairs is a hard but meaningful task. Although previous works have achieved great results on answer-aware question generation, it is difficult to apply them into practical application in the education field. This paper for the first time addresses the question-answer pair generation task on the real-world examination data, and proposes a new unified framework on RACE. To capture the important information of the input passage we first automatically generate(rather than extracting) keyphrases, thus this task is reduced to keyphrase-question-answer triplet joint generation. Accordingly, we propose a multi-agent communication model to generate and optimize the question and keyphrases iteratively, and then apply the generated question and keyphrases to guide the generation of answers. To establish a solid benchmark, we build our model on the strong generative pre-training model. Experimental results show that our model makes great breakthroughs in the question-answer pair generation task. Moreover, we make a comprehensive analysis on our model, suggesting new directions for this challenging task. ","[{'version': 'v1', 'created': 'Sat, 11 Sep 2021 04:10:57 GMT'}, {'version': 'v2', 'created': 'Fri, 17 Sep 2021 01:26:45 GMT'}]",2021-09-20,"[['Qu', 'Fanyi', ''], ['Jia', 'Xin', ''], ['Wu', 'Yunfang', '']]",0,1,2021-09-11,2,3,1,0,0,0,42568f7bf5dd040e04dcf176fe2d86df5c4beb86,237491433.0,https://www.semanticscholar.org/paper/42568f7bf5dd040e04dcf176fe2d86df5c4beb86,Conference on Empirical Methods in Natural Language Processing,2021.0,40.0,13.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2127464628', 'name': 'Fanyi Qu'}, {'authorId': '145650221', 'name': 'Xin Jia'}, {'authorId': '2115377800', 'name': 'Yunfang Wu'}]",['Peking University'],['China'],2021-09
2109.05256,Zewei Sun,"Zewei Sun, Mingxuan Wang and Lei Li",Multilingual Translation via Grafting Pre-trained Language Models,Accepted in EMNLP 2021 (Findings),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Can pre-trained BERT for one language and GPT for another be glued together to translate texts? Self-supervised training using only monolingual data has led to the success of pre-trained (masked) language models in many NLP tasks. However, directly connecting BERT as an encoder and GPT as a decoder can be challenging in machine translation, for GPT-like models lack a cross-attention component that is needed in seq2seq decoders. In this paper, we propose Graformer to graft separately pre-trained (masked) language models for machine translation. With monolingual data for pre-training and parallel data for grafting training, we maximally take advantage of the usage of both types of data. Experiments on 60 directions show that our method achieves average improvements of 5.8 BLEU in x2en and 2.9 BLEU in en2x directions comparing with the multilingual Transformer of the same size. ","[{'version': 'v1', 'created': 'Sat, 11 Sep 2021 10:57:45 GMT'}]",2021-09-14,"[['Sun', 'Zewei', ''], ['Wang', 'Mingxuan', ''], ['Li', 'Lei', '']]",0,1,2021-09-11,1,3,1,0,0,0,10b9a1f65d70964f85430d8edf419de736939d41,237490358.0,https://www.semanticscholar.org/paper/10b9a1f65d70964f85430d8edf419de736939d41,Conference on Empirical Methods in Natural Language Processing,2021.0,58.0,14.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '48064977', 'name': 'Zewei Sun'}, {'authorId': '50468534', 'name': 'Mingxuan Wang'}, {'authorId': '143900005', 'name': 'Lei Li'}]","['ByteDance', 'University of California, Santa Barbara']","['China', 'United States']",2021-09
2109.05687,Runxin Xu,"Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao Chang,
  Songfang Huang, Fei Huang",Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning,Accepted as a long paper to EMNLP 2021 Main Conference,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent pretrained language models extend from millions to billions of parameters. Thus the need to fine-tune an extremely large pretrained model with a limited training corpus arises in various downstream tasks. In this paper, we propose a straightforward yet effective fine-tuning technique, Child-Tuning, which updates a subset of parameters (called child network) of large pretrained models via strategically masking out the gradients of the non-child network during the backward process. Experiments on various downstream tasks in GLUE benchmark show that Child-Tuning consistently outperforms the vanilla fine-tuning by 1.5~8.6 average score among four different pretrained models, and surpasses the prior fine-tuning techniques by 0.6~1.3 points. Furthermore, empirical results on domain transfer and task transfer show that Child-Tuning can obtain better generalization performance by large margins. ","[{'version': 'v1', 'created': 'Mon, 13 Sep 2021 03:39:52 GMT'}]",2021-09-14,"[['Xu', 'Runxin', ''], ['Luo', 'Fuli', ''], ['Zhang', 'Zhiyuan', ''], ['Tan', 'Chuanqi', ''], ['Chang', 'Baobao', ''], ['Huang', 'Songfang', ''], ['Huang', 'Fei', '']]",0,0,2021-09-13,1,7,2,0,0,0,f45261b7b53043c316f45f613cb735907b93fb5a,237491053.0,https://www.semanticscholar.org/paper/f45261b7b53043c316f45f613cb735907b93fb5a,Conference on Empirical Methods in Natural Language Processing,2021.0,45.0,91.0,15.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1748844142', 'name': 'Runxin Xu'}, {'authorId': '2140495101', 'name': 'Fuli Luo'}, {'authorId': '50317060', 'name': 'Zhiyuan Zhang'}, {'authorId': '2111727840', 'name': 'Chuanqi Tan'}, {'authorId': '7267809', 'name': 'Baobao Chang'}, {'authorId': '2410938', 'name': 'Songfang Huang'}, {'authorId': '143857288', 'name': 'Fei Huang'}]","['Peking University', 'Alibaba']",['China'],2021-09
2109.06538,Yao Qiu,"Yao Qiu, Jinchao Zhang, Huiying Ren, Jie Zhou",Challenging Instances are Worth Learning: Generating Valuable Negative Samples for Response Selection Training,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Retrieval-based chatbot selects the appropriate response from candidates according to the context, which heavily depends on a response selection module. A response selection module is generally a scoring model to evaluate candidates and is usually trained on the annotated positive response and sampled negative responses. Sampling negative responses lead to two risks: a). The sampled negative instances, especially that from random sampling methods, are mostly irrelevant to the dialogue context and too easy to be fitted at the training stage while causing a weak model in the real scenario. b). The so-called negative instances may be positive, which is known as the fake negative problem. To address the above issue, we employ pre-trained language models, such as the DialoGPT to construct more challenging negative instances to enhance the model robustness. Specifically, we provide garbled context to the pre-trained model to generate responses and filter the fake negative ones. In this way, our negative instances are fluent, context-related, and more challenging for the model to learn, while can not be positive. Extensive experiments show that our method brings significant and stable improvements on the dialogue response selection capacity. ","[{'version': 'v1', 'created': 'Tue, 14 Sep 2021 09:16:24 GMT'}]",2021-09-15,"[['Qiu', 'Yao', ''], ['Zhang', 'Jinchao', ''], ['Ren', 'Huiying', ''], ['Zhou', 'Jie', '']]",0,1,2021-09-14,1,4,1,0,0,0,13c7580b0846416e9c024c027eaf33ed82a940bb,237503082.0,https://www.semanticscholar.org/paper/13c7580b0846416e9c024c027eaf33ed82a940bb,arXiv.org,2021.0,27.0,3.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2153278060', 'name': 'Yao Qiu'}, {'authorId': '2108970018', 'name': 'Jinchao Zhang'}, {'authorId': '2114293494', 'name': 'Huiying Ren'}, {'authorId': '2108485135', 'name': 'Jie Zhou'}]",['Tencent'],['China'],2021-09
2109.07377,Saneem Ahmed Chemmengath,"Saneem Ahmed Chemmengath, Vishwajeet Kumar, Samarth Bharadwaj, Jaydeep
  Sen, Mustafa Canim, Soumen Chakrabarti, Alfio Gliozzo, Karthik
  Sankaranarayanan",Topic Transferable Table Question Answering,To appear at EMNLP 2021,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Weakly-supervised table question-answering(TableQA) models have achieved state-of-art performance by using pre-trained BERT transformer to jointly encoding a question and a table to produce structured query for the question. However, in practical settings TableQA systems are deployed over table corpora having topic and word distributions quite distinct from BERT's pretraining corpus. In this work we simulate the practical topic shift scenario by designing novel challenge benchmarks WikiSQL-TS and WikiTQ-TS, consisting of train-dev-test splits in five distinct topic groups, based on the popular WikiSQL and WikiTableQuestions datasets. We empirically show that, despite pre-training on large open-domain text, performance of models degrades significantly when they are evaluated on unseen topics. In response, we propose T3QA (Topic Transferable Table Question Answering) a pragmatic adaptation framework for TableQA comprising of: (1) topic-specific vocabulary injection into BERT, (2) a novel text-to-text transformer generator (such as T5, GPT2) based natural language question generation pipeline focused on generating topic specific training data, and (3) a logical form reranker. We show that T3QA provides a reasonably good baseline for our topic shift benchmarks. We believe our topic split benchmarks will lead to robust TableQA solutions that are better suited for practical deployment. ","[{'version': 'v1', 'created': 'Wed, 15 Sep 2021 15:34:39 GMT'}]",2021-09-16,"[['Chemmengath', 'Saneem Ahmed', ''], ['Kumar', 'Vishwajeet', ''], ['Bharadwaj', 'Samarth', ''], ['Sen', 'Jaydeep', ''], ['Canim', 'Mustafa', ''], ['Chakrabarti', 'Soumen', ''], ['Gliozzo', 'Alfio', ''], ['Sankaranarayanan', 'Karthik', '']]",0,1,2021-09-15,1,8,2,2,2,0,55e226144b34f4efdf0e028e302e94658e0a1224,237513628.0,https://www.semanticscholar.org/paper/55e226144b34f4efdf0e028e302e94658e0a1224,Conference on Empirical Methods in Natural Language Processing,2021.0,29.0,7.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '27604943', 'name': 'Saneem A. Chemmengath'}, {'authorId': '2112128933', 'name': 'Vishwajeet Kumar'}, {'authorId': '34173298', 'name': 'Samarth Bharadwaj'}, {'authorId': '2450381', 'name': 'Jaydeep Sen'}, {'authorId': '1888104', 'name': 'Mustafa Canim'}, {'authorId': '2197703', 'name': 'Soumen Chakrabarti'}, {'authorId': '1711133', 'name': 'A. Gliozzo'}, {'authorId': '145590185', 'name': 'K. Sankaranarayanan'}]","['IBM Research - China', 'Indian Institute of Technology Bombay']","['China', 'India']",2021-09
2109.07684,Genta Indra Winata,"Genta Indra Winata, Andrea Madotto, Zhaojiang Lin, Rosanne Liu, Jason
  Yosinski, Pascale Fung",Language Models are Few-shot Multilingual Learners,14 pages,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  General-purpose language models have demonstrated impressive capabilities, performing on par with state-of-the-art approaches on a range of downstream natural language processing (NLP) tasks and benchmarks when inferring instructions from very few examples. Here, we evaluate the multilingual skills of the GPT and T5 models in conducting multi-class classification on non-English languages without any parameter updates. We show that, given a few English examples as context, pre-trained language models can predict not only English test samples but also non-English ones. Finally, we find the in-context few-shot cross-lingual prediction results of language models are significantly better than random prediction, and they are competitive compared to the existing state-of-the-art cross-lingual models. ","[{'version': 'v1', 'created': 'Thu, 16 Sep 2021 03:08:22 GMT'}]",2021-09-17,"[['Winata', 'Genta Indra', ''], ['Madotto', 'Andrea', ''], ['Lin', 'Zhaojiang', ''], ['Liu', 'Rosanne', ''], ['Yosinski', 'Jason', ''], ['Fung', 'Pascale', '']]",0,1,2021-09-16,1,6,2,1,1,0,42fc019b2668c9d9d984154d4c57f6c6d5a91619,237532173.0,https://www.semanticscholar.org/paper/42fc019b2668c9d9d984154d4c57f6c6d5a91619,MRL,2021.0,75.0,71.0,8.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '9162688', 'name': 'Genta Indra Winata'}, {'authorId': '2111680936', 'name': 'Andrea Madotto'}, {'authorId': '2146396528', 'name': 'Zhaojiang Lin'}, {'authorId': '48757909', 'name': 'Rosanne Liu'}, {'authorId': '2965424', 'name': 'J. Yosinski'}, {'authorId': '40539650', 'name': 'Pascale Fung'}]","['ML Collective', 'Google', 'Hong Kong University of Science and Technology']","['China', 'United States']",2021-09
2110.00987,Zaixi Zhang,"Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, Chee-Kong Lee",Motif-based Graph Self-Supervised Learning for Molecular Property Prediction,Accepted by NeurIPS'21,,,,q-bio.QM cs.AI cs.LG,http://creativecommons.org/licenses/by-sa/4.0/,"  Predicting molecular properties with data-driven methods has drawn much attention in recent years. Particularly, Graph Neural Networks (GNNs) have demonstrated remarkable success in various molecular generation and prediction tasks. In cases where labeled data is scarce, GNNs can be pre-trained on unlabeled molecular data to first learn the general semantic and structural information before being fine-tuned for specific tasks. However, most existing self-supervised pre-training frameworks for GNNs only focus on node-level or graph-level tasks. These approaches cannot capture the rich information in subgraphs or graph motifs. For example, functional groups (frequently-occurred subgraphs in molecular graphs) often carry indicative information about the molecular properties. To bridge this gap, we propose Motif-based Graph Self-supervised Learning (MGSSL) by introducing a novel self-supervised motif generation framework for GNNs. First, for motif extraction from molecular graphs, we design a molecule fragmentation method that leverages a retrosynthesis-based algorithm BRICS and additional rules for controlling the size of motif vocabulary. Second, we design a general motif-based generative pre-training framework in which GNNs are asked to make topological and label predictions. This generative framework can be implemented in two different ways, i.e., breadth-first or depth-first. Finally, to take the multi-scale information in molecular graphs into consideration, we introduce a multi-level self-supervised pre-training. Extensive experiments on various downstream benchmark tasks show that our methods outperform all state-of-the-art baselines. ","[{'version': 'v1', 'created': 'Sun, 3 Oct 2021 11:45:51 GMT'}, {'version': 'v2', 'created': 'Sat, 16 Oct 2021 00:48:00 GMT'}]",2021-10-19,"[['Zhang', 'Zaixi', ''], ['Liu', 'Qi', ''], ['Wang', 'Hao', ''], ['Lu', 'Chengqiang', ''], ['Lee', 'Chee-Kong', '']]",0,1,2021-10-03,2,5,3,0,0,0,2ced2ac19a88439b52e519d2e6ce44cccf08e191,238259534.0,https://www.semanticscholar.org/paper/2ced2ac19a88439b52e519d2e6ce44cccf08e191,Neural Information Processing Systems,2021.0,55.0,111.0,15.0,False,"['Computer Science', 'Biology']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Biology', 'source': 'external'}, {'category': 'Chemistry', 'source': 's2-fos-model'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2129455190', 'name': 'Zaixin Zhang'}, {'authorId': '2144831836', 'name': 'Qi Liu'}, {'authorId': '2144219662', 'name': 'Hao Wang'}, {'authorId': '46655401', 'name': 'Chengqiang Lu'}, {'authorId': '153897134', 'name': 'Chee-Kong Lee'}]","['Tencent', 'University of Science and Technology of China']","['China', 'United States']",2021-10
2110.03888,An Yang,"Junyang Lin, An Yang, Jinze Bai, Chang Zhou, Le Jiang, Xianyan Jia,
  Ang Wang, Jie Zhang, Yong Li, Wei Lin, Jingren Zhou, Hongxia Yang",M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining,"14 pages, 4 figures",,,,cs.LG cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  Recent expeditious developments in deep learning algorithms, distributed training, and even hardware design for large models have enabled training extreme-scale models, say GPT-3 and Switch Transformer possessing hundreds of billions or even trillions of parameters. However, under limited resources, extreme-scale model training that requires enormous amounts of computes and memory footprint suffers from frustratingly low efficiency in model convergence. In this paper, we propose a simple training strategy called ""Pseudo-to-Real"" for high-memory-footprint-required large models. Pseudo-to-Real is compatible with large models with architecture of sequential layers. We demonstrate a practice of pretraining unprecedented 10-trillion-parameter model, an order of magnitude larger than the state-of-the-art, on solely 512 GPUs within 10 days. Besides demonstrating the application of Pseudo-to-Real, we also provide a technique, Granular CPU offloading, to manage CPU memory for training large model and maintain high GPU utilities. Fast training of extreme-scale models on a decent amount of resources can bring much smaller carbon footprint and contribute to greener AI. ","[{'version': 'v1', 'created': 'Fri, 8 Oct 2021 04:24:51 GMT'}, {'version': 'v2', 'created': 'Wed, 13 Oct 2021 08:52:00 GMT'}, {'version': 'v3', 'created': 'Mon, 25 Oct 2021 06:24:41 GMT'}]",2021-10-26,"[['Lin', 'Junyang', ''], ['Yang', 'An', ''], ['Bai', 'Jinze', ''], ['Zhou', 'Chang', ''], ['Jiang', 'Le', ''], ['Jia', 'Xianyan', ''], ['Wang', 'Ang', ''], ['Zhang', 'Jie', ''], ['Li', 'Yong', ''], ['Lin', 'Wei', ''], ['Zhou', 'Jingren', ''], ['Yang', 'Hongxia', '']]",0,1,2021-10-08,3,12,2,1,0,1,24e775b20adf21e9b5b95c6a9b7a5c164d055849,238531482.0,https://www.semanticscholar.org/paper/24e775b20adf21e9b5b95c6a9b7a5c164d055849,arXiv.org,2021.0,52.0,27.0,1.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '35996608', 'name': 'Junyang Lin'}, {'authorId': '143936592', 'name': 'An Yang'}, {'authorId': '41211611', 'name': 'Jinze Bai'}, {'authorId': '144161025', 'name': 'Chang Zhou'}, {'authorId': '1485086888', 'name': 'Le Jiang'}, {'authorId': '3440541', 'name': 'Xianyan Jia'}, {'authorId': '153294981', 'name': 'Ang Wang'}, {'authorId': '40539618', 'name': 'J. Zhang'}, {'authorId': '1962987077', 'name': 'Yong Li'}, {'authorId': '71666838', 'name': 'Wei Lin'}, {'authorId': '1709595', 'name': 'Jingren Zhou'}, {'authorId': '38385080', 'name': 'Hongxia Yang'}]",['Alibaba'],['China'],2021-10
2110.07143,Cheng Chen,"Cheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, Fengyu
  Wang, Zhi Wang, Xiao Chen, Zhiyuan Liu, Qun Liu",bert2BERT: Towards Reusable Pretrained Language Models,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  In recent years, researchers tend to pre-train ever-larger language models to explore the upper limit of deep models. However, large language model pre-training costs intensive computational resources and most of the models are trained from scratch without reusing the existing pre-trained models, which is wasteful. In this paper, we propose bert2BERT, which can effectively transfer the knowledge of an existing smaller pre-trained model (e.g., BERT_BASE) to a large model (e.g., BERT_LARGE) through parameter initialization and significantly improve the pre-training efficiency of the large model. Specifically, we extend the previous function-preserving on Transformer-based language model, and further improve it by proposing advanced knowledge for large model's initialization. In addition, a two-stage pre-training method is proposed to further accelerate the training process. We did extensive experiments on representative PLMs (e.g., BERT and GPT) and demonstrate that (1) our method can save a significant amount of training cost compared with baselines including learning from scratch, StackBERT and MSLT; (2) our method is generic and applicable to different types of pre-trained models. In particular, bert2BERT saves about 45% and 47% computational cost of pre-training BERT_BASE and GPT_BASE by reusing the models of almost their half sizes. The source code will be publicly available upon publication. ","[{'version': 'v1', 'created': 'Thu, 14 Oct 2021 04:05:25 GMT'}]",2021-10-15,"[['Chen', 'Cheng', ''], ['Yin', 'Yichun', ''], ['Shang', 'Lifeng', ''], ['Jiang', 'Xin', ''], ['Qin', 'Yujia', ''], ['Wang', 'Fengyu', ''], ['Wang', 'Zhi', ''], ['Chen', 'Xiao', ''], ['Liu', 'Zhiyuan', ''], ['Liu', 'Qun', '']]",0,1,2021-10-14,1,10,1,0,0,0,7a49beff86a855f237f96ae3f0aefc9780cb31be,238856697.0,https://www.semanticscholar.org/paper/7a49beff86a855f237f96ae3f0aefc9780cb31be,Annual Meeting of the Association for Computational Linguistics,2021.0,46.0,25.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2145775089', 'name': 'Cheng Chen'}, {'authorId': '1384668226', 'name': 'Yichun Yin'}, {'authorId': '50812138', 'name': 'Lifeng Shang'}, {'authorId': '2110310493', 'name': 'Xin Jiang'}, {'authorId': '50625437', 'name': 'Yujia Qin'}, {'authorId': '2109126455', 'name': 'Fengyu Wang'}, {'authorId': '2135451922', 'name': 'Zhi Wang'}, {'authorId': '2117025507', 'name': 'Xiao Chen'}, {'authorId': '2146374807', 'name': 'Zhiyuan Liu'}, {'authorId': '30738758', 'name': 'Qun Liu'}]","['Tsinghua University', 'Huawei Technologies (China)']",['China'],2021-10
2110.07244,Quan Wang,"Quan Wang and Songtai Dai and Benfeng Xu and Yajuan Lyu and Yong Zhu
  and Hua Wu and Haifeng Wang",Building Chinese Biomedical Language Models via Multi-Level Text Discrimination,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-sa/4.0/,"  Pre-trained language models (PLMs), such as BERT and GPT, have revolutionized the field of NLP, not only in the general domain but also in the biomedical domain. Most prior efforts in building biomedical PLMs have resorted simply to domain adaptation and focused mainly on English. In this work we introduce eHealth, a Chinese biomedical PLM built from scratch with a new pre-training framework. This new framework pre-trains eHealth as a discriminator through both token- and sequence-level discrimination. The former is to detect input tokens corrupted by a generator and recover their original identities from plausible candidates, while the latter is to further distinguish corruptions of a same original sequence from those of others. As such, eHealth can learn language semantics at both token and sequence levels. Extensive experiments on 11 Chinese biomedical language understanding tasks of various forms verify the effectiveness and superiority of our approach. We release the pre-trained model at \url{https://github.com/PaddlePaddle/Research/tree/master/KG/eHealth} and will also release the code later. ","[{'version': 'v1', 'created': 'Thu, 14 Oct 2021 10:43:28 GMT'}, {'version': 'v2', 'created': 'Wed, 2 Mar 2022 10:04:24 GMT'}]",2022-03-03,"[['Wang', 'Quan', ''], ['Dai', 'Songtai', ''], ['Xu', 'Benfeng', ''], ['Lyu', 'Yajuan', ''], ['Zhu', 'Yong', ''], ['Wu', 'Hua', ''], ['Wang', 'Haifeng', '']]",0,1,2021-10-14,2,7,2,0,0,0,082ef2fa74671990f92f51f1590a3338493c4af5,238857039.0,https://www.semanticscholar.org/paper/082ef2fa74671990f92f51f1590a3338493c4af5,arXiv.org,2021.0,45.0,9.0,1.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '143906199', 'name': 'Quan Wang'}, {'authorId': '2052804457', 'name': 'Songtai Dai'}, {'authorId': '1754285124', 'name': 'Benfeng Xu'}, {'authorId': '8020700', 'name': 'Yajuan Lyu'}, {'authorId': '2116512598', 'name': 'Yong Zhu'}, {'authorId': '40354707', 'name': 'Hua Wu'}, {'authorId': '144270731', 'name': 'Haifeng Wang'}]","['University of Science and Technology of China', 'Baidu']",['China'],2021-10
2110.08118,Andrea Madotto Dr,"Andrea Madotto, Zhaojiang Lin, Genta Indra Winata, Pascale Fung",Few-Shot Bot: Prompt-Based Learning for Dialogue Systems,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Learning to converse using only a few examples is a great challenge in conversational AI. The current best conversational models, which are either good chit-chatters (e.g., BlenderBot) or goal-oriented systems (e.g., MinTL), are language models (LMs) fine-tuned on large conversational datasets. Training these models is expensive, both in terms of computational resources and time, and it is hard to keep them up to date with new conversational skills. A simple yet unexplored solution is prompt-based few-shot learning (Brown et al. 2020) which does not require gradient-based fine-tuning but instead uses a few examples in the LM context as the only source of learning. In this paper, we explore prompt-based few-shot learning in dialogue tasks. We benchmark LMs of different sizes in nine response generation tasks, which include four knowledge-grounded tasks, a task-oriented generations task, three open-chat tasks, and controlled stylistic generation, and five conversational parsing tasks, which include dialogue state tracking, graph path generation, persona information extraction, document retrieval, and internet query generation. The current largest released LM (GPT-J-6B) using prompt-based few-shot learning, and thus requiring no training, achieves competitive performance to fully trained state-of-the-art models. Moreover, we propose a novel prompt-based few-shot classifier, that also does not require any fine-tuning, to select the most appropriate prompt given a dialogue history. Finally, by combining the power of prompt-based few-shot learning and a Skill Selector, we create an end-to-end chatbot named the Few-Shot Bot (FSB), which automatically selects the most appropriate conversational skill, queries different knowledge bases or the internet, and uses the retrieved knowledge to generate a human-like response, all using only few dialogue examples per skill. ","[{'version': 'v1', 'created': 'Fri, 15 Oct 2021 14:36:45 GMT'}]",2021-10-18,"[['Madotto', 'Andrea', ''], ['Lin', 'Zhaojiang', ''], ['Winata', 'Genta Indra', ''], ['Fung', 'Pascale', '']]",0,1,2021-10-15,1,4,2,0,0,0,30873c32db5a219a58be928d5692cce48be1d3a0,239009514.0,https://www.semanticscholar.org/paper/30873c32db5a219a58be928d5692cce48be1d3a0,arXiv.org,2021.0,116.0,49.0,6.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2111680936', 'name': 'Andrea Madotto'}, {'authorId': '2146396528', 'name': 'Zhaojiang Lin'}, {'authorId': '9162688', 'name': 'Genta Indra Winata'}, {'authorId': '40539650', 'name': 'Pascale Fung'}]",['Hong Kong University of Science and Technology'],['China'],2021-10
2110.14468,David Mguni,"David Mguni, Usman Islam, Yaqi Sun, Xiuling Zhang, Joel Jennings,
  Aivar Sootla, Changmin Yu, Ziyan Wang, Jun Wang, Yaodong Yang",DESTA: A Framework for Safe Reinforcement Learning with Markov Games of Intervention,arXiv admin note: text overlap with arXiv:2103.09159,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Reinforcement learning (RL) involves performing exploratory actions in an unknown system. This can place a learning agent in dangerous and potentially catastrophic system states. Current approaches for tackling safe learning in RL simultaneously trade-off safe exploration and task fulfillment. In this paper, we introduce a new generation of RL solvers that learn to minimise safety violations while maximising the task reward to the extent that can be tolerated by the safe policy. Our approach introduces a novel two-player framework for safe RL called Distributive Exploration Safety Training Algorithm (DESTA). The core of DESTA is a game between two adaptive agents: Safety Agent that is delegated the task of minimising safety violations and Task Agent whose goal is to maximise the environment reward. Specifically, Safety Agent can selectively take control of the system at any given point to prevent safety violations while Task Agent is free to execute its policy at any other states. This framework enables Safety Agent to learn to take actions at certain states that minimise future safety violations, both during training and testing time, while Task Agent performs actions that maximise the task performance everywhere else. Theoretically, we prove that DESTA converges to stable points enabling safety violations of pretrained policies to be minimised. Empirically, we show DESTA's ability to augment the safety of existing policies and secondly, construct safe RL policies when the Task Agent and Safety Agent are trained concurrently. We demonstrate DESTA's superior performance against leading RL methods in Lunar Lander and Frozen Lake from OpenAI gym. ","[{'version': 'v1', 'created': 'Wed, 27 Oct 2021 14:35:00 GMT'}, {'version': 'v2', 'created': 'Thu, 30 Jun 2022 03:02:15 GMT'}, {'version': 'v3', 'created': 'Wed, 1 Mar 2023 12:25:12 GMT'}]",2023-03-02,"[['Mguni', 'David', ''], ['Islam', 'Usman', ''], ['Sun', 'Yaqi', ''], ['Zhang', 'Xiuling', ''], ['Jennings', 'Joel', ''], ['Sootla', 'Aivar', ''], ['Yu', 'Changmin', ''], ['Wang', 'Ziyan', ''], ['Wang', 'Jun', ''], ['Yang', 'Yaodong', '']]",0,0,2021-10-27,3,10,1,0,0,0,676b8462708f3851effee13c1b029ed0b79e82e9,239998202.0,https://www.semanticscholar.org/paper/676b8462708f3851effee13c1b029ed0b79e82e9,arXiv.org,2021.0,52.0,6.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '41127915', 'name': 'D. Mguni'}, {'authorId': '12417004', 'name': 'Joel Jennings'}, {'authorId': '1419479157', 'name': 'Taher Jafferjee'}, {'authorId': '144424020', 'name': 'Aivar Sootla'}, {'authorId': '47796324', 'name': 'Yaodong Yang'}, {'authorId': '2110962578', 'name': 'Changmin Yu'}, {'authorId': '2135162945', 'name': 'Usman Islam'}, {'authorId': '2142663126', 'name': 'Ziyan Wang'}, {'authorId': '48094081', 'name': 'Jun Wang'}]","['Peking University', 'University College London', 'Tsinghua University', 'Zhejiang Normal University']","['China', 'United Kingdom']",2021-10
2111.04613,Jiayu Chen,"Jiayu Chen, Yuanxin Zhang, Yuanfan Xu, Huimin Ma, Huazhong Yang,
  Jiaming Song, Yu Wang, Yi Wu",Variational Automatic Curriculum Learning for Sparse-Reward Cooperative Multi-Agent Problems,In NeurIPS 2021,,,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  We introduce a curriculum learning algorithm, Variational Automatic Curriculum Learning (VACL), for solving challenging goal-conditioned cooperative multi-agent reinforcement learning problems. We motivate our paradigm through a variational perspective, where the learning objective can be decomposed into two terms: task learning on the current task distribution, and curriculum update to a new task distribution. Local optimization over the second term suggests that the curriculum should gradually expand the training tasks from easy to hard. Our VACL algorithm implements this variational paradigm with two practical components, task expansion and entity progression, which produces training curricula over both the task configurations as well as the number of entities in the task. Experiment results show that VACL solves a collection of sparse-reward problems with a large number of agents. Particularly, using a single desktop machine, VACL achieves 98% coverage rate with 100 agents in the simple-spread benchmark and reproduces the ramp-use behavior originally shown in OpenAI's hide-and-seek project. Our project website is at https://sites.google.com/view/vacl-neurips-2021. ","[{'version': 'v1', 'created': 'Mon, 8 Nov 2021 16:35:08 GMT'}, {'version': 'v2', 'created': 'Wed, 22 Dec 2021 08:11:07 GMT'}]",2021-12-23,"[['Chen', 'Jiayu', ''], ['Zhang', 'Yuanxin', ''], ['Xu', 'Yuanfan', ''], ['Ma', 'Huimin', ''], ['Yang', 'Huazhong', ''], ['Song', 'Jiaming', ''], ['Wang', 'Yu', ''], ['Wu', 'Yi', '']]",0,0,2021-11-08,2,8,1,0,0,0,ef5c29f6d78f81c78b2d7a625d4d4053f51fcd6e,243848161.0,https://www.semanticscholar.org/paper/ef5c29f6d78f81c78b2d7a625d4d4053f51fcd6e,Neural Information Processing Systems,2021.0,36.0,20.0,4.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108327767', 'name': 'Jiayu Chen'}, {'authorId': '2145784019', 'name': 'Yuanxin Zhang'}, {'authorId': '2110355949', 'name': 'Yuanfan Xu'}, {'authorId': '2008031104', 'name': 'Huimin Ma'}, {'authorId': '39150998', 'name': 'Huazhong Yang'}, {'authorId': '2112626470', 'name': 'Jiaming Song'}, {'authorId': '2153607473', 'name': 'Yu Wang'}, {'authorId': '2108052525', 'name': 'Yi Wu'}]","['Stanford University', 'Tsinghua University', 'University of Science and Technology Beijing']","['China', 'United States']",2021-11
2111.07631,Qiyue Yin,"Qiyue Yin, Jun Yang, Kaiqi Huang, Meijing Zhao, Wancheng Ni, Bin
  Liang, Yan Huang, Shu Wu, Liang Wang","AI in Human-computer Gaming: Techniques, Challenges and Opportunities",,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With breakthrough of the AlphaGo, human-computer gaming AI has ushered in a big explosion, attracting more and more researchers all around the world. As a recognized standard for testing artificial intelligence, various human-computer gaming AI systems (AIs) have been developed such as the Libratus, OpenAI Five and AlphaStar, beating professional human players. The rapid development of human-computer gaming AIs indicate a big step of decision making intelligence, and it seems that current techniques can handle very complex human-computer games. So, one natural question raises: what are the possible challenges of current techniques in human-computer gaming, and what are the future trends? To answer the above question, in this paper, we survey recent successful game AIs, covering board game AIs, card game AIs, first-person shooting game AIs and real time strategy game AIs. Through this survey, we 1) compare the main difficulties among different kinds of games and the corresponding techniques utilized for achieving professional human level AIs; 2) summarize the mainstream frameworks and techniques that can be properly relied on for developing AIs for complex human-computer gaming; 3) raise the challenges or drawbacks of current techniques in the successful AIs; and 4) try to point out future trends in human-computer gaming AIs. Finally, we hope this brief review can provide an introduction for beginners, and inspire insights for researchers in the field of AI in human-computer gaming. ","[{'version': 'v1', 'created': 'Mon, 15 Nov 2021 09:35:53 GMT'}, {'version': 'v2', 'created': 'Thu, 18 Aug 2022 01:56:59 GMT'}]",2022-08-19,"[['Yin', 'Qiyue', ''], ['Yang', 'Jun', ''], ['Huang', 'Kaiqi', ''], ['Zhao', 'Meijing', ''], ['Ni', 'Wancheng', ''], ['Liang', 'Bin', ''], ['Huang', 'Yan', ''], ['Wu', 'Shu', ''], ['Wang', 'Liang', '']]",0,0,2021-11-15,2,9,1,0,0,0,4294e8a4a1ea06a3cfc55bbacc03299a72260473,251643883.0,https://www.semanticscholar.org/paper/4294e8a4a1ea06a3cfc55bbacc03299a72260473,Machine Intelligence Research,2021.0,73.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2397961', 'name': 'Qiyue Yin'}, {'authorId': '2146155252', 'name': 'Jun Yang'}, {'authorId': '2149775969', 'name': 'Kaiqi Huang'}, {'authorId': '7202473', 'name': 'Meijing Zhao'}, {'authorId': '2056887474', 'name': 'Wancheng Ni'}, {'authorId': '144691690', 'name': 'Bin Liang'}, {'authorId': '144368930', 'name': 'Yan Huang'}, {'authorId': '50425438', 'name': 'Shu Wu'}, {'authorId': '2144695405', 'name': 'Liangsheng Wang'}]","['Tsinghua University', 'Chinese Academy of Sciences']",['China'],2021-11
2111.07658,Sha Yuan,"Hanyu Zhao, Sha Yuan, Jiahong Leng, Xiang Pan, Guoqiang Wang, Ledell
  Wu and Jie Tang",Calculating Question Similarity is Enough: A New Method for KBQA Tasks,"We want to withdraw this submission, and add some experiments to make
  it more valuable",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge Base Question Answering (KBQA) aims to answer natural language questions with the help of an external knowledge base. The core idea is to find the link between the internal knowledge behind questions and known triples of the knowledge base. Traditional KBQA task pipelines contain several steps, including entity recognition, entity linking, answering selection, etc. In this kind of pipeline methods, errors in any procedure will inevitably propagate to the final prediction. To address this challenge, this paper proposes a Corpus Generation - Retrieve Method (CGRM) with Pre-training Language Model (PLM) for the KBQA task. The major novelty lies in the design of the new method, wherein our approach, the knowledge enhanced T5 (kT5) model aims to generate natural language QA pairs based on Knowledge Graph triples and directly solve the QA by retrieving the synthetic dataset. The new method can extract more information about the entities from PLM to improve accuracy and simplify the processes. We test our method on NLPCC-ICCPOL 2016 KBQA dataset, and the results show that our method improves the performance of KBQA and the out straight-forward method is competitive with the state-of-the-art. ","[{'version': 'v1', 'created': 'Mon, 15 Nov 2021 10:31:46 GMT'}, {'version': 'v2', 'created': 'Sat, 11 Dec 2021 05:44:46 GMT'}, {'version': 'v3', 'created': 'Wed, 9 Feb 2022 07:55:59 GMT'}, {'version': 'v4', 'created': 'Sun, 1 May 2022 02:49:10 GMT'}]",2022-05-03,"[['Zhao', 'Hanyu', ''], ['Yuan', 'Sha', ''], ['Leng', 'Jiahong', ''], ['Pan', 'Xiang', ''], ['Wang', 'Guoqiang', ''], ['Wu', 'Ledell', ''], ['Tang', 'Jie', '']]",0,0,2021-11-15,4,7,2,1,1,0,920598a31d3e6284129ad24ffcc8d957a88068d0,244117403.0,https://www.semanticscholar.org/paper/920598a31d3e6284129ad24ffcc8d957a88068d0,arXiv.org,2021.0,47.0,0.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1907469334', 'name': 'Hanyu Zhao'}, {'authorId': '3411347', 'name': 'Shaoqing Yuan'}, {'authorId': '2124048409', 'name': 'Jiahong Leng'}, {'authorId': '2119600282', 'name': 'X. Pan'}, {'authorId': '2142449761', 'name': 'Guoqiang Wang'}]","['New York University', 'Tsinghua University', 'Beijing Academy of Artificial Intelligence']","['China', 'United States']",2021-11
2111.10103,Hongyao Tang,"Tong Sang, Hongyao Tang, Jianye Hao, Yan Zheng, Zhaopeng Meng",Uncertainty-aware Low-Rank Q-Matrix Estimation for Deep Reinforcement Learning,"This paper is accepted by The 3rd International Conference on
  Distributed Artificial Intelligence (DAI 2021, Shanghai, China)",,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Value estimation is one key problem in Reinforcement Learning. Albeit many successes have been achieved by Deep Reinforcement Learning (DRL) in different fields, the underlying structure and learning dynamics of value function, especially with complex function approximation, are not fully understood. In this paper, we report that decreasing rank of $Q$-matrix widely exists during learning process across a series of continuous control tasks for different popular algorithms. We hypothesize that the low-rank phenomenon indicates the common learning dynamics of $Q$-matrix from stochastic high dimensional space to smooth low dimensional space. Moreover, we reveal a positive correlation between value matrix rank and value estimation uncertainty. Inspired by above evidence, we propose a novel Uncertainty-Aware Low-rank Q-matrix Estimation (UA-LQE) algorithm as a general framework to facilitate the learning of value function. Through quantifying the uncertainty of state-action value estimation, we selectively erase the entries of highly uncertain values in state-action value matrix and conduct low-rank matrix reconstruction for them to recover their values. Such a reconstruction exploits the underlying structure of value matrix to improve the value approximation, thus leading to a more efficient learning process of value function. In the experiments, we evaluate the efficacy of UA-LQE in several representative OpenAI MuJoCo continuous control tasks. ","[{'version': 'v1', 'created': 'Fri, 19 Nov 2021 09:00:38 GMT'}]",2021-11-22,"[['Sang', 'Tong', ''], ['Tang', 'Hongyao', ''], ['Hao', 'Jianye', ''], ['Zheng', 'Yan', ''], ['Meng', 'Zhaopeng', '']]",0,0,2021-11-19,1,5,3,0,0,0,07ae9fde025f71dc40f8b31c25b33469c9a5a5e8,244463040.0,https://www.semanticscholar.org/paper/07ae9fde025f71dc40f8b31c25b33469c9a5a5e8,International Conference on Distributed Artificial Intelligence,2021.0,28.0,1.0,0.0,True,"['Computer Science', 'Mathematics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Mathematics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2141512692', 'name': 'Tong Sang'}, {'authorId': '31190626', 'name': 'Hongyao Tang'}, {'authorId': '2069718816', 'name': 'Jianye Hao'}, {'authorId': '1752775197', 'name': 'Yan Zheng'}, {'authorId': '1889014', 'name': 'Zhaopeng Meng'}]","['Shanghai Construction Group (China)', 'Tianjin University']",['China'],2021-11
2111.14592,Yinpei Dai,"Wanwei He, Yinpei Dai, Yinhe Zheng, Yuchuan Wu, Zheng Cao, Dermot Liu,
  Peng Jiang, Min Yang, Fei Huang, Luo Si, Jian Sun, Yongbin Li",GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-Supervised Learning and Explicit Policy Injection,"7 pages, 5 figures. Accepted by AAAI 2022",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Pre-trained models have proved to be powerful in enhancing task-oriented dialog systems. However, current pre-training methods mainly focus on enhancing dialog understanding and generation tasks while neglecting the exploitation of dialog policy. In this paper, we propose GALAXY, a novel pre-trained dialog model that explicitly learns dialog policy from limited labeled dialogs and large-scale unlabeled dialog corpora via semi-supervised learning. Specifically, we introduce a dialog act prediction task for policy optimization during pre-training and employ a consistency regularization term to refine the learned representation with the help of unlabeled dialogs. We also implement a gating mechanism to weigh suitable unlabeled dialog samples. Empirical results show that GALAXY substantially improves the performance of task-oriented dialog systems, and achieves new state-of-the-art results on benchmark datasets: In-Car, MultiWOZ2.0 and MultiWOZ2.1, improving their end-to-end combined scores by 2.5, 5.3 and 5.5 points, respectively. We also show that GALAXY has a stronger few-shot ability than existing models under various low-resource settings. ","[{'version': 'v1', 'created': 'Mon, 29 Nov 2021 15:24:36 GMT'}, {'version': 'v2', 'created': 'Wed, 1 Dec 2021 02:15:16 GMT'}, {'version': 'v3', 'created': 'Tue, 7 Dec 2021 02:39:31 GMT'}, {'version': 'v4', 'created': 'Thu, 9 Dec 2021 13:43:16 GMT'}, {'version': 'v5', 'created': 'Mon, 13 Dec 2021 05:47:06 GMT'}, {'version': 'v6', 'created': 'Mon, 27 Dec 2021 09:35:34 GMT'}, {'version': 'v7', 'created': 'Thu, 6 Jan 2022 09:44:18 GMT'}, {'version': 'v8', 'created': 'Tue, 29 Mar 2022 09:37:19 GMT'}]",2022-03-30,"[['He', 'Wanwei', ''], ['Dai', 'Yinpei', ''], ['Zheng', 'Yinhe', ''], ['Wu', 'Yuchuan', ''], ['Cao', 'Zheng', ''], ['Liu', 'Dermot', ''], ['Jiang', 'Peng', ''], ['Yang', 'Min', ''], ['Huang', 'Fei', ''], ['Si', 'Luo', ''], ['Sun', 'Jian', ''], ['Li', 'Yongbin', '']]",0,1,2021-11-29,8,12,1,0,0,0,127ffc8697630a76b1b4149c24d1350f69205f41,244714676.0,https://www.semanticscholar.org/paper/127ffc8697630a76b1b4149c24d1350f69205f41,AAAI Conference on Artificial Intelligence,2021.0,80.0,102.0,12.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '66583249', 'name': 'Wanwei He'}, {'authorId': '30087809', 'name': 'Yinpei Dai'}, {'authorId': '51456789', 'name': 'Yinhe Zheng'}, {'authorId': '2142637404', 'name': 'Yuchuan Wu'}, {'authorId': '2114000051', 'name': 'Zhen Cao'}, {'authorId': '2217177435', 'name': 'Dermot Liu'}, {'authorId': '2061283210', 'name': 'Peng Jiang'}, {'authorId': '2144399900', 'name': 'Min Yang'}, {'authorId': '92451990', 'name': 'Feiling Huang'}, {'authorId': '2059080424', 'name': 'Luo Si'}, {'authorId': '2152147863', 'name': 'Jian Sun'}, {'authorId': '1527090216', 'name': 'Yongbin Li'}]","['Alibaba', 'University of Chinese Academy of Sciences', 'Shenzhen Institutes of Advanced Technology']",['China'],2021-11
2112.00029,Tri Dao,"Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri
  Rudra, Christopher R\'e",Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models,"International Conference on Learning Representations (ICLR) 2022
  spotlight",,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Overparameterized neural networks generalize well but are expensive to train. Ideally, one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty in sparsifying all model components. The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is difficult and expensive. To address this, our main insight is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. As butterfly matrices are not hardware efficient, we propose simple variants of butterfly (block and flat) to take advantage of modern hardware. Our method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP). We empirically validate that Pixelated Butterfly is 3x faster than butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs. On the ImageNet classification and WikiText-103 language modeling tasks, our sparse models train up to 2.5x faster than the dense MLP-Mixer, Vision Transformer, and GPT-2 medium with no drop in accuracy. ","[{'version': 'v1', 'created': 'Tue, 30 Nov 2021 19:00:03 GMT'}, {'version': 'v2', 'created': 'Wed, 11 May 2022 03:59:56 GMT'}]",2022-05-12,"[['Dao', 'Tri', ''], ['Chen', 'Beidi', ''], ['Liang', 'Kaizhao', ''], ['Yang', 'Jiaming', ''], ['Song', 'Zhao', ''], ['Rudra', 'Atri', ''], ['R', 'Christopher', '']]",0,1,2021-11-30,2,7,1,1,1,0,90b21dbad8969b74d704eed15a3d98722a88e464,244773609.0,https://www.semanticscholar.org/paper/90b21dbad8969b74d704eed15a3d98722a88e464,International Conference on Learning Representations,2021.0,108.0,45.0,7.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '4319427', 'name': 'Beidi Chen'}, {'authorId': '24593911', 'name': 'Tri Dao'}, {'authorId': '102461072', 'name': 'Kaizhao Liang'}, {'authorId': '2142772202', 'name': 'Jiaming Yang'}, {'authorId': '2119235975', 'name': 'Zhao Song'}, {'authorId': '1755572', 'name': 'A. Rudra'}, {'authorId': '2114485554', 'name': 'C. R'}]","['Adobe Research', 'University at Buffalo, State University of New York', 'Peking University']","['China', 'United States']",2021-11
2112.06222,Diandian Gu,"Xuanzhe Liu, Diandian Gu, Zhenpeng Chen, Jinfeng Wen, Zili Zhang, Yun
  Ma, Haoyu Wang, Xin Jin",Rise of Distributed Deep Learning Training in the Big Model Era: From a Software Engineering Perspective,"Accepted by ACM Transactions on Software Engineering and Methodology
  (TOSEM 2023). Please include TOSEM in any citations",,,,cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep learning (DL) has become a key component of modern software. In the ""big model"" era, the rich features of DL-based software substantially rely on powerful DL models, e.g., BERT, GPT-3, and the recently emerging GPT-4, which are trained on the powerful cloud with large datasets. Hence, training effective DL models has become a vital stage in the whole software lifecycle. When training deep learning models, especially those big models, developers need to parallelize and distribute the computation and memory resources amongst multiple devices in the training process, which is known as distributed deep learning training, or distributed training for short. However, the unique challenges that developers encounter in distributed training process have not been studied in the software engineering community. Given the increasingly heavy dependence of current DL-based software on distributed training, this paper aims to fill in the knowledge gap and presents the first comprehensive study on developers' issues in distributed training. To this end, we analyze 1,131 real-world developers' issues about using these frameworks reported on Stack Overflow and GitHub. We construct a fine-grained taxonomy consisting of 30 categories regarding the fault symptoms and summarize common fix patterns for different symptoms. Based on the results, we suggest actionable implications on research avenues that can potentially facilitate the distributed training to develop DL-based software, such as focusing on the frequent and common fix patterns when designing testing or debugging tools, developing efficient testing and debugging techniques for communication configuration along with the synthesis of network configuration analysis, designing new multi-device checkpoint-and-replay techniques to help reproduction, and designing serverless APIs for cloud platforms. ","[{'version': 'v1', 'created': 'Sun, 12 Dec 2021 12:58:48 GMT'}, {'version': 'v2', 'created': 'Fri, 28 Apr 2023 05:37:23 GMT'}]",2023-05-01,"[['Liu', 'Xuanzhe', ''], ['Gu', 'Diandian', ''], ['Chen', 'Zhenpeng', ''], ['Wen', 'Jinfeng', ''], ['Zhang', 'Zili', ''], ['Ma', 'Yun', ''], ['Wang', 'Haoyu', ''], ['Jin', 'Xin', '']]",0,1,2021-12-12,2,8,1,2,0,2,4870b00b7c0ac09d4eab15366d62fd8624ea076a,258418357.0,https://www.semanticscholar.org/paper/4870b00b7c0ac09d4eab15366d62fd8624ea076a,ACM Transactions on Software Engineering and Methodology,2021.0,92.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2110773587', 'name': 'Xuanzhe Liu'}, {'authorId': '1393328013', 'name': 'Diandian Gu'}, {'authorId': '115023982', 'name': 'Zhenpeng Chen'}, {'authorId': '145672103', 'name': 'Jinfeng Wen'}, {'authorId': '2144370396', 'name': 'Zili Zhang'}, {'authorId': '48521633', 'name': 'Yun Ma'}, {'authorId': '51225422', 'name': 'Haoyu Wang'}, {'authorId': '2149170624', 'name': 'Xin Jin'}]","['Peking University', 'University College London', 'Huazhong University of Science and Technology']","['China', 'United Kingdom']",2021-12
2112.12731,Shuohuan Wang,"Shuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong,
  Shikun Feng, Junyuan Shang, Yanbin Zhao, Chao Pang, Jiaxiang Liu, Xuyi Chen,
  Yuxiang Lu, Weixin Liu, Xi Wang, Yangfan Bai, Qiuliang Chen, Li Zhao, Shiyong
  Li, Peng Sun, Dianhai Yu, Yanjun Ma, Hao Tian, Hua Wu, Tian Wu, Wei Zeng, Ge
  Li, Wen Gao, Haifeng Wang",ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,arXiv admin note: text overlap with arXiv:2107.02137,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained language models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. GPT-3 has shown that scaling up pre-trained language models can further exploit their enormous potential. A unified framework named ERNIE 3.0 was recently proposed for pre-training large-scale knowledge enhanced models and trained a model with 10 billion parameters. ERNIE 3.0 outperformed the state-of-the-art models on various NLP tasks. In order to explore the performance of scaling up ERNIE 3.0, we train a hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters on the PaddlePaddle platform. Furthermore, we design a self-supervised adversarial loss and a controllable language modeling loss to make ERNIE 3.0 Titan generate credible and controllable texts. To reduce the computation overhead and carbon emission, we propose an online distillation framework for ERNIE 3.0 Titan, where the teacher model will teach students and train itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense pre-trained model so far. Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets. ","[{'version': 'v1', 'created': 'Thu, 23 Dec 2021 17:35:48 GMT'}]",2021-12-24,"[['Wang', 'Shuohuan', ''], ['Sun', 'Yu', ''], ['Xiang', 'Yang', ''], ['Wu', 'Zhihua', ''], ['Ding', 'Siyu', ''], ['Gong', 'Weibao', ''], ['Feng', 'Shikun', ''], ['Shang', 'Junyuan', ''], ['Zhao', 'Yanbin', ''], ['Pang', 'Chao', ''], ['Liu', 'Jiaxiang', ''], ['Chen', 'Xuyi', ''], ['Lu', 'Yuxiang', ''], ['Liu', 'Weixin', ''], ['Wang', 'Xi', ''], ['Bai', 'Yangfan', ''], ['Chen', 'Qiuliang', ''], ['Zhao', 'Li', ''], ['Li', 'Shiyong', ''], ['Sun', 'Peng', ''], ['Yu', 'Dianhai', ''], ['Ma', 'Yanjun', ''], ['Tian', 'Hao', ''], ['Wu', 'Hua', ''], ['Wu', 'Tian', ''], ['Zeng', 'Wei', ''], ['Li', 'Ge', ''], ['Gao', 'Wen', ''], ['Wang', 'Haifeng', '']]",0,1,2021-12-23,1,29,1,3,0,3,a3184d40d390793232c99c89b57b8f65c16320b2,245425057.0,https://www.semanticscholar.org/paper/a3184d40d390793232c99c89b57b8f65c16320b2,arXiv.org,2021.0,104.0,37.0,11.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '104463827', 'name': 'Shuohuan Wang'}, {'authorId': '2117103617', 'name': 'Yu Sun'}, {'authorId': '2068340960', 'name': 'Yang Xiang'}, {'authorId': '47039787', 'name': 'Zhihua Wu'}, {'authorId': '2152123817', 'name': 'Siyu Ding'}, {'authorId': '2117587198', 'name': 'Weibao Gong'}, {'authorId': '144588144', 'name': 'Shi Feng'}, {'authorId': '40861754', 'name': 'Junyuan Shang'}, {'authorId': '2117889541', 'name': 'Yanbin Zhao'}, {'authorId': '2054086618', 'name': 'Chao Pang'}, {'authorId': '2144130913', 'name': 'Jiaxiang Liu'}, {'authorId': '2109214103', 'name': 'Xuyi Chen'}, {'authorId': '2140025135', 'name': 'Yuxiang Lu'}, {'authorId': '2109563578', 'name': 'Weixin Liu'}, {'authorId': '2108249583', 'name': 'Xi Wang'}, {'authorId': '2153241479', 'name': 'Yangfan Bai'}, {'authorId': '2157776938', 'name': 'Qiuliang Chen'}, {'authorId': '2116548646', 'name': 'Li Zhao'}, {'authorId': '2155911527', 'name': 'Shiyong Li'}, {'authorId': '2075416480', 'name': 'Peng Sun'}, {'authorId': '3046102', 'name': 'Dianhai Yu'}, {'authorId': '2148985098', 'name': 'Yanjun Ma'}, {'authorId': '50007795', 'name': 'Hao Tian'}, {'authorId': '40354707', 'name': 'Hua Wu'}, {'authorId': '2112663486', 'name': 'Tian Wu'}, {'authorId': '144424265', 'name': 'Wei Zeng'}, {'authorId': '2154591487', 'name': 'Ge Li'}, {'authorId': '2153578299', 'name': 'Wen Gao'}, {'authorId': '144270731', 'name': 'Haifeng Wang'}]","['Peng Cheng Laboratory', 'Baidu']",['China'],2021-12
2112.14397,Xiaonan Nie,"Xiaonan Nie, Xupeng Miao, Shijie Cao, Lingxiao Ma, Qibin Liu, Jilong
  Xue, Youshan Miao, Yi Liu, Zhi Yang, Bin Cui",EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate,,,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Mixture-of-experts (MoE) is becoming popular due to its success in improving the model quality, especially in Transformers. By routing tokens with a sparse gate to a few experts (i.e., a small pieces of the full model), MoE can easily increase the model parameters to a very large scale while keeping the computation cost in a constant level. Most existing works just initialize some random experts, set a fixed gating strategy (e.g., Top-k), and train the model from scratch in an ad-hoc way. We identify that these MoE models are suffering from the immature experts and unstable sparse gate, which are harmful to the convergence performance. In this paper, we propose an efficient end-to-end MoE training framework called EvoMoE. EvoMoE starts from training one single expert and gradually evolves into a large and sparse MoE structure. EvoMoE mainly contains two phases: the expert-diversify phase to train the base expert for a while and spawn multiple diverse experts from it, and the gate-sparsify phase to learn an adaptive sparse gate and activate a dynamic number of experts. EvoMoE naturally decouples the joint learning of both the experts and the sparse gate and focuses on learning the basic knowledge with a single expert at the early training stage. Then it diversifies the experts and continues to train the MoE with a novel Dense-to-Sparse gate (DTS-Gate). Specifically, instead of using a permanent sparse gate, DTS-Gate begins as a dense gate that routes tokens to all experts, then gradually and adaptively becomes sparser while routes to fewer experts. Evaluations are conducted on three popular models and tasks, including RoBERTa for masked language modeling task, GPT for language modeling task and Transformer for machine translation task. The results show that EvoMoE outperforms existing baselines, including Switch, BASE Layer, Hash Layer and StableMoE. ","[{'version': 'v1', 'created': 'Wed, 29 Dec 2021 04:52:14 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Oct 2022 12:32:34 GMT'}]",2022-10-11,"[['Nie', 'Xiaonan', ''], ['Miao', 'Xupeng', ''], ['Cao', 'Shijie', ''], ['Ma', 'Lingxiao', ''], ['Liu', 'Qibin', ''], ['Xue', 'Jilong', ''], ['Miao', 'Youshan', ''], ['Liu', 'Yi', ''], ['Yang', 'Zhi', ''], ['Cui', 'Bin', '']]",0,1,2021-12-29,2,10,2,0,0,0,cf4f4b69b76dc58dc8c0d443ab88ceb461eec719,252780015.0,https://www.semanticscholar.org/paper/cf4f4b69b76dc58dc8c0d443ab88ceb461eec719,,2021.0,40.0,8.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2113588952', 'name': 'Xiaonan Nie'}, {'authorId': '1720763480', 'name': 'Xupeng Miao'}, {'authorId': '2072595192', 'name': 'Shijie Cao'}, {'authorId': '2492241', 'name': 'Lingxiao Ma'}, {'authorId': '2187417728', 'name': 'Qibin Liu'}, {'authorId': '2870618', 'name': 'Jilong Xue'}, {'authorId': '11009920', 'name': 'Youshan Miao'}, {'authorId': '2153630000', 'name': 'Yi Liu'}, {'authorId': '2109540175', 'name': 'Zhi Yang'}, {'authorId': '2068228300', 'name': 'Bin Cui'}]",['Peking University'],['China'],2021-12
2112.15283,Weichong Yin,"Han Zhang, Weichong Yin, Yewei Fang, Lanxin Li, Boqiang Duan, Zhihua
  Wu, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation,"15 pages, 7 figures",,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Conventional methods for the image-text generation tasks mainly tackle the naturally bidirectional generation tasks separately, focusing on designing task-specific frameworks to improve the quality and fidelity of the generated samples. Recently, Vision-Language Pre-training models have greatly improved the performance of the image-to-text generation tasks, but large-scale pre-training models for text-to-image synthesis task are still under-developed. In this paper, we propose ERNIE-ViLG, a unified generative pre-training framework for bidirectional image-text generation with transformer model. Based on the image quantization models, we formulate both image generation and text generation as autoregressive generative tasks conditioned on the text/image input. The bidirectional image-text generative modeling eases the semantic alignments across vision and language. For the text-to-image generation process, we further propose an end-to-end training method to jointly learn the visual sequence generator and the image reconstructor. To explore the landscape of large-scale pre-training for bidirectional text-image generation, we train a 10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million (Chinese) image-text pairs which achieves state-of-the-art performance for both text-to-image and image-to-text tasks, obtaining an FID of 7.9 on MS-COCO for text-to-image synthesis and best results on COCO-CN and AIC-ICC for image captioning. ","[{'version': 'v1', 'created': 'Fri, 31 Dec 2021 03:53:33 GMT'}]",2022-01-03,"[['Zhang', 'Han', ''], ['Yin', 'Weichong', ''], ['Fang', 'Yewei', ''], ['Li', 'Lanxin', ''], ['Duan', 'Boqiang', ''], ['Wu', 'Zhihua', ''], ['Sun', 'Yu', ''], ['Tian', 'Hao', ''], ['Wu', 'Hua', ''], ['Wang', 'Haifeng', '']]",0,1,2021-12-31,1,10,2,0,0,0,a02a3e4e3f8c1f185954af9b401f7100a45075a2,245634812.0,https://www.semanticscholar.org/paper/a02a3e4e3f8c1f185954af9b401f7100a45075a2,arXiv.org,2021.0,54.0,38.0,5.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '48213346', 'name': 'Han Zhang'}, {'authorId': '2318321', 'name': 'Weichong Yin'}, {'authorId': '37578937', 'name': 'Yewei Fang'}, {'authorId': '2145482889', 'name': 'Lanxin Li'}, {'authorId': '2148228671', 'name': 'Boqiang Duan'}, {'authorId': '47039787', 'name': 'Zhihua Wu'}, {'authorId': '2117103617', 'name': 'Yu Sun'}, {'authorId': '50007795', 'name': 'Hao Tian'}, {'authorId': '40354707', 'name': 'Hua Wu'}, {'authorId': '144270731', 'name': 'Haifeng Wang'}]",['Baidu'],['China'],2021-12
2201.05966,Chen Henry Wu,"Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak,
  Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang,
  Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao,
  Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke
  Zettlemoyer, Tao Yu",UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models,EMNLP 2022,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests, such as semantic parsing over databases and question answering over knowledge bases. Since the inputs and outputs of SKG tasks are heterogeneous, they have been studied separately by different communities, which limits systematic and compatible research on SKG. In this paper, we overcome this limitation by proposing the UnifiedSKG framework, which unifies 21 SKG tasks into a text-to-text format, aiming to promote systematic SKG research, instead of being exclusive to a single task, domain, or dataset. We use UnifiedSKG to benchmark T5 with different sizes and show that T5, with simple modifications when necessary, achieves state-of-the-art performance on almost all of the 21 tasks. We further demonstrate that multi-task prefix-tuning improves the performance on most tasks, largely improving the overall performance. UnifiedSKG also facilitates the investigation of zero-shot and few-shot learning, and we show that T0, GPT-3, and Codex struggle in zero-shot and few-shot learning for SKG. We also use UnifiedSKG to conduct a series of controlled experiments on structured knowledge encoding variants across SKG tasks. UnifiedSKG is easily extensible to more tasks, and it is open-sourced at https://github.com/hkunlp/unifiedskg. ","[{'version': 'v1', 'created': 'Sun, 16 Jan 2022 04:36:18 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Jan 2022 03:20:45 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Oct 2022 15:56:01 GMT'}]",2022-10-19,"[['Xie', 'Tianbao', ''], ['Wu', 'Chen Henry', ''], ['Shi', 'Peng', ''], ['Zhong', 'Ruiqi', ''], ['Scholak', 'Torsten', ''], ['Yasunaga', 'Michihiro', ''], ['Wu', 'Chien-Sheng', ''], ['Zhong', 'Ming', ''], ['Yin', 'Pengcheng', ''], ['Wang', 'Sida I.', ''], ['Zhong', 'Victor', ''], ['Wang', 'Bailin', ''], ['Li', 'Chengzu', ''], ['Boyle', 'Connor', ''], ['Ni', 'Ansong', ''], ['Yao', 'Ziyu', ''], ['Radev', 'Dragomir', ''], ['Xiong', 'Caiming', ''], ['Kong', 'Lingpeng', ''], ['Zhang', 'Rui', ''], ['Smith', 'Noah A.', ''], ['Zettlemoyer', 'Luke', ''], ['Yu', 'Tao', '']]",0,1,2022-01-16,3,23,1,4,2,2,53c0abe83fe9b4fdaf2208295d8504fcf5241694,246016124.0,https://www.semanticscholar.org/paper/53c0abe83fe9b4fdaf2208295d8504fcf5241694,Conference on Empirical Methods in Natural Language Processing,2022.0,121.0,189.0,12.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2057038673', 'name': 'Tianbao Xie'}, {'authorId': '114621402', 'name': 'Chen Henry Wu'}, {'authorId': '2055357805', 'name': 'Peng Shi'}, {'authorId': '51011000', 'name': 'Ruiqi Zhong'}, {'authorId': '11869783', 'name': 'Torsten Scholak'}, {'authorId': '19168196', 'name': 'Michihiro Yasunaga'}, {'authorId': '30340989', 'name': 'Chien-Sheng Wu'}, {'authorId': '1606040932', 'name': 'Ming Zhong'}, {'authorId': '38253388', 'name': 'Pengcheng Yin'}, {'authorId': '8729431', 'name': 'Sida I. Wang'}, {'authorId': '3428769', 'name': 'Victor Zhong'}, {'authorId': '2118640406', 'name': 'Bailin Wang'}, {'authorId': '2155795167', 'name': 'Chengzu Li'}, {'authorId': '2143195008', 'name': 'Connor Boyle'}, {'authorId': '33981736', 'name': 'Ansong Ni'}, {'authorId': '3366595', 'name': 'Ziyu Yao'}, {'authorId': '9215251', 'name': 'Dragomir R. Radev'}, {'authorId': '2054594326', 'name': 'Caiming Xiong'}, {'authorId': '47648549', 'name': 'Lingpeng Kong'}, {'authorId': '15176410', 'name': 'Rui Zhang'}, {'authorId': '144365875', 'name': 'Noah A. Smith'}, {'authorId': '1982950', 'name': 'Luke Zettlemoyer'}, {'authorId': '48881008', 'name': 'Tao Yu'}]","['University of Waterloo', 'Shanghai Artificial Intelligence Laboratory', 'Carnegie Mellon University', 'Allen Institute for Artificial Intelligence', 'University of Washington', 'University of Edinburgh', 'University of Hong Kong', 'Meta']","['Canada', 'United States', 'United Kingdom', 'China', 'Hong Kong']",2022-01
2201.08531,Shizhe Diao,"Shizhe Diao, Zhichao Huang, Ruijia Xu, Xuechun Li, Yong Lin, Xiao
  Zhou, Tong Zhang",Black-box Prompt Learning for Pre-trained Language Models,To appear in the Transactions on Machine Learning Research (TMLR),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The increasing scale of general-purpose Pre-trained Language Models (PLMs) necessitates the study of more efficient adaptation across different downstream tasks. In this paper, we establish a Black-box Discrete Prompt Learning (BDPL) to resonate with pragmatic interactions between the cloud infrastructure and edge devices. Particularly, instead of fine-tuning the model in the cloud, we adapt PLMs by prompt learning, which efficiently optimizes only a few parameters of the discrete prompts. Moreover, we consider the scenario that we do not have access to the parameters and gradients of the pre-trained models, except for its outputs given inputs. This black-box setting secures the cloud infrastructure from potential attack and misuse to cause a single-point failure, which is preferable to the white-box counterpart by current infrastructures. Under this black-box constraint, we apply a variance-reduced policy gradient algorithm to estimate the gradients of parameters in the categorical distribution of each discrete prompt. In light of our method, the user devices can efficiently tune their tasks by querying the PLMs bounded by a range of API calls. Our experiments on RoBERTa and GPT-3 demonstrate that the proposed algorithm achieves significant improvement on eight benchmarks in a cloud-device collaboration manner. Finally, we conduct in-depth case studies to comprehensively analyze our method in terms of various data sizes, prompt lengths, training budgets, optimization objectives, prompt transferability, and explanations of the learned prompts. Our code will be available at https://github.com/shizhediao/Black-Box-Prompt-Learning. ","[{'version': 'v1', 'created': 'Fri, 21 Jan 2022 03:53:19 GMT'}, {'version': 'v2', 'created': 'Mon, 14 Nov 2022 17:56:31 GMT'}, {'version': 'v3', 'created': 'Thu, 23 Feb 2023 18:42:07 GMT'}]",2023-02-24,"[['Diao', 'Shizhe', ''], ['Huang', 'Zhichao', ''], ['Xu', 'Ruijia', ''], ['Li', 'Xuechun', ''], ['Lin', 'Yong', ''], ['Zhou', 'Xiao', ''], ['Zhang', 'Tong', '']]",0,1,2022-01-21,3,7,1,1,0,1,5faa744dcc28cbbdd9bd67eb703320c6e2d85e52,246210164.0,https://www.semanticscholar.org/paper/5faa744dcc28cbbdd9bd67eb703320c6e2d85e52,Trans. Mach. Learn. Res.,2022.0,85.0,34.0,1.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '50826757', 'name': 'Shizhe Diao'}, {'authorId': '2181353249', 'name': 'Xuechun Li'}, {'authorId': '2154487280', 'name': 'Yong Lin'}, {'authorId': '50294051', 'name': 'Zhichao Huang'}, {'authorId': '2109127048', 'name': 'Xiao Zhou'}, {'authorId': '2117882335', 'name': 'Tong Zhang'}]","['Hong Kong University of Science and Technology', 'University of California, San Diego']","['China', 'United States']",2022-01
2201.08687,Vinsen Marselino Andreas,"Vinsen Marselino Andreas, Genta Indra Winata, Ayu Purwarianti",A Comparative Study on Language Models for Task-Oriented Dialogue Systems,"5 pages, 1 figure","2021 8th International Conference on Advanced Informatics:
  Concepts, Theory and Applications (ICAICTA) (pp. 1-5). IEEE",,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The recent development of language models has shown promising results by achieving state-of-the-art performance on various natural language tasks by fine-tuning pretrained models. In task-oriented dialogue (ToD) systems, language models can be used for end-to-end training without relying on dialogue state tracking to track the dialogue history but allowing the language models to generate responses according to the context given as input. This paper conducts a comparative study to show the effectiveness and strength of using recent pretrained models for fine-tuning, such as BART and T5, on endto-end ToD systems. The experimental results show substantial performance improvements after language model fine-tuning. The models produce more fluent responses after adding knowledge to the context that guides the model to avoid hallucination and generate accurate entities in the generated responses. Furthermore, we found that BART and T5 outperform GPT-based models in BLEU and F1 scores and achieve state-of-the-art performance in a ToD system. ","[{'version': 'v1', 'created': 'Fri, 21 Jan 2022 13:24:25 GMT'}]",2022-01-24,"[['Andreas', 'Vinsen Marselino', ''], ['Winata', 'Genta Indra', ''], ['Purwarianti', 'Ayu', '']]",0,1,2022-01-21,1,3,1,1,1,0,342a8bd5d9ca7f30ffa976476ac02926f0984b4c,245264067.0,https://www.semanticscholar.org/paper/342a8bd5d9ca7f30ffa976476ac02926f0984b4c,"2021 8th International Conference on Advanced Informatics: Concepts, Theory and Applications (ICAICTA)",2021.0,25.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2145961790', 'name': 'Vinsen Marselino Andreas'}, {'authorId': '9162688', 'name': 'Genta Indra Winata'}, {'authorId': '1962263', 'name': 'A. Purwarianti'}]","['Bandung Institute of Technology', 'Hong Kong University of Science and Technology']","['Indonesia', 'China']",2022-01
2201.09518,Varvara Guljajeva,Varvara Guljajeva,Synthetic Books,"7 pages, 5 figures",ARTECH 2021,10.1145/3483529.3483663,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  The article explores new ways of written language aided by AI technologies, like GPT-2 and GPT-3. The question that is stated in the paper is not about whether these novel technologies will eventually replace authored books, but how to relate to and contextualize such publications and what kind of new tools, processes, and ideas are behind them. For that purpose, a new concept of synthetic books is introduced in the article. It stands for the publications created by deploying AI technology, more precisely autoregressive language models that are able to generate human-like text. Supported by the case studies, the value and reasoning of the synthetic books are discussed. The paper emphasizes that artistic quality is an issue when it comes to AI-generated content. The article introduces projects that demonstrate an interactive input by an artist and/or audience combined with the deep-learning-based language models. In the end, the paper focuses on understanding the neural aesthetics of written language in the art context. ","[{'version': 'v1', 'created': 'Mon, 24 Jan 2022 08:26:28 GMT'}]",2022-01-25,"[['Guljajeva', 'Varvara', '']]",0,1,2022-01-24,1,1,2,2,1,1,f25e8603e2211fa03f7685d814e665cc0256a4b9,246240231.0,https://www.semanticscholar.org/paper/f25e8603e2211fa03f7685d814e665cc0256a4b9,ARTECH,2021.0,31.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Art', 'source': 's2-fos-model'}]","[{'authorId': '2733600', 'name': 'Varvara Guljajeva'}]",['Hong Kong University of Science and Technology'],['China'],2022-01
2202.05262,David Bau,"Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov",Locating and Editing Factual Associations in GPT,"NeurIPS 2022. 35 pages, 30 figures. Code and data at
  https://rome.baulab.info/",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/ ","[{'version': 'v1', 'created': 'Thu, 10 Feb 2022 18:59:54 GMT'}, {'version': 'v2', 'created': 'Mon, 21 Mar 2022 15:13:09 GMT'}, {'version': 'v3', 'created': 'Wed, 1 Jun 2022 18:56:44 GMT'}, {'version': 'v4', 'created': 'Sun, 23 Oct 2022 18:07:20 GMT'}, {'version': 'v5', 'created': 'Fri, 13 Jan 2023 15:16:16 GMT'}]",2023-01-16,"[['Meng', 'Kevin', ''], ['Bau', 'David', ''], ['Andonian', 'Alex', ''], ['Belinkov', 'Yonatan', '']]",0,1,2022-02-10,5,4,2,0,0,0,996445d847f06e99b0bd259345408a0cf1bce87e,255825985.0,https://www.semanticscholar.org/paper/996445d847f06e99b0bd259345408a0cf1bce87e,Neural Information Processing Systems,2022.0,56.0,235.0,44.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '153615419', 'name': 'Kevin Meng'}, {'authorId': '144159726', 'name': 'David Bau'}, {'authorId': '50112310', 'name': 'A. Andonian'}, {'authorId': '2083259', 'name': 'Yonatan Belinkov'}]","['Massachusetts Institute of Technology', 'Technion  Israel Institute of Technology', 'Northeastern University']","['China', 'United States', 'Israel']",2022-02
2202.06574,Ziyang Luo,"Ziyang Luo, Zhipeng Hu, Yadong Xi, Rongsheng Zhang, Jing Ma",I-Tuning: Tuning Frozen Language Models with Image for Lightweight Image Captioning,ICASSP 2023,,,,cs.CL cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Image Captioning is a traditional vision-and-language task that aims to generate the language description of an image. Recent studies focus on scaling up the model size and the number of training data, which significantly increase the cost of model training. Different to these heavy-cost models, we introduce a lightweight image captioning framework (I-Tuning), which contains a small number of trainable parameters. We design a novel I-Tuning cross-attention module to connect the non-trainable pre-trained language decoder GPT2 and vision encoder CLIP-ViT. Since most parameters are not required to be updated during training, our framework is lightweight and fast. Experimental results conducted on three image captioning benchmarks reveal that our framework achieves comparable or better performance than the large-scale baseline systems. But our models contain up to 10 times fewer trainable parameters and require much fewer data for training compared with state-of-the-art baselines. ","[{'version': 'v1', 'created': 'Mon, 14 Feb 2022 09:36:50 GMT'}, {'version': 'v2', 'created': 'Sat, 15 Oct 2022 01:05:02 GMT'}, {'version': 'v3', 'created': 'Mon, 13 Mar 2023 05:51:27 GMT'}]",2023-03-14,"[['Luo', 'Ziyang', ''], ['Hu', 'Zhipeng', ''], ['Xi', 'Yadong', ''], ['Zhang', 'Rongsheng', ''], ['Ma', 'Jing', '']]",0,1,2022-02-14,3,5,2,1,1,0,b19683b83c63cc245f78bd5bf095cbd41149be7e,252917565.0,https://www.semanticscholar.org/paper/b19683b83c63cc245f78bd5bf095cbd41149be7e,"IEEE International Conference on Acoustics, Speech, and Signal Processing",2022.0,25.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '23523733', 'name': 'Ziyang Luo'}, {'authorId': '2111296838', 'name': 'Zhipeng Hu'}, {'authorId': '2056826321', 'name': 'Yadong Xi'}, {'authorId': '48263731', 'name': 'Rongsheng Zhang'}, {'authorId': '2157405974', 'name': 'Jing Ma'}]","['NetEase', 'Hong Kong Baptist University', 'Zhejiang University']",['China'],2022-02
2202.07612,Hongwei Li,"Maosheng Zhong, Gen Liu, Hongwei Li, Jiangling Kuang, Jinshan Zeng,
  Mingwen Wang",CodeGen-Test: An Automatic Code Generation Model Integrating Program Test Information,"10 paper pages, 7 figures; 2 appendix pages, 5 appendix figures",,,,cs.SE,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Automatic code generation is to generate the program code according to the given natural language description. The current mainstream approach uses neural networks to encode natural language descriptions, and output abstract syntax trees (AST) at the decoder, then convert the AST into program code. While the generated code largely conforms to specific syntax rules, two problems are still ignored. One is missing program testing, an essential step in the process of complete code implementation; the other is only focusing on the syntax compliance of the generated code, while ignoring the more important program functional requirements. The paper proposes a CodeGen-Test model, which adds program testing steps and incorporates program testing information to iteratively generate code that meets the functional requirements of the program, thereby improving the quality of code generation. At the same time, the paper proposes a new evaluation metric, test accuracy (Test-Acc), which represents the proportion of passing program test in generated code. Different from the previous evaluation metric, which only evaluates the quality of code generation from the perspective of character similarity, the Test-Acc can evaluate the quality of code generation from the Program functions. Moreover, the paper evaluates the CodeGen-test model on a python data set ""hearthstone legend"". The experimental results show the proposed method can effectively improve the quality of generated code. Compared with the existing optimal model, CodeGen-Test model improves the Bleu value by 0.2%, Rouge-L value by 0.3% and Test-Acc by 6%. ","[{'version': 'v1', 'created': 'Mon, 14 Feb 2022 10:04:22 GMT'}]",2022-02-16,"[['Zhong', 'Maosheng', ''], ['Liu', 'Gen', ''], ['Li', 'Hongwei', ''], ['Kuang', 'Jiangling', ''], ['Zeng', 'Jinshan', ''], ['Wang', 'Mingwen', '']]",0,0,2022-02-14,1,6,1,1,1,0,24ae98c096dc358fe702fbffac8942421d3f997b,246863931.0,https://www.semanticscholar.org/paper/24ae98c096dc358fe702fbffac8942421d3f997b,arXiv.org,2022.0,39.0,6.0,1.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '40344592', 'name': 'M. Zhong'}, {'authorId': '2218198189', 'name': 'Gen Liu'}, {'authorId': '2115213856', 'name': 'HongWei Li'}, {'authorId': '2154575367', 'name': 'Jiangling Kuang'}, {'authorId': '7216245', 'name': 'Jinshan Zeng'}, {'authorId': '2140153996', 'name': 'Mingwen Wang'}]","[' ZHONG Maosheng, LIU Gen, LI Hongwei, KUANG Jiangling, ZENG Jinshan, and WANG MingWen are with the School of Computer and Information Engineering, Jiangxi Normal University, Nanchang,China, 330022.']",['China'],2022-02
2202.07922,Jiacheng Ye,"Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong
  Wu, Tao Yu, Lingpeng Kong",ZeroGen: Efficient Zero-shot Learning via Dataset Generation,Accepted by EMNLP 2022 (Main Conference),,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There is a growing interest in dataset generation recently due to the superior generative capacity of large pre-trained language models (PLMs). In this paper, we study a flexible and efficient zero-short learning method, \textsc{ZeroGen}. Given a zero-shot task, we first generate a dataset from scratch using PLMs in an unsupervised manner. Then, we train a tiny task model (e.g., LSTM) under the supervision of the synthesized dataset. This approach allows highly efficient inference as the final task model only has orders of magnitude fewer parameters comparing to PLMs (e.g., GPT2-XL). Apart from being annotation-free and efficient, we argue that \textsc{ZeroGen} can also provide useful insights from the perspective of data-free model-agnostic knowledge distillation, and unreferenced text generation evaluation. Experiments and analysis on different NLP tasks, namely, text classification, question answering, and natural language inference, show the effectiveness of \textsc{ZeroGen}. ","[{'version': 'v1', 'created': 'Wed, 16 Feb 2022 08:18:02 GMT'}, {'version': 'v2', 'created': 'Sat, 22 Oct 2022 01:32:03 GMT'}]",2022-10-25,"[['Ye', 'Jiacheng', ''], ['Gao', 'Jiahui', ''], ['Li', 'Qintong', ''], ['Xu', 'Hang', ''], ['Feng', 'Jiangtao', ''], ['Wu', 'Zhiyong', ''], ['Yu', 'Tao', ''], ['Kong', 'Lingpeng', '']]",0,1,2022-02-16,2,8,2,1,1,0,2145fcceeb69385e108bf1796d52f974854d4c0b,246867045.0,https://www.semanticscholar.org/paper/2145fcceeb69385e108bf1796d52f974854d4c0b,Conference on Empirical Methods in Natural Language Processing,2022.0,95.0,65.0,7.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '65846898', 'name': 'Jiacheng Ye'}, {'authorId': '144407296', 'name': 'Jiahui Gao'}, {'authorId': '47422209', 'name': 'Qintong Li'}, {'authorId': '47995165', 'name': 'Hang Xu'}, {'authorId': '2093485', 'name': 'Jiangtao Feng'}, {'authorId': '150358371', 'name': 'Zhiyong Wu'}, {'authorId': '2117900202', 'name': 'Tao Yu'}, {'authorId': '47648549', 'name': 'Lingpeng Kong'}]","['Shanghai Artificial Intelligence Laboratory', 'University of Washington', 'University of Hong Kong', 'Huawei Technologies (China)']","['China', 'United States', 'Hong Kong']",2022-02
2202.08904,Niklas Muennighoff,Niklas Muennighoff,SGPT: GPT Sentence Embeddings for Semantic Search,"19 pages, 3 figures, 12 tables. v2 corrects a misreported nDCG@10
  number for the SGPT-BE-5.8B model. v3 updates SGPT-BE-5.8B scores based on
  retrained models with larger batch sizes v4 removes a superfluous table. v5
  adds OpenAI scores on USEB and makes the paper easier to read",,,,cs.CL cs.AI cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Decoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt. ","[{'version': 'v1', 'created': 'Thu, 17 Feb 2022 21:35:56 GMT'}, {'version': 'v2', 'created': 'Mon, 21 Feb 2022 17:42:49 GMT'}, {'version': 'v3', 'created': 'Wed, 16 Mar 2022 11:33:40 GMT'}, {'version': 'v4', 'created': 'Wed, 23 Mar 2022 19:16:55 GMT'}, {'version': 'v5', 'created': 'Fri, 5 Aug 2022 09:33:10 GMT'}]",2022-08-08,"[['Muennighoff', 'Niklas', '']]",0,1,2022-02-17,5,1,3,0,0,0,cf8235e0b592f52848c3dc4a9b76222c25d172cb,246996947.0,https://www.semanticscholar.org/paper/cf8235e0b592f52848c3dc4a9b76222c25d172cb,arXiv.org,2022.0,54.0,60.0,6.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2037383772', 'name': 'Niklas Muennighoff'}]",['Peking University'],['China'],2022-02
2202.10587,Yin Fang,"Yin Fang, Zhuo Chen, Xiaohui Fan and Ningyu Zhang",Knowledge-informed Molecular Learning: A Survey on Paradigm Transfer,"8 pages, 3 figures",,,,cs.LG cs.AI physics.chem-ph q-bio.QM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine learning, notably deep learning, has significantly propelled molecular investigations within the biochemical sphere. Traditionally, modeling for such research has centered around a handful of paradigms. For instance, the prediction paradigm is frequently deployed for tasks such as molecular property prediction. To enhance the generation and decipherability of purely data-driven models, scholars have integrated biochemical domain knowledge into these molecular study models. This integration has sparked a surge in paradigm transfer, which is solving one molecular learning task by reformulating it as another one. With the emergence of Large Language Models, these paradigms have demonstrated an escalating trend towards harmonized unification. In this work, we delineate a literature survey focused on knowledge-informed molecular learning from the perspective of paradigm transfer. We classify the paradigms, scrutinize their methodologies, and dissect the contribution of domain knowledge. Moreover, we encapsulate prevailing trends and identify intriguing avenues for future exploration in molecular learning. ","[{'version': 'v1', 'created': 'Thu, 17 Feb 2022 06:18:02 GMT'}, {'version': 'v2', 'created': 'Tue, 5 Sep 2023 10:46:44 GMT'}]",2023-09-06,"[['Fang', 'Yin', ''], ['Chen', 'Zhuo', ''], ['Fan', 'Xiaohui', ''], ['Zhang', 'Ningyu', '']]",0,0,2022-02-17,2,4,4,0,0,0,32eba408f74b4e07d283cbe63f4648f3cc98e2ea,247025592.0,https://www.semanticscholar.org/paper/32eba408f74b4e07d283cbe63f4648f3cc98e2ea,arXiv.org,2022.0,68.0,1.0,0.0,False,"['Computer Science', 'Physics', 'Biology']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Physics', 'source': 'external'}, {'category': 'Biology', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2112787103', 'name': 'Yin Fang'}, {'authorId': '2145895211', 'name': 'Qiang Zhang'}, {'authorId': '2111498496', 'name': 'Zhuo Chen'}, {'authorId': '2152774386', 'name': 'Xiaohui Fan'}, {'authorId': '1729778', 'name': 'Huajun Chen'}]",['Zhejiang University'],['China'],2022-02
2202.13047,Chujie Zheng,"Chujie Zheng, Sahand Sabour, Jiaxin Wen, Zheng Zhang, Minlie Huang",AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation,Findings of ACL 2023,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Crowdsourced dialogue corpora are usually limited in scale and topic coverage due to the expensive cost of data curation. This would hinder the generalization of downstream dialogue models to open-domain topics. In this work, we leverage large language models for dialogue augmentation in the task of emotional support conversation (ESC). By treating dialogue augmentation as a dialogue completion task, we prompt a fine-tuned language model to complete full dialogues from available dialogue posts of various topics, which are then postprocessed based on heuristics. Applying this approach, we construct AugESC, an augmented dataset for the ESC task, which largely extends the scale and topic coverage of the crowdsourced ESConv corpus. Through comprehensive human evaluation, we demonstrate that our approach is superior to strong baselines of dialogue augmentation and that AugESC has comparable dialogue quality to the crowdsourced corpus. We also conduct human interactive evaluation and prove that post-training on AugESC improves downstream dialogue models' generalization ability to open-domain topics. These results suggest the utility of AugESC and highlight the potential of large language models in improving data-scarce dialogue generation tasks. ","[{'version': 'v1', 'created': 'Sat, 26 Feb 2022 03:17:08 GMT'}, {'version': 'v2', 'created': 'Wed, 10 May 2023 08:34:56 GMT'}, {'version': 'v3', 'created': 'Thu, 18 May 2023 05:25:49 GMT'}]",2023-05-19,"[['Zheng', 'Chujie', ''], ['Sabour', 'Sahand', ''], ['Wen', 'Jiaxin', ''], ['Zhang', 'Zheng', ''], ['Huang', 'Minlie', '']]",0,0,2022-02-26,3,5,1,0,0,0,de4635e95259118a545fdc0682407f416c16086a,258588110.0,https://www.semanticscholar.org/paper/de4635e95259118a545fdc0682407f416c16086a,Annual Meeting of the Association for Computational Linguistics,2022.0,56.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '146452866', 'name': 'Chujie Zheng'}, {'authorId': '2106627931', 'name': 'Sahand Sabour'}, {'authorId': '2104586007', 'name': 'Jiaxin Wen'}, {'authorId': '47294008', 'name': 'Zheng Zhang'}, {'authorId': '2196817617', 'name': 'Minlie Huang'}]","['State Key Lab of Intelligent Technology and Systems,', 'Tsinghua University', 'Artificial Intelligence Research Institute']","['China', 'Spain']",2022-02
2203.00281,Xiang Hu,"Xiang Hu, Haitao Mi, Liang Li, Gerard de Melo",Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for Grammar Induction and Text Representation,EMNLP 2022,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently CKY-based models show great potential in unsupervised grammar induction thanks to their human-like encoding paradigm, which runs recursively and hierarchically, but requires $O(n^3)$ time-complexity. Recursive Transformer based on Differentiable Trees (R2D2) makes it possible to scale to large language model pre-training even with complex tree encoder by introducing a heuristic pruning method. However, the rule-based pruning approach suffers from local optimum and slow inference issues. In this paper, we fix those issues in a unified method. We propose to use a top-down parser as a model-based pruning method, which also enables parallel encoding during inference. Typically, our parser casts parsing as a split point scoring task, which first scores all split points for a given sentence, and then recursively splits a span into two by picking a split point with the highest score in the current span. The reverse order of the splits is considered as the order of pruning in R2D2 encoder. Beside the bi-directional language model loss, we also optimize the parser by minimizing the KL distance between tree probabilities from parser and R2D2. Our experiments show that our Fast-R2D2 improves performance significantly in grammar induction and achieves competitive results in downstream classification tasks. ","[{'version': 'v1', 'created': 'Tue, 1 Mar 2022 07:54:44 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Oct 2022 16:17:19 GMT'}, {'version': 'v3', 'created': 'Wed, 2 Nov 2022 08:01:07 GMT'}]",2022-11-03,"[['Hu', 'Xiang', ''], ['Mi', 'Haitao', ''], ['Li', 'Liang', ''], ['de Melo', 'Gerard', '']]",0,0,2022-03-01,3,4,1,0,0,0,dc85180153687539724d20a5927b2fbdf5f8e2a4,247187477.0,https://www.semanticscholar.org/paper/dc85180153687539724d20a5927b2fbdf5f8e2a4,Conference on Empirical Methods in Natural Language Processing,2022.0,47.0,8.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '144219881', 'name': 'Xiang Hu'}, {'authorId': '2013337', 'name': 'Haitao Mi'}, {'authorId': '2154884927', 'name': 'Liang Li'}, {'authorId': '144608002', 'name': 'Gerard de Melo'}]","['Hasso Plattner Institute', 'Shandong University', 'Quancheng Laboratory, China ']","['Germany', 'China']",2022-03
2203.01104,Ze-Feng Gao,"Ze-Feng Gao, Peiyu Liu, Wayne Xin Zhao, Zhong-Yi Lu, Ji-Rong Wen",Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models,"11 pages, 2 figures, 2 tables",COLING2022 Oral Presentation,,,cs.CL cs.AI cs.LG quant-ph,http://creativecommons.org/publicdomain/zero/1.0/,"  Recently, Mixture-of-Experts (short as MoE) architecture has achieved remarkable success in increasing the model capacity of large-scale language models. However, MoE requires incorporating significantly more parameters than the base model being extended. In this paper, we propose building a parameter-efficient MoE architecture by sharing information among experts. We adopt the matrix product operator (MPO, a tensor decomposition from quantum many-body physics) to reconstruct the parameter matrix in the expert layer and increase model capacity for pre-trained language models by sharing parameters of the central tensor (containing the core information) among different experts while enabling the specificity through the auxiliary tensors (complementing the central tensor) of different experts. To address the unbalanced optimization issue, we further design the gradient mask strategy for the MPO-based MoE architecture. Extensive experiments based on T5 and GPT-2 show improved performance and efficiency of the pre-trained language model (27.2x reduction in total parameters for the superior model performance, compared with the Switch Transformers). Our code is publicly available at https://github.com/RUCAIBox/MPOE. ","[{'version': 'v1', 'created': 'Wed, 2 Mar 2022 13:44:49 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Sep 2022 14:31:24 GMT'}, {'version': 'v3', 'created': 'Thu, 15 Sep 2022 06:59:19 GMT'}, {'version': 'v4', 'created': 'Mon, 10 Oct 2022 06:41:52 GMT'}]",2022-10-14,"[['Gao', 'Ze-Feng', ''], ['Liu', 'Peiyu', ''], ['Zhao', 'Wayne Xin', ''], ['Lu', 'Zhong-Yi', ''], ['Wen', 'Ji-Rong', '']]",0,1,2022-03-02,4,5,4,2,2,0,f677ef460670e63a0e9a0bd048cd881b4b55d92f,247218249.0,https://www.semanticscholar.org/paper/f677ef460670e63a0e9a0bd048cd881b4b55d92f,International Conference on Computational Linguistics,2022.0,46.0,12.0,1.0,True,"['Computer Science', 'Physics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Physics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '9136116', 'name': 'Ze-Feng Gao'}, {'authorId': '2108129670', 'name': 'Peiyu Liu'}, {'authorId': '2542603', 'name': 'Wayne Xin Zhao'}, {'authorId': '48462104', 'name': 'Zhong-Yi Lu'}, {'authorId': '153693432', 'name': 'Ji-rong Wen'}]","['Beijing Key Laboratory of Big Data Management and Analysis Methods', 'Beijing Academy of Artificial Intelligence', 'Renmin University of China']",['China'],2022-03
2203.05132,Xin Wang,"Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li, Pingyi Zhou, Jin
  Liu, Hao Wu, Xin Jiang, Qun Liu",Compilable Neural Code Generation with Compiler Feedback,Accepted by ACL 2022,,,,cs.CL cs.AI cs.PL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automatically generating compilable programs with (or without) natural language descriptions has always been a touchstone problem for computational linguistics and automated software engineering. Existing deep-learning approaches model code generation as text generation, either constrained by grammar structures in decoder, or driven by pre-trained language models on large-scale code corpus (e.g., CodeGPT, PLBART, and CodeT5). However, few of them account for compilability of the generated programs. To improve compilability of the generated programs, this paper proposes COMPCODER, a three-stage pipeline utilizing compiler feedback for compilable code generation, including language model fine-tuning, compilability reinforcement, and compilability discrimination. Comprehensive experiments on two code generation tasks demonstrate the effectiveness of our proposed approach, improving the success rate of compilation from 44.18 to 89.18 in code completion on average and from 70.3 to 96.2 in text-to-code generation, respectively, when comparing with the state-of-the-art CodeGPT. ","[{'version': 'v1', 'created': 'Thu, 10 Mar 2022 03:15:17 GMT'}]",2022-03-11,"[['Wang', 'Xin', ''], ['Wang', 'Yasheng', ''], ['Wan', 'Yao', ''], ['Mi', 'Fei', ''], ['Li', 'Yitong', ''], ['Zhou', 'Pingyi', ''], ['Liu', 'Jin', ''], ['Wu', 'Hao', ''], ['Jiang', 'Xin', ''], ['Liu', 'Qun', '']]",0,1,2022-03-10,1,10,3,0,0,0,7a1069dafaeb484e22f2473d5545f1e45ce30656,247362946.0,https://www.semanticscholar.org/paper/7a1069dafaeb484e22f2473d5545f1e45ce30656,Findings,2022.0,43.0,25.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2153689125', 'name': 'Xin Wang'}, {'authorId': '2136912252', 'name': 'Yasheng Wang'}, {'authorId': '2147200426', 'name': 'Yao Wan'}, {'authorId': '33727421', 'name': 'Fei Mi'}, {'authorId': '50024168', 'name': 'Yitong Li'}, {'authorId': '8157288', 'name': 'Pingyi Zhou'}, {'authorId': '2155352529', 'name': 'Jin Liu'}, {'authorId': '2119797360', 'name': 'Hao Wu'}, {'authorId': '2110310493', 'name': 'Xin Jiang'}, {'authorId': '30738758', 'name': 'Qun Liu'}]","['Wuhan University', 'Huazhong University of Science and Technology', 'Yunnan University', 'Huawei Technologies (China)']",['China'],2022-03
2203.06311,Yujia Qin,"Yujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong
  Sun, Jie Zhou",ELLE: Efficient Lifelong Pre-training for Emerging Data,Findings of ACL 2022,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Current pre-trained language models (PLM) are typically trained with static data, ignoring that in real-world scenarios, streaming data of various sources may continuously grow. This requires PLMs to integrate the information from all the sources in a lifelong manner. Although this goal could be achieved by exhaustive pre-training on all the existing data, such a process is known to be computationally expensive. To this end, we propose ELLE, aiming at efficient lifelong pre-training for emerging data. Specifically, ELLE consists of (1) function preserved model expansion, which flexibly expands an existing PLM's width and depth to improve the efficiency of knowledge acquisition; and (2) pre-trained domain prompts, which disentangle the versatile knowledge learned during pre-training and stimulate the proper knowledge for downstream tasks. We experiment ELLE with streaming data from 5 domains on BERT and GPT. The results show the superiority of ELLE over various lifelong learning baselines in both pre-training efficiency and downstream performances. The codes are publicly available at https://github.com/thunlp/ELLE. ","[{'version': 'v1', 'created': 'Sat, 12 Mar 2022 01:53:53 GMT'}, {'version': 'v2', 'created': 'Mon, 11 Jul 2022 06:07:22 GMT'}]",2022-07-12,"[['Qin', 'Yujia', ''], ['Zhang', 'Jiajie', ''], ['Lin', 'Yankai', ''], ['Liu', 'Zhiyuan', ''], ['Li', 'Peng', ''], ['Sun', 'Maosong', ''], ['Zhou', 'Jie', '']]",0,1,2022-03-12,2,7,2,0,0,0,bc7984bfcfae537dbe633eeeb8d69c42a994c724,247447415.0,https://www.semanticscholar.org/paper/bc7984bfcfae537dbe633eeeb8d69c42a994c724,Findings,2022.0,59.0,34.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '50625437', 'name': 'Yujia Qin'}, {'authorId': '2107983722', 'name': 'Jiajie Zhang'}, {'authorId': '2149202150', 'name': 'Yankai Lin'}, {'authorId': '2141313179', 'name': 'Zhiyuan Liu'}, {'authorId': '144326610', 'name': 'Peng Li'}, {'authorId': '1753344', 'name': 'Maosong Sun'}, {'authorId': '49178343', 'name': 'Jie Zhou'}]","['Beijing Academy of Artificial Intelligence', 'Beijing Information Science & Technology University', 'Jiangsu Collaborative Innovation Center for Language Ability, Xuzhou, China', 'Tencent', 'Tsinghua University']",['China'],2022-03
2203.10256,Zhixian Yang,"Zhixian Yang, Xiaojun Wan",Dependency-based Mixture Language Models,Accepted to ACL 2022 Main Conference,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Various models have been proposed to incorporate knowledge of syntactic structures into neural language models. However, previous works have relied heavily on elaborate components for a specific language model, usually recurrent neural network (RNN), which makes themselves unwieldy in practice to fit into other neural language models, such as Transformer and GPT-2. In this paper, we introduce the Dependency-based Mixture Language Models. In detail, we first train neural language models with a novel dependency modeling objective to learn the probability distribution of future dependent tokens given context. We then formulate the next-token probability by mixing the previous dependency modeling probability distributions with self-attention. Extensive experiments and human evaluations show that our method can be easily and effectively applied to different neural language models while improving neural text generation on various tasks. ","[{'version': 'v1', 'created': 'Sat, 19 Mar 2022 06:28:30 GMT'}]",2022-03-22,"[['Yang', 'Zhixian', ''], ['Wan', 'Xiaojun', '']]",0,1,2022-03-19,1,2,1,1,1,0,7f39cc53d4f41dbc05887388c5202e38c97722f8,247594788.0,https://www.semanticscholar.org/paper/7f39cc53d4f41dbc05887388c5202e38c97722f8,Annual Meeting of the Association for Computational Linguistics,2022.0,58.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2140193848', 'name': 'Zhixian Yang'}, {'authorId': '117908148', 'name': 'Xiaojun Wan'}]",['Peking University'],['China'],2022-03
2203.10705,Chaofan Tao,"Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping
  Luo, Ngai Wong",Compression of Generative Pre-trained Language Models via Quantization,ACL 2022,,,,cs.CL cs.CV,http://creativecommons.org/licenses/by/4.0/,"  The increasing size of generative Pre-trained Language Models (PLMs) has greatly increased the demand for model compression. Despite various methods to compress BERT or its variants, there are few attempts to compress generative PLMs, and the underlying difficulty remains unclear. In this paper, we compress generative PLMs by quantization. We find that previous quantization methods fail on generative tasks due to the \textit{homogeneous word embeddings} caused by reduced capacity, and \textit{varied distribution of weights}. Correspondingly, we propose a token-level contrastive distillation to learn distinguishable word embeddings, and a module-wise dynamic scaling to make quantizers adaptive to different modules. Empirical results on various tasks show that our proposed method outperforms the state-of-the-art compression methods on generative PLMs by a clear margin. With comparable performance with the full-precision models, we achieve 14.4x and 13.4x compression rates on GPT-2 and BART, respectively. ","[{'version': 'v1', 'created': 'Mon, 21 Mar 2022 02:11:35 GMT'}, {'version': 'v2', 'created': 'Sat, 16 Jul 2022 07:09:25 GMT'}]",2022-07-19,"[['Tao', 'Chaofan', ''], ['Hou', 'Lu', ''], ['Zhang', 'Wei', ''], ['Shang', 'Lifeng', ''], ['Jiang', 'Xin', ''], ['Liu', 'Qun', ''], ['Luo', 'Ping', ''], ['Wong', 'Ngai', '']]",0,1,2022-03-21,2,8,2,1,1,0,e4f82c0a13cae6739239ae0c25a554b6daff35af,247593909.0,https://www.semanticscholar.org/paper/e4f82c0a13cae6739239ae0c25a554b6daff35af,Annual Meeting of the Association for Computational Linguistics,2022.0,42.0,47.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '144259094', 'name': 'Chaofan Tao'}, {'authorId': '48557122', 'name': 'Lu Hou'}, {'authorId': '2155470511', 'name': 'Wei Zhang'}, {'authorId': '50812138', 'name': 'Lifeng Shang'}, {'authorId': '2110310493', 'name': 'Xin Jiang'}, {'authorId': '30738758', 'name': 'Qun Liu'}, {'authorId': '2143481782', 'name': 'Ping Luo'}, {'authorId': '1873081', 'name': 'Ngai Wong'}]","['University of Hong Kong', 'Huawei Technologies (China)']","['China', 'Hong Kong']",2022-03
2203.12054,Tianyu Hua,"Tianyu Hua, Yonglong Tian, Sucheng Ren, Michalis Raptis, Hang Zhao,
  Leonid Sigal",Self-supervision through Random Segments with Autoregressive Coding (RandSAC),,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Inspired by the success of self-supervised autoregressive representation learning in natural language (GPT and its variants), and advances in recent visual architecture design with Vision Transformers (ViTs), in this paper, we explore the effect various design choices have on the success of applying such training strategies for visual feature learning. Specifically, we introduce a novel strategy that we call Random Segments with Autoregressive Coding (RandSAC). In RandSAC, we group patch representations (image tokens) into hierarchically arranged segments; within each segment, tokens are predicted in parallel, similar to BERT, while across segment predictions are sequential, similar to GPT. We illustrate that randomized serialization of the segments significantly improves the performance and results in distribution over spatially-long (across-segments) and -short (within-segment) predictions which are effective for feature learning. We illustrate the pertinence of these design choices and explore alternatives on a number of datasets (e.g., CIFAR10, CIFAR100, ImageNet). While our pre-training strategy works with a vanilla Transformer, we also propose a conceptually simple, but highly effective, addition to the decoder that allows learnable skip-connections to encoder$'$s feature layers, which further improves the performance. ","[{'version': 'v1', 'created': 'Tue, 22 Mar 2022 21:28:55 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Oct 2022 03:59:43 GMT'}]",2022-10-27,"[['Hua', 'Tianyu', ''], ['Tian', 'Yonglong', ''], ['Ren', 'Sucheng', ''], ['Raptis', 'Michalis', ''], ['Zhao', 'Hang', ''], ['Sigal', 'Leonid', '']]",0,1,2022-03-22,2,6,2,0,0,0,4bc8fa1ca83cf4a6b93044b3103fe622b012d90d,247618909.0,https://www.semanticscholar.org/paper/4bc8fa1ca83cf4a6b93044b3103fe622b012d90d,International Conference on Learning Representations,2022.0,64.0,6.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1419971650', 'name': 'Tianyu Hua'}, {'authorId': '2476765', 'name': 'Yonglong Tian'}, {'authorId': '1823941979', 'name': 'Sucheng Ren'}, {'authorId': '2146231364', 'name': 'Hang Zhao'}, {'authorId': '144398147', 'name': 'L. Sigal'}]","['South China University of Technology', 'Google', 'Vector Institute', 'University of British Columbia', 'Tsinghua University', 'Canadian Institute for Advanced Research', 'Massachusetts Institute of Technology']","['China', 'United States', 'Canada']",2022-03
2203.13055,Li Siyao,"Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang, Chen Qian,
  Chen Change Loy, Ziwei Liu",Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory,"Accepted by CVPR 2022. Code and video link:
  https://github.com/lisiyao21/Bailando/",,,,cs.SD cs.CV eess.AS,http://creativecommons.org/licenses/by/4.0/,"  Driving 3D characters to dance following a piece of music is highly challenging due to the spatial constraints applied to poses by choreography norms. In addition, the generated dance sequence also needs to maintain temporal coherency with different music genres. To tackle these challenges, we propose a novel music-to-dance framework, Bailando, with two powerful components: 1) a choreographic memory that learns to summarize meaningful dancing units from 3D pose sequence to a quantized codebook, 2) an actor-critic Generative Pre-trained Transformer (GPT) that composes these units to a fluent dance coherent to the music. With the learned choreographic memory, dance generation is realized on the quantized units that meet high choreography standards, such that the generated dancing sequences are confined within the spatial constraints. To achieve synchronized alignment between diverse motion tempos and music beats, we introduce an actor-critic-based reinforcement learning scheme to the GPT with a newly-designed beat-align reward function. Extensive experiments on the standard benchmark demonstrate that our proposed framework achieves state-of-the-art performance both qualitatively and quantitatively. Notably, the learned choreographic memory is shown to discover human-interpretable dancing-style poses in an unsupervised manner. ","[{'version': 'v1', 'created': 'Thu, 24 Mar 2022 13:06:43 GMT'}, {'version': 'v2', 'created': 'Fri, 25 Mar 2022 03:07:26 GMT'}]",2022-03-28,"[['Siyao', 'Li', ''], ['Yu', 'Weijiang', ''], ['Gu', 'Tianpei', ''], ['Lin', 'Chunze', ''], ['Wang', 'Quan', ''], ['Qian', 'Chen', ''], ['Loy', 'Chen Change', ''], ['Liu', 'Ziwei', '']]",0,1,2022-03-24,2,8,3,0,0,0,45b0f30ee83f324ccdffd608818ffb2d50de4a8f,247627867.0,https://www.semanticscholar.org/paper/45b0f30ee83f324ccdffd608818ffb2d50de4a8f,Computer Vision and Pattern Recognition,2022.0,42.0,63.0,22.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '31354625', 'name': 'Lian Siyao'}, {'authorId': '2856505', 'name': 'Weijiang Yu'}, {'authorId': '2124439724', 'name': 'Tianpei Gu'}, {'authorId': '51510731', 'name': 'Chunze Lin'}, {'authorId': '2145348695', 'name': 'Quan Wang'}, {'authorId': '144461220', 'name': 'Chen Qian'}, {'authorId': '1717179', 'name': 'Chen Change Loy'}, {'authorId': '2117940996', 'name': 'Ziwei Liu'}]","['Sun Yat-sen University', 'Nanyang Technological University']","['China', 'Singapore']",2022-03
2203.17113,Junyi Ao,"Junyi Ao, Ziqiang Zhang, Long Zhou, Shujie Liu, Haizhou Li, Tom Ko,
  Lirong Dai, Jinyu Li, Yao Qian, Furu Wei",Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data,Accepted by Interspeech 2022,,,,cs.SD cs.LG eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper studies a novel pre-training technique with unpaired speech data, Speech2C, for encoder-decoder based automatic speech recognition (ASR). Within a multi-task learning framework, we introduce two pre-training tasks for the encoder-decoder network using acoustic units, i.e., pseudo codes, derived from an offline clustering model. One is to predict the pseudo codes via masked language modeling in encoder output, like HuBERT model, while the other lets the decoder learn to reconstruct pseudo codes autoregressively instead of generating textual scripts. In this way, the decoder learns to reconstruct original speech information with codes before learning to generate correct text. Comprehensive experiments on the LibriSpeech corpus show that the proposed Speech2C can relatively reduce the word error rate (WER) by 19.2% over the method without decoder pre-training, and also outperforms significantly the state-of-the-art wav2vec 2.0 and HuBERT on fine-tuning subsets of 10h and 100h. We release our code and model at https://github.com/microsoft/SpeechT5/tree/main/Speech2C. ","[{'version': 'v1', 'created': 'Thu, 31 Mar 2022 15:33:56 GMT'}, {'version': 'v2', 'created': 'Mon, 20 Jun 2022 07:00:49 GMT'}]",2022-06-22,"[['Ao', 'Junyi', ''], ['Zhang', 'Ziqiang', ''], ['Zhou', 'Long', ''], ['Liu', 'Shujie', ''], ['Li', 'Haizhou', ''], ['Ko', 'Tom', ''], ['Dai', 'Lirong', ''], ['Li', 'Jinyu', ''], ['Qian', 'Yao', ''], ['Wei', 'Furu', '']]",0,0,2022-03-31,2,10,3,0,0,0,49995802ad0d6ef327647868868458d7619d430d,247839669.0,https://www.semanticscholar.org/paper/49995802ad0d6ef327647868868458d7619d430d,Interspeech,2022.0,25.0,14.0,3.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2052108603', 'name': 'Junyi Ao'}, {'authorId': '2116460844', 'name': 'Zi-Hua Zhang'}, {'authorId': '2135918679', 'name': 'Long Zhou'}, {'authorId': '2107983441', 'name': 'Shujie Liu'}, {'authorId': '2108493009', 'name': 'Haizhou Li'}, {'authorId': '3023507', 'name': 'Tom Ko'}, {'authorId': '2147422894', 'name': 'Lirong Dai'}, {'authorId': '152319568', 'name': 'Jinyu Li'}, {'authorId': '48549092', 'name': 'Yao Qian'}, {'authorId': '49807919', 'name': 'Furu Wei'}]","['University of Science and Technology of China', 'Chinese University of Hong Kong']",['China'],2022-03
2204.07014,Carina Negreanu,"Carina Negreanu, Alperen Karaoglu, Jack Williams, Shuang Chen, Daniel
  Fabian, Andrew Gordon, Chin-Yew Lin",Rows from Many Sources: Enriching row completions from Wikidata with a pre-trained Language Model,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Row completion is the task of augmenting a given table of text and numbers with additional, relevant rows. The task divides into two steps: subject suggestion, the task of populating the main column; and gap filling, the task of populating the remaining columns. We present state-of-the-art results for subject suggestion and gap filling measured on a standard benchmark (WikiTables). Our idea is to solve this task by harmoniously combining knowledge base table interpretation and free text generation. We interpret the table using the knowledge base to suggest new rows and generate metadata like headers through property linking. To improve candidate diversity, we synthesize additional rows using free text generation via GPT-3, and crucially, we exploit the metadata we interpret to produce better prompts for text generation. Finally, we verify that the additional synthesized content can be linked to the knowledge base or a trusted web source such as Wikipedia. ","[{'version': 'v1', 'created': 'Thu, 14 Apr 2022 15:11:52 GMT'}]",2022-04-15,"[['Negreanu', 'Carina', ''], ['Karaoglu', 'Alperen', ''], ['Williams', 'Jack', ''], ['Chen', 'Shuang', ''], ['Fabian', 'Daniel', ''], ['Gordon', 'Andrew', ''], ['Lin', 'Chin-Yew', '']]",0,1,2022-04-14,1,7,2,1,0,1,846031341785391cc7d5d4844e2e10ace110b184,248177853.0,https://www.semanticscholar.org/paper/846031341785391cc7d5d4844e2e10ace110b184,The Web Conference,2022.0,53.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '21777559', 'name': 'C. Negreanu'}, {'authorId': '51112250', 'name': 'Alperen Karaoglu'}, {'authorId': '2111787940', 'name': 'Jack Williams'}, {'authorId': '23548372', 'name': 'Shuang Chen'}, {'authorId': '2162473933', 'name': 'Daniel Fabian'}, {'authorId': '2054050322', 'name': 'Andrew Gordon'}, {'authorId': '50554693', 'name': 'Chin-Yew Lin'}]","['Harbin Institute of Technology', 'Microsoft']","['China', 'United Kingdom']",2022-04
2204.09714,Qihao Zhu,"Qihao Zhu, Xinyu Zhang, Jianxi Luo",Generative Pre-Trained Transformers for Biologically Inspired Design,"Accepted by the 34th ASME IDETC-CIE International Conference on
  Design Theory & Methodology",,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Biological systems in nature have evolved for millions of years to adapt and survive the environment. Many features they developed can be inspirational and beneficial for solving technical problems in modern industries. This leads to a novel form of design-by-analogy called bio-inspired design (BID). Although BID as a design method has been proven beneficial, the gap between biology and engineering continuously hinders designers from effectively applying the method. Therefore, we explore the recent advance of artificial intelligence (AI) for a computational approach to bridge the gap. This paper proposes a generative design approach based on the pre-trained language model (PLM) to automatically retrieve and map biological analogy and generate BID in the form of natural language. The latest generative pre-trained transformer, namely GPT-3, is used as the base PLM. Three types of design concept generators are identified and fine-tuned from the PLM according to the looseness of the problem space representation. Machine evaluators are also fine-tuned to assess the correlation between the domains within the generated BID concepts. The approach is then tested via a case study in which the fine-tuned models are applied to generate and evaluate light-weighted flying car concepts inspired by nature. The results show our approach can generate BID concepts with good performance. ","[{'version': 'v1', 'created': 'Thu, 31 Mar 2022 11:13:22 GMT'}]",2022-04-22,"[['Zhu', 'Qihao', ''], ['Zhang', 'Xinyu', ''], ['Luo', 'Jianxi', '']]",0,1,2022-03-31,1,3,2,1,0,1,fae63a876176e6c3e9166132433a127f11483f7c,248299706.0,https://www.semanticscholar.org/paper/fae63a876176e6c3e9166132433a127f11483f7c,Volume 6: 34th International Conference on Design Theory and Methodology (DTM),2022.0,48.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2152203384', 'name': 'Qihao Zhu'}, {'authorId': None, 'name': 'Xinyu Zhang'}, {'authorId': '145990580', 'name': 'Jianxi Luo'}]","['University of Technology', 'Tsinghua University']","['China', 'Russia']",2022-03
2204.13498,Qi Jia,"Qi Jia, Yizhu Liu, Haifeng Tang, Kenny Q. Zhu",Post-Training Dialogue Summarization using Pseudo-Paraphrasing,Findings of NAACL 2022,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Previous dialogue summarization techniques adapt large language models pretrained on the narrative text by injecting dialogue-specific features into the models. These features either require additional knowledge to recognize or make the resulting models harder to tune. To bridge the format gap between dialogues and narrative summaries in dialogue summarization tasks, we propose to post-train pretrained language models (PLMs) to rephrase from dialogue to narratives. After that, the model is fine-tuned for dialogue summarization as usual. Comprehensive experiments show that our approach significantly improves vanilla PLMs on dialogue summarization and outperforms other SOTA models by the summary quality and implementation costs. ","[{'version': 'v1', 'created': 'Thu, 28 Apr 2022 13:42:19 GMT'}]",2022-04-29,"[['Jia', 'Qi', ''], ['Liu', 'Yizhu', ''], ['Tang', 'Haifeng', ''], ['Zhu', 'Kenny Q.', '']]",0,0,2022-04-28,1,4,1,0,0,0,80c05e4a7d2a4c838e1cb697571b3ac89f8a0b53,248426849.0,https://www.semanticscholar.org/paper/80c05e4a7d2a4c838e1cb697571b3ac89f8a0b53,NAACL-HLT,2022.0,33.0,6.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2056108122', 'name': 'Qi Jia'}, {'authorId': '5826956', 'name': 'Yizhu Liu'}, {'authorId': '2112389755', 'name': 'Haifeng Tang'}, {'authorId': '1796651', 'name': 'Kenny Q. Zhu'}]","['China Merchants Bank Credit Card Center, Shanghai, China', 'Shanghai Jiao Tong University']",['China'],2022-04
2204.13921,Xiaoqiang Wang,"Xiaoqiang Wang, Bang Liu, Siliang Tang, Lingfei Wu",QRelScore: Better Evaluating Generated Questions with Deeper Understanding of Context-aware Relevance,"16 pages, 7 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing metrics for assessing question generation not only require costly human reference but also fail to take into account the input context of generation, rendering the lack of deep understanding of the relevance between the generated questions and input contexts. As a result, they may wrongly penalize a legitimate and reasonable candidate question when it (i) involves complicated reasoning with the context or (ii) can be grounded by multiple evidences in the context. In this paper, we propose $\textbf{QRelScore}$, a context-aware $\underline{\textbf{Rel}}$evance evaluation metric for $\underline{\textbf{Q}}$uestion Generation. Based on off-the-shelf language models such as BERT and GPT2, QRelScore employs both word-level hierarchical matching and sentence-level prompt-based generation to cope with the complicated reasoning and diverse generation from multiple evidences, respectively. Compared with existing metrics, our experiments demonstrate that QRelScore is able to achieve a higher correlation with human judgments while being much more robust to adversarial samples. ","[{'version': 'v1', 'created': 'Fri, 29 Apr 2022 07:39:53 GMT'}]",2022-05-02,"[['Wang', 'Xiaoqiang', ''], ['Liu', 'Bang', ''], ['Tang', 'Siliang', ''], ['Wu', 'Lingfei', '']]",0,1,2022-04-29,1,4,1,1,1,0,e6f44759d5d03d7018319d507ee5ac19485596a5,248476204.0,https://www.semanticscholar.org/paper/e6f44759d5d03d7018319d507ee5ac19485596a5,Conference on Empirical Methods in Natural Language Processing,2022.0,69.0,2.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108158210', 'name': 'Xiaoqiang Wang'}, {'authorId': '2116441692', 'name': 'Bang Liu'}, {'authorId': '2118071462', 'name': 'Siliang Tang'}, {'authorId': '3008832', 'name': 'Lingfei Wu'}]","['Jingdong', 'Universit de Montral', 'Canadian Institute for Advanced Research', 'Zhejiang University']","['Canada', 'China']",2022-04
2205.02655,Yixuan Su,"Yixuan Su and Tian Lan and Yahui Liu and Fangyu Liu and Dani Yogatama
  and Yan Wang and Lingpeng Kong and Nigel Collier",Language Models Can See: Plugging Visual Controls in Text Generation,"21 pages, 5 figures, 5 tables; (v2 adds some experimental details)",,,,cs.CV cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Generative language models (LMs) such as GPT-2/3 can be prompted to generate text with remarkable quality. While they are designed for text-prompted generation, it remains an open question how the generation process could be guided by modalities beyond text such as images. In this work, we propose a training-free framework, called MAGIC (iMAge-Guided text generatIon with CLIP), for plugging in visual controls in the generation process and enabling LMs to perform multimodal tasks (e.g., image captioning) in a zero-shot manner. MAGIC is a simple yet efficient plug-and-play framework, which directly combines an off-the-shelf LM (i.e., GPT-2) and an image-text matching model (i.e., CLIP) for image-grounded text generation. During decoding, MAGIC influences the generation of the LM by introducing a CLIP-induced score, called magic score, which regularizes the generated result to be semantically related to a given image while being coherent to the previously generated context. Notably, the proposed decoding scheme does not involve any gradient update operation, therefore being computationally efficient. On the challenging task of zero-shot image captioning, MAGIC outperforms the state-of-the-art method by notable margins with a nearly 27 times decoding speedup. MAGIC is a flexible framework and is theoretically compatible with any text generation tasks that incorporate image grounding. In the experiments, we showcase that it is also capable of performing visually grounded story generation given both an image and a text prompt. ","[{'version': 'v1', 'created': 'Thu, 5 May 2022 13:56:18 GMT'}, {'version': 'v2', 'created': 'Mon, 30 May 2022 19:45:05 GMT'}]",2022-06-01,"[['Su', 'Yixuan', ''], ['Lan', 'Tian', ''], ['Liu', 'Yahui', ''], ['Liu', 'Fangyu', ''], ['Yogatama', 'Dani', ''], ['Wang', 'Yan', ''], ['Kong', 'Lingpeng', ''], ['Collier', 'Nigel', '']]",0,1,2022-05-05,2,8,2,1,1,0,05bcf9999525656cfaa59bc71f8572d771ff3776,248525064.0,https://www.semanticscholar.org/paper/05bcf9999525656cfaa59bc71f8572d771ff3776,arXiv.org,2022.0,95.0,60.0,16.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '50087162', 'name': 'Yixuan Su'}, {'authorId': '1684523', 'name': 'Tian Lan'}, {'authorId': '2144475412', 'name': 'Yahui Liu'}, {'authorId': '144097210', 'name': 'Fangyu Liu'}, {'authorId': '1755465', 'name': 'Dani Yogatama'}, {'authorId': '2152546690', 'name': 'Yan Wang'}, {'authorId': '47648549', 'name': 'Lingpeng Kong'}, {'authorId': '50638196', 'name': 'Nigel Collier'}]","['Google', 'University of Trento', 'Tencent', 'University of Cambridge', 'University of Hong Kong']","['Italy', 'China', 'United Kingdom', 'Hong Kong']",2022-05
2205.05862,Haoqin Tu,"Haoqin Tu, Zhongliang Yang, Jinshuai Yang, Yongfeng Huang",AdaVAE: Exploring Adaptive GPT-2s in Variational Auto-Encoders for Language Modeling,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Variational Auto-Encoder (VAE) has become the de-facto learning paradigm in achieving representation learning and generation for natural language at the same time. Nevertheless, existing VAE-based language models either employ elementary RNNs, which is not powerful to handle complex works in the multi-task situation, or fine-tunes two pre-trained language models (PLMs) for any downstream task, which is a huge drain on resources. In this paper, we propose the first VAE framework empowered with adaptive GPT-2s (AdaVAE). Different from existing systems, we unify both the encoder\&decoder of the VAE model using GPT-2s with adaptive parameter-efficient components, and further introduce Latent Attention operation to better construct latent space from transformer models. Experiments from multiple dimensions validate that AdaVAE is competent to effectively organize language in three related tasks (language modeling, representation modeling and guided text generation) even with less than $15\%$ activated parameters in training. Our code is available at \url{https://github.com/ImKeTT/AdaVAE}. ","[{'version': 'v1', 'created': 'Thu, 12 May 2022 03:22:07 GMT'}, {'version': 'v2', 'created': 'Fri, 20 May 2022 05:52:50 GMT'}, {'version': 'v3', 'created': 'Sat, 19 Nov 2022 12:41:26 GMT'}]",2022-11-22,"[['Tu', 'Haoqin', ''], ['Yang', 'Zhongliang', ''], ['Yang', 'Jinshuai', ''], ['Huang', 'Yongfeng', '']]",0,1,2022-05-12,3,4,1,0,0,0,1668da91ad76aad91e401986136833e2a23f2599,248721899.0,https://www.semanticscholar.org/paper/1668da91ad76aad91e401986136833e2a23f2599,arXiv.org,2022.0,42.0,4.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2136194164', 'name': 'Haoqin Tu'}, {'authorId': '143932857', 'name': 'Zhongliang Yang'}, {'authorId': '2130044491', 'name': 'Jinshuai Yang'}, {'authorId': '2108337221', 'name': 'Siyu Zhang'}, {'authorId': '2145587670', 'name': 'Yong Huang'}]",['Tsinghua University'],['China'],2022-05
2205.05989,Yejin Bang,"Yejin Bang, Nayeon Lee, Tiezheng Yu, Leila Khalatbari, Yan Xu, Samuel
  Cahyawijaya, Dan Su, Bryan Wilie, Romain Barraud, Elham J. Barezi, Andrea
  Madotto, Hayden Kee, Pascale Fung",Towards Answering Open-ended Ethical Quandary Questions,16 pages,,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Considerable advancements have been made in various NLP tasks based on the impressive power of large language models (LLMs) and many NLP applications are deployed in our daily lives. In this work, we challenge the capability of LLMs with the new task of Ethical Quandary Generative Question Answering. Ethical quandary questions are more challenging to address because multiple conflicting answers may exist to a single quandary. We explore the current capability of LLMs in providing an answer with a deliberative exchange of different perspectives to an ethical quandary, in the approach of Socratic philosophy, instead of providing a closed answer like an oracle. We propose a model that searches for different ethical principles applicable to the ethical quandary and generates an answer conditioned on the chosen principles through prompt-based few-shot learning. We also discuss the remaining challenges and ethical issues involved in this task and suggest the direction toward developing responsible NLP systems by incorporating human values explicitly. ","[{'version': 'v1', 'created': 'Thu, 12 May 2022 09:52:59 GMT'}, {'version': 'v2', 'created': 'Tue, 24 May 2022 15:16:11 GMT'}, {'version': 'v3', 'created': 'Wed, 1 Feb 2023 07:25:10 GMT'}]",2023-02-02,"[['Bang', 'Yejin', ''], ['Lee', 'Nayeon', ''], ['Yu', 'Tiezheng', ''], ['Khalatbari', 'Leila', ''], ['Xu', 'Yan', ''], ['Cahyawijaya', 'Samuel', ''], ['Su', 'Dan', ''], ['Wilie', 'Bryan', ''], ['Barraud', 'Romain', ''], ['Barezi', 'Elham J.', ''], ['Madotto', 'Andrea', ''], ['Kee', 'Hayden', ''], ['Fung', 'Pascale', '']]",0,0,2022-05-12,3,13,3,0,0,0,0376b7ff6bd5fd3df5dc766cb24f9ca8736ea34e,256459930.0,https://www.semanticscholar.org/paper/0376b7ff6bd5fd3df5dc766cb24f9ca8736ea34e,,2022.0,62.0,2.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '23672613', 'name': 'Yejin Bang'}, {'authorId': '40221187', 'name': 'Nayeon Lee'}, {'authorId': '1660855299', 'name': 'Tiezheng Yu'}, {'authorId': '50824937', 'name': 'Leila Khalatbari'}, {'authorId': '98271906', 'name': 'Yan Xu'}, {'authorId': '2220548276', 'name': 'Samuel Cahyawijaya'}, {'authorId': '144610224', 'name': 'Dan Su'}, {'authorId': '150048491', 'name': 'Bryan Wilie'}, {'authorId': '2184144605', 'name': 'Romain Barraud'}, {'authorId': '26418289', 'name': 'Elham J. Barezi'}, {'authorId': '2111680936', 'name': 'Andrea Madotto'}, {'authorId': '46195163', 'name': 'Hayden Kee'}, {'authorId': '2057151752', 'name': 'Pascale Fung'}]","['Chinese University of Hong Kong', 'Hong Kong University of Science and Technology']",['China'],2022-05
2205.06036,Shangda Wu,"Shangda Wu, Maosong Sun",Efficient and Training-Free Control of Language Generation,"8 pages, 4 figures, 3 tables",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  In recent years, there has been a growing interest in the development of language models capable of generating text with controllable attributes. While several approaches have been proposed, many of these methods require condition-specific data or significant computational resources. In this study, we propose a novel method called Gamma Sampling, which enables controllable language generation without the need for any training data and maintains a fast generation speed. Gamma Sampling incorporates attribute-related information into the sampling process, effectively guiding the language model to produce text with desired attributes. Our experimental results demonstrate that Gamma Sampling, when applied to GPT2, outperforms representative baselines in terms of diversity, attribute relevance, and overall quality of the generated samples. ","[{'version': 'v1', 'created': 'Thu, 12 May 2022 11:48:11 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Sep 2022 12:12:50 GMT'}, {'version': 'v3', 'created': 'Mon, 12 Sep 2022 15:12:59 GMT'}, {'version': 'v4', 'created': 'Sat, 17 Sep 2022 03:14:34 GMT'}, {'version': 'v5', 'created': 'Tue, 21 Feb 2023 07:48:50 GMT'}]",2023-02-22,"[['Wu', 'Shangda', ''], ['Sun', 'Maosong', '']]",0,1,2022-05-12,5,2,1,1,1,0,54ddd5df6d1189ea45cac08a4eddbf00a742068d,257050203.0,https://www.semanticscholar.org/paper/54ddd5df6d1189ea45cac08a4eddbf00a742068d,,2022.0,29.0,1.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2152536983', 'name': 'Shangda Wu'}, {'authorId': '1753344', 'name': 'Maosong Sun'}]",['Tsinghua University'],['China'],2022-05
2205.07162,Junjun Jiang,Zeyu Lu and Junjun Jiang and Junqin Huang and Gang Wu and Xianming Liu,GLaMa: Joint Spatial and Frequency Loss for General Image Inpainting,"IEEE/CVF Conference on Computer Vision and Pattern Recognition
  Workshops (CVPRW)",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The purpose of image inpainting is to recover scratches and damaged areas using context information from remaining parts. In recent years, thanks to the resurgence of convolutional neural networks (CNNs), image inpainting task has made great breakthroughs. However, most of the work consider insufficient types of mask, and their performance will drop dramatically when encountering unseen masks. To combat these challenges, we propose a simple yet general method to solve this problem based on the LaMa image inpainting framework, dubbed GLaMa. Our proposed GLaMa can better capture different types of missing information by using more types of masks. By incorporating more degraded images in the training phase, we can expect to enhance the robustness of the model with respect to various masks. In order to yield more reasonable results, we further introduce a frequency-based loss in addition to the traditional spatial reconstruction loss and adversarial loss. In particular, we introduce an effective reconstruction loss both in the spatial and frequency domain to reduce the chessboard effect and ripples in the reconstructed image. Extensive experiments demonstrate that our method can boost the performance over the original LaMa method for each type of mask on FFHQ, ImageNet, Places2 and WikiArt dataset. The proposed GLaMa was ranked first in terms of PSNR, LPIPS and SSIM in the NTIRE 2022 Image Inpainting Challenge Track 1 Unsupervised. ","[{'version': 'v1', 'created': 'Sun, 15 May 2022 02:18:59 GMT'}]",2022-05-17,"[['Lu', 'Zeyu', ''], ['Jiang', 'Junjun', ''], ['Huang', 'Junqin', ''], ['Wu', 'Gang', ''], ['Liu', 'Xianming', '']]",0,0,2022-05-15,1,5,1,0,0,0,0c606469d17650290dbbd7bd087853ad8d28f518,248810839.0,https://www.semanticscholar.org/paper/0c606469d17650290dbbd7bd087853ad8d28f518,2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),2022.0,42.0,10.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2049016304', 'name': 'Zeyu Lu'}, {'authorId': '2141510194', 'name': 'Junjun Jiang'}, {'authorId': '2118225524', 'name': 'Jun Huang'}, {'authorId': '2152004466', 'name': 'Gang Wu'}, {'authorId': '2108681776', 'name': 'Xianming Liu'}]",['Harbin Institute of Technology'],['China'],2022-05
2205.10183,Li Dong,"Zhixiong Han, Yaru Hao, Li Dong, Yutao Sun, Furu Wei",Prototypical Calibration for Few-shot Learning of Language Models,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In-context learning of GPT-like models has been recognized as fragile across different hand-crafted templates, and demonstration permutations. In this work, we propose prototypical calibration to adaptively learn a more robust decision boundary for zero- and few-shot classification, instead of greedy decoding. Concretely, our method first adopts Gaussian mixture distribution to estimate the prototypical clusters for all categories. Then we assign each cluster to the corresponding label by solving a weighted bipartite matching problem. Given an example, its prediction is calibrated by the likelihood of prototypical clusters. Experimental results show that prototypical calibration yields a substantial improvement on a diverse set of tasks. Extensive analysis across different scales also indicates that our method calibrates the decision boundary as expected, greatly improving the robustness of GPT to templates, permutations, and class imbalance. ","[{'version': 'v1', 'created': 'Fri, 20 May 2022 13:50:07 GMT'}, {'version': 'v2', 'created': 'Wed, 5 Oct 2022 01:33:07 GMT'}]",2022-10-06,"[['Han', 'Zhixiong', ''], ['Hao', 'Yaru', ''], ['Dong', 'Li', ''], ['Sun', 'Yutao', ''], ['Wei', 'Furu', '']]",0,1,2022-05-20,2,5,1,0,0,0,6a483cd1cbecd66150c9bbcd01606723950281bc,248964978.0,https://www.semanticscholar.org/paper/6a483cd1cbecd66150c9bbcd01606723950281bc,International Conference on Learning Representations,2022.0,31.0,18.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2835766', 'name': 'Zhixiong Han'}, {'authorId': '34128716', 'name': 'Y. Hao'}, {'authorId': '145307652', 'name': 'Li Dong'}, {'authorId': '2108540694', 'name': 'Yutao Sun'}, {'authorId': '49807919', 'name': 'Furu Wei'}]",['Microsoft'],"['China', 'India']",2022-05
2205.10583,Zhiyu Fan,"Zhiyu Fan, Xiang Gao, Martin Mirchev, Abhik Roychoudhury, Shin Hwei
  Tan",Automated Repair of Programs from Large Language Models,"12 pages, To appear in ICSE 2023",,,,cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Large language models such as Codex, have shown the capability to produce code for many programming tasks. However, the success rate of existing models is low, especially for complex programming tasks. One of the reasons is that language models lack awareness of program semantics, resulting in incorrect programs, or even programs which do not compile. In this paper, we systematically study whether automated program repair (APR) techniques can fix the incorrect solutions produced by language models in LeetCode contests. The goal is to study whether APR techniques can enhance reliability in the code produced by large language models. Our study revealed that: (1) automatically generated code shares common programming mistakes with human-crafted solutions, indicating APR techniques may have potential to fix auto-generated code; (2) given bug location information provided by a statistical fault localization approach, the newly released Codex edit mode, which supports editing code, is similar to or better than existing Java repair tools TBar and Recoder in fixing incorrect solutions. By analyzing the experimental results generated by these tools, we provide several suggestions: (1) enhancing APR tools to surpass limitations in patch space (e.g., introducing more flexible fault localization) is desirable; (2) as large language models can derive more fix patterns by training on more data, future APR tools could shift focus from adding more fix patterns to synthesis/semantics based approaches, (3) combination of language models with APR to curate patch ingredients, is worth studying. ","[{'version': 'v1', 'created': 'Sat, 21 May 2022 12:48:27 GMT'}, {'version': 'v2', 'created': 'Tue, 24 May 2022 02:03:34 GMT'}, {'version': 'v3', 'created': 'Fri, 9 Dec 2022 17:57:41 GMT'}, {'version': 'v4', 'created': 'Mon, 2 Jan 2023 04:32:26 GMT'}]",2023-01-03,"[['Fan', 'Zhiyu', ''], ['Gao', 'Xiang', ''], ['Mirchev', 'Martin', ''], ['Roychoudhury', 'Abhik', ''], ['Tan', 'Shin Hwei', '']]",0,0,2022-05-21,4,5,1,1,0,1,0bcd59da541fdae66884afba8d25475a54a9da1a,255372224.0,https://www.semanticscholar.org/paper/0bcd59da541fdae66884afba8d25475a54a9da1a,International Conference on Software Engineering,2022.0,53.0,33.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2113846907', 'name': 'Zhiyu Fan'}, {'authorId': '143856160', 'name': 'Xiang Gao'}, {'authorId': '88369696', 'name': 'M. Mirchev'}, {'authorId': '1789700', 'name': 'Abhik Roychoudhury'}, {'authorId': '1747226', 'name': 'Shin Hwei Tan'}]","['National University of Singapore', 'Beihang University', 'Southern University of Science and Technology']","['China', 'Singapore']",2022-05
2205.10687,Abbas Ghaddar,"Abbas Ghaddar, Yimeng Wu, Sunyam Bagga, Ahmad Rashid, Khalil Bibi,
  Mehdi Rezagholizadeh, Chao Xing, Yasheng Wang, Duan Xinyu, Zhefeng Wang,
  Baoxing Huai, Xin Jiang, Qun Liu, Philippe Langlais",Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Understanding,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  There is a growing body of work in recent years to develop pre-trained language models (PLMs) for the Arabic language. This work concerns addressing two major problems in existing Arabic PLMs which constraint progress of the Arabic NLU and NLG fields.First, existing Arabic PLMs are not well-explored and their pre-trainig can be improved significantly using a more methodical approach. Second, there is a lack of systematic and reproducible evaluation of these models in the literature. In this work, we revisit both the pre-training and evaluation of Arabic PLMs. In terms of pre-training, we explore improving Arabic LMs from three perspectives: quality of the pre-training data, size of the model, and incorporating character-level information. As a result, we release three new Arabic BERT-style models ( JABER, Char-JABER, and SABER), and two T5-style models (AT5S and AT5B). In terms of evaluation, we conduct a comprehensive empirical study to systematically evaluate the performance of existing state-of-the-art models on ALUE that is a leaderboard-powered benchmark for Arabic NLU tasks, and on a subset of the ARGEN benchmark for Arabic NLG tasks. We show that our models significantly outperform existing Arabic PLMs and achieve a new state-of-the-art performance on discriminative and generative Arabic NLU and NLG tasks. Our models and source code to reproduce of results will be made available shortly. ","[{'version': 'v1', 'created': 'Sat, 21 May 2022 22:38:19 GMT'}]",2022-05-24,"[['Ghaddar', 'Abbas', ''], ['Wu', 'Yimeng', ''], ['Bagga', 'Sunyam', ''], ['Rashid', 'Ahmad', ''], ['Bibi', 'Khalil', ''], ['Rezagholizadeh', 'Mehdi', ''], ['Xing', 'Chao', ''], ['Wang', 'Yasheng', ''], ['Xinyu', 'Duan', ''], ['Wang', 'Zhefeng', ''], ['Huai', 'Baoxing', ''], ['Jiang', 'Xin', ''], ['Liu', 'Qun', ''], ['Langlais', 'Philippe', '']]",0,0,2022-05-21,1,14,1,1,1,0,24fbe3af030f393e55bda3dc7dae0f57bd270d04,248986492.0,https://www.semanticscholar.org/paper/24fbe3af030f393e55bda3dc7dae0f57bd270d04,arXiv.org,2022.0,83.0,3.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '3448973', 'name': 'Abbas Ghaddar'}, {'authorId': '2000863903', 'name': 'Yimeng Wu'}, {'authorId': '32559189', 'name': 'Sunyam Bagga'}, {'authorId': '2064509318', 'name': 'Ahmad Rashid'}, {'authorId': '2127601849', 'name': 'Khalil Bibi'}, {'authorId': '2066076226', 'name': 'Mehdi Rezagholizadeh'}, {'authorId': '2064677272', 'name': 'Chao Xing'}, {'authorId': '2136912252', 'name': 'Yasheng Wang'}, {'authorId': '2289790', 'name': 'Duan Xinyu'}, {'authorId': '2108271253', 'name': 'Zhefeng Wang'}, {'authorId': '2422046', 'name': 'Baoxing Huai'}, {'authorId': '2110310493', 'name': 'Xin Jiang'}, {'authorId': '30738758', 'name': 'Qun Liu'}, {'authorId': '1706916', 'name': 'P. Langlais'}]","['Universit de Montral', 'Huawei Technologies (China)']","['China', 'Canada']",2022-05
2205.10803,Zhenyu Hou,"Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie
  Wang, Jie Tang",GraphMAE: Self-Supervised Masked Graph Autoencoders,11 pages; Accepted to KDD'22,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Self-supervised learning (SSL) has been extensively explored in recent years. Particularly, generative SSL has seen emerging success in natural language processing and other AI fields, such as the wide adoption of BERT and GPT. Despite this, contrastive learning-which heavily relies on structural data augmentation and complicated training strategies-has been the dominant approach in graph SSL, while the progress of generative SSL on graphs, especially graph autoencoders (GAEs), has thus far not reached the potential as promised in other fields. In this paper, we identify and examine the issues that negatively impact the development of GAEs, including their reconstruction objective, training robustness, and error metric. We present a masked graph autoencoder GraphMAE that mitigates these issues for generative self-supervised graph pretraining. Instead of reconstructing graph structures, we propose to focus on feature reconstruction with both a masking strategy and scaled cosine error that benefit the robust training of GraphMAE. We conduct extensive experiments on 21 public datasets for three different graph learning tasks. The results manifest that GraphMAE-a simple graph autoencoder with careful designs-can consistently generate outperformance over both contrastive and generative state-of-the-art baselines. This study provides an understanding of graph autoencoders and demonstrates the potential of generative self-supervised pre-training on graphs. ","[{'version': 'v1', 'created': 'Sun, 22 May 2022 11:57:08 GMT'}, {'version': 'v2', 'created': 'Tue, 24 May 2022 13:52:48 GMT'}, {'version': 'v3', 'created': 'Wed, 13 Jul 2022 13:53:31 GMT'}]",2022-07-14,"[['Hou', 'Zhenyu', ''], ['Liu', 'Xiao', ''], ['Cen', 'Yukuo', ''], ['Dong', 'Yuxiao', ''], ['Yang', 'Hongxia', ''], ['Wang', 'Chunjie', ''], ['Tang', 'Jie', '']]",0,1,2022-05-22,3,7,1,0,0,0,b161c4aaddd2983a9d4d5a240bd5ffa84b36c4e7,248987361.0,https://www.semanticscholar.org/paper/b161c4aaddd2983a9d4d5a240bd5ffa84b36c4e7,Knowledge Discovery and Data Mining,2022.0,61.0,116.0,22.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2068251467', 'name': 'Zhenyu Hou'}, {'authorId': '2111312892', 'name': 'Xiao Liu'}, {'authorId': '83546711', 'name': 'Yukuo Cen'}, {'authorId': '2047998', 'name': 'Yuxiao Dong'}, {'authorId': '38385080', 'name': 'Hongxia Yang'}, {'authorId': '47074042', 'name': 'C. Wang'}, {'authorId': '2148911956', 'name': 'Jie Tang'}]","['BirenTech Research', 'Alibaba', 'Tsinghua University']",['China'],2022-05
2205.11005,Yuchao Li,"Yuchao Li, Fuli Luo, Chuanqi Tan, Mengdi Wang, Songfang Huang, Shen
  Li, Junjie Bai",Parameter-Efficient Sparsity for Large Language Models Fine-Tuning,This paper is published in IJCAI 2022,,,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  With the dramatically increased number of parameters in language models, sparsity methods have received ever-increasing research focus to compress and accelerate the models. While most research focuses on how to accurately retain appropriate weights while maintaining the performance of the compressed model, there are challenges in the computational overhead and memory footprint of sparse training when compressing large-scale language models. To address this problem, we propose a Parameter-efficient Sparse Training (PST) method to reduce the number of trainable parameters during sparse-aware training in downstream tasks. Specifically, we first combine the data-free and data-driven criteria to efficiently and accurately measure the importance of weights. Then we investigate the intrinsic redundancy of data-driven weight importance and derive two obvious characteristics i.e., low-rankness and structuredness. Based on that, two groups of small matrices are introduced to compute the data-driven importance of weights, instead of using the original large importance score matrix, which therefore makes the sparse training resource-efficient and parameter-efficient. Experiments with diverse networks (i.e., BERT, RoBERTa and GPT-2) on dozens of datasets demonstrate PST performs on par or better than previous sparsity methods, despite only training a small number of parameters. For instance, compared with previous sparsity methods, our PST only requires 1.5% trainable parameters to achieve comparable performance on BERT. ","[{'version': 'v1', 'created': 'Mon, 23 May 2022 02:43:45 GMT'}]",2022-05-24,"[['Li', 'Yuchao', ''], ['Luo', 'Fuli', ''], ['Tan', 'Chuanqi', ''], ['Wang', 'Mengdi', ''], ['Huang', 'Songfang', ''], ['Li', 'Shen', ''], ['Bai', 'Junjie', '']]",0,1,2022-05-23,1,7,1,1,1,0,03d19fde1df67c7ea8dedc750dcd3a6291032577,248986195.0,https://www.semanticscholar.org/paper/03d19fde1df67c7ea8dedc750dcd3a6291032577,International Joint Conference on Artificial Intelligence,2022.0,27.0,7.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2110482496', 'name': 'Yuchao Li'}, {'authorId': '2140495101', 'name': 'Fuli Luo'}, {'authorId': '2111727840', 'name': 'Chuanqi Tan'}, {'authorId': '50468734', 'name': 'Mengdi Wang'}, {'authorId': '2410938', 'name': 'Songfang Huang'}, {'authorId': '2153701890', 'name': 'Shen Li'}, {'authorId': '2113829116', 'name': 'Junjie Bai'}]",['Alibaba'],['China'],2022-05
2205.12682,Haoyu Dong,"Fan Zhou, Mengkang Hu, Haoyu Dong, Zhoujun Cheng, Shi Han, Dongmei
  Zhang",TaCube: Pre-computing Data Cubes for Answering Numerical-Reasoning Questions over Tabular Data,,,,,cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing auto-regressive pre-trained language models (PLMs) like T5 and BART, have been well applied to table question answering by UNIFIEDSKG and TAPEX, respectively, and demonstrated state-of-the-art results on multiple benchmarks. However, auto-regressive PLMs are challenged by recent emerging numerical reasoning datasets, such as TAT-QA, due to the error-prone implicit calculation. In this paper, we present TaCube, to pre-compute aggregation/arithmetic results for the table in advance, so that they are handy and readily available for PLMs to answer numerical reasoning questions. TaCube systematically and comprehensively covers a collection of computational operations over table segments. By simply concatenating TaCube to the input sequence of PLMs, it shows significant experimental effectiveness. TaCube promotes the F1 score from 49.6% to 66.2% on TAT-QA and achieves new state-of-the-art results on WikiTQ (59.6% denotation accuracy). TaCube's improvements on numerical reasoning cases are even more notable: on TAT-QA, TaCube promotes the exact match accuracy of BART-large by 39.6% on sum, 52.5% on average, 36.6% on substraction, and 22.2% on division. We believe that TaCube is a general and portable pre-computation solution that can be potentially integrated to various numerical reasoning frameworks ","[{'version': 'v1', 'created': 'Wed, 25 May 2022 11:44:11 GMT'}]",2022-05-26,"[['Zhou', 'Fan', ''], ['Hu', 'Mengkang', ''], ['Dong', 'Haoyu', ''], ['Cheng', 'Zhoujun', ''], ['Han', 'Shi', ''], ['Zhang', 'Dongmei', '']]",0,0,2022-05-25,1,6,1,1,1,0,52b3087525b262f6f467453e22fdfa843353d40c,249062802.0,https://www.semanticscholar.org/paper/52b3087525b262f6f467453e22fdfa843353d40c,Conference on Empirical Methods in Natural Language Processing,2022.0,39.0,12.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2153433679', 'name': 'Fan Zhou'}, {'authorId': '2166455111', 'name': 'Mengkang Hu'}, {'authorId': '2113413583', 'name': 'Haoyu Dong'}, {'authorId': '1471878967', 'name': 'Zhoujun Cheng'}, {'authorId': '2109750123', 'name': 'Shi Han'}, {'authorId': '2140415600', 'name': 'Dongmei Zhang'}]","['Harbin Institute of Technology', 'Shanghai Jiao Tong University', 'Microsoft']","['China', 'India']",2022-05
2205.12986,Kaitao Song,"Kaitao Song, Yichong Leng, Xu Tan, Yicheng Zou, Tao Qin, Dongsheng Li",Transcormer: Transformer for Sentence Scoring with Sliding Language Modeling,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Sentence scoring aims at measuring the likelihood score of a sentence and is widely used in many natural language processing scenarios, like reranking, which is to select the best sentence from multiple candidates. Previous works on sentence scoring mainly adopted either causal language modeling (CLM) like GPT or masked language modeling (MLM) like BERT, which have some limitations: 1) CLM only utilizes unidirectional information for the probability estimation of a sentence without considering bidirectional context, which affects the scoring quality; 2) MLM can only estimate the probability of partial tokens at a time and thus requires multiple forward passes to estimate the probability of the whole sentence, which incurs large computation and time cost. In this paper, we propose \textit{Transcormer} -- a Transformer model with a novel \textit{sliding language modeling} (SLM) for sentence scoring. Specifically, our SLM adopts a triple-stream self-attention mechanism to estimate the probability of all tokens in a sentence with bidirectional context and only requires a single forward pass. SLM can avoid the limitations of CLM (only unidirectional context) and MLM (multiple forward passes) and inherit their advantages, and thus achieve high effectiveness and efficiency in scoring. Experimental results on multiple tasks demonstrate that our method achieves better performance than other language modelings. ","[{'version': 'v1', 'created': 'Wed, 25 May 2022 18:00:09 GMT'}, {'version': 'v2', 'created': 'Sat, 28 May 2022 09:04:28 GMT'}, {'version': 'v3', 'created': 'Sun, 5 Jun 2022 14:55:32 GMT'}, {'version': 'v4', 'created': 'Wed, 19 Oct 2022 03:15:21 GMT'}]",2022-10-20,"[['Song', 'Kaitao', ''], ['Leng', 'Yichong', ''], ['Tan', 'Xu', ''], ['Zou', 'Yicheng', ''], ['Qin', 'Tao', ''], ['Li', 'Dongsheng', '']]",0,1,2022-05-25,4,6,1,0,0,0,49c7599fad96dbb571f7b2e9d6b6c733b4bf6d6d,249097571.0,https://www.semanticscholar.org/paper/49c7599fad96dbb571f7b2e9d6b6c733b4bf6d6d,Neural Information Processing Systems,2022.0,70.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '50982078', 'name': 'Kaitao Song'}, {'authorId': '148398655', 'name': 'Yichong Leng'}, {'authorId': '48391466', 'name': 'Xu Tan'}, {'authorId': '51192034', 'name': 'Yicheng Zou'}, {'authorId': '143826491', 'name': 'Tao Qin'}, {'authorId': '2119081394', 'name': 'Dongsheng Li'}]","['University of Science and Technology of China', 'Microsoft']",['China'],2022-05
2205.14953,Muning Wen,"Muning Wen, Jakub Grudzien Kuba, Runji Lin, Weinan Zhang, Ying Wen,
  Jun Wang and Yaodong Yang",Multi-Agent Reinforcement Learning is a Sequence Modeling Problem,,,,,cs.MA cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large sequence model (SM) such as GPT series and BERT has displayed outstanding performance and generalization capabilities on vision, language, and recently reinforcement learning tasks. A natural follow-up question is how to abstract multi-agent decision making into an SM problem and benefit from the prosperous development of SMs. In this paper, we introduce a novel architecture named Multi-Agent Transformer (MAT) that effectively casts cooperative multi-agent reinforcement learning (MARL) into SM problems wherein the task is to map agents' observation sequence to agents' optimal action sequence. Our goal is to build the bridge between MARL and SMs so that the modeling power of modern sequence models can be unleashed for MARL. Central to our MAT is an encoder-decoder architecture which leverages the multi-agent advantage decomposition theorem to transform the joint policy search problem into a sequential decision making process; this renders only linear time complexity for multi-agent problems and, most importantly, endows MAT with monotonic performance improvement guarantee. Unlike prior arts such as Decision Transformer fit only pre-collected offline data, MAT is trained by online trials and errors from the environment in an on-policy fashion. To validate MAT, we conduct extensive experiments on StarCraftII, Multi-Agent MuJoCo, Dexterous Hands Manipulation, and Google Research Football benchmarks. Results demonstrate that MAT achieves superior performance and data efficiency compared to strong baselines including MAPPO and HAPPO. Furthermore, we demonstrate that MAT is an excellent few-short learner on unseen tasks regardless of changes in the number of agents. See our project page at https://sites.google.com/view/multi-agent-transformer. ","[{'version': 'v1', 'created': 'Mon, 30 May 2022 09:39:45 GMT'}, {'version': 'v2', 'created': 'Tue, 28 Jun 2022 13:55:00 GMT'}, {'version': 'v3', 'created': 'Fri, 28 Oct 2022 10:41:49 GMT'}]",2022-10-31,"[['Wen', 'Muning', ''], ['Kuba', 'Jakub Grudzien', ''], ['Lin', 'Runji', ''], ['Zhang', 'Weinan', ''], ['Wen', 'Ying', ''], ['Wang', 'Jun', ''], ['Yang', 'Yaodong', '']]",0,1,2022-05-30,3,7,2,0,0,0,5bc0a77d99831f8303d6755719c2447038061189,249192420.0,https://www.semanticscholar.org/paper/5bc0a77d99831f8303d6755719c2447038061189,Neural Information Processing Systems,2022.0,47.0,49.0,6.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2111875607', 'name': 'Muning Wen'}, {'authorId': '2126704872', 'name': 'J. Kuba'}, {'authorId': '2167032295', 'name': 'Runji Lin'}, {'authorId': '2108309275', 'name': 'Weinan Zhang'}, {'authorId': '50531782', 'name': 'Ying Wen'}, {'authorId': '66063792', 'name': 'J. Wang'}, {'authorId': '47796324', 'name': 'Yaodong Yang'}]","['University College London', 'Beijing Institute of Technology', 'Chinese Academy of Sciences', 'Shanghai Jiao Tong University', 'University of Oxford', 'Digital Brain Lab,', 'Peking University']","['China', 'United Kingdom']",2022-05
2205.15868,Ming Ding,"Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Jie Tang",CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers,,,,,cs.CV cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large-scale pretrained transformers have created milestones in text (GPT-3) and text-to-image (DALL-E and CogView) generation. Its application to video generation is still facing many challenges: The potential huge computation cost makes the training from scratch unaffordable; The scarcity and weak relevance of text-video datasets hinder the model understanding complex movement semantics. In this work, we present 9B-parameter transformer CogVideo, trained by inheriting a pretrained text-to-image model, CogView2. We also propose multi-frame-rate hierarchical training strategy to better align text and video clips. As (probably) the first open-source large-scale pretrained text-to-video model, CogVideo outperforms all publicly available models at a large margin in machine and human evaluations. ","[{'version': 'v1', 'created': 'Sun, 29 May 2022 19:02:15 GMT'}]",2022-06-01,"[['Hong', 'Wenyi', ''], ['Ding', 'Ming', ''], ['Zheng', 'Wendi', ''], ['Liu', 'Xinghan', ''], ['Tang', 'Jie', '']]",0,1,2022-05-29,1,5,3,1,0,1,707bd332d2c21dc5eb1f02a52d4a0506199aae76,249209614.0,https://www.semanticscholar.org/paper/707bd332d2c21dc5eb1f02a52d4a0506199aae76,International Conference on Learning Representations,2022.0,45.0,138.0,28.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2105844599', 'name': 'Wenyi Hong'}, {'authorId': '2055623340', 'name': 'Ming Ding'}, {'authorId': '2163967642', 'name': 'Wendi Zheng'}, {'authorId': '46522721', 'name': 'Xinghan Liu'}, {'authorId': '2148911990', 'name': 'Jie Tang'}]",['Tsinghua University'],['China'],2022-05
2205.15953,David Mguni,"David Mguni, Aivar Sootla, Juliusz Ziomek, Oliver Slumbers, Zipeng
  Dai, Kun Shao, Jun Wang",Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints,,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many real-world settings involve costs for performing actions; transaction costs in financial systems and fuel costs being common examples. In these settings, performing actions at each time step quickly accumulates costs leading to vastly suboptimal outcomes. Additionally, repeatedly acting produces wear and tear and ultimately, damage. Determining \textit{when to act} is crucial for achieving successful outcomes and yet, the challenge of efficiently \textit{learning} to behave optimally when actions incur minimally bounded costs remains unresolved. In this paper, we introduce a reinforcement learning (RL) framework named \textbf{L}earnable \textbf{I}mpulse \textbf{C}ontrol \textbf{R}einforcement \textbf{A}lgorithm (LICRA), for learning to optimally select both when to act and which actions to take when actions incur costs. At the core of LICRA is a nested structure that combines RL and a form of policy known as \textit{impulse control} which learns to maximise objectives when actions incur costs. We prove that LICRA, which seamlessly adopts any RL method, converges to policies that optimally select when to perform actions and their optimal magnitudes. We then augment LICRA to handle problems in which the agent can perform at most $k<\infty$ actions and more generally, faces a budget constraint. We show LICRA learns the optimal value function and ensures budget constraints are satisfied almost surely. We demonstrate empirically LICRA's superior performance against benchmark RL methods in OpenAI gym's \textit{Lunar Lander} and in \textit{Highway} environments and a variant of the Merton portfolio problem within finance. ","[{'version': 'v1', 'created': 'Tue, 31 May 2022 16:50:46 GMT'}, {'version': 'v2', 'created': 'Mon, 6 Jun 2022 12:26:45 GMT'}, {'version': 'v3', 'created': 'Wed, 17 May 2023 01:41:04 GMT'}, {'version': 'v4', 'created': 'Sun, 4 Jun 2023 23:59:39 GMT'}]",2023-06-06,"[['Mguni', 'David', ''], ['Sootla', 'Aivar', ''], ['Ziomek', 'Juliusz', ''], ['Slumbers', 'Oliver', ''], ['Dai', 'Zipeng', ''], ['Shao', 'Kun', ''], ['Wang', 'Jun', '']]",0,0,2022-05-31,4,7,1,0,0,0,2bb383228065a17cbb3de273d2572e2097115612,249209650.0,https://www.semanticscholar.org/paper/2bb383228065a17cbb3de273d2572e2097115612,International Conference on Learning Representations,2022.0,41.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Economics', 'source': 's2-fos-model'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '41127915', 'name': 'D. Mguni'}, {'authorId': '144424020', 'name': 'Aivar Sootla'}, {'authorId': '1884921320', 'name': 'Juliusz Ziomek'}, {'authorId': '2050145993', 'name': 'Oliver Slumbers'}, {'authorId': '1382484054', 'name': 'Zipeng Dai'}, {'authorId': '1704229853', 'name': 'Kun Shao'}, {'authorId': '66063792', 'name': 'J. Wang'}]","['University College London', 'Huawei Technologies (China)']","['China', 'United Kingdom']",2022-05
2206.02369,Jin Xu,"Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, Jian Li",Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation,"Accepted by NeurIPS 2022. Code is released at
  https://github.com/Jxu-Thu/DITTO",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  While large-scale neural language models, such as GPT2 and BART, have achieved impressive results on various text generation tasks, they tend to get stuck in undesirable sentence-level loops with maximization-based decoding algorithms (\textit{e.g.}, greedy search). This phenomenon is counter-intuitive since there are few consecutive sentence-level repetitions in human corpora (e.g., 0.02\% in Wikitext-103). To investigate the underlying reasons for generating consecutive sentence-level repetitions, we study the relationship between the probabilities of the repetitive tokens and their previous repetitions in the context. Through our quantitative experiments, we find that 1) Language models have a preference to repeat the previous sentence; 2) The sentence-level repetitions have a \textit{self-reinforcement effect}: the more times a sentence is repeated in the context, the higher the probability of continuing to generate that sentence; 3) The sentences with higher initial probabilities usually have a stronger self-reinforcement effect. Motivated by our findings, we propose a simple and effective training method \textbf{DITTO} (Pseu\underline{D}o-Repet\underline{IT}ion Penaliza\underline{T}i\underline{O}n), where the model learns to penalize probabilities of sentence-level repetitions from pseudo repetitive data. Although our method is motivated by mitigating repetitions, experiments show that DITTO not only mitigates the repetition issue without sacrificing perplexity, but also achieves better generation quality. Extensive experiments on open-ended text generation (Wikitext-103) and text summarization (CNN/DailyMail) demonstrate the generality and effectiveness of our method. ","[{'version': 'v1', 'created': 'Mon, 6 Jun 2022 05:51:12 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Oct 2022 07:53:01 GMT'}]",2022-10-11,"[['Xu', 'Jin', ''], ['Liu', 'Xiaojiang', ''], ['Yan', 'Jianhao', ''], ['Cai', 'Deng', ''], ['Li', 'Huayang', ''], ['Li', 'Jian', '']]",0,1,2022-06-06,2,6,1,1,1,0,6151ee4af6a3fe78f2df7c605598cd9e02b23c5b,249395390.0,https://www.semanticscholar.org/paper/6151ee4af6a3fe78f2df7c605598cd9e02b23c5b,Neural Information Processing Systems,2022.0,44.0,24.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2110641353', 'name': 'Jin Xu'}, {'authorId': '3028405', 'name': 'Xiaojiang Liu'}, {'authorId': '134233854', 'name': 'Jianhao Yan'}, {'authorId': '2053327987', 'name': 'Deng Cai'}, {'authorId': '91956362', 'name': 'Huayang Li'}, {'authorId': '2151968158', 'name': 'Jian Li'}]","['Westlake University', 'Apple', 'Chinese University of Hong Kong', 'Tsinghua University']","['China', 'United States']",2022-06
2206.06009,Lei Han,"Lei Han, Jiawei Xu, Cheng Zhou, Yizheng Zhang, Zhengyou Zhang",Relative Policy-Transition Optimization for Fast Policy Transfer,,,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We consider the problem of policy transfer between two Markov Decision Processes (MDPs). We introduce a lemma based on existing theoretical results in reinforcement learning (RL) to measure the relativity between two arbitrary MDPs, that is the difference between any two cumulative expected returns defined on different policies and environment dynamics. Based on this lemma, we propose two new algorithms referred to as Relative Policy Optimization (RPO) and Relative Transition Optimization (RTO), which can offer fast policy transfer and dynamics modeling, respectively. RPO updates the policy using the relative policy gradient to transfer the policy evaluated in one environment to maximize the return in another, while RTO updates the parameterized dynamics model (if there exists) using the relative transition gradient to reduce the gap between the dynamics of the two environments. Then, integrating the two algorithms offers the complete algorithm Relative Policy-Transition Optimization (RPTO), in which the policy interacts with the two environments simultaneously, such that data collections from two environments, policy and transition updates are completed in one closed loop to form a principled learning framework for policy transfer. We demonstrate the effectiveness of RPTO in OpenAI gym's classic control tasks by creating policy transfer problems via variant dynamics. ","[{'version': 'v1', 'created': 'Mon, 13 Jun 2022 09:55:04 GMT'}]",2022-06-14,"[['Han', 'Lei', ''], ['Xu', 'Jiawei', ''], ['Zhou', 'Cheng', ''], ['Zhang', 'Yizheng', ''], ['Zhang', 'Zhengyou', '']]",0,0,2022-06-13,1,5,2,0,0,0,478c1c3b3b344c22bc7bc6cd41f21ecd4ea6f261,249626165.0,https://www.semanticscholar.org/paper/478c1c3b3b344c22bc7bc6cd41f21ecd4ea6f261,arXiv.org,2022.0,25.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2112661118', 'name': 'Lei Han'}, {'authorId': '2110476942', 'name': 'Jiawei Xu'}, {'authorId': '2111168222', 'name': 'Cheng Zhou'}, {'authorId': '1591122785', 'name': 'Yizheng Zhang'}, {'authorId': '2148905709', 'name': 'Zhengyou Zhang'}]","['Chinese University of Hong Kong, Shenzhen', 'Tencent']",['China'],2022-06
2206.06888,Daoguang Zan,"Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan,
  Yongji Wang, Weizhu Chen, Jian-Guang Lou",CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation,Accepted for publication at IJCAI-ECAI 2022,,,,cs.SE cs.CL cs.PL,http://creativecommons.org/licenses/by/4.0/,"  Code generation is a longstanding challenge, aiming to generate a code snippet based on a natural language description. Usually, expensive text-code paired data is essential for training a code generation model. Recently, thanks to the success of pre-training techniques, large language models are trained on large-scale unlabelled code corpora and perform well in code generation. In this paper, we investigate how to leverage an unlabelled code corpus to train a model for library-oriented code generation. Since it is a common practice for programmers to reuse third-party libraries, in which case the text-code paired data are harder to obtain due to the huge number of libraries. We observe that library-oriented code snippets are more likely to share similar code sketches. Hence, we present CERT with two steps: a sketcher generates the sketch, then a generator fills the details in the sketch. Both the sketcher and the generator are continually pre-trained upon a base model using unlabelled data. Furthermore, we craft two benchmarks named PandasEval and NumpyEval to evaluate library-oriented code generation. Experimental results demonstrate the impressive performance of CERT. For example, it surpasses the base model by an absolute 15.67% improvement in terms of pass@1 on PandasEval. Our work is available at https://github.com/microsoft/PyCodeGPT. ","[{'version': 'v1', 'created': 'Tue, 14 Jun 2022 14:44:34 GMT'}]",2022-06-15,"[['Zan', 'Daoguang', ''], ['Chen', 'Bei', ''], ['Yang', 'Dejian', ''], ['Lin', 'Zeqi', ''], ['Kim', 'Minsu', ''], ['Guan', 'Bei', ''], ['Wang', 'Yongji', ''], ['Chen', 'Weizhu', ''], ['Lou', 'Jian-Guang', '']]",0,1,2022-06-14,1,9,3,0,0,0,a08a3b08a5a1de6462a7da2906b1cd81691d6c18,249642442.0,https://www.semanticscholar.org/paper/a08a3b08a5a1de6462a7da2906b1cd81691d6c18,International Joint Conference on Artificial Intelligence,2022.0,27.0,36.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2134434187', 'name': 'Daoguang Zan'}, {'authorId': None, 'name': 'Bei Chen'}, {'authorId': '2111181262', 'name': 'Dejian Yang'}, {'authorId': '2284174', 'name': 'Zeqi Lin'}, {'authorId': '2174816942', 'name': 'Minsu Kim'}, {'authorId': '36923691', 'name': 'Bei Guan'}, {'authorId': '2108097250', 'name': 'Yongji Wang'}, {'authorId': '2109136147', 'name': 'Weizhu Chen'}, {'authorId': '153249455', 'name': 'Jian-Guang Lou'}]","['University of Chinese Academy of Sciences', 'Microsoft', 'Chinese Academy of Sciences', 'Korea University']","['China', 'United States', 'South Korea']",2022-06
2206.07699,Shizhe Diao,"Shizhe Diao, Wangchunshu Zhou, Xinsong Zhang, Jiawei Wang",Write and Paint: Generative Vision-Language Models are Unified Modal Learners,ICLR 2023,,,,cs.CV cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advances in vision-language pre-training have pushed the state-of-the-art on various vision-language tasks, making machines more capable of multi-modal writing (image-to-text generation) and painting (text-to-image generation). However, few studies investigate if these two essential capabilities can be learned together and boost each other, making a versatile and powerful multi-modal foundation model. In this work, we disclose the potential of symmetric generative vision-language pre-training in learning to write and paint concurrently, and propose a new unified modal model, named DaVinci, trained with prefix language modeling and prefix image modeling, a simple generative self-supervised objective on image-text pairs. Thanks to the proposed prefix multi-modal modeling framework, DaVinci is simple to train, scalable to huge data, adaptable to both writing and painting tasks, and also strong on other vision, text, and multi-modal understanding tasks. DaVinci achieves competitive performance on a wide range of 27 generation/understanding tasks and demonstrates the superiority of combining vision/language generative pre-training. Furthermore, we carefully benchmark the performance of different vision-language pre-training objectives on different scales of pre-training datasets on a heterogeneous and broad distribution coverage. Our results demonstrate the potential of exploiting self-supervision in both language and vision inputs, and establish new, stronger baselines for future comparisons at different data scales. The code and pre-trained models are available at https://github.com/shizhediao/DaVinci. ","[{'version': 'v1', 'created': 'Wed, 15 Jun 2022 17:49:38 GMT'}, {'version': 'v2', 'created': 'Thu, 16 Feb 2023 17:01:44 GMT'}, {'version': 'v3', 'created': 'Fri, 17 Feb 2023 02:58:03 GMT'}]",2023-02-20,"[['Diao', 'Shizhe', ''], ['Zhou', 'Wangchunshu', ''], ['Zhang', 'Xinsong', ''], ['Wang', 'Jiawei', '']]",0,1,2022-06-15,3,4,3,0,0,0,ef57279f080e6e305c5cd216fd8f22ade2c903da,256901258.0,https://www.semanticscholar.org/paper/ef57279f080e6e305c5cd216fd8f22ade2c903da,International Conference on Learning Representations,2022.0,118.0,6.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '50826757', 'name': 'Shizhe Diao'}, {'authorId': '150341221', 'name': 'Wangchunshu Zhou'}, {'authorId': '47957145', 'name': 'Xinsong Zhang'}, {'authorId': '2110132746', 'name': 'Jiawei Wang'}]","['ByteDance', 'Shanghai Jiao Tong University', 'Hong Kong University of Science and Technology']",['China'],2022-06
2206.10265,Yufei Wang,"Yufei Wang, Jiayi Zheng, Can Xu, Xiubo Geng, Tao Shen, Chongyang Tao,
  Daxin Jiang",KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Low-Resource NLP,"Accepted by ICLR 2023 main track at
  https://openreview.net/forum?id=2nocgE1m0A",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  This paper focuses on the data augmentation for low-resource NLP tasks where the training set is limited. The existing solutions either leverage task-independent heuristic rules (e.g., Synonym Replacement) or fine-tune general-purpose pre-trained language models (e.g., GPT2) using the limited training instances to produce new synthetic data. Consequently, they have trivial task-specific knowledge and are limited to yielding low-quality synthetic data. To combat this issue, we propose Knowledge Mixture Data Augmentation Model (KnowDA) which is an Seq2Seq language model pre-trained on a mixture of diverse NLP tasks under a novel framework of Knowledge Mixture Training (KoMT). The goal of KoMT is to condense diverse NLP task-specific knowledge into the single KnowDA model (i.e., all-in-one) such that KnowDA could utilize these knowledge to quickly grasp the inherent synthesis law of the target task through limited training instances. Specifically, KoMT reformulates input examples from various heterogeneous NLP tasks into a unified text-to-text format, and employs denoising training objectives in different granularity to learn to reconstruct partial or complete samples. To the best of our knowledge, we are the first attempt to apply 100+ NLP multi-task training for data augmentation. Extensive experiments show that i) the synthetic data produced by KnowDA successfully improves performance of the strong pre-trained language models (i.e., Bert, ALBert and Deberta) by a large margin on the low-resource NLP benchmark FewGLUE, CoNLL'03 and WikiAnn; ii) KnowDA successfully transfers the task knowledge to NLP tasks whose types are seen and unseen in KoMT. ","[{'version': 'v1', 'created': 'Tue, 21 Jun 2022 11:34:02 GMT'}, {'version': 'v2', 'created': 'Fri, 27 Jan 2023 11:11:58 GMT'}]",2023-01-30,"[['Wang', 'Yufei', ''], ['Zheng', 'Jiayi', ''], ['Xu', 'Can', ''], ['Geng', 'Xiubo', ''], ['Shen', 'Tao', ''], ['Tao', 'Chongyang', ''], ['Jiang', 'Daxin', '']]",0,1,2022-06-21,2,7,1,1,1,0,e852c5ed7f4f1a4375f0f3bce1b4e445c7684e7e,256358906.0,https://www.semanticscholar.org/paper/e852c5ed7f4f1a4375f0f3bce1b4e445c7684e7e,International Conference on Learning Representations,2022.0,118.0,2.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2143696239', 'name': 'Yufei Wang'}, {'authorId': '1820918457', 'name': 'Jiayi Zheng'}, {'authorId': '2110091832', 'name': 'Can Xu'}, {'authorId': '2442662', 'name': 'Xiubo Geng'}, {'authorId': '143681703', 'name': 'Tao Shen'}, {'authorId': '8801869', 'name': 'Chongyang Tao'}, {'authorId': '2086994543', 'name': 'Daxin Jiang'}]","['Peking University', 'Macquarie University', 'Microsoft']","['China', 'Australia']",2022-06
2206.11064,Ning Gui Prof. dr.,"Jiawen Wei, Fangyuan Wang, Wanxin Zeng, Wenwei Lin and Ning Gui",An Embedded Feature Selection Framework for Control,"9 pages, 9 figures, accepted by SIGKDD 2022",,10.1145/3534678.3539290,,cs.LG cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Reducing sensor requirements while keeping optimal control performance is crucial to many industrial control applications to achieve robust, low-cost, and computation-efficient controllers. However, existing feature selection solutions for the typical machine learning domain can hardly be applied in the domain of control with changing dynamics. In this paper, a novel framework, namely the Dual-world embedded Attentive Feature Selection (D-AFS), can efficiently select the most relevant sensors for the system under dynamic control. Rather than the one world used in most Deep Reinforcement Learning (DRL) algorithms, D-AFS has both the real world and its virtual peer with twisted features. By analyzing the DRL's response in two worlds, D-AFS can quantitatively identify respective features' importance towards control. A well-known active flow control problem, cylinder drag reduction, is used for evaluation. Results show that D-AFS successfully finds an optimized five-probes layout with 18.7\% drag reduction than the state-of-the-art solution with 151 probes and 49.2\% reduction than five-probes layout by human experts. We also apply this solution to four OpenAI classical control cases. In all cases, D-AFS achieves the same or better sensor configurations than originally provided solutions. Results highlight, we argued, a new way to achieve efficient and optimal sensor designs for experimental or industrial systems. Our source codes are made publicly available at https://github.com/G-AILab/DAFSFluid. ","[{'version': 'v1', 'created': 'Sun, 19 Jun 2022 07:03:40 GMT'}]",2022-06-23,"[['Wei', 'Jiawen', ''], ['Wang', 'Fangyuan', ''], ['Zeng', 'Wanxin', ''], ['Lin', 'Wenwei', ''], ['Gui', 'Ning', '']]",0,0,2022-06-19,1,5,2,0,0,0,f03ae909e81f0489bee1cfe6c20664e47edcc4f3,249926422.0,https://www.semanticscholar.org/paper/f03ae909e81f0489bee1cfe6c20664e47edcc4f3,Knowledge Discovery and Data Mining,2022.0,35.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '145604950', 'name': 'Jiawen Wei'}, {'authorId': '2145902572', 'name': 'Fangyuan Wang'}, {'authorId': '3013856', 'name': 'Wan-Ting Zeng'}, {'authorId': '49661300', 'name': 'Wen-Jiun Lin'}, {'authorId': '1780829', 'name': 'Ning Gui'}]","['New York, NY, USA, 10 pages.', 'Central South University', 'Zhejiang Sci-Tech University']","['China', 'United States']",2022-06
2206.11494,Tairan Huang,"Tairan Huang, Xu Li, Hao Li, Mingming Sun, Ping Li",CGAR: Critic Guided Action Redistribution in Reinforcement Leaning,"IEEE Conference on Games (CoG), 2022",,,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Training a game-playing reinforcement learning agent requires multiple interactions with the environment. Ignorant random exploration may cause a waste of time and resources. It's essential to alleviate such waste. As discussed in this paper, under the settings of the off-policy actor critic algorithms, we demonstrate that the critic can bring more expected discounted rewards than or at least equal to the actor. Thus, the Q value predicted by the critic is a better signal to redistribute the action originally sampled from the policy distribution predicted by the actor. This paper introduces the novel Critic Guided Action Redistribution (CGAR) algorithm and tests it on the OpenAI MuJoCo tasks. The experimental results demonstrate that our method improves the sample efficiency and achieves state-of-the-art performance. Our code can be found at https://github.com/tairanhuang/CGAR. ","[{'version': 'v1', 'created': 'Thu, 23 Jun 2022 06:33:14 GMT'}]",2022-06-24,"[['Huang', 'Tairan', ''], ['Li', 'Xu', ''], ['Li', 'Hao', ''], ['Sun', 'Mingming', ''], ['Li', 'Ping', '']]",0,0,2022-06-23,1,5,1,0,0,0,c5db0f5620a9d7be50248ef90ba140cfca7682da,249954000.0,https://www.semanticscholar.org/paper/c5db0f5620a9d7be50248ef90ba140cfca7682da,2022 IEEE Conference on Games (CoG),2022.0,16.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '51932854', 'name': 'Tairan Huang'}, {'authorId': '2116327105', 'name': 'Xu Li'}, {'authorId': '2145537996', 'name': 'Haoyuan Li'}, {'authorId': '1893044063', 'name': 'Mingming Sun'}, {'authorId': '2420746', 'name': 'P. Li'}]","['Peking University', 'Beihang University', 'Baidu']","['China', 'United States']",2022-06
2206.12045,Jiajun Deng,"Jiajun Deng, Xurong Xie, Tianzi Wang, Mingyu Cui, Boyang Xue, Zengrui
  Jin, Mengzhe Geng, Guinan Li, Xunying Liu, Helen Meng",Confidence Score Based Conformer Speaker Adaptation for Speech Recognition,"It's accepted to INTERSPEECH 2022. arXiv admin note: text overlap
  with arXiv:2206.11596",,,,eess.AS cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A key challenge for automatic speech recognition (ASR) systems is to model the speaker level variability. In this paper, compact speaker dependent learning hidden unit contributions (LHUC) are used to facilitate both speaker adaptive training (SAT) and test time unsupervised speaker adaptation for state-of-the-art Conformer based end-to-end ASR systems. The sensitivity during adaptation to supervision error rate is reduced using confidence score based selection of the more ""trustworthy"" subset of speaker specific data. A confidence estimation module is used to smooth the over-confident Conformer decoder output probabilities before serving as confidence scores. The increased data sparsity due to speaker level data selection is addressed using Bayesian estimation of LHUC parameters. Experiments on the 300-hour Switchboard corpus suggest that the proposed LHUC-SAT Conformer with confidence score based test time unsupervised adaptation outperformed the baseline speaker independent and i-vector adapted Conformer systems by up to 1.0%, 1.0%, and 1.2% absolute (9.0%, 7.9%, and 8.9% relative) word error rate (WER) reductions on the NIST Hub5'00, RT02, and RT03 evaluation sets respectively. Consistent performance improvements were retained after external Transformer and LSTM language models were used for rescoring. ","[{'version': 'v1', 'created': 'Fri, 24 Jun 2022 02:48:00 GMT'}]",2022-06-27,"[['Deng', 'Jiajun', ''], ['Xie', 'Xurong', ''], ['Wang', 'Tianzi', ''], ['Cui', 'Mingyu', ''], ['Xue', 'Boyang', ''], ['Jin', 'Zengrui', ''], ['Geng', 'Mengzhe', ''], ['Li', 'Guinan', ''], ['Liu', 'Xunying', ''], ['Meng', 'Helen', '']]",0,0,2022-06-24,1,10,2,0,0,0,a2febd2b78c8cf7e76b12aa7c210d461ad5599b7,250048413.0,https://www.semanticscholar.org/paper/a2febd2b78c8cf7e76b12aa7c210d461ad5599b7,Interspeech,2022.0,52.0,9.0,0.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2153627064', 'name': 'Jiajun Deng'}, {'authorId': '3064192', 'name': 'Xurong Xie'}, {'authorId': '2118915448', 'name': 'Tianzi Wang'}, {'authorId': '2055099014', 'name': 'Mingyu Cui'}, {'authorId': '30790846', 'name': 'Boyang Xue'}, {'authorId': '2125082024', 'name': 'Zengrui Jin'}, {'authorId': '1654167366', 'name': 'Mengzhe Geng'}, {'authorId': '2108696929', 'name': 'Guinan Li'}, {'authorId': '150344273', 'name': 'Xunying Liu'}, {'authorId': '2057833292', 'name': 'Helen M. Meng'}]","['Chinese University of Hong Kong', 'Chinese Academy of Sciences']",['China'],2022-06
2206.12131,Tianyi Tang,"Tianyi Tang, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen",MVP: Multi-task Supervised Pre-training for Natural Language Generation,Accepted by ACL 2023,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained language models (PLMs) have achieved remarkable success in natural language generation (NLG) tasks. Up to now, most NLG-oriented PLMs are pre-trained in an unsupervised manner using the large-scale general corpus. In the meanwhile, an increasing number of models pre-trained with labeled data (i.e. ""supervised pre-training"") showcase superior performance compared to unsupervised pre-trained models. Motivated by the success of supervised pre-training, we propose Multi-task superVised Pre-training (MVP) for natural language generation. We collect a large-scale natural language generation corpus, MVPCorpus, from $77$ datasets over $11$ diverse NLG tasks. Then we unify these examples into a general text-to-text format to pre-train the text generation model MVP in a supervised manner. For each task, we further pre-train specific soft prompts to stimulate the model's capacity to perform a specific task. Our MVP model can be seen as a practice that utilizes recent instruction tuning on relatively small PLMs. Extensive experiments have demonstrated the effectiveness and generality of our MVP model in a number of NLG tasks, which achieves state-of-the-art performance on $13$ out of $17$ datasets, outperforming BART by $9.3\%$ and Flan-T5 by $5.8\%$. ","[{'version': 'v1', 'created': 'Fri, 24 Jun 2022 07:49:47 GMT'}, {'version': 'v2', 'created': 'Mon, 19 Dec 2022 11:44:38 GMT'}, {'version': 'v3', 'created': 'Sun, 28 May 2023 14:41:31 GMT'}]",2023-05-30,"[['Tang', 'Tianyi', ''], ['Li', 'Junyi', ''], ['Zhao', 'Wayne Xin', ''], ['Wen', 'Ji-Rong', '']]",0,0,2022-06-24,3,4,1,3,2,1,5b0855b9b9361839844ae86277e8189a957d9d23,250048838.0,https://www.semanticscholar.org/paper/5b0855b9b9361839844ae86277e8189a957d9d23,Annual Meeting of the Association for Computational Linguistics,2022.0,200.0,14.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1997234792', 'name': 'Tianyi Tang'}, {'authorId': '2018027', 'name': 'Junyi Li'}, {'authorId': '2542603', 'name': 'Wayne Xin Zhao'}, {'authorId': '153693432', 'name': 'Ji-rong Wen'}]","['Beijing Key Laboratory of Big Data Management and Analysis Methods', 'Universit de Montral', 'Renmin University of China']","['Canada', 'China']",2022-06
2206.13778,Fan Xu,Fan Xu and Yunxiang Zhang and Xiaojun Wan,CC-Riddle: A Question Answering Dataset of Chinese Character Riddles,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The Chinese character riddle is a unique form of cultural entertainment specific to the Chinese language. It typically comprises two parts: the riddle description and the solution. The solution to the riddle is a single character, while the riddle description primarily describes the glyph of the solution, occasionally supplemented with its explanation and pronunciation. Solving Chinese character riddles is a challenging task that demands understanding of character glyph, general knowledge, and a grasp of figurative language. In this paper, we construct a \textbf{C}hinese \textbf{C}haracter riddle dataset named CC-Riddle, which covers the majority of common simplified Chinese characters. The construction process is a combination of web crawling, language model generation and manual filtering. In generation stage, we input the Chinese phonetic alphabet, glyph and meaning of the solution character into the generation model, which then produces multiple riddle descriptions. The generated riddles are then manually filtered and the final CC-Riddle dataset is composed of both human-written riddles and these filtered, generated riddles. In order to assess the performance of language models on the task of solving character riddles, we use retrieval-based, generative and multiple-choice QA strategies to test three language models: BERT, ChatGPT and ChatGLM. The test results reveal that current language models still struggle to solve Chinese character riddles. CC-Riddle is publicly available at \url{https://github.com/pku0xff/CC-Riddle}. ","[{'version': 'v1', 'created': 'Tue, 28 Jun 2022 06:23:13 GMT'}, {'version': 'v2', 'created': 'Sun, 24 Sep 2023 05:15:51 GMT'}]",2023-09-26,"[['Xu', 'Fan', ''], ['Zhang', 'Yunxiang', ''], ['Wan', 'Xiaojun', '']]",1,1,2022-06-28,2,3,1,2,1,1,db4906c7cc08cfd324bcc8a78a8faa747b78ddff,249172717.0,https://www.semanticscholar.org/paper/db4906c7cc08cfd324bcc8a78a8faa747b78ddff,arXiv.org,2022.0,29.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2111431951', 'name': 'Fan Xu'}, {'authorId': '2124948532', 'name': 'Yunxiang Zhang'}, {'authorId': '9714242', 'name': 'Xiao-Yi Wan'}]","['Peking University', 'University of MichiganAnn Arbor']","['China', 'United States']",2022-06
2208.00399,Damai Dai,"Damai Dai, Wenbin Jiang, Qingxiu Dong, Yajuan Lyu, Qiaoqiao She,
  Zhifang Sui",Neural Knowledge Bank for Pretrained Transformers,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The ability of pretrained Transformers to remember factual knowledge is essential but still limited for existing models. Inspired by existing work that regards Feed-Forward Networks (FFNs) in Transformers as key-value memories, we design a Neural Knowledge Bank (NKB) and a knowledge injection strategy to introduce extra factual knowledge for pretrained Transformers. The NKB is in the form of additional knowledgeable memory slots to the FFN and the memory-like architecture makes it highly interpretable and flexible. When injecting extra knowledge with the Salient Span Masking (SSM) pretraining objective, we fix the original pretrained model and train only the NKB. This training strategy makes sure the general language modeling ability of the original pretrained model is not influenced. By mounting the NKB onto the T5 model, we verify its strong ability to store extra factual knowledge based on three closed-book question answering datasets. Also, we prove that mounting the NKB will not degrade the general language modeling ability of T5 through two representative tasks, summarization and machine translation. Further, we thoroughly analyze the interpretability of the NKB and reveal the meaning of its keys and values in a human-readable way. Finally, we show the flexibility of the NKB by directly modifying its value vectors to update the factual knowledge stored in it. ","[{'version': 'v1', 'created': 'Sun, 31 Jul 2022 09:14:34 GMT'}, {'version': 'v2', 'created': 'Tue, 16 Aug 2022 09:29:18 GMT'}]",2022-08-17,"[['Dai', 'Damai', ''], ['Jiang', 'Wenbin', ''], ['Dong', 'Qingxiu', ''], ['Lyu', 'Yajuan', ''], ['She', 'Qiaoqiao', ''], ['Sui', 'Zhifang', '']]",0,0,2022-07-31,2,6,2,1,1,0,422d8c989adeb904563d0c96d5038f6c8596fa99,251223709.0,https://www.semanticscholar.org/paper/422d8c989adeb904563d0c96d5038f6c8596fa99,Natural Language Processing and Chinese Computing,2022.0,39.0,10.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '10780897', 'name': 'Damai Dai'}, {'authorId': '2155573246', 'name': 'Wen-Jie Jiang'}, {'authorId': '2047143813', 'name': 'Qingxiu Dong'}, {'authorId': '8020700', 'name': 'Yajuan Lyu'}, {'authorId': '40430110', 'name': 'Qiaoqiao She'}, {'authorId': '3335836', 'name': 'Zhifang Sui'}]","['Peking University', 'Baidu']",['China'],2022-07
2208.00638,Guangyi Liu,"Guangyi Liu, Zeyu Feng, Yuan Gao, Zichao Yang, Xiaodan Liang, Junwei
  Bao, Xiaodong He, Shuguang Cui, Zhen Li, Zhiting Hu",Composable Text Controls in Latent Space with ODEs,"27 Pages, Code: https://github.com/guangyliu/LatentOps",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Real-world text applications often involve composing a wide range of text control operations, such as editing the text w.r.t. an attribute, manipulating keywords and structure, and generating new text of desired properties. Prior work typically learns/finetunes a language model (LM) to perform individual or specific subsets of operations. Recent research has studied combining operations in a plug-and-play manner, often with costly search or optimization in the complex sequence space. This paper proposes a new efficient approach for composable text operations in the compact latent space of text. The low-dimensionality and differentiability of the text latent vector allow us to develop an efficient sampler based on ordinary differential equations (ODEs) given arbitrary plug-in operators (e.g., attribute classifiers). By connecting pretrained LMs (e.g., GPT2) to the latent space through efficient adaption, we then decode the sampled vectors into desired text sequences. The flexible approach permits diverse control operators (sentiment, tense, formality, keywords, etc.) acquired using any relevant data from different domains. Experiments show that composing those operators within our approach manages to generate or edit high-quality text, substantially improving over previous methods in terms of generation quality and efficiency. ","[{'version': 'v1', 'created': 'Mon, 1 Aug 2022 06:51:45 GMT'}, {'version': 'v2', 'created': 'Thu, 15 Sep 2022 06:16:01 GMT'}]",2022-09-16,"[['Liu', 'Guangyi', ''], ['Feng', 'Zeyu', ''], ['Gao', 'Yuan', ''], ['Yang', 'Zichao', ''], ['Liang', 'Xiaodan', ''], ['Bao', 'Junwei', ''], ['He', 'Xiaodong', ''], ['Cui', 'Shuguang', ''], ['Li', 'Zhen', ''], ['Hu', 'Zhiting', '']]",0,1,2022-08-01,2,10,2,1,1,0,291a32c20db76eab85d4f626d0a8c16bcf965379,252280511.0,https://www.semanticscholar.org/paper/291a32c20db76eab85d4f626d0a8c16bcf965379,,2022.0,65.0,9.0,2.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2152756082', 'name': 'Guangyi Liu'}, {'authorId': '35398411', 'name': 'Zeyu Feng'}, {'authorId': '2185168382', 'name': 'Yuan Gao'}, {'authorId': '8387085', 'name': 'Zichao Yang'}, {'authorId': '2153397698', 'name': 'Xiaodan Liang'}, {'authorId': '3299718', 'name': 'Junwei Bao'}, {'authorId': '144137069', 'name': 'Xiaodong He'}, {'authorId': '1745056', 'name': 'Shuguang Cui'}, {'authorId': '2110120874', 'name': 'Zhen Li'}, {'authorId': '2749311', 'name': 'Zhiting Hu'}]","['Jingdong', 'Carnegie Mellon University', 'DarkMatter AI Research,']","['China', 'United States']",2022-08
2208.06677,Xingyu Xie,Xingyu Xie and Pan Zhou and Huan Li and Zhouchen Lin and Shuicheng Yan,Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models,,,,,cs.LG math.OC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In deep learning, different kinds of deep networks typically need different optimizers, which have to be chosen after multiple trials, making the training process inefficient. To relieve this issue and consistently improve the model training speed across deep networks, we propose the ADAptive Nesterov momentum algorithm, Adan for short. Adan first reformulates the vanilla Nesterov acceleration to develop a new Nesterov momentum estimation (NME) method, which avoids the extra overhead of computing gradient at the extrapolation point. Then Adan adopts NME to estimate the gradient's first- and second-order moments in adaptive gradient algorithms for convergence acceleration. Besides, we prove that Adan finds an $\epsilon$-approximate first-order stationary point within $O(\epsilon^{-3.5})$ stochastic gradient complexity on the non-convex stochastic problems (e.g., deep learning problems), matching the best-known lower bound. Extensive experimental results show that Adan consistently surpasses the corresponding SoTA optimizers on vision, language, and RL tasks and sets new SoTAs for many popular networks and frameworks, e.g., ResNet, ConvNext, ViT, Swin, MAE, DETR, GPT-2, Transformer-XL, and BERT. More surprisingly, Adan can use half of the training cost (epochs) of SoTA optimizers to achieve higher or comparable performance on ViT, GPT-2, MAE, e.t.c., and also shows great tolerance to a large range of minibatch size, e.g., from 1k to 32k. Code is released at https://github.com/sail-sg/Adan, and has been used in multiple popular deep learning frameworks or projects. ","[{'version': 'v1', 'created': 'Sat, 13 Aug 2022 16:04:39 GMT'}, {'version': 'v2', 'created': 'Thu, 1 Sep 2022 15:48:38 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Oct 2022 09:32:37 GMT'}, {'version': 'v4', 'created': 'Mon, 27 Feb 2023 14:58:59 GMT'}]",2023-02-28,"[['Xie', 'Xingyu', ''], ['Zhou', 'Pan', ''], ['Li', 'Huan', ''], ['Lin', 'Zhouchen', ''], ['Yan', 'Shuicheng', '']]",0,1,2022-08-13,4,5,2,1,1,0,568eb10d17f1643228303670fe0f1d6608bd6f4d,251564316.0,https://www.semanticscholar.org/paper/568eb10d17f1643228303670fe0f1d6608bd6f4d,arXiv.org,2022.0,92.0,29.0,3.0,True,"['Computer Science', 'Mathematics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Mathematics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2543387', 'name': 'Xingyu Xie'}, {'authorId': '2153245275', 'name': 'Pan Zhou'}, {'authorId': '3092681', 'name': 'Huan Li'}, {'authorId': '33383055', 'name': 'Zhouchen Lin'}, {'authorId': '143653681', 'name': 'Shuicheng Yan'}]","['Peking University', 'Nankai University', 'Applied Research Laboratory at the University of Hawaii']","['China', 'United States']",2022-08
2208.08289,Zongjie Li,"Zongjie Li, Chaozheng Wang, Zhibo Liu, Haoxuan Wang, Dong Chen, Shuai
  Wang, Cuiyun Gao",CCTEST: Testing and Repairing Code Completion Systems,"13 pages, 10 figures, 5 tables. Accepted by ICSE 2023",,,,cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Code completion, a highly valuable topic in the software development domain, has been increasingly promoted for use by recent advances in large language models (LLMs). To date, visible LLM-based code completion frameworks such as GitHub Copilot and GPT are trained using deep learning over vast quantities of unstructured text and open source code. As the paramount component and the cornerstone in daily programming tasks, code completion has largely boosted professionals' efficiency in building real-world software systems. In contrast to this flourishing market, we find that code completion systems often output suspicious results, and to date, an automated testing and enhancement framework for code completion systems is not available. This research proposes CCTEST, a framework to test and repair code completion systems in blackbox settings. CCTEST features a set of novel mutation strategies, namely program structure-correlated (PSC) mutations, to generate mutated code completion inputs. Then, it detects inconsistent outputs, representing possibly erroneous cases, from all the completed code cases. Moreover, CCTEST repairs the code completion outputs by selecting the output that mostly reflects the ""average"" appearance of all output cases, as the final output of the code completion systems. We detected a total of 33,540 inputs (with a true positive rate of 86%) that can trigger erroneous cases from eight popular LLM-based code completion systems. With repairing, we show that the accuracy of code completion systems is notably increased by 40% and 67% with respect to BLEU score and Levenshtein edit similarity. ","[{'version': 'v1', 'created': 'Wed, 17 Aug 2022 13:37:03 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Dec 2022 12:17:28 GMT'}, {'version': 'v3', 'created': 'Mon, 8 May 2023 13:01:08 GMT'}]",2023-05-09,"[['Li', 'Zongjie', ''], ['Wang', 'Chaozheng', ''], ['Liu', 'Zhibo', ''], ['Wang', 'Haoxuan', ''], ['Chen', 'Dong', ''], ['Wang', 'Shuai', ''], ['Gao', 'Cuiyun', '']]",0,1,2022-08-17,3,7,1,0,0,0,e61c6213bd7e4d3ca3156723c2146294ea1b4414,251623193.0,https://www.semanticscholar.org/paper/e61c6213bd7e4d3ca3156723c2146294ea1b4414,International Conference on Software Engineering,2022.0,88.0,19.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2118207559', 'name': 'Zongjie Li'}, {'authorId': '2135764153', 'name': 'Chaozheng Wang'}, {'authorId': '2109253887', 'name': 'Zhibo Liu'}, {'authorId': '48016595', 'name': 'Hao Wang'}, {'authorId': '2146514698', 'name': 'Shuai Wang'}, {'authorId': '2337550', 'name': 'Cuiyun Gao'}]","['Harbin Institute of Technology', 'cole Polytechnique Fdrale de Lausanne', 'Hong Kong University of Science and Technology']","['China', 'Switzerland']",2022-08
2208.08629,Yongkang Liu,"Yongkang Liu, Shi Feng, Daling Wang, Yifei Zhang",MulZDG: Multilingual Code-Switching Framework for Zero-shot Dialogue Generation,COLING 2022,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Building dialogue generation systems in a zero-shot scenario remains a huge challenge, since the typical zero-shot approaches in dialogue generation rely heavily on large-scale pre-trained language generation models such as GPT-3 and T5. The research on zero-shot dialogue generation without cumbersome language models is limited due to lacking corresponding parallel dialogue corpora. In this paper, we propose a simple but effective Multilingual learning framework for Zero-shot Dialogue Generation (dubbed as MulZDG) that can effectively transfer knowledge from an English corpus with large-scale training samples to a non-English corpus with zero samples. Besides, MulZDG can be viewed as a multilingual data augmentation method to improve the performance of the resource-rich language. First, we construct multilingual code-switching dialogue datasets via translation utterances randomly selected from monolingual English datasets. Then we employ MulZDG to train a unified multilingual dialogue model based on the code-switching datasets. The MulZDG can conduct implicit semantic alignment between different languages. Experiments on DailyDialog and DSTC7 datasets demonstrate that MulZDG not only achieve competitive performance under zero-shot case compared to training with sufficient examples but also greatly improve the performance of the source language. ","[{'version': 'v1', 'created': 'Thu, 18 Aug 2022 04:28:20 GMT'}]",2022-08-19,"[['Liu', 'Yongkang', ''], ['Feng', 'Shi', ''], ['Wang', 'Daling', ''], ['Zhang', 'Yifei', '']]",0,1,2022-08-18,1,4,1,2,1,1,cdb85bb09495fde3031469da0567f66d7b497768,251643779.0,https://www.semanticscholar.org/paper/cdb85bb09495fde3031469da0567f66d7b497768,International Conference on Computational Linguistics,2022.0,63.0,6.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108142844', 'name': 'Yongkang Liu'}, {'authorId': '144588144', 'name': 'Shi Feng'}, {'authorId': '2111226672', 'name': 'Daling Wang'}, {'authorId': '2108463824', 'name': 'Yifei Zhang'}]",['Northeastern University'],['China'],2022-08
2208.09398,Mustafa Yiildirim,"Mustafa Yildirim, Ilker Oguz, Fabian Kaufmann, Marc Reig Escale,
  Rachel Grange, Demetri Psaltis and Christophe Moser",Nonlinear Optical Data Transformer for Machine Learning,"13 pages, 3 figures and 1 table",,,,physics.optics cs.AI cs.ET cs.LG physics.app-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Modern machine learning models use an ever-increasing number of parameters to train (175 billion parameters for GPT-3) with large datasets to obtain better performance. Bigger is better has been the norm. Optical computing has been reawakened as a potential solution to large-scale computing through optical accelerators that carry out linear operations while reducing electrical power. However, to achieve efficient computing with light, creating and controlling nonlinearity optically rather than electronically remains a challenge. This study explores a reservoir computing (RC) approach whereby a 14 mm long few-mode waveguide in LiNbO3 on insulator is used as a complex nonlinear optical processor. A dataset is encoded digitally on the spectrum of a femtosecond pulse which is then launched in the waveguide. The output spectrum depends nonlinearly on the input. We experimentally show that a simple digital linear classifier with 784 parameters using the output spectrum from the waveguide as input increased the classification accuracy of several databases compared to non-transformed data, approximately 10$\%$. In comparison, a deep digital neural network (NN) with 40000 parameters was necessary to achieve the same accuracy. Reducing the number of parameters by a factor of $\sim$50 illustrates that a compact optical RC approach can perform on par with a deep digital NN. ","[{'version': 'v1', 'created': 'Fri, 19 Aug 2022 15:28:48 GMT'}]",2022-08-22,"[['Yildirim', 'Mustafa', ''], ['Oguz', 'Ilker', ''], ['Kaufmann', 'Fabian', ''], ['Escale', 'Marc Reig', ''], ['Grange', 'Rachel', ''], ['Psaltis', 'Demetri', ''], ['Moser', 'Christophe', '']]",0,1,2022-08-19,1,7,5,1,0,1,1d1e89d2daf9d66a720d02053980feaafa8e2d8b,251710538.0,https://www.semanticscholar.org/paper/1d1e89d2daf9d66a720d02053980feaafa8e2d8b,arXiv.org,2022.0,31.0,3.0,0.0,True,"['Computer Science', 'Physics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Physics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Physics', 'source': 's2-fos-model'}]","[{'authorId': '2054293300', 'name': 'Mustafa Yildirim'}, {'authorId': '123792875', 'name': 'Ilker Oguz'}, {'authorId': '102360141', 'name': 'Fabian Kaufmann'}, {'authorId': '7294556', 'name': 'M. R. Escal'}, {'authorId': '5002406', 'name': 'R. Grange'}, {'authorId': '1784553', 'name': 'D. Psaltis'}, {'authorId': '32557253', 'name': 'C. Moser'}]","['ETH Zurich', 'cole Polytechnique Fdrale de Lausanne', 'Sichuan University']","['China', 'Switzerland']",2022-08
2208.12461,Junwei Bao Doctor,"Guanming Xiong, Junwei Bao, Wen Zhao, Youzheng Wu, Xiaodong He",AutoQGS: Auto-Prompt for Low-Resource Knowledge-based Question Generation from SPARQL,Accepted to CIKM2022,,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  This study investigates the task of knowledge-based question generation (KBQG). Conventional KBQG works generated questions from fact triples in the knowledge graph, which could not express complex operations like aggregation and comparison in SPARQL. Moreover, due to the costly annotation of large-scale SPARQL-question pairs, KBQG from SPARQL under low-resource scenarios urgently needs to be explored. Recently, since the generative pre-trained language models (PLMs) typically trained in natural language (NL)-to-NL paradigm have been proven effective for low-resource generation, e.g., T5 and BART, how to effectively utilize them to generate NL-question from non-NL SPARQL is challenging. To address these challenges, AutoQGS, an auto-prompt approach for low-resource KBQG from SPARQL, is proposed. Firstly, we put forward to generate questions directly from SPARQL for the KBQG task to handle complex operations. Secondly, we propose an auto-prompter trained on large-scale unsupervised data to rephrase SPARQL into NL description, smoothing the low-resource transformation from non-NL SPARQL to NL question with PLMs. Experimental results on the WebQuestionsSP, ComlexWebQuestions 1.1, and PathQuestions show that our model achieves state-of-the-art performance, especially in low-resource settings. Furthermore, a corpus of 330k factoid complex question-SPARQL pairs is generated for further KBQG research. ","[{'version': 'v1', 'created': 'Fri, 26 Aug 2022 06:53:46 GMT'}]",2022-08-29,"[['Xiong', 'Guanming', ''], ['Bao', 'Junwei', ''], ['Zhao', 'Wen', ''], ['Wu', 'Youzheng', ''], ['He', 'Xiaodong', '']]",0,1,2022-08-26,1,5,1,1,1,0,3b5f33c57e4ed2067fa0aaf87bcb6750f9f7936d,251881730.0,https://www.semanticscholar.org/paper/3b5f33c57e4ed2067fa0aaf87bcb6750f9f7936d,International Conference on Information and Knowledge Management,2022.0,30.0,4.0,1.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2059069090', 'name': 'Guanming Xiong'}, {'authorId': '3299718', 'name': 'Junwei Bao'}, {'authorId': '2118223762', 'name': 'Wen Zhao'}, {'authorId': '2115860568', 'name': 'Youzheng Wu'}, {'authorId': '144137069', 'name': 'Xiaodong He'}]","['Peking University', 'Jingdong']",['China'],2022-08
2209.01638,Holy Lovenia,"Holy Lovenia, Bryan Wilie, Romain Barraud, Samuel Cahyawijaya, Willy
  Chung, Pascale Fung",Every picture tells a story: Image-grounded controllable stylistic story generation,"Accepted in LaTeCH-CLfL 2022 (6th Joint SIGHUM Workshop on
  Computational Linguistics for Cultural Heritage, Social Sciences, Humanities
  and Literature), COLING 2022",,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  Generating a short story out of an image is arduous. Unlike image captioning, story generation from an image poses multiple challenges: preserving the story coherence, appropriately assessing the quality of the story, steering the generated story into a certain style, and addressing the scarcity of image-story pair reference datasets limiting supervision during training. In this work, we introduce Plug-and-Play Story Teller (PPST) and improve image-to-story generation by: 1) alleviating the data scarcity problem by incorporating large pre-trained models, namely CLIP and GPT-2, to facilitate a fluent image-to-text generation with minimal supervision, and 2) enabling a more style-relevant generation by incorporating stylistic adapters to control the story generation. We conduct image-to-story generation experiments with non-styled, romance-styled, and action-styled PPST approaches and compare our generated stories with those of previous work over three aspects, i.e., story coherence, image-story relevance, and style fitness, using both automatic and human evaluation. The results show that PPST improves story coherence and has better image-story relevance, but has yet to be adequately stylistic. ","[{'version': 'v1', 'created': 'Sun, 4 Sep 2022 15:07:53 GMT'}, {'version': 'v2', 'created': 'Sun, 11 Sep 2022 06:08:45 GMT'}]",2022-09-13,"[['Lovenia', 'Holy', ''], ['Wilie', 'Bryan', ''], ['Barraud', 'Romain', ''], ['Cahyawijaya', 'Samuel', ''], ['Chung', 'Willy', ''], ['Fung', 'Pascale', '']]",0,1,2022-09-04,2,6,1,1,1,0,37a03639f555bcd450aa47d78f634f9853b62819,252090065.0,https://www.semanticscholar.org/paper/37a03639f555bcd450aa47d78f634f9853b62819,LATECHCLFL,2022.0,78.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '116344405', 'name': 'Holy Lovenia'}, {'authorId': '150048491', 'name': 'Bryan Wilie'}, {'authorId': '2184144605', 'name': 'Romain Barraud'}, {'authorId': '2220548276', 'name': 'Samuel Cahyawijaya'}, {'authorId': '2160716372', 'name': 'Willy Chung'}, {'authorId': '2057151752', 'name': 'Pascale Fung'}]",['Hong Kong University of Science and Technology'],['China'],2022-09
2210.00185,Zhenhailong Wang,"Zhenhailong Wang, Xiaoman Pan, Dian Yu, Dong Yu, Jianshu Chen, Heng Ji",Zemi: Learning Zero-Shot Semi-Parametric Language Models from Multiple Tasks,Accepted as a conference paper at Findings of ACL 2023,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Although large language models have achieved impressive zero-shot ability, the huge model size generally incurs high cost. Recently, semi-parametric language models, which augment a smaller language model with an external retriever, have demonstrated promising language modeling capabilities. However, it remains unclear whether such semi-parametric language models can perform competitively well as their fully-parametric counterparts on zero-shot generalization to downstream tasks. In this work, we introduce $\text{Zemi}$, a zero-shot semi-parametric language model. To our best knowledge, this is the first semi-parametric language model that can demonstrate strong zero-shot performance on a wide range of held-out unseen tasks. We train $\text{Zemi}$ with a novel semi-parametric multitask prompted training paradigm, which shows significant improvement compared with the parametric multitask training as proposed by T0. Specifically, we augment the multitask training and zero-shot evaluation with retrieval from a large-scale task-agnostic unlabeled corpus. In order to incorporate multiple potentially noisy retrieved augmentations, we further propose a novel $\text{augmentation fusion}$ module leveraging perceiver resampler and gated cross-attention. Notably, our proposed $\text{Zemi}_\text{LARGE}$ outperforms T0-3B by 16% on all seven evaluation tasks while being 3.9x smaller in model size. ","[{'version': 'v1', 'created': 'Sat, 1 Oct 2022 04:08:50 GMT'}, {'version': 'v2', 'created': 'Tue, 23 May 2023 00:49:44 GMT'}]",2023-05-24,"[['Wang', 'Zhenhailong', ''], ['Pan', 'Xiaoman', ''], ['Yu', 'Dian', ''], ['Yu', 'Dong', ''], ['Chen', 'Jianshu', ''], ['Ji', 'Heng', '']]",0,0,2022-10-01,2,6,1,1,1,0,ec97c3248537bb0b455b3fe9bc341110cfceffde,252683285.0,https://www.semanticscholar.org/paper/ec97c3248537bb0b455b3fe9bc341110cfceffde,Annual Meeting of the Association for Computational Linguistics,2022.0,88.0,5.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2052036545', 'name': 'Zhenhailong Wang'}, {'authorId': '34741133', 'name': 'Xiaoman Pan'}, {'authorId': '41190054', 'name': 'Dian Yu'}, {'authorId': '144580027', 'name': 'Dong Yu'}, {'authorId': '2108276402', 'name': 'Jianshu Chen'}, {'authorId': '2113323573', 'name': 'Heng Ji'}]",['Tencent'],['China'],2022-10
2210.00305,Ningyu Zhang,"Xin Xie, Zhoubo Li, Xiaohan Wang, Zekun Xi, Ningyu Zhang",LambdaKG: A Library for Pre-trained Language Model-Based Knowledge Graph Embeddings,"AACL 2023 System Demonstrations, the project website is
  https://zjunlp.github.io/project/promptkg/",,,,cs.CL cs.AI cs.DB cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge Graphs (KGs) often have two characteristics: heterogeneous graph structure and text-rich entity/relation information. Text-based KG embeddings can represent entities by encoding descriptions with pre-trained language models, but no open-sourced library is specifically designed for KGs with PLMs at present. In this paper, we present LambdaKG, a library for KGE that equips with many pre-trained language models (e.g., BERT, BART, T5, GPT-3), and supports various tasks (e.g., knowledge graph completion, question answering, recommendation, and knowledge probing). LambdaKG is publicly open-sourced at https://github.com/zjunlp/PromptKG/tree/main/lambdaKG, with a demo video at http://deepke.zjukg.cn/lambdakg.mp4 and long-term maintenance. ","[{'version': 'v1', 'created': 'Sat, 1 Oct 2022 16:01:53 GMT'}, {'version': 'v2', 'created': 'Tue, 7 Mar 2023 14:35:33 GMT'}, {'version': 'v3', 'created': 'Thu, 14 Sep 2023 07:06:03 GMT'}]",2023-09-15,"[['Xie', 'Xin', ''], ['Li', 'Zhoubo', ''], ['Wang', 'Xiaohan', ''], ['Xi', 'Zekun', ''], ['Zhang', 'Ningyu', '']]",0,1,2022-10-01,3,5,5,2,1,1,2b4f212522b1744fcfdad2d591abda2db66db7d8,257378687.0,https://www.semanticscholar.org/paper/2b4f212522b1744fcfdad2d591abda2db66db7d8,arXiv.org,2022.0,36.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2110972563', 'name': 'Xin Xie'}, {'authorId': '9956037', 'name': 'Zhoubo Li'}, {'authorId': '2141660877', 'name': 'Xiaohan Wang'}, {'authorId': '2127757530', 'name': 'Yuqi Zhu'}, {'authorId': '2153010067', 'name': 'Ningyu Zhang'}, {'authorId': '2107967596', 'name': 'Jintian Zhang'}, {'authorId': '46378881', 'name': 'Siyuan Cheng'}, {'authorId': '2064522174', 'name': 'Bo Tian'}, {'authorId': '152931849', 'name': 'Shumin Deng'}, {'authorId': '2068169902', 'name': 'Feiyu Xiong'}, {'authorId': '2144200945', 'name': 'Huajun Chen'}]",['Zhejiang University'],['China'],2022-10
2210.02414,Xiao Liu,"Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding,
  Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei
  Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, Jie Tang",GLM-130B: An Open Bilingual Pre-trained Model,47 pages,,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and disconvergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization, without quantization aware training and with almost no performance loss, making it the first among 100B-scale models. More importantly, the property allows its effective inference on 4$\times$RTX 3090 (24G) or 8$\times$RTX 2080 Ti (11G) GPUs, the most ever affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at https://github.com/THUDM/GLM-130B . ","[{'version': 'v1', 'created': 'Wed, 5 Oct 2022 17:34:44 GMT'}]",2022-10-06,"[['Zeng', 'Aohan', ''], ['Liu', 'Xiao', ''], ['Du', 'Zhengxiao', ''], ['Wang', 'Zihan', ''], ['Lai', 'Hanyu', ''], ['Ding', 'Ming', ''], ['Yang', 'Zhuoyi', ''], ['Xu', 'Yifan', ''], ['Zheng', 'Wendi', ''], ['Xia', 'Xiao', ''], ['Tam', 'Weng Lam', ''], ['Ma', 'Zixuan', ''], ['Xue', 'Yufei', ''], ['Zhai', 'Jidong', ''], ['Chen', 'Wenguang', ''], ['Zhang', 'Peng', ''], ['Dong', 'Yuxiao', ''], ['Tang', 'Jie', '']]",0,1,2022-10-05,1,18,3,4,3,1,1d26c947406173145a4665dd7ab255e03494ea28,252715691.0,https://www.semanticscholar.org/paper/1d26c947406173145a4665dd7ab255e03494ea28,International Conference on Learning Representations,2022.0,154.0,359.0,66.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '2051712753', 'name': 'Aohan Zeng'}, {'authorId': '2111312892', 'name': 'Xiao Liu'}, {'authorId': '66395694', 'name': 'Zhengxiao Du'}, {'authorId': '2240689', 'name': 'Zihan Wang'}, {'authorId': '2051311700', 'name': 'Hanyu Lai'}, {'authorId': '145573466', 'name': 'Ming Ding'}, {'authorId': '2109506541', 'name': 'Zhuoyi Yang'}, {'authorId': '2125063007', 'name': 'Yifan Xu'}, {'authorId': '2163967642', 'name': 'Wendi Zheng'}, {'authorId': '2186982651', 'name': 'Xiao Xia'}, {'authorId': '1403621152', 'name': 'W. Tam'}, {'authorId': '2124489983', 'name': 'Zixuan Ma'}, {'authorId': '2114921664', 'name': 'Yufei Xue'}, {'authorId': '2467444', 'name': 'Jidong Zhai'}, {'authorId': '1712168', 'name': 'Wenguang Chen'}, {'authorId': '47243067', 'name': 'P. Zhang'}, {'authorId': '2047998', 'name': 'Yuxiao Dong'}, {'authorId': '2148911956', 'name': 'Jie Tang'}]",['Tsinghua University'],['China'],2022-10
2210.02875,Tianbao Xie,"Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni,
  Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer,
  Noah A. Smith, Tao Yu",Binding Language Models in Symbolic Languages,"ICLR 2023 camera ready, 27 pages, 10 figures",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at https://github.com/HKUNLP/Binder . ","[{'version': 'v1', 'created': 'Thu, 6 Oct 2022 12:55:17 GMT'}, {'version': 'v2', 'created': 'Wed, 1 Mar 2023 03:21:40 GMT'}]",2023-03-02,"[['Cheng', 'Zhoujun', ''], ['Xie', 'Tianbao', ''], ['Shi', 'Peng', ''], ['Li', 'Chengzu', ''], ['Nadkarni', 'Rahul', ''], ['Hu', 'Yushi', ''], ['Xiong', 'Caiming', ''], ['Radev', 'Dragomir', ''], ['Ostendorf', 'Mari', ''], ['Zettlemoyer', 'Luke', ''], ['Smith', 'Noah A.', ''], ['Yu', 'Tao', '']]",0,1,2022-10-06,2,12,1,2,0,2,f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab,252734772.0,https://www.semanticscholar.org/paper/f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab,International Conference on Learning Representations,2022.0,67.0,77.0,6.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1471878967', 'name': 'Zhoujun Cheng'}, {'authorId': '2057038673', 'name': 'Tianbao Xie'}, {'authorId': '2055356856', 'name': 'Peng Shi'}, {'authorId': '2155795167', 'name': 'Chengzu Li'}, {'authorId': '40027281', 'name': 'R.K. Nadkarni'}, {'authorId': '2112209725', 'name': 'Yushi Hu'}, {'authorId': '2054594326', 'name': 'Caiming Xiong'}, {'authorId': '9215251', 'name': 'Dragomir R. Radev'}, {'authorId': '81444299', 'name': 'M. Ostendorf'}, {'authorId': '2137813791', 'name': 'Luke Zettlemoyer'}, {'authorId': '2116827887', 'name': 'N. A. Smith'}, {'authorId': None, 'name': 'Tao Yu'}]","['University of Waterloo', 'Yale University', 'Shanghai Jiao Tong University', 'Allen Institute for Artificial Intelligence', 'University of Washington', 'Salesforce', 'University of Hong Kong', 'Meta']","['China', 'United States', 'Canada', 'Hong Kong']",2022-10
2210.03493,Aston Zhang,"Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola",Automatic Chain of Thought Prompting in Large Language Models,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-sa/4.0/,"  Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like ""Let's think step by step"" to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the ""Let's think step by step"" prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot ","[{'version': 'v1', 'created': 'Fri, 7 Oct 2022 12:28:21 GMT'}]",2022-10-10,"[['Zhang', 'Zhuosheng', ''], ['Zhang', 'Aston', ''], ['Li', 'Mu', ''], ['Smola', 'Alex', '']]",0,1,2022-10-07,1,4,2,1,0,1,90350aa626bed47b02d0c162462e5b0ca82be6b2,252762275.0,https://www.semanticscholar.org/paper/90350aa626bed47b02d0c162462e5b0ca82be6b2,International Conference on Learning Representations,2022.0,39.0,214.0,34.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Physics', 'source': 's2-fos-model'}]","[{'authorId': '3322871', 'name': 'Zhuosheng Zhang'}, {'authorId': '2085709', 'name': 'Aston Zhang'}, {'authorId': '1701799', 'name': 'Mu Li'}, {'authorId': '78088877', 'name': 'Alexander J. Smola'}]",['Shanghai Jiao Tong University'],['China'],2022-10
2210.05075,Haoyu Dong,"Fan Zhou, Haoyu Dong, Qian Liu, Zhoujun Cheng, Shi Han, Dongmei Zhang",Reflection of Thought: Inversely Eliciting Numerical Reasoning in Language Models via Solving Linear Systems,,,,,cs.CL cs.IR cs.NA math.NA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Numerical reasoning over natural language has been a long-standing goal for the research community. However, cutting-edge language models have proven difficult to reliably generalize to a broad range of numbers, although they have shown proficiency in reasoning over common and simple numbers. In this paper, we propose a novel method to elicit and exploit the numerical reasoning knowledge hidden in pre-trained language models using simple anchor numbers. Concretely, we first leverage simple numbers as anchors to probe the implicitly inferred arithmetic expressions from language models, and then explicitly apply the expressions on complex numbers to get corresponding answers. To inversely elicit arithmetic expressions, we transform and formulate the task as an analytically solvable linear system. Experimental results on several numerical reasoning benchmarks demonstrate that our approach significantly improves numerical reasoning capabilities of existing LMs. More importantly, our approach is training-free and simply works in the inference phase, making it highly portable and achieving consistent performance benefits across a variety of language models (GPT-3, T5, BART, etc) in all zero-shot, few-shot, and fine-tuning scenarios. ","[{'version': 'v1', 'created': 'Tue, 11 Oct 2022 00:57:19 GMT'}]",2022-10-12,"[['Zhou', 'Fan', ''], ['Dong', 'Haoyu', ''], ['Liu', 'Qian', ''], ['Cheng', 'Zhoujun', ''], ['Han', 'Shi', ''], ['Zhang', 'Dongmei', '']]",0,1,2022-10-11,1,6,4,2,1,1,712f21411526e8450036d7199637808590be3579,252815431.0,https://www.semanticscholar.org/paper/712f21411526e8450036d7199637808590be3579,arXiv.org,2022.0,37.0,5.0,1.0,True,"['Computer Science', 'Mathematics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Mathematics', 'source': 'external'}, {'category': 'Mathematics', 'source': 's2-fos-model'}]","[{'authorId': '2153433679', 'name': 'Fan Zhou'}, {'authorId': '2153721991', 'name': 'Haoyu Dong'}, {'authorId': '1409707585', 'name': 'Qian Liu'}, {'authorId': '1471878967', 'name': 'Zhoujun Cheng'}, {'authorId': '2109750123', 'name': 'Shi Han'}, {'authorId': '2140415600', 'name': 'Dongmei Zhang'}]","['Shanghai Jiao Tong University', 'Microsoft', 'Applied Research Laboratory at the University of Hawaii']","['China', 'United States']",2022-10
2210.05549,Zixuan Ke,"Zixuan Ke, Haowei Lin, Yijia Shao, Hu Xu, Lei Shu, and Bing Liu",Continual Training of Language Models for Few-Shot Learning,,EMNLP 2022,,,cs.CL cs.AI cs.LG cs.NE,http://creativecommons.org/publicdomain/zero/1.0/,"  Recent work on applying large language models (LMs) achieves impressive performance in many NLP applications. Adapting or posttraining an LM using an unlabeled domain corpus can produce even better performance for end-tasks in the domain. This paper proposes the problem of continually extending an LM by incrementally post-train the LM with a sequence of unlabeled domain corpora to expand its knowledge without forgetting its previous skills. The goal is to improve the few-shot end-task learning in these domains. The resulting system is called CPT (Continual PostTraining), which to our knowledge, is the first continual post-training system. Experimental results verify its effectiveness. ","[{'version': 'v1', 'created': 'Tue, 11 Oct 2022 15:43:58 GMT'}]",2022-10-12,"[['Ke', 'Zixuan', ''], ['Lin', 'Haowei', ''], ['Shao', 'Yijia', ''], ['Xu', 'Hu', ''], ['Shu', 'Lei', ''], ['Liu', 'Bing', '']]",0,0,2022-10-11,1,6,4,0,0,0,e053be7f36a0772b68eaaa14f15650c14071e4ab,252815848.0,https://www.semanticscholar.org/paper/e053be7f36a0772b68eaaa14f15650c14071e4ab,Conference on Empirical Methods in Natural Language Processing,2022.0,62.0,15.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '9623089', 'name': 'Zixuan Ke'}, {'authorId': '2152782471', 'name': 'Hao Lin'}, {'authorId': '74175857', 'name': 'Yijia Shao'}, {'authorId': '33464127', 'name': 'Hu Xu'}, {'authorId': '145142456', 'name': 'Lei Shu'}, {'authorId': '47655556', 'name': 'Bin Liu'}]","['Peking University', 'University of Illinois at Chicago', 'Google']","['China', 'United States']",2022-10
2210.06706,Hong Liu,"Hong Liu, Zhijian Ou, Yi Huang and Junlan Feng",Jointly Reinforced User Simulator and Task-oriented Dialog System with Simplified Generative Architecture,"An early version of Markovian Generative Architectures (MGA) and
  Generative User Simulator (GUS)",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Recently, there has been progress in supervised funetuning pretrained GPT-2 to build end-to-end task-oriented dialog (TOD) systems. However, online reinforcement learning of a GPT-2 based dialog system (DS), together with a end-to-end user simulator (US), has not ever been explored. Moreover, a drawback with existing GPT-2 based TOD systems is that they mostly employ the whole dialog history as input, which brings inefficiencies in memory and compute. In this paper, we first propose Simplified Generative Architectures (SGA) for DS and US respectively, both based on GPT-2 but using shortened history. Then, we successfully develop Jointly Reinforced US and DS, called SGA-JRUD. Our DS with the proposed SGA, when only supervised trained, achieves state-of-the-art performance on MultiWOZ2.1 and is more compute-efficient in both training and generation. Extensive experiments on MultiWOZ2.1 further show the superiority of SGA-JRUD in both offline and online evaluations. ","[{'version': 'v1', 'created': 'Thu, 13 Oct 2022 03:57:17 GMT'}]",2022-10-14,"[['Liu', 'Hong', ''], ['Ou', 'Zhijian', ''], ['Huang', 'Yi', ''], ['Feng', 'Junlan', '']]",0,1,2022-10-13,1,4,2,1,1,0,df5ff7707cafd339a439ed0e8105b0e7530b23c8,252546616.0,https://www.semanticscholar.org/paper/df5ff7707cafd339a439ed0e8105b0e7530b23c8,arXiv.org,2022.0,47.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '14214710', 'name': 'Abhishek Sethi'}]","['Tsinghua University', 'China Mobile (China)']",['China'],2022-10
2210.06726,Shiyang Li,"Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun
  Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, Wenhu Chen and Xifeng Yan",Explanations from Large Language Models Make Small Reasoners Better,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Integrating free-text explanations to in-context learning of large language models (LLM) is shown to elicit strong reasoning capabilities along with reasonable explanations. In this paper, we consider the problem of leveraging the explanations generated by LLM to improve the training of small reasoners, which are more favorable in real-production deployment due to their low cost. We systematically explore three explanation generation approaches from LLM and utilize a multi-task learning framework to facilitate small models to acquire strong reasoning power together with explanation generation capabilities. Experiments on multiple reasoning tasks show that our method can consistently and significantly outperform finetuning baselines across different settings, and even perform better than finetuning/prompting a 60x larger GPT-3 (175B) model by up to 9.5% in accuracy. As a side benefit, human evaluation further shows that our method can generate high-quality explanations to justify its predictions, moving towards the goal of explainable AI. ","[{'version': 'v1', 'created': 'Thu, 13 Oct 2022 04:50:02 GMT'}]",2022-10-14,"[['Li', 'Shiyang', ''], ['Chen', 'Jianshu', ''], ['Shen', 'Yelong', ''], ['Chen', 'Zhiyu', ''], ['Zhang', 'Xinlu', ''], ['Li', 'Zekun', ''], ['Wang', 'Hong', ''], ['Qian', 'Jing', ''], ['Peng', 'Baolin', ''], ['Mao', 'Yi', ''], ['Chen', 'Wenhu', ''], ['Yan', 'Xifeng', '']]",0,1,2022-10-13,1,12,1,1,0,1,7d29a84a589aa5655e5d3fed8d725ea472816599,252873123.0,https://www.semanticscholar.org/paper/7d29a84a589aa5655e5d3fed8d725ea472816599,arXiv.org,2022.0,54.0,46.0,7.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '50341591', 'name': 'SHIYANG LI'}, {'authorId': '2108276402', 'name': 'Jianshu Chen'}, {'authorId': '1752875', 'name': 'Yelong Shen'}, {'authorId': '2142370346', 'name': 'Zhiyu Chen'}, {'authorId': '2108030191', 'name': 'Xinlu Zhang'}, {'authorId': '2109964198', 'name': 'Zekun Li'}, {'authorId': '46507182', 'name': 'Hong Wang'}, {'authorId': '66735072', 'name': 'Jingu Qian'}, {'authorId': '1780690', 'name': 'Baolin Peng'}, {'authorId': '145469202', 'name': 'Yi Mao'}, {'authorId': '2928777', 'name': 'Wenhu Chen'}, {'authorId': '1740249', 'name': 'Xifeng Yan'}]","['Tencent', 'Microsoft', 'University of Waterloo', 'University of California, Santa Barbara']","['Canada', 'United States', 'China']",2022-10
2210.07054,Jinhui Ye,"Jinhui Ye, Wenxiang Jiao, Xing Wang and Zhaopeng Tu",Scaling Back-Translation with Domain Text Generation for Sign Language Gloss Translation,Accepted at EACL 2023 (main conference),,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Sign language gloss translation aims to translate the sign glosses into spoken language texts, which is challenging due to the scarcity of labeled gloss-text parallel data. Back translation (BT), which generates pseudo-parallel data by translating in-domain spoken language texts into sign glosses, has been applied to alleviate the data scarcity problem. However, the lack of large-scale high-quality domain spoken language text data limits the effect of BT. In this paper, to overcome the limitation, we propose a Prompt based domain text Generation (PGEN) approach to produce the large-scale in-domain spoken language text data. Specifically, PGEN randomly concatenates sentences from the original in-domain spoken language text data as prompts to induce a pre-trained language model (i.e., GPT-2) to generate spoken language texts in a similar style. Experimental results on three benchmarks of sign language gloss translation in varied languages demonstrate that BT with spoken language texts generated by PGEN significantly outperforms the compared methods. In addition, as the scale of spoken language texts generated by PGEN increases, the BT technique can achieve further improvements, demonstrating the effectiveness of our approach. We release the code and data for facilitating future research in this field. ","[{'version': 'v1', 'created': 'Thu, 13 Oct 2022 14:25:08 GMT'}, {'version': 'v2', 'created': 'Tue, 7 Feb 2023 09:19:20 GMT'}]",2023-02-08,"[['Ye', 'Jinhui', ''], ['Jiao', 'Wenxiang', ''], ['Wang', 'Xing', ''], ['Tu', 'Zhaopeng', '']]",0,1,2022-10-13,2,4,2,1,1,0,7d7fa25e5cc7a805d5c6dab85eeeb19c96f28af1,252873427.0,https://www.semanticscholar.org/paper/7d7fa25e5cc7a805d5c6dab85eeeb19c96f28af1,Conference of the European Chapter of the Association for Computational Linguistics,2022.0,48.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '153173929', 'name': 'Jinhui Ye'}, {'authorId': '12386833', 'name': 'Wenxiang Jiao'}, {'authorId': '48631170', 'name': 'Xing Wang'}, {'authorId': '2909321', 'name': 'Zhaopeng Tu'}]","['Tencent', 'Hong Kong University of Science and Technology']",['China'],2022-10
2210.07229,Arnab Sen Sharma,"Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, David
  Bau",Mass-Editing Memory in a Transformer,"18 pages, 11 figures. Code and data at https://memit.baulab.info",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by orders of magnitude. Our code and data are at https://memit.baulab.info. ","[{'version': 'v1', 'created': 'Thu, 13 Oct 2022 17:55:53 GMT'}, {'version': 'v2', 'created': 'Tue, 1 Aug 2023 18:41:52 GMT'}]",2023-08-03,"[['Meng', 'Kevin', ''], ['Sharma', 'Arnab Sen', ''], ['Andonian', 'Alex', ''], ['Belinkov', 'Yonatan', ''], ['Bau', 'David', '']]",0,1,2022-10-13,2,5,2,0,0,0,2fe1ac0b09cc0f50eb83eef6c7c6b45ac8b12413,252873467.0,https://www.semanticscholar.org/paper/2fe1ac0b09cc0f50eb83eef6c7c6b45ac8b12413,International Conference on Learning Representations,2022.0,59.0,101.0,15.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '153615419', 'name': 'Kevin Meng'}, {'authorId': '2109373982', 'name': 'Arnab Sharma'}, {'authorId': '50112310', 'name': 'A. Andonian'}, {'authorId': '2083259', 'name': 'Yonatan Belinkov'}, {'authorId': '144159726', 'name': 'David Bau'}]","['Massachusetts Institute of Technology', 'Technion  Israel Institute of Technology', 'Northeastern University']","['China', 'United States', 'Israel']",2022-10
2210.07652,Yejin Bang,"Yejin Bang, Tiezheng Yu, Andrea Madotto, Zhaojiang Lin, Mona Diab,
  Pascale Fung",Enabling Classifiers to Make Judgements Explicitly Aligned with Human Values,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many NLP classification tasks, such as sexism/racism detection or toxicity detection, are based on human values. Yet, human values can vary under diverse cultural conditions. Therefore, we introduce a framework for value-aligned classification that performs prediction based on explicitly written human values in the command. Along with the task, we propose a practical approach that distills value-aligned knowledge from large-scale language models (LLMs) to construct value-aligned classifiers in two steps. First, we generate value-aligned training data from LLMs by prompt-based few-shot learning. Next, we fine-tune smaller classification models with the generated data for the task. Empirical results show that our VA-Models surpass multiple baselines by at least 15.56% on the F1-score, including few-shot learning with OPT-175B and existing text augmentation methods. We suggest that using classifiers with explicit human value input improves both inclusivity & explainability in AI. ","[{'version': 'v1', 'created': 'Fri, 14 Oct 2022 09:10:49 GMT'}]",2022-10-17,"[['Bang', 'Yejin', ''], ['Yu', 'Tiezheng', ''], ['Madotto', 'Andrea', ''], ['Lin', 'Zhaojiang', ''], ['Diab', 'Mona', ''], ['Fung', 'Pascale', '']]",0,0,2022-10-14,1,6,2,1,1,0,9c36c8f398a074801d6098287c4353bcf87a1d6c,252907375.0,https://www.semanticscholar.org/paper/9c36c8f398a074801d6098287c4353bcf87a1d6c,TRUSTNLP,2022.0,49.0,5.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '23672613', 'name': 'Yejin Bang'}, {'authorId': '1660855299', 'name': 'Tiezheng Yu'}, {'authorId': '2111680936', 'name': 'Andrea Madotto'}, {'authorId': '100466830', 'name': 'Zhaojiang Lin'}, {'authorId': '2138579860', 'name': 'Mona T. Diab'}, {'authorId': '2057151752', 'name': 'Pascale Fung'}]","['Hong Kong University of Science and Technology', 'Meta']","['China', 'United States']",2022-10
2210.08692,Hong Liu,"Hong Liu, Yucheng Cai, Zhijian Ou, Yi Huang, Junlan Feng",A Generative User Simulator with GPT-based Architecture and Goal State Tracking for Reinforced Multi-Domain Dialog Systems,Accepted by EMNLP 2022 SereTOD Workshop,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Building user simulators (USs) for reinforcement learning (RL) of task-oriented dialog systems (DSs) has gained more and more attention, which, however, still faces several fundamental challenges. First, it is unclear whether we can leverage pretrained language models to design, for example, GPT-2 based USs, to catch up and interact with the recently advanced GPT-2 based DSs. Second, an important ingredient in a US is that the user goal can be effectively incorporated and tracked; but how to flexibly integrate goal state tracking and develop an end-to-end trainable US for multi-domains has remained to be a challenge. In this work, we propose a generative user simulator (GUS) with GPT-2 based architecture and goal state tracking towards addressing the above two challenges. Extensive experiments are conducted on MultiWOZ2.1. Different DSs are trained via RL with GUS, the classic agenda-based user simulator (ABUS) and other ablation simulators respectively, and are compared for cross-model evaluation, corpus-based evaluation and human evaluation. The GUS achieves superior results in all three evaluation tasks. ","[{'version': 'v1', 'created': 'Mon, 17 Oct 2022 01:57:50 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Oct 2022 06:41:39 GMT'}]",2022-10-19,"[['Liu', 'Hong', ''], ['Cai', 'Yucheng', ''], ['Ou', 'Zhijian', ''], ['Huang', 'Yi', ''], ['Feng', 'Junlan', '']]",0,1,2022-10-17,2,5,2,1,1,0,bba250f9ac68dfeb9dd90b09bb3ed0da31543ea5,252917630.0,https://www.semanticscholar.org/paper/bba250f9ac68dfeb9dd90b09bb3ed0da31543ea5,SERETOD,2022.0,36.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2175085268', 'name': 'Hong Liu'}, {'authorId': '2116362074', 'name': 'Yucheng Cai'}, {'authorId': '1717830', 'name': 'Zhijian Ou'}, {'authorId': '2143931690', 'name': 'Yi Huang'}, {'authorId': '39729308', 'name': 'Junlan Feng'}]","['Tsinghua University', 'China Mobile (China)']",['China'],2022-10
2210.09549,Weihua Li,"Ruijun Li, Weihua Li, Yi Yang, Hanyu Wei, Jianhua Jiang and Quan Bai",Swinv2-Imagen: Hierarchical Vision Transformer Diffusion Models for Text-to-Image Generation,,,,,cs.CV cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Recently, diffusion models have been proven to perform remarkably well in text-to-image synthesis tasks in a number of studies, immediately presenting new study opportunities for image generation. Google's Imagen follows this research trend and outperforms DALLE2 as the best model for text-to-image generation. However, Imagen merely uses a T5 language model for text processing, which cannot ensure learning the semantic information of the text. Furthermore, the Efficient UNet leveraged by Imagen is not the best choice in image processing. To address these issues, we propose the Swinv2-Imagen, a novel text-to-image diffusion model based on a Hierarchical Visual Transformer and a Scene Graph incorporating a semantic layout. In the proposed model, the feature vectors of entities and relationships are extracted and involved in the diffusion model, effectively improving the quality of generated images. On top of that, we also introduce a Swin-Transformer-based UNet architecture, called Swinv2-Unet, which can address the problems stemming from the CNN convolution operations. Extensive experiments are conducted to evaluate the performance of the proposed model by using three real-world datasets, i.e., MSCOCO, CUB and MM-CelebA-HQ. The experimental results show that the proposed Swinv2-Imagen model outperforms several popular state-of-the-art methods. ","[{'version': 'v1', 'created': 'Tue, 18 Oct 2022 02:50:34 GMT'}]",2022-10-19,"[['Li', 'Ruijun', ''], ['Li', 'Weihua', ''], ['Yang', 'Yi', ''], ['Wei', 'Hanyu', ''], ['Jiang', 'Jianhua', ''], ['Bai', 'Quan', '']]",0,0,2022-10-18,1,6,2,1,1,0,3de95f33c2b4f61a9c0f335b4810a966e209a47a,252967839.0,https://www.semanticscholar.org/paper/3de95f33c2b4f61a9c0f335b4810a966e209a47a,Neural computing & applications (Print),2022.0,75.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2150926914', 'name': 'Rui Li'}, {'authorId': '2108711879', 'name': 'Weihua Li'}, {'authorId': '2143686207', 'name': 'Yi Yang'}, {'authorId': '2158648839', 'name': 'Hanyu Wei'}, {'authorId': '2149533009', 'name': 'Jianhua Jiang'}, {'authorId': '2059035600', 'name': 'Quan Bai'}]","['Jilin University of Finance and Economics', 'Auckland University of Technology', 'University of Tasmania', 'Hefei University of Technology']","['China', 'New Zealand', 'Australia']",2022-10
2210.10246,Muralidhar Andoorveedu,"Muralidhar Andoorveedu, Zhanda Zhu, Bojian Zheng, Gennady Pekhimenko",Tempo: Accelerating Transformer-Based Model Training through Memory Footprint Reduction,"Accepted to NeurIPS 2022. Fixed some minor typos and added some small
  clarifications",,,,cs.LG cs.PF,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Training deep learning models can be computationally expensive. Prior works have shown that increasing the batch size can potentially lead to better overall throughput. However, the batch size is frequently limited by the accelerator memory capacity due to the activations/feature maps stored for the training backward pass, as larger batch sizes require larger feature maps to be stored. Transformer-based models, which have recently seen a surge in popularity due to their good performance and applicability to a variety of tasks, have a similar problem. To remedy this issue, we propose Tempo, a new approach to efficiently use accelerator (e.g., GPU) memory resources for training Transformer-based models. Our approach provides drop-in replacements for the GELU, LayerNorm, and Attention layers, reducing the memory usage and ultimately leading to more efficient training. We implement Tempo and evaluate the throughput, memory usage, and accuracy/loss on the BERT Large pre-training task. We demonstrate that Tempo enables up to 2x higher batch sizes and 16% higher training throughput over the state-of-the-art baseline. We also evaluate Tempo on GPT2 and RoBERTa models, showing 19% and 26% speedup over the baseline. ","[{'version': 'v1', 'created': 'Wed, 19 Oct 2022 01:59:37 GMT'}, {'version': 'v2', 'created': 'Tue, 24 Jan 2023 01:37:30 GMT'}]",2023-01-25,"[['Andoorveedu', 'Muralidhar', ''], ['Zhu', 'Zhanda', ''], ['Zheng', 'Bojian', ''], ['Pekhimenko', 'Gennady', '']]",0,1,2022-10-19,2,4,2,1,1,0,94d23df13520db6d48f72621fc72049e640ca9f2,252992718.0,https://www.semanticscholar.org/paper/94d23df13520db6d48f72621fc72049e640ca9f2,Neural Information Processing Systems,2022.0,89.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2188240827', 'name': 'Muralidhar Andoorveedu'}, {'authorId': '11594634', 'name': 'Zhanda Zhu'}, {'authorId': '2112670252', 'name': 'Bojian Zheng'}, {'authorId': '3257164', 'name': 'Gennady Pekhimenko'}]","['University of Toronto', 'Shanghai Jiao Tong University', 'Vector Institute']","['China', 'Canada']",2022-10
2210.10341,Renqian Luo,"Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon,
  Tie-Yan Liu",BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining,"Published at Briefings in Bioinformatics. Code is available at
  https://github.com/microsoft/BioGPT","Briefings in Bioinformatics, 2022;, bbac409",10.1093/bib/bbac409,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e., BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large scale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms. Code is available at https://github.com/microsoft/BioGPT. ","[{'version': 'v1', 'created': 'Wed, 19 Oct 2022 07:17:39 GMT'}, {'version': 'v2', 'created': 'Fri, 13 Jan 2023 12:00:04 GMT'}, {'version': 'v3', 'created': 'Mon, 3 Apr 2023 06:49:33 GMT'}]",2023-04-04,"[['Luo', 'Renqian', ''], ['Sun', 'Liai', ''], ['Xia', 'Yingce', ''], ['Qin', 'Tao', ''], ['Zhang', 'Sheng', ''], ['Poon', 'Hoifung', ''], ['Liu', 'Tie-Yan', '']]",0,1,2022-10-19,3,7,2,0,0,0,44279244407a64431810f982be6d0c7da4429dd7,252542956.0,https://www.semanticscholar.org/paper/44279244407a64431810f982be6d0c7da4429dd7,Briefings Bioinform.,2022.0,58.0,230.0,29.0,True,"['Computer Science', 'Medicine']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Medicine', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Medicine', 'source': 's2-fos-model'}]","[{'authorId': '3440939', 'name': 'Renqian Luo'}, {'authorId': '2186266093', 'name': 'Liai Sun'}, {'authorId': '2111056280', 'name': 'Yingce Xia'}, {'authorId': '143826491', 'name': 'Tao Qin'}, {'authorId': '72655349', 'name': 'Sheng Zhang'}, {'authorId': '1759772', 'name': 'Hoifung Poon'}, {'authorId': '2110264337', 'name': 'Tie-Yan Liu'}]","['Peking University', 'Microsoft']","['China', 'United States', 'Netherlands']",2022-10
2210.11599,Xian Qian,"Xian Qian, Kai Hu, Jiaqiang Wang, Yifeng Liu, Xingyuan Pan, Jun Cao,
  Mingxuan Wang",The VolcTrans System for WMT22 Multilingual Machine Translation Task,"WMT 2022, 8 pages",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  This report describes our VolcTrans system for the WMT22 shared task on large-scale multilingual machine translation. We participated in the unconstrained track which allows the use of external resources. Our system is a transformerbased multilingual model trained on data from multiple sources including the public training set from the data track, NLLB data provided by Meta AI, self-collected parallel corpora, and pseudo bitext from back-translation. A series of heuristic rules clean both bilingual and monolingual texts. On the official test set, our system achieves 17.3 BLEU, 21.9 spBLEU, and 41.9 chrF2++ on average over all language pairs. The average inference speed is 11.5 sentences per second using a single Nvidia Tesla V100 GPU. Our code and trained models are available at https://github.com/xian8/wmt22 ","[{'version': 'v1', 'created': 'Thu, 20 Oct 2022 21:18:03 GMT'}]",2022-10-24,"[['Qian', 'Xian', ''], ['Hu', 'Kai', ''], ['Wang', 'Jiaqiang', ''], ['Liu', 'Yifeng', ''], ['Pan', 'Xingyuan', ''], ['Cao', 'Jun', ''], ['Wang', 'Mingxuan', '']]",0,0,2022-10-20,1,7,1,1,1,0,a30318cc5281187bd315915f0d0cccd7007f75b9,253080360.0,https://www.semanticscholar.org/paper/a30318cc5281187bd315915f0d0cccd7007f75b9,Conference on Machine Translation,2022.0,27.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2067733840', 'name': 'Xian Qian'}, {'authorId': '2055736269', 'name': 'Kai Hu'}, {'authorId': '2110239173', 'name': 'Jiaqiang Wang'}, {'authorId': '2143063229', 'name': 'Yifeng Liu'}, {'authorId': '3265780', 'name': 'Xingyuan Pan'}, {'authorId': '2109830278', 'name': 'Jun Cao'}, {'authorId': '2067908', 'name': 'Mingxuan Wang'}]","['Wuhan University', 'ByteDance', 'Tsinghua University']",['China'],2022-10
2210.13382,Kenneth Li,"Kenneth Li, Aspen K. Hopkins, David Bau, Fernanda Vi\'egas, Hanspeter
  Pfister, Martin Wattenberg",Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task,"ICLR 2023 oral (notable-top-5%):
  https://openreview.net/forum?id=DeG07_TcZvT ; code:
  https://github.com/likenneth/othello_world",,,,cs.LG cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Language models show a surprising range of capabilities, but the source of their apparent competence is unclear. Do these networks just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see? We investigate this question by applying a variant of the GPT model to the task of predicting legal moves in a simple board game, Othello. Although the network has no a priori knowledge of the game or its rules, we uncover evidence of an emergent nonlinear internal representation of the board state. Interventional experiments indicate this representation can be used to control the output of the network and create ""latent saliency maps"" that can help explain predictions in human terms. ","[{'version': 'v1', 'created': 'Mon, 24 Oct 2022 16:29:55 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Oct 2022 13:47:00 GMT'}, {'version': 'v3', 'created': 'Wed, 25 Jan 2023 20:05:29 GMT'}, {'version': 'v4', 'created': 'Mon, 27 Feb 2023 17:09:15 GMT'}]",2023-02-28,"[['Li', 'Kenneth', ''], ['Hopkins', 'Aspen K.', ''], ['Bau', 'David', ''], ['Vigas', 'Fernanda', ''], ['Pfister', 'Hanspeter', ''], ['Wattenberg', 'Martin', '']]",0,1,2022-10-24,4,6,3,0,0,0,b34a7d4f40d7653b15f4e2e04405cf59b8821d5d,253098566.0,https://www.semanticscholar.org/paper/b34a7d4f40d7653b15f4e2e04405cf59b8821d5d,International Conference on Learning Representations,2022.0,34.0,67.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2149140708', 'name': 'Kenneth Li'}, {'authorId': '1934459700', 'name': 'Aspen K. Hopkins'}, {'authorId': '144159726', 'name': 'David Bau'}, {'authorId': '2064951472', 'name': ""Fernanda Vi'egas""}, {'authorId': '143758236', 'name': 'H. Pfister'}, {'authorId': '145233583', 'name': 'M. Wattenberg'}]","['Massachusetts Institute of Technology', 'Harvard University', 'Northeastern University']","['China', 'United States']",2022-10
2210.13432,Hao Liu,"Hao Liu, Xinyang Geng, Lisa Lee, Igor Mordatch, Sergey Levine, Sharan
  Narang, Pieter Abbeel",Towards Better Few-Shot and Finetuning Performance with Forgetful Causal Language Models,Added T-FCM and better FCM results,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLM) trained using the next-token-prediction objective, such as GPT3 and PaLM, have revolutionized natural language processing in recent years by showing impressive zero-shot and few-shot capabilities across a wide range of tasks. In this work, we propose a simple technique that significantly boosts the performance of LLMs without adding computational cost. Our key observation is that, by performing the next token prediction task with randomly selected past tokens masked out, we can improve the quality of the learned representations for downstream language understanding tasks. We hypothesize that randomly masking past tokens prevents over-attending to recent tokens and encourages attention to tokens in the distant past. We find that our method, Forgetful Causal Masking (FCM), significantly improves both few-shot and finetuning performance of PaLM. We further consider a simple extension, T-FCM, which introduces bidirectional context to causal language model without altering the sequence order, and further improves finetuning performance. ","[{'version': 'v1', 'created': 'Mon, 24 Oct 2022 17:46:57 GMT'}, {'version': 'v2', 'created': 'Tue, 31 Jan 2023 08:08:18 GMT'}]",2023-02-01,"[['Liu', 'Hao', ''], ['Geng', 'Xinyang', ''], ['Lee', 'Lisa', ''], ['Mordatch', 'Igor', ''], ['Levine', 'Sergey', ''], ['Narang', 'Sharan', ''], ['Abbeel', 'Pieter', '']]",0,1,2022-10-24,2,7,1,2,0,2,075f83cac2742dbb36ee49d30f7aee2a322f3127,256416540.0,https://www.semanticscholar.org/paper/075f83cac2742dbb36ee49d30f7aee2a322f3127,,2022.0,62.0,2.0,2.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2143856672', 'name': 'Hao Liu'}, {'authorId': '3468192', 'name': 'Xinyang Geng'}, {'authorId': '87068304', 'name': 'Lisa Lee'}, {'authorId': '2080746', 'name': 'Igor Mordatch'}, {'authorId': '1736651', 'name': 'S. Levine'}, {'authorId': '46617804', 'name': 'Sharan Narang'}, {'authorId': '1689992', 'name': 'P. Abbeel'}]","['Google', 'Xinyang Normal University', 'University of California, Berkeley']","['China', 'United States']",2022-10
2210.13617,Yifan Hou,"Yifan Hou, Wenxiang Jiao, Meizhen Liu, Carl Allen, Zhaopeng Tu,
  Mrinmaya Sachan",Adapters for Enhanced Modeling of Multilingual Knowledge and Text,"Our code, models, and data (e.g., integration corpus and extended
  datasets) are available: https://github.com/yifan-h/Multilingual_Space",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models appear to learn facts from the large text corpora they are trained on. Such facts are encoded implicitly within their many parameters, making it difficult to verify or manipulate what knowledge has been learned. Language models have recently been extended to multilingual language models (MLLMs), enabling knowledge to be learned across hundreds of languages. Meanwhile, knowledge graphs contain facts in an explicit triple format, which require careful and costly curation and are only available in a few high-resource languages, restricting their research and application. To address these issues, we propose to enhance MLLMs with knowledge from multilingual knowledge graphs (MLKGs) so as to tackle language and knowledge graph tasks across many languages, including low-resource ones. Specifically, we introduce a lightweight adapter set to enhance MLLMs with cross-lingual entity alignment and facts from MLKGs for many languages. Experiments on common benchmarks show that such enhancement benefits both MLLMs and MLKGs, achieving: (1) comparable or improved performance for knowledge graph completion and entity alignment relative to baselines, especially for low-resource languages (for which knowledge graphs are unavailable); and (2) improved MLLM performance on language understanding tasks that require multilingual factual knowledge; all while maintaining performance on other general language tasks. ","[{'version': 'v1', 'created': 'Mon, 24 Oct 2022 21:33:42 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Oct 2022 09:03:41 GMT'}]",2022-10-27,"[['Hou', 'Yifan', ''], ['Jiao', 'Wenxiang', ''], ['Liu', 'Meizhen', ''], ['Allen', 'Carl', ''], ['Tu', 'Zhaopeng', ''], ['Sachan', 'Mrinmaya', '']]",0,0,2022-10-24,2,6,2,0,0,0,9519b1cdad21528632819e9ecece00f987e3fd8c,253107898.0,https://www.semanticscholar.org/paper/9519b1cdad21528632819e9ecece00f987e3fd8c,Conference on Empirical Methods in Natural Language Processing,2022.0,36.0,3.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2088768581', 'name': 'Yifan Hou'}, {'authorId': '12386833', 'name': 'Wenxiang Jiao'}, {'authorId': '2152974289', 'name': 'Mei-Jun Liu'}, {'authorId': '2909321', 'name': 'Zhaopeng Tu'}, {'authorId': '4054662', 'name': 'Carl Allen'}, {'authorId': '2790926', 'name': 'Mrinmaya Sachan'}]","['Tencent', 'ETH Zurich', 'Shandong University']","['China', 'Switzerland']",2022-10
2210.13673,Peng Xu,"Peng Xu, Mostofa Patwary, Shrimai Prabhumoye, Virginia Adams, Ryan J.
  Prenger, Wei Ping, Nayeon Lee, Mohammad Shoeybi and Bryan Catanzaro",Evaluating Parameter Efficient Learning for Generation,Accepted to EMNLP 2022 main conference,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Parameter efficient learning methods (PERMs) have recently gained significant attention as they provide an efficient way for pre-trained language models (PLMs) to adapt to a downstream task. However, these conclusions are mostly drawn from in-domain evaluations over the full training set. In this paper, we present comparisons between PERMs and finetuning from three new perspectives: (1) the effect of sample and model size to in-domain evaluations, (2) generalization to unseen domains and new datasets, and (3) the faithfulness of generations. Our results show that for in-domain settings (a) there is a cross point of sample size for which PERMs will perform better than finetuning when training with fewer samples, and (b) larger PLMs have larger cross points. For cross-domain and cross-dataset cases, we show that (a) Adapter (Houlsby et al., 2019) performs the best amongst all the PERMs studied here, and (b) it outperforms finetuning if the task dataset is below a certain size. We also compare the faithfulness of generations and show that PERMs can achieve better faithfulness score than finetuning, especially for small training set, by as much as 6%. Finally, we apply Adapter to MT-NLG 530b (Smith et al., 2022) and achieve new state-of-the-art results on Xsum (Narayan et al., 2018) for all ROUGE scores (ROUGE-1 49.17, ROUGE-2 27.20, ROUGE-L 40.98). ","[{'version': 'v1', 'created': 'Tue, 25 Oct 2022 00:14:48 GMT'}]",2022-10-26,"[['Xu', 'Peng', ''], ['Patwary', 'Mostofa', ''], ['Prabhumoye', 'Shrimai', ''], ['Adams', 'Virginia', ''], ['Prenger', 'Ryan J.', ''], ['Ping', 'Wei', ''], ['Lee', 'Nayeon', ''], ['Shoeybi', 'Mohammad', ''], ['Catanzaro', 'Bryan', '']]",0,0,2022-10-25,1,9,1,1,0,1,5f25d61830314a4302cf07feaf8be221d5e2d2fc,253107420.0,https://www.semanticscholar.org/paper/5f25d61830314a4302cf07feaf8be221d5e2d2fc,Conference on Empirical Methods in Natural Language Processing,2022.0,54.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '145011005', 'name': 'Peng Xu'}, {'authorId': '66870756', 'name': 'M. Patwary'}, {'authorId': '9358910', 'name': 'Shrimai Prabhumoye'}, {'authorId': '2139781966', 'name': 'Virginia Adams'}, {'authorId': '3283879', 'name': 'R. Prenger'}, {'authorId': '2056440915', 'name': 'Wei Ping'}, {'authorId': '40221187', 'name': 'Nayeon Lee'}, {'authorId': '1911755', 'name': 'M. Shoeybi'}, {'authorId': '2301680', 'name': 'Bryan Catanzaro'}]",['Hong Kong University of Science and Technology'],['China'],2022-10
2210.14128,Xiao Liu,"Chenguang Wang, Xiao Liu, Dawn Song",IELM: An Open Information Extraction Benchmark for Pre-Trained Language Models,"EMNLP 2022. arXiv admin note: substantial text overlap with
  arXiv:2010.11967",,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  We introduce a new open information extraction (OIE) benchmark for pre-trained language models (LM). Recent studies have demonstrated that pre-trained LMs, such as BERT and GPT, may store linguistic and relational knowledge. In particular, LMs are able to answer ``fill-in-the-blank'' questions when given a pre-defined relation category. Instead of focusing on pre-defined relations, we create an OIE benchmark aiming to fully examine the open relational information present in the pre-trained LMs. We accomplish this by turning pre-trained LMs into zero-shot OIE systems. Surprisingly, pre-trained LMs are able to obtain competitive performance on both standard OIE datasets (CaRB and Re-OIE2016) and two new large-scale factual OIE datasets (TAC KBP-OIE and Wikidata-OIE) that we establish via distant supervision. For instance, the zero-shot pre-trained LMs outperform the F1 score of the state-of-the-art supervised OIE methods on our factual OIE datasets without needing to use any training sets. Our code and datasets are available at https://github.com/cgraywang/IELM ","[{'version': 'v1', 'created': 'Tue, 25 Oct 2022 16:25:00 GMT'}]",2022-10-26,"[['Wang', 'Chenguang', ''], ['Liu', 'Xiao', ''], ['Song', 'Dawn', '']]",0,1,2022-10-25,1,3,3,0,0,0,1933a0ef47f8d2ba4a8277d702d522a06319302c,253107490.0,https://www.semanticscholar.org/paper/1933a0ef47f8d2ba4a8277d702d522a06319302c,Conference on Empirical Methods in Natural Language Processing,2022.0,70.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108755854', 'name': 'Chenguang Wang'}, {'authorId': '2111312892', 'name': 'Xiao Liu'}, {'authorId': '2112739650', 'name': 'Dawn Song'}]","['Tsinghua University', 'Washington University in St. Louis']","['China', 'United States']",2022-10
2210.14709,Jianan Zhao,"Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing
  Xie, Jian Tang",Learning on Large-scale Text-attributed Graphs via Variational Inference,ICLR 2023,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper studies learning on text-attributed graphs (TAGs), where each node is associated with a text description. An ideal solution for such a problem would be integrating both the text and graph structure information with large language models and graph neural networks (GNNs). However, the problem becomes very challenging when graphs are large due to the high computational complexity brought by training large language models and GNNs together. In this paper, we propose an efficient and effective solution to learning on large text-attributed graphs by fusing graph structure and language learning with a variational Expectation-Maximization (EM) framework, called GLEM. Instead of simultaneously training large language models and GNNs on big graphs, GLEM proposes to alternatively update the two modules in the E-step and M-step. Such a procedure allows training the two modules separately while simultaneously allowing the two modules to interact and mutually enhance each other. Extensive experiments on multiple data sets demonstrate the efficiency and effectiveness of the proposed approach. ","[{'version': 'v1', 'created': 'Wed, 26 Oct 2022 13:40:57 GMT'}, {'version': 'v2', 'created': 'Wed, 1 Mar 2023 14:09:04 GMT'}]",2023-03-02,"[['Zhao', 'Jianan', ''], ['Qu', 'Meng', ''], ['Li', 'Chaozhuo', ''], ['Yan', 'Hao', ''], ['Liu', 'Qian', ''], ['Li', 'Rui', ''], ['Xie', 'Xing', ''], ['Tang', 'Jian', '']]",0,0,2022-10-26,2,8,1,0,0,0,8bb37e8ae7dd6fa8cab2407f63a61f697152717f,253117079.0,https://www.semanticscholar.org/paper/8bb37e8ae7dd6fa8cab2407f63a61f697152717f,International Conference on Learning Representations,2022.0,37.0,26.0,6.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '48018967', 'name': 'Jianan Zhao'}, {'authorId': '145252498', 'name': 'Meng Qu'}, {'authorId': '2869810', 'name': 'Chaozhuo Li'}, {'authorId': '2152208710', 'name': 'Hao Yan'}, {'authorId': '1409707585', 'name': 'Qian Liu'}, {'authorId': '1500522974', 'name': 'Rui Li'}, {'authorId': '2110972816', 'name': 'Xing Xie'}, {'authorId': '152226504', 'name': 'Jian Tang'}]","['Universit de Montral', 'Canadian Institute for Advanced Research', 'Microsoft', 'Dalian University of Technology', 'University of Milan', 'Central South University']","['Canada', 'China', 'Italy']",2022-10
2210.14739,Yara Rizk,"Yara Rizk, Praveen Venkateswaran, Vatche Isahagian, Vinod Muthusamy",A Case for Business Process-Specific Foundation Models,,,,,cs.AI cs.NE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The inception of large language models has helped advance state-of-the-art performance on numerous natural language tasks. This has also opened the door for the development of foundation models for other domains and data modalities such as images, code, and music. In this paper, we argue that business process data representations have unique characteristics that warrant the development of a new class of foundation models to handle tasks like process mining, optimization, and decision making. These models should also tackle the unique challenges of applying AI to business processes which include data scarcity, multi-modal representations, domain specific terminology, and privacy concerns. ","[{'version': 'v1', 'created': 'Wed, 26 Oct 2022 14:17:47 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Nov 2022 19:35:57 GMT'}]",2022-12-02,"[['Rizk', 'Yara', ''], ['Venkateswaran', 'Praveen', ''], ['Isahagian', 'Vatche', ''], ['Muthusamy', 'Vinod', '']]",0,0,2022-10-26,2,4,2,0,0,0,30477855d76058a9b542cabea3058aad1a837d51,253116622.0,https://www.semanticscholar.org/paper/30477855d76058a9b542cabea3058aad1a837d51,arXiv.org,2022.0,51.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2587541', 'name': 'Yara Rizk'}, {'authorId': '2077182665', 'name': 'P. Venkateswaran'}, {'authorId': '1482469814', 'name': 'Vatche Ishakian'}, {'authorId': '46761645', 'name': 'Vinod Muthusamy'}]",['IBM Research - China'],['China'],2022-10
2210.15456,Yi Gu,"Mo Yu, Yi Gu, Xiaoxiao Guo, Yufei Feng, Xiaodan Zhu, Michael
  Greenspan, Murray Campbell, Chuang Gan",JECC: Commonsense Reasoning Tasks Derived from Interactive Fictions,arXiv admin note: text overlap with arXiv:2010.09788,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Commonsense reasoning simulates the human ability to make presumptions about our physical world, and it is an essential cornerstone in building general AI systems. We propose a new commonsense reasoning dataset based on human's Interactive Fiction (IF) gameplay walkthroughs as human players demonstrate plentiful and diverse commonsense reasoning. The new dataset provides a natural mixture of various reasoning types and requires multi-hop reasoning. Moreover, the IF game-based construction procedure requires much less human interventions than previous ones. Different from existing benchmarks, our dataset focuses on the assessment of functional commonsense knowledge rules rather than factual knowledge. Hence, in order to achieve higher performance on our tasks, models need to effectively utilize such functional knowledge to infer the outcomes of actions, rather than relying solely on memorizing facts. Experiments show that the introduced dataset is challenging to previous machine reading models as well as the new large language models with a significant 20% performance gap compared to human experts. ","[{'version': 'v1', 'created': 'Tue, 18 Oct 2022 19:20:53 GMT'}, {'version': 'v2', 'created': 'Fri, 26 May 2023 05:40:19 GMT'}]",2023-05-29,"[['Yu', 'Mo', ''], ['Gu', 'Yi', ''], ['Guo', 'Xiaoxiao', ''], ['Feng', 'Yufei', ''], ['Zhu', 'Xiaodan', ''], ['Greenspan', 'Michael', ''], ['Campbell', 'Murray', ''], ['Gan', 'Chuang', '']]",0,0,2022-10-18,2,8,2,0,0,0,868f9bb603dfa8f6951787040fc6d62c909a15c2,237262875.0,https://www.semanticscholar.org/paper/868f9bb603dfa8f6951787040fc6d62c909a15c2,Annual Meeting of the Association for Computational Linguistics,2022.0,30.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2115621395', 'name': 'Mo Yu'}, {'authorId': '121433787', 'name': 'Xiaoxiao Guo'}, {'authorId': '2150671498', 'name': 'Yufei Feng'}, {'authorId': '2112578816', 'name': 'Yi Gu'}, {'authorId': '150345740', 'name': 'Xiaodan Zhu'}, {'authorId': '35019530', 'name': 'M. Greenspan'}, {'authorId': '2114676694', 'name': 'Murray Campbell'}, {'authorId': '2056157586', 'name': 'Chuang Gan'}]","[""Queen's University"", 'IBM Research - China']","['China', 'Canada']",2022-10
2211.00818,Yihong Dong,"Yihong Dong, Xue Jiang, Yuchen Liu, Ge Li, Zhi Jin",CodePAD: Sequence-based Code Generation with Pushdown Automaton,Accepted to ISSTA 2023 (Technical Papers),,,,cs.SE cs.AI,http://creativecommons.org/licenses/by/4.0/,"  In the process of code generation, it is essential to guarantee the generated code satisfies grammar constraints of programming language (PL). However, neglecting grammar constraints is a fatal drawback of commonly used sequence-based code generation. In this paper, we devise a pushdown automaton (PDA)-based methodology to address this problem, exploiting the principle that PL is a subset of PDA recognizable language and code accepted by PDA is grammatical. Specifically, we construct a PDA module and design an algorithm to constrain the generation of sequence-based models to ensure grammatical correctness. Guided by this methodology, we further propose CodePAD, a sequence-based code generation framework equipped with a PDA module, to integrate the deduction of PDA into deep learning. Additionally, this framework can leverage states of PDA deduction (including state representation, state prediction task, and joint prediction with state) to assist models in learning PDA deduction. To comprehensively evaluate CodePAD, we construct a PDA for Python and conduct extensive experiments on four public benchmark datasets. CodePAD can leverage existing sequence-based models, and we show that it can achieve 100\% grammatical correctness percentage on these benchmark datasets. Thus, it relatively improve 17\% CodeBLEU on CONALA, 8\% EM on DJANGO, and 15\% CodeBLEU on JUICE-10K compared to base models. In addition, our method significantly enhances pre-trained models, e.g., CodeBLEU of CodeGen-350M improvement from 3.21 to 21.54 on MBPP in zero-shot setting. ","[{'version': 'v1', 'created': 'Wed, 2 Nov 2022 01:40:18 GMT'}, {'version': 'v2', 'created': 'Mon, 14 Nov 2022 16:53:15 GMT'}, {'version': 'v3', 'created': 'Mon, 9 Jan 2023 06:14:56 GMT'}, {'version': 'v4', 'created': 'Tue, 17 Jan 2023 03:14:35 GMT'}]",2023-01-18,"[['Dong', 'Yihong', ''], ['Jiang', 'Xue', ''], ['Liu', 'Yuchen', ''], ['Li', 'Ge', ''], ['Jin', 'Zhi', '']]",0,0,2022-11-02,4,5,2,1,1,0,3cba16fc46ac5b35c1cc72a822208aa0097384cc,255546659.0,https://www.semanticscholar.org/paper/3cba16fc46ac5b35c1cc72a822208aa0097384cc,,2022.0,40.0,1.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '26845858', 'name': 'Yihong Dong'}, {'authorId': '2199808863', 'name': 'Xue Jiang'}, {'authorId': '2183078810', 'name': 'Yuchen Liu'}, {'authorId': '2154591375', 'name': 'Ge Li'}, {'authorId': '2152843753', 'name': 'Zhi Jin'}]",['Peking University'],['China'],2022-11
2211.01334,Zhang Junlin,Pengtao Zhang and Junlin Zhang,MemoNet: Memorizing All Cross Features' Representations Efficiently via Multi-Hash Codebook Network for CTR Prediction,,"ACM International Conference on Information and Knowledge
  Management(CIKM '23), October 21-25,2023,Birmingham,United Kingdom",10.1145/3583780.3614963,,cs.IR cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  New findings in natural language processing (NLP) demonstrate that the strong memorization capability contributes a lot to the success of Large Language Models (LLM). This inspires us to explicitly bring an independent memory mechanism into CTR ranking model to learn and memorize cross features' representations. In this paper, we propose multi-Hash Codebook NETwork (HCNet) as the memory mechanism for efficiently learning and memorizing representations of cross features in CTR tasks. HCNet uses a multi-hash codebook as the main memory place and the whole memory procedure consists of three phases: multi-hash addressing, memory restoring, and feature shrinking. We also propose a new CTR model named MemoNet which combines HCNet with a DNN backbone. Extensive experimental results on three public datasets and online test show that MemoNet reaches superior performance over state-of-the-art approaches. Besides, MemoNet shows scaling law of large language model in NLP, which means we can enlarge the size of the codebook in HCNet to sustainably obtain performance gains. Our work demonstrates the importance and feasibility of learning and memorizing representations of cross features, which sheds light on a new promising research direction. ","[{'version': 'v1', 'created': 'Tue, 25 Oct 2022 12:08:14 GMT'}, {'version': 'v2', 'created': 'Thu, 3 Nov 2022 06:49:56 GMT'}, {'version': 'v3', 'created': 'Mon, 4 Sep 2023 08:58:04 GMT'}]",2023-09-06,"[['Zhang', 'Pengtao', ''], ['Zhang', 'Junlin', '']]",0,0,2022-10-25,3,2,3,0,0,0,a43101d6dcaf417cdd5b143654cb51273422a49e,253254821.0,https://www.semanticscholar.org/paper/a43101d6dcaf417cdd5b143654cb51273422a49e,International Conference on Information and Knowledge Management,2022.0,36.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '40627025', 'name': 'P. Zhang'}, {'authorId': '2048571', 'name': 'Junlin Zhang'}]","['Sina Weibo Beijing, China']",['China'],2022-10
2211.02556,Lingxi Xie,"Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, Qi Tian",Pangu-Weather: A 3D High-Resolution Model for Fast and Accurate Global Weather Forecast,"19 pages, 13 figures: the first ever AI-based method that outperforms
  traditional numerical weather prediction methods",,,,physics.ao-ph cs.AI cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we present Pangu-Weather, a deep learning based system for fast and accurate global weather forecast. For this purpose, we establish a data-driven environment by downloading $43$ years of hourly global weather data from the 5th generation of ECMWF reanalysis (ERA5) data and train a few deep neural networks with about $256$ million parameters in total. The spatial resolution of forecast is $0.25^\circ\times0.25^\circ$, comparable to the ECMWF Integrated Forecast Systems (IFS). More importantly, for the first time, an AI-based method outperforms state-of-the-art numerical weather prediction (NWP) methods in terms of accuracy (latitude-weighted RMSE and ACC) of all factors (e.g., geopotential, specific humidity, wind speed, temperature, etc.) and in all time ranges (from one hour to one week). There are two key strategies to improve the prediction accuracy: (i) designing a 3D Earth Specific Transformer (3DEST) architecture that formulates the height (pressure level) information into cubic data, and (ii) applying a hierarchical temporal aggregation algorithm to alleviate cumulative forecast errors. In deterministic forecast, Pangu-Weather shows great advantages for short to medium-range forecast (i.e., forecast time ranges from one hour to one week). Pangu-Weather supports a wide range of downstream forecast scenarios, including extreme weather forecast (e.g., tropical cyclone tracking) and large-member ensemble forecast in real-time. Pangu-Weather not only ends the debate on whether AI-based methods can surpass conventional NWP methods, but also reveals novel directions for improving deep learning weather forecast systems. ","[{'version': 'v1', 'created': 'Thu, 3 Nov 2022 17:19:43 GMT'}]",2022-11-07,"[['Bi', 'Kaifeng', ''], ['Xie', 'Lingxi', ''], ['Zhang', 'Hengheng', ''], ['Chen', 'Xin', ''], ['Gu', 'Xiaotao', ''], ['Tian', 'Qi', '']]",0,0,2022-11-03,1,6,4,0,0,0,4405874879827cacf199d26e8e23e4f547f72a2c,253370735.0,https://www.semanticscholar.org/paper/4405874879827cacf199d26e8e23e4f547f72a2c,arXiv.org,2022.0,59.0,52.0,9.0,True,"['Computer Science', 'Physics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Physics', 'source': 'external'}, {'category': 'Environmental Science', 'source': 's2-fos-model'}]","[{'authorId': '94365920', 'name': 'Kaifeng Bi'}, {'authorId': '3041937', 'name': 'Lingxi Xie'}, {'authorId': '1983351', 'name': 'Hengheng Zhang'}, {'authorId': '2145229597', 'name': 'Xin Chen'}, {'authorId': '7787721', 'name': 'Xiaotao Gu'}, {'authorId': '1400120070', 'name': 'Qi Tian'}]",['Huawei Technologies (China)'],['China'],2022-11
2211.04668,Chonghua Liao,"Chonghua Liao, Yanan Zheng, Zhilin Yang",Zero-Label Prompt Selection,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Natural language prompts have been shown to facilitate cross-task generalization for large language models. However, with no or limited labeled examples, the cross-task performance is highly sensitive to the choice of prompts, while selecting a high-performing prompt is challenging given the scarcity of labels. To address the issue, we propose a Zero-Label Prompt Selection (ZPS) method that selects prompts without any labeled data or gradient update. Specifically, given the candidate human-written prompts for a task, ZPS labels a set of unlabeled data with a prompt ensemble and uses the pseudo-labels for prompt selection. Experiments show that ZPS improves over prior methods by a sizeable margin in zero-label performance. We also extend ZPS to a few-shot setting and show its advantages over strong baselines such as prompt tuning and model tuning. ","[{'version': 'v1', 'created': 'Wed, 9 Nov 2022 04:13:31 GMT'}]",2022-11-10,"[['Liao', 'Chonghua', ''], ['Zheng', 'Yanan', ''], ['Yang', 'Zhilin', '']]",0,0,2022-11-09,1,3,1,0,0,0,5de4860323ffaba9b7f5aefeedc2d8db2a529a96,253420466.0,https://www.semanticscholar.org/paper/5de4860323ffaba9b7f5aefeedc2d8db2a529a96,arXiv.org,2022.0,51.0,3.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2150047631', 'name': 'Chonghua Liao'}, {'authorId': '2111090780', 'name': 'Yanan Zheng'}, {'authorId': '2109512754', 'name': 'Zhilin Yang'}]","['Shanghai Artificial Intelligence Laboratory', 'Tsinghua University', 'Shanghai Qizhi Institute']",['China'],2022-11
2211.06679,Guang Liu,"Zhongzhi Chen, Guang Liu, Bo-Wen Zhang, Fulong Ye, Qinghong Yang,
  Ledell Wu",AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work, we present a conceptually simple and effective method to train a strong bilingual/multilingual multimodal representation model. Starting from the pre-trained multimodal representation model CLIP released by OpenAI, we altered its text encoder with a pre-trained multilingual text encoder XLM-R, and aligned both languages and image representations by a two-stage training schema consisting of teacher learning and contrastive learning. We validate our method through evaluations of a wide range of tasks. We set new state-of-the-art performances on a bunch of tasks including ImageNet-CN, Flicker30k-CN, COCO-CN and XTD. Further, we obtain very close performances with CLIP on almost all tasks, suggesting that one can simply alter the text encoder in CLIP for extended capabilities such as multilingual understanding. Our models and code are available at https://github.com/FlagAI-Open/FlagAI. ","[{'version': 'v1', 'created': 'Sat, 12 Nov 2022 14:48:55 GMT'}, {'version': 'v2', 'created': 'Mon, 21 Nov 2022 15:39:52 GMT'}]",2022-11-22,"[['Chen', 'Zhongzhi', ''], ['Liu', 'Guang', ''], ['Zhang', 'Bo-Wen', ''], ['Ye', 'Fulong', ''], ['Yang', 'Qinghong', ''], ['Wu', 'Ledell', '']]",0,0,2022-11-12,2,6,1,0,0,0,4cee5b151e0f309b070525a4252a25bbb10bc0a7,253511222.0,https://www.semanticscholar.org/paper/4cee5b151e0f309b070525a4252a25bbb10bc0a7,Annual Meeting of the Association for Computational Linguistics,2022.0,60.0,27.0,7.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '3012401', 'name': 'Zhongzhi Chen'}, {'authorId': '2152756082', 'name': 'Guangyi Liu'}, {'authorId': '2141897317', 'name': 'Bo Zhang'}, {'authorId': '2190755364', 'name': 'Fulong Ye'}, {'authorId': '2164215173', 'name': 'Qinghong Yang'}, {'authorId': '51183248', 'name': 'Ledell Yu Wu'}]","['Beijing University of Posts and Telecommunications', 'Beihang University', 'Beijing Academy of Artificial Intelligence']",['China'],2022-11
2211.06778,Qiuhao Lu,"Qiuhao Lu, Dejing Dou, Thien Huu Nguyen",Textual Data Augmentation for Patient Outcomes Prediction,BIBM 2021,,10.1109/BIBM52615.2021.9669861,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep learning models have demonstrated superior performance in various healthcare applications. However, the major limitation of these deep models is usually the lack of high-quality training data due to the private and sensitive nature of this field. In this study, we propose a novel textual data augmentation method to generate artificial clinical notes in patients' Electronic Health Records (EHRs) that can be used as additional training data for patient outcomes prediction. Essentially, we fine-tune the generative language model GPT-2 to synthesize labeled text with the original training data. More specifically, We propose a teacher-student framework where we first pre-train a teacher model on the original data, and then train a student model on the GPT-augmented data under the guidance of the teacher. We evaluate our method on the most common patient outcome, i.e., the 30-day readmission rate. The experimental results show that deep models can improve their predictive performance with the augmented data, indicating the effectiveness of the proposed architecture. ","[{'version': 'v1', 'created': 'Sun, 13 Nov 2022 01:07:23 GMT'}]",2022-11-15,"[['Lu', 'Qiuhao', ''], ['Dou', 'Dejing', ''], ['Nguyen', 'Thien Huu', '']]",0,1,2022-11-13,1,3,2,1,1,0,3fed6f440e4f2cc67312524f2444434f733811a7,245142427.0,https://www.semanticscholar.org/paper/3fed6f440e4f2cc67312524f2444434f733811a7,IEEE International Conference on Bioinformatics and Biomedicine,2021.0,24.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '35539899', 'name': 'Qiuhao Lu'}, {'authorId': '1721158', 'name': 'D. Dou'}, {'authorId': '1811211', 'name': 'Thien Huu Nguyen'}]","['University of Oregon', 'Baidu']","['China', 'United States']",2022-11
2211.08073,Linyi Yang,"Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng
  Liu, Jindong Wang, Xing Xie, Yue Zhang",GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective,Accepted to ACL-23 Findings,,,,cs.CL cs.AI cs.LG cs.PF,http://creativecommons.org/licenses/by-sa/4.0/,"  Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods. This paper presents the first attempt at creating a unified benchmark named GLUE-X for evaluating OOD robustness in NLP models, highlighting the importance of OOD robustness and providing insights on how to measure the robustness of a model and how to improve it. The benchmark includes 13 publicly available datasets for OOD testing, and evaluations are conducted on 8 classic NLP tasks over 21 popularly used PLMs, including GPT-3 and GPT-3.5. Our findings confirm the need for improved OOD accuracy in NLP tasks, as significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy. ","[{'version': 'v1', 'created': 'Tue, 15 Nov 2022 11:53:55 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Dec 2022 16:03:56 GMT'}, {'version': 'v3', 'created': 'Fri, 12 May 2023 02:14:58 GMT'}, {'version': 'v4', 'created': 'Mon, 22 May 2023 11:55:52 GMT'}]",2023-05-23,"[['Yang', 'Linyi', ''], ['Zhang', 'Shuibai', ''], ['Qin', 'Libo', ''], ['Li', 'Yafu', ''], ['Wang', 'Yidong', ''], ['Liu', 'Hanmeng', ''], ['Wang', 'Jindong', ''], ['Xie', 'Xing', ''], ['Zhang', 'Yue', '']]",0,1,2022-11-15,4,9,4,1,0,1,0e127ff323cf37cc32bdb227ca8498ea7e6ee389,253523094.0,https://www.semanticscholar.org/paper/0e127ff323cf37cc32bdb227ca8498ea7e6ee389,Annual Meeting of the Association for Computational Linguistics,2022.0,101.0,26.0,6.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2145500840', 'name': 'Linyi Yang'}, {'authorId': '2190935184', 'name': 'Shuibai Zhang'}, {'authorId': '49169076', 'name': 'Libo Qin'}, {'authorId': '2110450452', 'name': 'Yafu Li'}, {'authorId': '2108024273', 'name': 'Yidong Wang'}, {'authorId': '2118960911', 'name': 'Hanmeng Liu'}, {'authorId': '1519290245', 'name': 'Jindong Wang'}, {'authorId': '1576441343', 'name': 'Xingxu Xie'}, {'authorId': '39939186', 'name': 'Yue Zhang'}]","['Westlake University', 'Xidian University', 'Institute for Advanced Study', 'Microsoft', 'Central South University']","['China', 'United States']",2022-11
2211.08316,Changlong Yu,"Changlong Yu, Weiqi Wang, Xin Liu, Jiaxin Bai, Yangqiu Song, Zheng Li,
  Yifan Gao, Tianyu Cao, and Bing Yin",FolkScope: Intention Knowledge Graph Construction for E-commerce Commonsense Discovery,ACL Findings 2023,,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  Understanding users' intentions in e-commerce platforms requires commonsense knowledge. In this paper, we present FolkScope, an intention knowledge graph construction framework to reveal the structure of humans' minds about purchasing items. As commonsense knowledge is usually ineffable and not expressed explicitly, it is challenging to perform information extraction. Thus, we propose a new approach that leverages the generation power of large language models~(LLMs) and human-in-the-loop annotation to semi-automatically construct the knowledge graph. LLMs first generate intention assertions via e-commerce-specific prompts to explain shopping behaviors, where the intention can be an open reason or a predicate falling into one of 18 categories aligning with ConceptNet, e.g., IsA, MadeOf, UsedFor, etc. Then we annotate plausibility and typicality labels of sampled intentions as training data in order to populate human judgments to all automatic generations. Last, to structurize the assertions, we propose pattern mining and conceptualization to form more condensed and abstract knowledge. Extensive evaluations and studies demonstrate that our constructed knowledge graph can well model e-commerce knowledge and have many potential applications. ","[{'version': 'v1', 'created': 'Tue, 15 Nov 2022 17:20:40 GMT'}, {'version': 'v2', 'created': 'Thu, 11 May 2023 16:33:50 GMT'}]",2023-05-12,"[['Yu', 'Changlong', ''], ['Wang', 'Weiqi', ''], ['Liu', 'Xin', ''], ['Bai', 'Jiaxin', ''], ['Song', 'Yangqiu', ''], ['Li', 'Zheng', ''], ['Gao', 'Yifan', ''], ['Cao', 'Tianyu', ''], ['Yin', 'Bing', '']]",0,0,2022-11-15,2,9,1,0,0,0,af1ec939e26e2467fb4c51f4ca3c85f12579c5a1,258615139.0,https://www.semanticscholar.org/paper/af1ec939e26e2467fb4c51f4ca3c85f12579c5a1,Annual Meeting of the Association for Computational Linguistics,2022.0,78.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '49778308', 'name': 'Changlong Yu'}, {'authorId': '1587728690', 'name': 'Weiqi Wang'}, {'authorId': '89121677', 'name': 'Xin Liu'}, {'authorId': '145677395', 'name': 'Jiaxin Bai'}, {'authorId': '1809614', 'name': 'Yangqiu Song'}, {'authorId': '2146249169', 'name': 'Zheng Li'}, {'authorId': '1921742', 'name': 'Yifan Gao'}, {'authorId': '2151070', 'name': 'Tianyu Cao'}, {'authorId': '2021632793', 'name': 'Bing Yin'}]","['Amazon', 'Hong Kong University of Science and Technology']","['China', 'United States']",2022-11
2211.09783,Yulong Chen,"Yulong Chen, Yang Liu, Ruochen Xu, Ziyi Yang, Chenguang Zhu, Michael
  Zeng, Yue Zhang",UniSumm and SummZoo: Unified Model and Diverse Benchmark for Few-Shot Summarization,ACL2023 main conference,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The high annotation costs and diverse demands of various summarization tasks motivate the development of few-shot summarization. However, despite the emergence of many summarization tasks and datasets, the current training paradigm for few-shot summarization systems ignores potentially shareable knowledge in heterogeneous datasets. To this end, we propose \textsc{UniSumm}, a unified few-shot summarization model pre-trained with multiple summarization tasks and can be prefix-tuned to excel at any few-shot summarization task. Meanwhile, to better evaluate few-shot summarizers, under the principles of diversity and robustness, we assemble and release a new benchmark \textsc{SummZoo}. It consists of $8$ summarization tasks with multiple sets of few-shot samples for each task, covering diverse domains. Experimental results and analysis show that \textsc{UniSumm} outperforms strong baselines by a large margin across all sub-tasks in \textsc{SummZoo} under both automatic and human evaluations and achieves comparable results in human evaluation compared with a GPT-3.5 model. ","[{'version': 'v1', 'created': 'Thu, 17 Nov 2022 18:54:47 GMT'}, {'version': 'v2', 'created': 'Mon, 21 Nov 2022 15:16:40 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Dec 2022 08:54:22 GMT'}, {'version': 'v4', 'created': 'Tue, 13 Dec 2022 14:57:14 GMT'}, {'version': 'v5', 'created': 'Mon, 19 Dec 2022 05:15:58 GMT'}, {'version': 'v6', 'created': 'Sat, 27 May 2023 19:28:00 GMT'}]",2023-05-30,"[['Chen', 'Yulong', ''], ['Liu', 'Yang', ''], ['Xu', 'Ruochen', ''], ['Yang', 'Ziyi', ''], ['Zhu', 'Chenguang', ''], ['Zeng', 'Michael', ''], ['Zhang', 'Yue', '']]",0,1,2022-11-17,6,7,1,1,0,1,732fbf857fb46cbafb19eba691cded67be43143d,258958993.0,https://www.semanticscholar.org/paper/732fbf857fb46cbafb19eba691cded67be43143d,Annual Meeting of the Association for Computational Linguistics,2022.0,72.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2109404730', 'name': 'Yulong Chen'}, {'authorId': '2152797401', 'name': 'Yang Liu'}, {'authorId': '8233965', 'name': 'Ruochen Xu'}, {'authorId': '2155459391', 'name': 'Ziyi Yang'}, {'authorId': '8652308', 'name': 'Chenguang Zhu'}, {'authorId': '48262024', 'name': 'Michael Zeng'}, {'authorId': '2145913600', 'name': 'Yue Zhang'}]","['Westlake University', 'Institute for Advanced Study', 'Microsoft', 'Zhejiang University']","['China', 'India', 'United States']",2022-11
2211.10438,Guangxuan Xiao,"Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, Song
  Han",SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models,ICML 2023. First two authors contributed equally to this work,,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, and LLaMA family. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs. Code is available at https://github.com/mit-han-lab/smoothquant. ","[{'version': 'v1', 'created': 'Fri, 18 Nov 2022 18:59:33 GMT'}, {'version': 'v2', 'created': 'Mon, 28 Nov 2022 18:59:16 GMT'}, {'version': 'v3', 'created': 'Wed, 4 Jan 2023 14:36:07 GMT'}, {'version': 'v4', 'created': 'Tue, 14 Feb 2023 21:31:03 GMT'}, {'version': 'v5', 'created': 'Mon, 5 Jun 2023 21:21:28 GMT'}]",2023-06-07,"[['Xiao', 'Guangxuan', ''], ['Lin', 'Ji', ''], ['Seznec', 'Mickael', ''], ['Wu', 'Hao', ''], ['Demouth', 'Julien', ''], ['Han', 'Song', '']]",0,0,2022-11-18,5,6,3,5,4,1,2c994fadbb84fb960d8306ee138dbeef41a5b323,253708271.0,https://www.semanticscholar.org/paper/2c994fadbb84fb960d8306ee138dbeef41a5b323,International Conference on Machine Learning,2022.0,45.0,116.0,28.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2046958974', 'name': 'Guangxuan Xiao'}, {'authorId': '46698300', 'name': 'Ji Lin'}, {'authorId': '66890356', 'name': 'Mickael Seznec'}, {'authorId': '32604218', 'name': 'Julien Demouth'}, {'authorId': '143840275', 'name': 'Song Han'}]","['Jilin University', 'Massachusetts Institute of Technology']","['China', 'United States']",2022-11
2211.11216,Shangda Wu,"Shangda Wu, Maosong Sun",Exploring the Efficacy of Pre-trained Checkpoints in Text-to-Music Generation Task,Accepted by the Creative AI Across Modalities workshop at AAAI 2023,,,,cs.SD cs.CL eess.AS,http://creativecommons.org/licenses/by/4.0/,"  Benefiting from large-scale datasets and pre-trained models, the field of generative models has recently gained significant momentum. However, most datasets for symbolic music are very small, which potentially limits the performance of data-driven multimodal models. An intuitive solution to this problem is to leverage pre-trained models from other modalities (e.g., natural language) to improve the performance of symbolic music-related multimodal tasks. In this paper, we carry out the first study of generating complete and semantically consistent symbolic music scores from text descriptions, and explore the efficacy of using publicly available checkpoints (i.e., BERT, GPT-2, and BART) for natural language processing in the task of text-to-music generation. Our experimental results show that the improvement from using pre-trained checkpoints is statistically significant in terms of BLEU score and edit distance similarity. We analyse the capabilities and limitations of our model to better understand the potential of language-music models. ","[{'version': 'v1', 'created': 'Mon, 21 Nov 2022 07:19:17 GMT'}, {'version': 'v2', 'created': 'Wed, 4 Jan 2023 01:06:19 GMT'}]",2023-01-05,"[['Wu', 'Shangda', ''], ['Sun', 'Maosong', '']]",0,1,2022-11-21,2,2,3,1,1,0,120f4e798d78c3f6dceed218bee1ca83a5855f55,253734672.0,https://www.semanticscholar.org/paper/120f4e798d78c3f6dceed218bee1ca83a5855f55,arXiv.org,2022.0,24.0,6.0,1.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2152536983', 'name': 'Shangda Wu'}, {'authorId': '1753344', 'name': 'Maosong Sun'}]",['Tsinghua University'],['China'],2022-11
2211.12787,Xiang Gao,Xiang Gao and Yannic Noller and Abhik Roychoudhury,Program Repair,arXiv admin note: text overlap with arXiv:2012.06824 by other authors,,,,cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Automated program repair is an emerging technology which consists of a suite of techniques to automatically fix bugs or vulnerabilities in programs. In this paper, we present a comprehensive survey of the state of the art in program repair. We first study the different suite of techniques used including search based repair, constraint based repair and learning based repair. We then discuss one of the main challenges in program repair namely patch overfitting, by distilling a class of techniques which can alleviate patch overfitting. We then discuss classes of program repair tools, applications of program repair as well as uses of program repair in industry. We conclude the survey with a forward looking outlook on future usages of program repair, as well as research opportunities arising from work on code from large language models. ","[{'version': 'v1', 'created': 'Wed, 23 Nov 2022 09:04:45 GMT'}]",2022-11-24,"[['Gao', 'Xiang', ''], ['Noller', 'Yannic', ''], ['Roychoudhury', 'Abhik', '']]",0,0,2022-11-23,1,3,1,0,0,0,30624a18720bf93a85dc3efe570df271a8c9f4c3,253801580.0,https://www.semanticscholar.org/paper/30624a18720bf93a85dc3efe570df271a8c9f4c3,arXiv.org,2022.0,105.0,9.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '143856160', 'name': 'Xiang Gao'}, {'authorId': '32628820', 'name': 'Yannic Noller'}, {'authorId': '1789700', 'name': 'Abhik Roychoudhury'}]","['National University of Singapore', 'Beihang University']","['China', 'Singapore']",2022-11
2211.13112,Lukasz Augustyniak,"{\L}ukasz Augustyniak, Kamil Tagowski, Albert Sawczyn, Denis Janiak,
  Roman Bartusiak, Adrian Szymczak, Marcin W\k{a}troba, Arkadiusz Janz, Piotr
  Szyma\'nski, Miko{\l}aj Morzy, Tomasz Kajdanowicz, Maciej Piasecki","This is the way: designing and compiling LEPISZCZE, a comprehensive NLP benchmark for Polish","10 pages, 8 pages appendix","Thirty-sixth Conference on Neural Information Processing Systems
  Datasets and Benchmarks Track (NeurIPS 2022) - https://lepiszcze.ml",,,cs.CL cs.IR cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  The availability of compute and data to train larger and larger language models increases the demand for robust methods of benchmarking the true progress of LM training. Recent years witnessed significant progress in standardized benchmarking for English. Benchmarks such as GLUE, SuperGLUE, or KILT have become de facto standard tools to compare large language models. Following the trend to replicate GLUE for other languages, the KLEJ benchmark has been released for Polish. In this paper, we evaluate the progress in benchmarking for low-resourced languages. We note that only a handful of languages have such comprehensive benchmarks. We also note the gap in the number of tasks being evaluated by benchmarks for resource-rich English/Chinese and the rest of the world. In this paper, we introduce LEPISZCZE (the Polish word for glew, the Middle English predecessor of glue), a new, comprehensive benchmark for Polish NLP with a large variety of tasks and high-quality operationalization of the benchmark. We design LEPISZCZE with flexibility in mind. Including new models, datasets, and tasks is as simple as possible while still offering data versioning and model tracking. In the first run of the benchmark, we test 13 experiments (task and dataset pairs) based on the five most recent LMs for Polish. We use five datasets from the Polish benchmark and add eight novel datasets. As the paper's main contribution, apart from LEPISZCZE, we provide insights and experiences learned while creating the benchmark for Polish as the blueprint to design similar benchmarks for other low-resourced languages. ","[{'version': 'v1', 'created': 'Wed, 23 Nov 2022 16:51:09 GMT'}]",2022-11-24,"[['Augustyniak', 'ukasz', ''], ['Tagowski', 'Kamil', ''], ['Sawczyn', 'Albert', ''], ['Janiak', 'Denis', ''], ['Bartusiak', 'Roman', ''], ['Szymczak', 'Adrian', ''], ['Wtroba', 'Marcin', ''], ['Janz', 'Arkadiusz', ''], ['Szymaski', 'Piotr', ''], ['Morzy', 'Mikoaj', ''], ['Kajdanowicz', 'Tomasz', ''], ['Piasecki', 'Maciej', '']]",0,0,2022-11-23,1,12,3,0,0,0,e7dfa8ef33f961315837dcd808856a45fc9e97c1,252724666.0,https://www.semanticscholar.org/paper/e7dfa8ef33f961315837dcd808856a45fc9e97c1,Neural Information Processing Systems,2022.0,62.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2556544', 'name': 'Lukasz Augustyniak'}, {'authorId': '51187565', 'name': 'Kamil Tagowski'}, {'authorId': '2141196841', 'name': 'Albert Sawczyn'}, {'authorId': '2141195404', 'name': 'Denis Janiak'}, {'authorId': '3207129', 'name': 'Roman Bartusiak'}, {'authorId': '51121039', 'name': 'Adrian Szymczak'}, {'authorId': '2191898935', 'name': 'Marcin Wkatroba'}, {'authorId': '32559047', 'name': 'Arkadiusz Janz'}, {'authorId': '2065846509', 'name': ""Piotr Szyma'nski""}, {'authorId': '121244853', 'name': 'M. Morzy'}, {'authorId': '1787971', 'name': 'Tomasz Kajdanowicz'}, {'authorId': '144205338', 'name': 'Maciej Piasecki'}]","['Wrocaw University of Science and Technology', 'Pozna University of Technology', 'Wuhan University of Science and Technology']","['China', 'Poland']",2022-11
2211.15844,Guang Yang,"Guang Yang, Yu Zhou, Wenhua Yang, Tao Yue, Xiang Chen, Taolue Chen",How Important are Good Method Names in Neural Code Generation? A Model Robustness Perspective,UNDER REVIEW,,,,cs.SE,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Pre-trained code generation models (PCGMs) have been widely applied in neural code generation which can generate executable code from functional descriptions in natural languages, possibly together with signatures. Despite substantial performance improvement of PCGMs, the role of method names in neural code generation has not been thoroughly investigated. In this paper, we study and demonstrate the potential of benefiting from method names to enhance the performance of PCGMs, from a model robustness perspective. Specifically, we propose a novel approach, named RADAR (neuRAl coDe generAtor Robustifier). RADAR consists of two components: RADAR-Attack and RADAR-Defense. The former attacks a PCGM by generating adversarial method names as part of the input, which are semantic and visual similar to the original input, but may trick the PCGM to generate completely unrelated code snippets. As a countermeasure to such attacks, RADAR-Defense synthesizes a new method name from the functional description and supplies it to the PCGM. Evaluation results show that RADAR-Attack can reduce the CodeBLEU of generated code by 19.72% to 38.74% in three state-of-the-art PCGMs (i.e., CodeGPT, PLBART, and CodeT5) in the fine-tuning code generation task, and reduce the Pass@1 of generated code by 32.28% to 44.42% in three state-of-the-art PCGMs (i.e., Replit, CodeGen, and CodeT5+) in the zero-shot code generation task. Moreover, RADAR-Defense is able to reinstate the performance of PCGMs with synthesized method names. These results highlight the importance of good method names in neural code generation and implicate the benefits of studying model robustness in software engineering. ","[{'version': 'v1', 'created': 'Tue, 29 Nov 2022 00:37:35 GMT'}, {'version': 'v2', 'created': 'Sun, 30 Jul 2023 12:21:39 GMT'}]",2023-08-01,"[['Yang', 'Guang', ''], ['Zhou', 'Yu', ''], ['Yang', 'Wenhua', ''], ['Yue', 'Tao', ''], ['Chen', 'Xiang', ''], ['Chen', 'Taolue', '']]",0,0,2022-11-29,2,6,1,1,1,0,e99422aa7044c8a42d9b69c833addd7c49800e45,254070024.0,https://www.semanticscholar.org/paper/e99422aa7044c8a42d9b69c833addd7c49800e45,ACM Transactions on Software Engineering and Methodology,2022.0,106.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2149522863', 'name': 'Guang Yang'}, {'authorId': '144452434', 'name': 'Yu Zhou'}, {'authorId': '3199434', 'name': 'Wenhua Yang'}, {'authorId': '2192706593', 'name': 'Tao Yue'}, {'authorId': '2143738759', 'name': 'Xiang Chen'}, {'authorId': '3273975', 'name': 'Taolue Chen'}]","['Simula Research Laboratory', 'Nantong University', 'Nanjing University of Aeronautics and Astronautics', 'Birkbeck, University of London', 'University of London']","['China', 'United Kingdom', 'Norway']",2022-11
2211.16175,Xiaofeng Mao,"Xiaofeng Mao, Yuefeng Chen, Xiaojun Jia, Rong Zhang, Hui Xue, Zhao Li",Context-Aware Robust Fine-Tuning,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Contrastive Language-Image Pre-trained (CLIP) models have zero-shot ability of classifying an image belonging to ""[CLASS]"" by using similarity between the image and the prompt sentence ""a [CONTEXT] of [CLASS]"". Based on exhaustive text cues in ""[CONTEXT]"", CLIP model is aware of different contexts, e.g. background, style, viewpoint, and exhibits unprecedented robustness against a wide range of distribution shifts. However, recent works find further fine-tuning of CLIP models improves accuracy but sacrifices the robustness on downstream tasks. We conduct an empirical investigation to show fine-tuning will corrupt the context-aware ability of pre-trained CLIP features. To solve this problem, we propose Context-Aware Robust Fine-tuning (CAR-FT). CAR-FT regularizes the model during fine-tuning to capture the context information. Specifically, we use zero-shot prompt weights to get the context distribution contained in the image. By minimizing the Kullback-Leibler Divergence (KLD) between context distributions induced by original/fine-tuned CLIP models, CAR-FT makes the context-aware ability of CLIP inherited into downstream tasks, and achieves both higher In-Distribution (ID) and Out-Of-Distribution (OOD) accuracy. The experimental results show CAR-FT achieves superior robustness on five OOD test datasets of ImageNet, and meanwhile brings accuracy gains on nine downstream tasks. Additionally, CAR-FT surpasses previous Domain Generalization (DG) methods and gets 78.5% averaged accuracy on DomainBed benchmark, building the new state-of-the-art. ","[{'version': 'v1', 'created': 'Tue, 29 Nov 2022 13:07:41 GMT'}]",2022-11-30,"[['Mao', 'Xiaofeng', ''], ['Chen', 'Yuefeng', ''], ['Jia', 'Xiaojun', ''], ['Zhang', 'Rong', ''], ['Xue', 'Hui', ''], ['Li', 'Zhao', '']]",0,0,2022-11-29,1,6,1,0,0,0,adb89ea270e47809d3341679a2d8fe2900a4bf97,254069926.0,https://www.semanticscholar.org/paper/adb89ea270e47809d3341679a2d8fe2900a4bf97,arXiv.org,2022.0,61.0,5.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2067697557', 'name': 'Xiaofeng Mao'}, {'authorId': '47557806', 'name': 'YueFeng Chen'}, {'authorId': '144890263', 'name': 'Xiaojun Jia'}, {'authorId': '2119062118', 'name': 'Rong Zhang'}, {'authorId': '1645209767', 'name': 'Hui Xue'}, {'authorId': '2156575246', 'name': 'Zhao Li'}]","['Alibaba', 'Institute of Information Engineering', 'Zhejiang University']",['China'],2022-11
2212.01539,Xuechen Li,"Jiyan He, Xuechen Li, Da Yu, Huishuai Zhang, Janardhan Kulkarni, Yin
  Tat Lee, Arturs Backurs, Nenghai Yu, Jiang Bian",Exploring the Limits of Differentially Private Deep Learning with Group-wise Clipping,25 pages,,,,cs.LG stat.ML,http://creativecommons.org/licenses/by/4.0/,"  Differentially private deep learning has recently witnessed advances in computational efficiency and privacy-utility trade-off. We explore whether further improvements along the two axes are possible and provide affirmative answers leveraging two instantiations of \emph{group-wise clipping}. To reduce the compute time overhead of private learning, we show that \emph{per-layer clipping}, where the gradient of each neural network layer is clipped separately, allows clipping to be performed in conjunction with backpropagation in differentially private optimization. This results in private learning that is as memory-efficient and almost as fast per training update as non-private learning for many workflows of interest. While per-layer clipping with constant thresholds tends to underperform standard flat clipping, per-layer clipping with adaptive thresholds matches or outperforms flat clipping under given training epoch constraints, hence attaining similar or better task performance within less wall time. To explore the limits of scaling (pretrained) models in differentially private deep learning, we privately fine-tune the 175 billion-parameter GPT-3. We bypass scaling challenges associated with clipping gradients that are distributed across multiple devices with \emph{per-device clipping} that clips the gradient of each model piece separately on its host device. Privately fine-tuning GPT-3 with per-device clipping achieves a task performance at $\epsilon=1$ better than what is attainable by non-privately fine-tuning the largest GPT-2 on a summarization task. ","[{'version': 'v1', 'created': 'Sat, 3 Dec 2022 05:20:15 GMT'}]",2022-12-06,"[['He', 'Jiyan', ''], ['Li', 'Xuechen', ''], ['Yu', 'Da', ''], ['Zhang', 'Huishuai', ''], ['Kulkarni', 'Janardhan', ''], ['Lee', 'Yin Tat', ''], ['Backurs', 'Arturs', ''], ['Yu', 'Nenghai', ''], ['Bian', 'Jiang', '']]",0,1,2022-12-03,1,9,2,2,1,1,0ba0091c60c0346493b9ffb46ac682eee5453a53,254247299.0,https://www.semanticscholar.org/paper/0ba0091c60c0346493b9ffb46ac682eee5453a53,International Conference on Learning Representations,2022.0,70.0,16.0,3.0,True,"['Computer Science', 'Mathematics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Mathematics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2167236091', 'name': 'Jiyan He'}, {'authorId': '2145429039', 'name': 'Xuechen Li'}, {'authorId': '2111467167', 'name': 'Da Yu'}, {'authorId': '2973831', 'name': 'Huishuai Zhang'}, {'authorId': '2876647', 'name': 'Janardhan Kulkarni'}, {'authorId': '2109308930', 'name': 'Y. Lee'}, {'authorId': '2064251', 'name': 'A. Backurs'}, {'authorId': '1708598', 'name': 'Nenghai Yu'}, {'authorId': '143901037', 'name': 'J. Bian'}]","['Stanford University', 'Sun Yat-sen University', 'University of Science and Technology of China', 'Microsoft']","['China', 'United States', 'India']",2022-12
2212.03827,Collin Burns,"Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt",Discovering Latent Knowledge in Language Models Without Supervision,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels. ","[{'version': 'v1', 'created': 'Wed, 7 Dec 2022 18:17:56 GMT'}]",2022-12-08,"[['Burns', 'Collin', ''], ['Ye', 'Haotian', ''], ['Klein', 'Dan', ''], ['Steinhardt', 'Jacob', '']]",0,0,2022-12-07,1,4,3,0,0,0,89c3bd70ad33c4f8832f00ab98872b77861ee0ec,254366253.0,https://www.semanticscholar.org/paper/89c3bd70ad33c4f8832f00ab98872b77861ee0ec,International Conference on Learning Representations,2022.0,75.0,72.0,13.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '90909974', 'name': 'Collin Burns'}, {'authorId': '5935398', 'name': 'Hao-Tong Ye'}, {'authorId': '38666915', 'name': 'D. Klein'}, {'authorId': '5164568', 'name': 'J. Steinhardt'}]","['Peking University', 'University of California, Berkeley']","['China', 'United States']",2022-12
2212.04732,Zhe Liu,"Zhe Liu, Chunyang Chen, Junjie Wang, Xing Che, Yuekai Huang, Jun Hu,
  Qing Wang",Fill in the Blank: Context-aware Automated Text Input Generation for Mobile GUI Testing,"Accepted by IEEE/ACM International Conference on Software Engineering
  2023 (ICSE 2023)",,,,cs.SE,http://creativecommons.org/publicdomain/zero/1.0/,"  Automated GUI testing is widely used to help ensure the quality of mobile apps. However, many GUIs require appropriate text inputs to proceed to the next page which remains a prominent obstacle for testing coverage. Considering the diversity and semantic requirement of valid inputs (e.g., flight departure, movie name), it is challenging to automate the text input generation. Inspired by the fact that the pre-trained Large Language Model (LLM) has made outstanding progress in text generation, we propose an approach named QTypist based on LLM for intelligently generating semantic input text according to the GUI context. To boost the performance of LLM in the mobile testing scenario, we develop a prompt-based data construction and tuning method which automatically extracts the prompts and answers for model tuning. We evaluate QTypist on 106 apps from Google Play and the result shows that the passing rate of QTypist is 87%, which is 93% higher than the best baseline. We also integrate QTypist with the automated GUI testing tools and it can cover 42% more app activities, 52% more pages, and subsequently help reveal 122% more bugs compared with the raw tool. ","[{'version': 'v1', 'created': 'Fri, 9 Dec 2022 09:00:38 GMT'}]",2022-12-12,"[['Liu', 'Zhe', ''], ['Chen', 'Chunyang', ''], ['Wang', 'Junjie', ''], ['Che', 'Xing', ''], ['Huang', 'Yuekai', ''], ['Hu', 'Jun', ''], ['Wang', 'Qing', '']]",0,0,2022-12-09,1,7,1,0,0,0,ddd49e6fae7c8e081b76865a841d217008b8f3b9,254535728.0,https://www.semanticscholar.org/paper/ddd49e6fae7c8e081b76865a841d217008b8f3b9,International Conference on Software Engineering,2022.0,92.0,22.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2116746462', 'name': 'Zhe Liu'}, {'authorId': '46729152', 'name': 'Chunyang Chen'}, {'authorId': '2109763128', 'name': 'Junjie Wang'}, {'authorId': '2184146378', 'name': 'Xing Che'}, {'authorId': '2108716128', 'name': 'Yuekai Huang'}, {'authorId': '2117230675', 'name': 'Jun Hu'}, {'authorId': '2145464944', 'name': 'Qing Wang'}]","['Monash University', 'Chinese Academy of Sciences']","['China', 'Australia']",2022-12
2212.05339,Yang You,"Haichen Huang and Jiarui Fang and Hongxin Liu and Shenggui Li and Yang
  You",Elixir: Train a Large Language Model on a Small GPU Cluster,,,,,cs.DC cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In recent years, large language models have achieved great success due to their unprecedented size. However, training these models poses a challenge for most researchers as it requires a substantial number of GPUs. To reduce GPU memory usage, memory partitioning, and memory offloading have been proposed. These approaches eliminate memory redundancies and offload memory usage to the CPU and NVMe memory, respectively, enabling training on small GPU clusters. However, directly deploying these solutions often leads to suboptimal efficiency. Only experienced experts can unleash the full potential of hardware by carefully tuning the distributed configuration. Thus, we present a novel solution, Elixir, which automates efficient large-model training based on pre-runtime model profiling. Elixir aims to identify the optimal combination of partitioning and offloading techniques to maximize training throughput. In our experiments, Elixir significantly outperforms the current state-of-the-art baseline. Our optimal configuration achieves up to a 3.4$\times$ speedup on GPT-2 models compared with SOTA solutions. We hope that our work will benefit individuals who lack computing resources and expertise, granting them access to large models. The beta version of Elixir is now available at https://github.com/hpcaitech/ColossalAI/tree/feature/elixir. ","[{'version': 'v1', 'created': 'Sat, 10 Dec 2022 17:26:05 GMT'}, {'version': 'v2', 'created': 'Sun, 26 Feb 2023 14:38:09 GMT'}, {'version': 'v3', 'created': 'Wed, 31 May 2023 13:56:53 GMT'}]",2023-06-01,"[['Huang', 'Haichen', ''], ['Fang', 'Jiarui', ''], ['Liu', 'Hongxin', ''], ['Li', 'Shenggui', ''], ['You', 'Yang', '']]",0,1,2022-12-10,3,5,3,1,1,0,be157d55b4afd5be9c81619d75aa4897f5e201e4,254564302.0,https://www.semanticscholar.org/paper/be157d55b4afd5be9c81619d75aa4897f5e201e4,arXiv.org,2022.0,38.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2146286212', 'name': 'Haichen Huang'}, {'authorId': '30760010', 'name': 'Jiarui Fang'}, {'authorId': '2110176595', 'name': 'Hongxin Liu'}, {'authorId': '2156702577', 'name': 'Shenggui Li'}, {'authorId': '144259229', 'name': 'Yang You'}]","['National University of Singapore', 'HPC-AI Tech']","['China', 'Singapore']",2022-12
2212.07937,Hangyu Guo,"Hangyu Guo, Kun Zhou, Wayne Xin Zhao, Qinyu Zhang, and Ji-Rong Wen",Visually-augmented pretrained language models for NLP tasks without images,"16 pages, 2 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although pre-trained language models~(PLMs) have shown impressive performance by text-only self-supervised training, they are found lack of visual semantics or commonsense. Existing solutions often rely on explicit images for visual knowledge augmentation (requiring time-consuming retrieval or generation), and they also conduct the augmentation for the whole input text, without considering whether it is actually needed in specific inputs or tasks. To address these issues, we propose a novel \textbf{V}isually-\textbf{A}ugmented fine-tuning approach that can be generally applied to various PLMs or NLP tasks, \textbf{W}ithout using any retrieved or generated \textbf{I}mages, namely \textbf{VAWI}. Experimental results show that our approach can consistently improve the performance of BERT, RoBERTa, BART, and T5 at different scales, and outperform several competitive baselines on ten tasks. Our codes and data are publicly available at~\url{https://github.com/RUCAIBox/VAWI}. ","[{'version': 'v1', 'created': 'Thu, 15 Dec 2022 16:13:25 GMT'}, {'version': 'v2', 'created': 'Fri, 26 May 2023 14:09:49 GMT'}]",2023-05-29,"[['Guo', 'Hangyu', ''], ['Zhou', 'Kun', ''], ['Zhao', 'Wayne Xin', ''], ['Zhang', 'Qinyu', ''], ['Wen', 'Ji-Rong', '']]",0,0,2022-12-15,2,5,1,1,1,0,59d440de6378de10bf8da3995e64c70be23aa88e,254685570.0,https://www.semanticscholar.org/paper/59d440de6378de10bf8da3995e64c70be23aa88e,Annual Meeting of the Association for Computational Linguistics,2022.0,43.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '138404610', 'name': 'Hangyu Guo'}, {'authorId': '1423651904', 'name': 'Kun Zhou'}, {'authorId': '2542603', 'name': 'Wayne Xin Zhao'}, {'authorId': '2190563903', 'name': 'Qinyu Zhang'}, {'authorId': '153693432', 'name': 'Ji-rong Wen'}]","['Harbin Institute of Technology', 'Shenzhen University', 'Beijing Key Laboratory of Big Data Management and Analysis Methods.', 'Renmin University of China']",['China'],2022-12
2212.08354,Willow Dong,"Weilong Dong, Xinwei Wu, Junzhuo Li, Shuangzhi Wu, Chao Bian, Deyi
  Xiong",FewFedWeight: Few-shot Federated Learning Framework across Multiple NLP Tasks,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Massively multi-task learning with large language models has recently made substantial progress on few-shot generalization. However, this is usually performed in a centralized learning fashion, ignoring the privacy sensitivity issue of (annotated) data used in multiple tasks. To mitigate this issue, we propose FewFedWeight, a few-shot federated learning framework across multiple tasks, to achieve the best of both worlds: privacy preservation and cross-task generalization. FewFedWeight trains client models in isolated devices without sharing data. It broadcasts the global model in the server to each client and produces pseudo data for clients so that knowledge from the global model can be explored to enhance few-shot learning of each client model. An energy-based algorithm is further proposed to weight pseudo samples in order to reduce the negative impact of noise from the generated pseudo data. Adaptive model weights of client models are also tuned according to their performance. We use these model weights to dynamically aggregate client models to update the global model. Experiments on 118 NLP tasks show that FewFedWeight can significantly improve the performance of client models on 61% tasks with an average performance improvement rate of 30.5% over the baseline and substantially outperform FedAvg and other decentralized learning methods. ","[{'version': 'v1', 'created': 'Fri, 16 Dec 2022 09:01:56 GMT'}]",2022-12-19,"[['Dong', 'Weilong', ''], ['Wu', 'Xinwei', ''], ['Li', 'Junzhuo', ''], ['Wu', 'Shuangzhi', ''], ['Bian', 'Chao', ''], ['Xiong', 'Deyi', '']]",0,0,2022-12-16,1,6,1,0,0,0,3585e08f2491859679b761eae8444afe7ec62f74,254823272.0,https://www.semanticscholar.org/paper/3585e08f2491859679b761eae8444afe7ec62f74,arXiv.org,2022.0,44.0,3.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2114049130', 'name': 'Weilong Dong'}, {'authorId': '2051972795', 'name': 'Xinwei Wu'}, {'authorId': '2138202959', 'name': 'Junzhuo Li'}, {'authorId': '2362902', 'name': 'Shuangzhi Wu'}, {'authorId': '2150136794', 'name': 'Chao Bian'}, {'authorId': '2140188860', 'name': 'Deyi Xiong'}]","['ByteDance', 'Tianjin University']",['China'],2022-12
2212.08635,Junlong Li,"Junlong Li, Zhuosheng Zhang, Hai Zhao",Self-Prompting Large Language Models for Zero-Shot Open-Domain QA,Work in progress,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Open-Domain Question Answering (ODQA) aims at answering factoid questions without explicitly providing specific background documents. In a zero-shot setting, this task is more challenging since no data is available to train customized models like Retriever-Readers. Recently, Large Language Models (LLMs) like GPT-3 have shown their power in zero-shot ODQA with direct prompting methods, but these methods are still far from releasing the full powerfulness of LLMs only in an implicitly invoking way. In this paper, we propose a Self-Prompting framework to explicitly utilize the massive knowledge stored in the parameters of LLMs and their strong instruction understanding abilities. Concretely, we prompt LLMs step by step to generate multiple pseudo QA pairs with background passages and explanations from scratch and then use those generated elements for in-context learning. Experimental results show our method surpasses previous SOTA methods significantly on three widely-used ODQA datasets, and even achieves comparable performance with some Retriever-Reader models fine-tuned on full training data. ","[{'version': 'v1', 'created': 'Fri, 16 Dec 2022 18:23:43 GMT'}, {'version': 'v2', 'created': 'Tue, 16 May 2023 11:29:15 GMT'}]",2023-05-17,"[['Li', 'Junlong', ''], ['Zhang', 'Zhuosheng', ''], ['Zhao', 'Hai', '']]",0,1,2022-12-16,2,3,2,1,0,1,9cd329e3b86e6869e73a91c467459b1947655b07,258715365.0,https://www.semanticscholar.org/paper/9cd329e3b86e6869e73a91c467459b1947655b07,,2022.0,44.0,0.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '46276940', 'name': 'Junlong Li'}, {'authorId': '3322871', 'name': 'Zhuosheng Zhang'}, {'authorId': '2146232510', 'name': 'Hai Zhao'}]",['Shanghai Jiao Tong University'],['China'],2022-12
2212.08966,Shaopeng Wei,"Shaopeng Wei, Yu Zhao, Xingyan Chen, Qing Li, Fuzhen Zhuang, Ji Liu,
  Gang Kou",Graph Learning and Its Applications: A Holistic Survey,"20 pages, 8 figures, 3 tables",,,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Graph learning is a prevalent domain that endeavors to learn the intricate relationships among nodes and the topological structure of graphs. Over the years, graph learning has transcended from graph theory to graph data mining. With the advent of representation learning, it has attained remarkable performance in diverse scenarios. Owing to its extensive application prospects, graph learning attracts copious attention. While some researchers have accomplished impressive surveys on graph learning, they failed to connect related objectives, methods, and applications in a more coherent way. As a result, they did not encompass current ample scenarios and challenging problems due to the rapid expansion of graph learning. Particularly, large language models have recently had a disruptive effect on human life, but they also show relative weakness in structured scenarios. The question of how to make these models more powerful with graph learning remains open. Different from previous surveys on graph learning, we provide a holistic review that analyzes current works from the perspective of graph structure, and discusses the latest applications, trends, and challenges in graph learning. Specifically, we commence by proposing a taxonomy and then summarize the methods employed in graph learning. We then provide a detailed elucidation of mainstream applications. Finally, we propose future directions. ","[{'version': 'v1', 'created': 'Sat, 17 Dec 2022 22:05:07 GMT'}, {'version': 'v2', 'created': 'Sat, 11 Mar 2023 17:00:20 GMT'}, {'version': 'v3', 'created': 'Sat, 3 Jun 2023 18:36:37 GMT'}]",2023-06-06,"[['Wei', 'Shaopeng', ''], ['Zhao', 'Yu', ''], ['Chen', 'Xingyan', ''], ['Li', 'Qing', ''], ['Zhuang', 'Fuzhen', ''], ['Liu', 'Ji', ''], ['Kou', 'Gang', '']]",0,0,2022-12-17,3,7,1,0,0,0,5c743bea9cdd2bf0a3efb603c67309ee27907aa0,257496081.0,https://www.semanticscholar.org/paper/5c743bea9cdd2bf0a3efb603c67309ee27907aa0,,2022.0,248.0,1.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1409866591', 'name': 'Shaopeng Wei'}, {'authorId': '97522134', 'name': 'Yu Zhao'}, {'authorId': '2143791763', 'name': 'Xingyan Chen'}, {'authorId': '2117895423', 'name': 'Qing Li'}, {'authorId': '2162961864', 'name': 'Fuzhen Zhuang'}, {'authorId': '2155375528', 'name': 'Ji Liu'}, {'authorId': '2147326459', 'name': 'Gang Kou'}]","['Xidian University', 'Beihang University', 'are with Fintech Innovation Center, Financial Intelligence and Financial Engineering Key Laboratory of Sichuan Province,', 'Southwestern University of Finance and Economics', 'Meta', 'Zhongguancun Laboratory, Beijing, China. J.']","['China', 'United States']",2022-12
2212.09278,Yingwen Fu,"Yingwen Fu, Wenjie Ou, Zhou Yu, and Yue Lin",MIGA: A Unified Multi-task Generation Framework for Conversational Text-to-SQL,Accepted by AAAI23,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Conversational text-to-SQL is designed to translate multi-turn natural language questions into their corresponding SQL queries. Most state-of-the-art conversational text- to-SQL methods are incompatible with generative pre-trained language models (PLMs), such as T5. In this paper, we present a two-stage unified MultI-task Generation frAmework (MIGA) that leverages PLMs' ability to tackle conversational text-to-SQL. In the pre-training stage, MIGA first decomposes the main task into several related sub-tasks and then unifies them into the same sequence-to-sequence (Seq2Seq) paradigm with task-specific natural language prompts to boost the main task from multi-task training. Later in the fine-tuning stage, we propose four SQL perturbations to alleviate the error propagation problem. MIGA tends to achieve state-of-the-art performance on two benchmarks (SparC and CoSQL). We also provide extensive analyses and discussions to shed light on some new perspectives for conversational text-to-SQL. ","[{'version': 'v1', 'created': 'Mon, 19 Dec 2022 07:14:32 GMT'}]",2022-12-20,"[['Fu', 'Yingwen', ''], ['Ou', 'Wenjie', ''], ['Yu', 'Zhou', ''], ['Lin', 'Yue', '']]",0,1,2022-12-19,1,4,2,1,1,0,642e04ece55c5b532ff7e5408d8723c7d9c835db,254853639.0,https://www.semanticscholar.org/paper/642e04ece55c5b532ff7e5408d8723c7d9c835db,AAAI Conference on Artificial Intelligence,2022.0,38.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2110659921', 'name': 'Yingwen Fu'}, {'authorId': '46223131', 'name': 'Wenjie Ou'}, {'authorId': '144007938', 'name': 'Zhou Yu'}, {'authorId': '2143785210', 'name': 'Yue Lin'}]","['Guangdong University of Foreign Studies', 'NetEase', 'Columbia University']","['China', 'United States']",2022-12
2212.09535,Zheng-Xin Yong,"Zheng-Xin Yong, Hailey Schoelkopf, Niklas Muennighoff, Alham Fikri
  Aji, David Ifeoluwa Adelani, Khalid Almubarak, M Saiful Bari, Lintang
  Sutawika, Jungo Kasai, Ahmed Baruwa, Genta Indra Winata, Stella Biderman,
  Edward Raff, Dragomir Radev and Vassilina Nikoulina",BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting,ACL 2023,,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  The BLOOM model is a large publicly available multilingual language model, but its pretraining was limited to 46 languages. To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining. In this work, we apply existing language adaptation strategies to BLOOM and benchmark its zero-shot prompting performance on eight new languages in a resource-constrained setting. We find language adaptation to be effective at improving zero-shot performance in new languages. Surprisingly, we find that adapter-based finetuning is more effective than continued pretraining for large models. In addition, we discover that prompting performance is not significantly affected by language specifics, such as the writing system. It is primarily determined by the size of the language adaptation data. We also add new languages to BLOOMZ, which is a multitask finetuned version of BLOOM capable of following task instructions zero-shot. We find including a new language in the multitask fine-tuning mixture to be the most effective method to teach BLOOMZ a new language. We conclude that with sufficient training data language adaptation can generalize well to diverse languages. Our code is available at https://github.com/bigscience-workshop/multilingual-modeling. ","[{'version': 'v1', 'created': 'Mon, 19 Dec 2022 15:24:45 GMT'}, {'version': 'v2', 'created': 'Thu, 25 May 2023 10:50:40 GMT'}, {'version': 'v3', 'created': 'Sat, 27 May 2023 05:48:38 GMT'}]",2023-05-30,"[['Yong', 'Zheng-Xin', ''], ['Schoelkopf', 'Hailey', ''], ['Muennighoff', 'Niklas', ''], ['Aji', 'Alham Fikri', ''], ['Adelani', 'David Ifeoluwa', ''], ['Almubarak', 'Khalid', ''], ['Bari', 'M Saiful', ''], ['Sutawika', 'Lintang', ''], ['Kasai', 'Jungo', ''], ['Baruwa', 'Ahmed', ''], ['Winata', 'Genta Indra', ''], ['Biderman', 'Stella', ''], ['Raff', 'Edward', ''], ['Radev', 'Dragomir', ''], ['Nikoulina', 'Vassilina', '']]",0,0,2022-12-19,3,15,3,2,2,0,34c2939d3147946b2ac218e7857e1bc4c8902679,254854009.0,https://www.semanticscholar.org/paper/34c2939d3147946b2ac218e7857e1bc4c8902679,Annual Meeting of the Association for Computational Linguistics,2022.0,78.0,20.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '1725420331', 'name': 'Zheng-Xin Yong'}, {'authorId': '2184031883', 'name': 'Hailey Schoelkopf'}, {'authorId': '2037383772', 'name': 'Niklas Muennighoff'}, {'authorId': '8129718', 'name': 'Alham Fikri Aji'}, {'authorId': '2518906', 'name': 'David Ifeoluwa Adelani'}, {'authorId': '90615055', 'name': 'Khalid Almubarak'}, {'authorId': '31773000', 'name': 'M Saiful Bari'}, {'authorId': '35566806', 'name': 'Lintang Sutawika'}, {'authorId': '11348687', 'name': 'Jungo Kasai'}, {'authorId': '114850513', 'name': 'Ahmed Baruwa'}, {'authorId': '9162688', 'name': 'Genta Indra Winata'}, {'authorId': '103476203', 'name': 'Stella Rose Biderman'}, {'authorId': '9215251', 'name': 'Dragomir R. Radev'}, {'authorId': '2841761', 'name': 'Vassilina Nikoulina'}]","['University College London', 'Hong Kong University of Science and Technology', 'Yale University', 'Hugging Face', 'University of Oregon', 'NAVER', 'Nanyang Technological University', 'Brown University', 'University of Washington', 'Mohamed bin Zayed University of Artificial Intelligence']","['Singapore', 'Canada', 'United States', 'United Kingdom', 'France', 'China', 'United Arab Emirates']",2022-12
2212.09561,Yixuan Weng,"Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, Jun
  Zhao",Large Language Models are Better Reasoners with Self-Verification,,,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, with the chain of thought (CoT) prompting, large language models (LLMs), e.g., GPT-3, have shown strong reasoning ability in several natural language processing tasks such as arithmetic, commonsense, and logical reasoning. However, LLMs with CoT require multi-step prompting and multi-token prediction, which is highly sensitive to individual mistakes and vulnerable to error accumulation. The above issues make the LLMs need the ability to verify the answers. In fact, after inferring conclusions in some thinking decision tasks, people often check them by re-verifying steps to avoid some mistakes. In this paper, we propose and prove that LLMs also have similar self-verification abilities. We take the conclusion obtained by CoT as one of the conditions for solving the original problem. By taking turns masking the original conditions and predicting their results, we calculate an explainable answer verification score based on whether the re-predicted conditions are correct. Experimental results demonstrate that the proposed method can improve the reasoning performance on various arithmetic, commonsense, and logical reasoning datasets. Our code is publicly available at: https://github.com/WENGSYX/Self-Verification. ","[{'version': 'v1', 'created': 'Mon, 19 Dec 2022 15:51:52 GMT'}, {'version': 'v2', 'created': 'Wed, 29 Mar 2023 11:52:10 GMT'}, {'version': 'v3', 'created': 'Tue, 23 May 2023 16:39:09 GMT'}, {'version': 'v4', 'created': 'Wed, 24 May 2023 09:34:20 GMT'}]",2023-05-25,"[['Weng', 'Yixuan', ''], ['Zhu', 'Minjun', ''], ['Xia', 'Fei', ''], ['Li', 'Bin', ''], ['He', 'Shizhu', ''], ['Liu', 'Kang', ''], ['Zhao', 'Jun', '']]",0,1,2022-12-19,4,7,2,1,0,1,7715ba5e75f5256e1061c7473afe61bb0dbb9065,258840837.0,https://www.semanticscholar.org/paper/7715ba5e75f5256e1061c7473afe61bb0dbb9065,,2022.0,55.0,19.0,2.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2142839441', 'name': 'Yixuan Weng'}, {'authorId': '2187504043', 'name': 'Minjun Zhu'}, {'authorId': '2066079622', 'name': 'Fei Xia'}, {'authorId': '2156072001', 'name': 'Bin Li'}, {'authorId': '1954845', 'name': 'Shizhu He'}, {'authorId': '2200096', 'name': 'Kang Liu'}, {'authorId': '11447228', 'name': 'Jun Zhao'}]","['University of Chinese Academy of Sciences', 'Hunan University', 'Shanghai Artificial Intelligence Laboratory', 'Case Western Reserve University', 'Tianjin University']","['China', 'United States']",2022-12
2212.10461,Jingjing Xu,"Jingjing Xu, Qingxiu Dong, Hongyi Liu and Lei Li",Go-tuning: Improving Zero-shot Learning Abilities of Smaller Language Models,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  With increasing scale, large language models demonstrate both quantitative improvement and new qualitative capabilities, especially as zero-shot learners, like GPT-3. However, these results rely heavily on delicate prompt design and large computation. In this work, we explore whether the strong zero-shot ability could be achieved at a smaller model scale without any external supervised data. To achieve this goal, we revisit masked language modeling and present a geometry-guided self-supervised learning method (Go-tuningfor short) by taking a small number of task-aware self-supervised data to update language models further. Experiments show that Go-tuning can enable T5-small (80M) competitive zero-shot results compared with large language models, such as T5-XL (3B). We also apply Go-tuning on multi-task settings and develop a multi-task model, mgo-T5 (250M). It can reach the average performance of OPT (175B) on 9 datasets. ","[{'version': 'v1', 'created': 'Tue, 20 Dec 2022 17:36:49 GMT'}]",2022-12-21,"[['Xu', 'Jingjing', ''], ['Dong', 'Qingxiu', ''], ['Liu', 'Hongyi', ''], ['Li', 'Lei', '']]",0,1,2022-12-20,1,4,1,3,2,1,96bdc84fba47a71f2a4dbdeb58439fa16693873f,254877425.0,https://www.semanticscholar.org/paper/96bdc84fba47a71f2a4dbdeb58439fa16693873f,arXiv.org,2022.0,25.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '47883405', 'name': 'Jingjing Xu'}, {'authorId': '2047143813', 'name': 'Qingxiu Dong'}, {'authorId': '2115669628', 'name': 'Hongyi Liu'}, {'authorId': None, 'name': 'Lei Li'}]","['Peking University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'University of California, Santa Barbara']","['China', 'United States']",2022-12
2212.10559,Damai Dai,"Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, Furu
  Wei",Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers,Accepted to ACL 2023 findings,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as meta-optimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. We comprehensively compare the behaviors of in-context learning and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspectives. Inspired by the dual form between Transformer attention and gradient descent, we design a momentum-based attention by analogy with gradient descent with momentum. The improved performance over vanilla attention further supports our understanding from another perspective, and more importantly, shows the potential to utilize our understanding for future model design. The code is available at \url{https://aka.ms/icl}. ","[{'version': 'v1', 'created': 'Tue, 20 Dec 2022 18:58:48 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Dec 2022 08:36:47 GMT'}, {'version': 'v3', 'created': 'Mon, 15 May 2023 11:45:12 GMT'}]",2023-05-16,"[['Dai', 'Damai', ''], ['Sun', 'Yutao', ''], ['Dong', 'Li', ''], ['Hao', 'Yaru', ''], ['Ma', 'Shuming', ''], ['Sui', 'Zhifang', ''], ['Wei', 'Furu', '']]",0,1,2022-12-20,3,7,1,0,0,0,69c85405cc1986a41f6387d869aa1648a5668d6f,258686544.0,https://www.semanticscholar.org/paper/69c85405cc1986a41f6387d869aa1648a5668d6f,,2022.0,29.0,31.0,3.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '10780897', 'name': 'Damai Dai'}, {'authorId': '2108540694', 'name': 'Yutao Sun'}, {'authorId': '145307652', 'name': 'Li Dong'}, {'authorId': '34128716', 'name': 'Y. Hao'}, {'authorId': '2118866998', 'name': 'Shuming Ma'}, {'authorId': '3335836', 'name': 'Zhifang Sui'}, {'authorId': '49807919', 'name': 'Furu Wei'}]","['Peking University', 'Tsinghua University', 'Microsoft']","['China', 'United States']",2022-12
2212.13196,Qihao Zhu,"Qihao Zhu, Xinyu Zhang, Jianxi Luo",Biologically Inspired Design Concept Generation Using Generative Pre-Trained Transformers,"Accepted by J. Mech. Des. arXiv admin note: substantial text overlap
  with arXiv:2204.09714",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Biological systems in nature have evolved for millions of years to adapt and survive the environment. Many features they developed can be inspirational and beneficial for solving technical problems in modern industries. This leads to a specific form of design-by-analogy called bio-inspired design (BID). Although BID as a design method has been proven beneficial, the gap between biology and engineering continuously hinders designers from effectively applying the method. Therefore, we explore the recent advance of artificial intelligence (AI) for a data-driven approach to bridge the gap. This paper proposes a generative design approach based on the generative pre-trained language model (PLM) to automatically retrieve and map biological analogy and generate BID in the form of natural language. The latest generative pre-trained transformer, namely GPT-3, is used as the base PLM. Three types of design concept generators are identified and fine-tuned from the PLM according to the looseness of the problem space representation. Machine evaluators are also fine-tuned to assess the mapping relevancy between the domains within the generated BID concepts. The approach is evaluated and then employed in a real-world project of designing light-weighted flying cars during its conceptual design phase The results show our approach can generate BID concepts with good performance. ","[{'version': 'v1', 'created': 'Mon, 26 Dec 2022 16:06:04 GMT'}]",2022-12-27,"[['Zhu', 'Qihao', ''], ['Zhang', 'Xinyu', ''], ['Luo', 'Jianxi', '']]",0,1,2022-12-26,1,3,1,1,0,1,18fa8810f5c5f0e9d3785f21be1415897eea08c6,255125347.0,https://www.semanticscholar.org/paper/18fa8810f5c5f0e9d3785f21be1415897eea08c6,Journal of Mechanical Design,2022.0,61.0,15.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Art', 'source': 's2-fos-model'}]","[{'authorId': '2152203384', 'name': 'Qihao Zhu'}, {'authorId': '120070100', 'name': 'Xinyu Zhang'}, {'authorId': '145990580', 'name': 'Jianxi Luo'}]","['University of Technology', 'Tsinghua University']","['China', 'Russia']",2022-12
2212.13456,Yong Chen,Wang Qi and Rui Liu and Yuan Zuo and Yong Chen and Dell Zhang,TegFormer: Topic-to-Essay Generation with Good Topic Coverage and High Text Coherence,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Creating an essay based on a few given topics is a challenging NLP task. Although several effective methods for this problem, topic-to-essay generation, have appeared recently, there is still much room for improvement, especially in terms of the coverage of the given topics and the coherence of the generated text. In this paper, we propose a novel approach called TegFormer which utilizes the Transformer architecture where the encoder is enriched with domain-specific contexts while the decoder is enhanced by a large-scale pre-trained language model. Specifically, a \emph{Topic-Extension} layer capturing the interaction between the given topics and their domain-specific contexts is plugged into the encoder. Since the given topics are usually concise and sparse, such an additional layer can bring more topic-related semantics in to facilitate the subsequent natural language generation. Moreover, an \emph{Embedding-Fusion} module that combines the domain-specific word embeddings learnt from the given corpus and the general-purpose word embeddings provided by a GPT-2 model pre-trained on massive text data is integrated into the decoder. Since GPT-2 is at a much larger scale, it contains a lot more implicit linguistic knowledge which would help the decoder to produce more grammatical and readable text. Extensive experiments have shown that the pieces of text generated by TegFormer have better topic coverage and higher text coherence than those from SOTA topic-to-essay techniques, according to automatic and human evaluations. As revealed by ablation studies, both the Topic-Extension layer and the Embedding-Fusion module contribute substantially to TegFormer's performance advantage. ","[{'version': 'v1', 'created': 'Tue, 27 Dec 2022 11:50:14 GMT'}]",2022-12-29,"[['Qi', 'Wang', ''], ['Liu', 'Rui', ''], ['Zuo', 'Yuan', ''], ['Chen', 'Yong', ''], ['Zhang', 'Dell', '']]",0,1,2022-12-27,1,5,1,1,1,0,d184b066aca3e5a5a0bab8da6edd1c6f58bce9a8,255186608.0,https://www.semanticscholar.org/paper/d184b066aca3e5a5a0bab8da6edd1c6f58bce9a8,arXiv.org,2022.0,41.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2190955031', 'name': 'Wang Qi'}, {'authorId': '144207288', 'name': 'R. Liu'}, {'authorId': '2125171036', 'name': 'Y. Zuo'}, {'authorId': '2144257959', 'name': 'Yong Chen'}, {'authorId': '2145943595', 'name': 'Dell Zhang'}]","['Beijing University of Posts and Telecommunications', 'Beihang University', 'Thomson Reuters (United Kingdom)']","['China', 'United Kingdom']",2022-12
2212.14232,Xin Hu,"Xin Hu, Lingling Zhang, Jun Liu, Jinfu Fan, Yang You, Yaqiang Wu",GPTR: Gestalt-Perception Transformer for Diagram Object Detection,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Diagram object detection is the key basis of practical applications such as textbook question answering. Because the diagram mainly consists of simple lines and color blocks, its visual features are sparser than those of natural images. In addition, diagrams usually express diverse knowledge, in which there are many low-frequency object categories in diagrams. These lead to the fact that traditional data-driven detection model is not suitable for diagrams. In this work, we propose a gestalt-perception transformer model for diagram object detection, which is based on an encoder-decoder architecture. Gestalt perception contains a series of laws to explain human perception, that the human visual system tends to perceive patches in an image that are similar, close or connected without abrupt directional changes as a perceptual whole object. Inspired by these thoughts, we build a gestalt-perception graph in transformer encoder, which is composed of diagram patches as nodes and the relationships between patches as edges. This graph aims to group these patches into objects via laws of similarity, proximity, and smoothness implied in these edges, so that the meaningful objects can be effectively detected. The experimental results demonstrate that the proposed GPTR achieves the best results in the diagram object detection task. Our model also obtains comparable results over the competitors in natural image object detection. ","[{'version': 'v1', 'created': 'Thu, 29 Dec 2022 09:03:05 GMT'}]",2023-01-02,"[['Hu', 'Xin', ''], ['Zhang', 'Lingling', ''], ['Liu', 'Jun', ''], ['Fan', 'Jinfu', ''], ['You', 'Yang', ''], ['Wu', 'Yaqiang', '']]",0,1,2022-12-29,1,6,1,0,0,0,6329ee4cf6bbd30c5d76d6caa78959946c656c56,255340892.0,https://www.semanticscholar.org/paper/6329ee4cf6bbd30c5d76d6caa78959946c656c56,AAAI Conference on Artificial Intelligence,2022.0,38.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2110048361', 'name': 'Xin Hu'}, {'authorId': '2145417611', 'name': 'Lingling Zhang'}, {'authorId': '2155648333', 'name': 'Jun Liu'}, {'authorId': '2151857494', 'name': 'Jinfu Fan'}, {'authorId': '2147330214', 'name': 'Yang You'}, {'authorId': '2107894795', 'name': 'Yaqiang Wu'}]","[""Xi'an Jiaotong University"", 'National University of Singapore', 'Lenovo Research, Beijing, China', 'Tongji University']","['China', 'Singapore']",2022-12
2212.14548,Liwen Jing,"Bowen Zhang, Daijun Ding, Liwen Jing",How would Stance Detection Techniques Evolve after the Launch of ChatGPT?,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Stance detection refers to the task of extracting the standpoint (Favor, Against or Neither) towards a target in given texts. Such research gains increasing attention with the proliferation of social media contents. The conventional framework of handling stance detection is converting it into text classification tasks. Deep learning models have already replaced rule-based models and traditional machine learning models in solving such problems. Current deep neural networks are facing two main challenges which are insufficient labeled data and information in social media posts and the unexplainable nature of deep learning models. A new pre-trained language model chatGPT was launched on Nov 30, 2022. For the stance detection tasks, our experiments show that ChatGPT can achieve SOTA or similar performance for commonly used datasets including SemEval-2016 and P-Stance. At the same time, ChatGPT can provide explanation for its own prediction, which is beyond the capability of any existing model. The explanations for the cases it cannot provide classification results are especially useful. ChatGPT has the potential to be the best AI model for stance detection tasks in NLP, or at least change the research paradigm of this field. ChatGPT also opens up the possibility of building explanatory AI for stance detection. ","[{'version': 'v1', 'created': 'Fri, 30 Dec 2022 05:03:15 GMT'}, {'version': 'v2', 'created': 'Sat, 18 Feb 2023 11:55:12 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Apr 2023 11:43:12 GMT'}]",2023-04-11,"[['Zhang', 'Bowen', ''], ['Ding', 'Daijun', ''], ['Jing', 'Liwen', '']]",1,1,2022-12-30,3,3,1,1,0,1,e7a8e426709967154c00c0529e0e9fd9ffdd0e95,255340709.0,https://www.semanticscholar.org/paper/e7a8e426709967154c00c0529e0e9fd9ffdd0e95,arXiv.org,2022.0,37.0,40.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': None, 'name': 'Bowen Zhang'}, {'authorId': '2199013050', 'name': 'Daijun Ding'}, {'authorId': '2147240380', 'name': 'Liwen Jing'}]","['Shenzhen Technology University', 'Shenzhen University']",['China'],2022-12
2301.00184,Wenhao Wu,"Wenhao Wu, Haipeng Luo, Bo Fang, Jingdong Wang, Wanli Ouyang",Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?,"Accepted by CVPR 2023. Selected as a Highlight (Top 2.5% of ALL
  submissions)",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Most existing text-video retrieval methods focus on cross-modal matching between the visual content of videos and textual query sentences. However, in real-world scenarios, online videos are often accompanied by relevant text information such as titles, tags, and even subtitles, which can be utilized to match textual queries. This insight has motivated us to propose a novel approach to text-video retrieval, where we directly generate associated captions from videos using zero-shot video captioning with knowledge from web-scale pre-trained models (e.g., CLIP and GPT-2). Given the generated captions, a natural question arises: what benefits do they bring to text-video retrieval? To answer this, we introduce Cap4Video, a new framework that leverages captions in three ways: i) Input data: video-caption pairs can augment the training data. ii) Intermediate feature interaction: we perform cross-modal feature interaction between the video and caption to produce enhanced video representations. iii) Output score: the Query-Caption matching branch can complement the original Query-Video matching branch for text-video retrieval. We conduct comprehensive ablation studies to demonstrate the effectiveness of our approach. Without any post-processing, Cap4Video achieves state-of-the-art performance on four standard text-video retrieval benchmarks: MSR-VTT (51.4%), VATEX (66.6%), MSVD (51.8%), and DiDeMo (52.0%). The code is available at https://github.com/whwu95/Cap4Video . ","[{'version': 'v1', 'created': 'Sat, 31 Dec 2022 11:50:32 GMT'}, {'version': 'v2', 'created': 'Sun, 26 Mar 2023 09:39:45 GMT'}, {'version': 'v3', 'created': 'Tue, 28 Mar 2023 07:09:59 GMT'}]",2023-03-29,"[['Wu', 'Wenhao', ''], ['Luo', 'Haipeng', ''], ['Fang', 'Bo', ''], ['Wang', 'Jingdong', ''], ['Ouyang', 'Wanli', '']]",0,1,2022-12-31,3,5,1,1,1,0,9d40a8a4dda60bcc94effac56ee800da5017efc7,255372517.0,https://www.semanticscholar.org/paper/9d40a8a4dda60bcc94effac56ee800da5017efc7,Computer Vision and Pattern Recognition,2022.0,49.0,16.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '50224945', 'name': 'Wenhao Wu'}, {'authorId': '2131127', 'name': 'Haipeng Luo'}, {'authorId': '2144213014', 'name': 'Bo Fang'}, {'authorId': '2109534192', 'name': 'Jingdong Wang'}, {'authorId': '3001348', 'name': 'Wanli Ouyang'}]","['Shanghai Artificial Intelligence Laboratory', 'University of Chinese Academy of Sciences', 'University of Sydney', 'Baidu']","['China', 'Australia']",2022-12
2301.00234,Qingxiu Dong,"Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang,
  Xu Sun, Jingjing Xu, Lei Li and Zhifang Sui",A Survey on In-context Learning,Papers collected until 2023/05/22,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few examples. It has been a new trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL. ","[{'version': 'v1', 'created': 'Sat, 31 Dec 2022 15:57:09 GMT'}, {'version': 'v2', 'created': 'Wed, 8 Feb 2023 02:59:46 GMT'}, {'version': 'v3', 'created': 'Thu, 1 Jun 2023 12:23:40 GMT'}]",2023-06-02,"[['Dong', 'Qingxiu', ''], ['Li', 'Lei', ''], ['Dai', 'Damai', ''], ['Zheng', 'Ce', ''], ['Wu', 'Zhiyong', ''], ['Chang', 'Baobao', ''], ['Sun', 'Xu', ''], ['Xu', 'Jingjing', ''], ['Li', 'Lei', ''], ['Sui', 'Zhifang', '']]",0,0,2022-12-31,3,10,2,0,0,0,30c0cdc414f68211d5d0514df027cec22e005174,255372865.0,https://www.semanticscholar.org/paper/30c0cdc414f68211d5d0514df027cec22e005174,,2022.0,128.0,39.0,1.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2047143813', 'name': 'Qingxiu Dong'}, {'authorId': '49192881', 'name': 'Lei Li'}, {'authorId': '10780897', 'name': 'Damai Dai'}, {'authorId': '2113919886', 'name': 'Ce Zheng'}, {'authorId': '150358371', 'name': 'Zhiyong Wu'}, {'authorId': '7267809', 'name': 'Baobao Chang'}, {'authorId': '2116530295', 'name': 'Xu Sun'}, {'authorId': '47883405', 'name': 'Jingjing Xu'}, {'authorId': '3335836', 'name': 'Zhifang Sui'}]","['Shanghai Artificial Intelligence Laboratory', 'Peking University', 'University of California, Santa Barbara']","['China', 'United States']",2022-12
2301.00303,Hangfeng He,"Hangfeng He, Hongming Zhang, Dan Roth",Rethinking with Retrieval: Faithful Large Language Model Inference,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs. ","[{'version': 'v1', 'created': 'Sat, 31 Dec 2022 22:35:34 GMT'}]",2023-01-03,"[['He', 'Hangfeng', ''], ['Zhang', 'Hongming', ''], ['Roth', 'Dan', '']]",0,1,2022-12-31,1,3,2,1,0,1,490d8006851b1562cfd9ec1f057471f2868289d1,255372320.0,https://www.semanticscholar.org/paper/490d8006851b1562cfd9ec1f057471f2868289d1,arXiv.org,2022.0,58.0,59.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '7146703', 'name': 'Hangfeng He'}, {'authorId': '2111112132', 'name': 'Hongming Zhang'}, {'authorId': '144590225', 'name': 'D. Roth'}]","['University of Rochester', 'University of Pennsylvania', 'Tencent']","['China', 'United States']",2022-12
2301.02884,Shangda Wu,"Shangda Wu, Xiaobing Li, Feng Yu, Maosong Sun",TunesFormer: Forming Irish Tunes with Control Codes by Bar Patching,"5 pages, 3 figures, 1 table",,,,cs.SD eess.AS,http://creativecommons.org/licenses/by/4.0/,"  This paper introduces TunesFormer, an efficient Transformer-based dual-decoder model specifically designed for the generation of melodies that adhere to user-defined musical forms. Trained on 214,122 Irish tunes, TunesFormer utilizes techniques including bar patching and control codes. Bar patching reduces sequence length and generation time, while control codes guide TunesFormer in producing melodies that conform to desired musical forms. Our evaluation demonstrates TunesFormer's superior efficiency, being 3.22 times faster than GPT-2 and 1.79 times faster than a model with linear complexity of equal scale while offering comparable performance in controllability and other metrics. TunesFormer provides a novel tool for musicians, composers, and music enthusiasts alike to explore the vast landscape of Irish music. Our model and code are available at https://github.com/sander-wood/tunesformer. ","[{'version': 'v1', 'created': 'Sat, 7 Jan 2023 16:11:55 GMT'}, {'version': 'v2', 'created': 'Sun, 20 Aug 2023 07:28:16 GMT'}]",2023-08-22,"[['Wu', 'Shangda', ''], ['Li', 'Xiaobing', ''], ['Yu', 'Feng', ''], ['Sun', 'Maosong', '']]",0,1,2023-01-07,2,4,2,1,1,0,8ca9e32b779bc2e23b6dfcfd00abb7d89c0b81f1,261046903.0,https://www.semanticscholar.org/paper/8ca9e32b779bc2e23b6dfcfd00abb7d89c0b81f1,,2023.0,21.0,0.0,0.0,False,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2152536983', 'name': 'Shangda Wu'}, {'authorId': '2109349380', 'name': 'Xiaobing Li'}, {'authorId': '2164906956', 'name': 'Feng Yu'}, {'authorId': '1753344', 'name': 'Maosong Sun'}]","['2nd Workshop on Human-Centric Music Information Research, November 10th, 2023, Milan, Italy', 'Tsinghua University', 'Central Conservatory of Music']","['China', 'Italy']",2023-01
2301.03416,Jiaxiang Liu,"Weixin Liu, Xuyi Chen, Jiaxiang Liu, Shikun Feng, Yu Sun, Hao Tian,
  Hua Wu",ERNIE 3.0 Tiny: Frustratingly Simple Method to Improve Task-Agnostic Distillation Generalization,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Task-agnostic knowledge distillation attempts to address the problem of deploying large pretrained language model in resource-constrained scenarios by compressing a large pretrained model called teacher into a smaller one called student such that the student can be directly finetuned on downstream tasks and retains comparable performance. However, we empirically find that there is a generalization gap between the student and the teacher in existing methods. In this work, we show that we can leverage multi-task learning in task-agnostic distillation to advance the generalization of the resulted student. In particular, we propose Multi-task Infused Task-agnostic Knowledge Distillation (MITKD). We first enhance the teacher by multi-task training it on multiple downstream tasks and then perform distillation to produce the student. Experimental results demonstrate that our method yields a student with much better generalization, significantly outperforms existing baselines, and establishes a new state-of-the-art result on in-domain, out-domain, and low-resource datasets in the setting of task-agnostic distillation. Moreover, our method even exceeds an 8x larger BERT$_{\text{Base}}$ on SQuAD and four GLUE tasks. In addition, by combining ERNIE 3.0, our method achieves state-of-the-art results on 10 Chinese datasets. ","[{'version': 'v1', 'created': 'Mon, 9 Jan 2023 15:12:50 GMT'}]",2023-01-10,"[['Liu', 'Weixin', ''], ['Chen', 'Xuyi', ''], ['Liu', 'Jiaxiang', ''], ['Feng', 'Shikun', ''], ['Sun', 'Yu', ''], ['Tian', 'Hao', ''], ['Wu', 'Hua', '']]",0,0,2023-01-09,1,7,1,1,0,1,44d02cf6cd5ab64a2eaa3f724545ca849cecc164,255546131.0,https://www.semanticscholar.org/paper/44d02cf6cd5ab64a2eaa3f724545ca849cecc164,arXiv.org,2023.0,98.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2109563578', 'name': 'Weixin Liu'}, {'authorId': '2109214103', 'name': 'Xuyi Chen'}, {'authorId': '2144130913', 'name': 'Jiaxiang Liu'}, {'authorId': '144588144', 'name': 'Shi Feng'}, {'authorId': '2117103617', 'name': 'Yu Sun'}, {'authorId': '50007795', 'name': 'Hao Tian'}, {'authorId': '2190177674', 'name': 'Hua Wu'}]",['Baidu'],['China'],2023-01
2301.07597,Biyang Guo,"Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan
  Ding, Jianwei Yue, Yupeng Wu","How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection",https://github.com/Hello-SimpleAI/chatgpt-comparison-detection,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The introduction of ChatGPT has garnered widespread attention in both academic and industrial communities. ChatGPT is able to respond effectively to a wide range of human questions, providing fluent and comprehensive answers that significantly surpass previous public chatbots in terms of security and usefulness. On one hand, people are curious about how ChatGPT is able to achieve such strength and how far it is from human experts. On the other hand, people are starting to worry about the potential negative impacts that large language models (LLMs) like ChatGPT could have on society, such as fake news, plagiarism, and social security issues. In this work, we collected tens of thousands of comparison responses from both human experts and ChatGPT, with questions ranging from open-domain, financial, medical, legal, and psychological areas. We call the collected dataset the Human ChatGPT Comparison Corpus (HC3). Based on the HC3 dataset, we study the characteristics of ChatGPT's responses, the differences and gaps from human experts, and future directions for LLMs. We conducted comprehensive human evaluations and linguistic analyses of ChatGPT-generated content compared with that of humans, where many interesting results are revealed. After that, we conduct extensive experiments on how to effectively detect whether a certain text is generated by ChatGPT or humans. We build three different detection systems, explore several key factors that influence their effectiveness, and evaluate them in different scenarios. The dataset, code, and models are all publicly available at https://github.com/Hello-SimpleAI/chatgpt-comparison-detection. ","[{'version': 'v1', 'created': 'Wed, 18 Jan 2023 15:23:25 GMT'}]",2023-01-19,"[['Guo', 'Biyang', ''], ['Zhang', 'Xin', ''], ['Wang', 'Ziyuan', ''], ['Jiang', 'Minqi', ''], ['Nie', 'Jinran', ''], ['Ding', 'Yuxuan', ''], ['Yue', 'Jianwei', ''], ['Wu', 'Yupeng', '']]",1,1,2023-01-18,1,8,1,1,0,1,cb29cf52f0f7d2e4324c68690a55b22890f2212d,255998637.0,https://www.semanticscholar.org/paper/cb29cf52f0f7d2e4324c68690a55b22890f2212d,arXiv.org,2023.0,47.0,198.0,37.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Biology', 'source': 's2-fos-model'}]","[{'authorId': '2034015747', 'name': 'Biyang Guo'}, {'authorId': '2149173726', 'name': 'Xin Zhang'}, {'authorId': '2108416148', 'name': 'Ziyuan Wang'}, {'authorId': '2152154941', 'name': 'Minqi Jiang'}, {'authorId': '2147399904', 'name': 'Jinran Nie'}, {'authorId': '2193663564', 'name': 'Yuxuan Ding'}, {'authorId': '2170160137', 'name': 'Jianwei Yue'}, {'authorId': '2107934757', 'name': 'Yupeng Wu'}]","['Xidian University', 'Queens University', 'Shanghai University of Finance and Economics', 'Harbin Institute of Technology', 'Shenzhen University', 'Beijing Language and Culture University']","['China', 'Bangladesh']",2023-01
2301.08721,Zhoujun Cheng,"Zhoujun Cheng, Jungo Kasai, Tao Yu",Batch Prompting: Efficient Inference with Large Language Model APIs,"18 pages, 9 figures",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Performing inference on hundreds of thousands of samples with large language models (LLMs) can be computationally and financially costly. We propose batch prompting, a simple alternative prompting approach that enables the LLM to run inference in batches, instead of one sample at a time. Our method reduces both token and time costs while retaining downstream performance. We theoretically demonstrate that under a few-shot in-context learning setting, the inference costs decrease almost inverse linearly with the number of samples in each batch. We extensively validate the effectiveness of batch prompting on ten datasets across commonsense QA, arithmetic reasoning, and NLI/NLU: batch prompting significantly~(up to $5\times$ with six samples in batch) reduces the LLM (Codex) inference token and time costs while achieving better or comparable performance. Our analysis shows that the number of samples in each batch and the complexity of tasks affect its performance. Further, batch prompting can be applied across different LLMs and reasoning methods. ","[{'version': 'v1', 'created': 'Thu, 19 Jan 2023 02:29:23 GMT'}]",2023-01-23,"[['Cheng', 'Zhoujun', ''], ['Kasai', 'Jungo', ''], ['Yu', 'Tao', '']]",0,0,2023-01-19,1,3,2,1,0,1,27f0ce04403158b61328716ae4aaab5840c0d123,256080546.0,https://www.semanticscholar.org/paper/27f0ce04403158b61328716ae4aaab5840c0d123,arXiv.org,2023.0,80.0,13.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1471878967', 'name': 'Zhoujun Cheng'}, {'authorId': '11348687', 'name': 'Jungo Kasai'}, {'authorId': '2117900202', 'name': 'Tao Yu'}]","['University of Washington', 'Shanghai Jiao Tong University', 'University of Hong Kong']","['China', 'United States', 'Hong Kong']",2023-01
2301.08745,Wenxiang Jiao,"Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, Zhaopeng Tu",Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine,8 pages; added GPT-3 data statistics reference; added GPT-4 results,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This report provides a preliminary evaluation of ChatGPT for machine translation, including translation prompt, multilingual translation, and translation robustness. We adopt the prompts advised by ChatGPT to trigger its translation ability and find that the candidate prompts generally work well and show minor performance differences. By evaluating on a number of benchmark test sets, we find that ChatGPT performs competitively with commercial translation products (e.g., Google Translate) on high-resource European languages but lags behind significantly on low-resource or distant languages. For distant languages, we explore an interesting strategy named $\mathbf{pivot~prompting}$ that asks ChatGPT to translate the source sentence into a high-resource pivot language before into the target language, which improves the translation performance significantly. As for the translation robustness, ChatGPT does not perform as well as the commercial systems on biomedical abstracts or Reddit comments but exhibits good results on spoken language. With the launch of the GPT-4 engine, the translation performance of ChatGPT is significantly boosted, becoming comparable to commercial translation products, even for distant languages. In other words, $\mathbf{ChatGPT~has~already~become~a~good~translator!}$ Scripts and data: https://github.com/wxjiao/Is-ChatGPT-A-Good-Translator ","[{'version': 'v1', 'created': 'Fri, 20 Jan 2023 08:51:36 GMT'}, {'version': 'v2', 'created': 'Tue, 31 Jan 2023 16:39:51 GMT'}, {'version': 'v3', 'created': 'Sun, 19 Mar 2023 11:53:20 GMT'}]",2023-03-21,"[['Jiao', 'Wenxiang', ''], ['Wang', 'Wenxuan', ''], ['Huang', 'Jen-tse', ''], ['Wang', 'Xing', ''], ['Tu', 'Zhaopeng', '']]",1,1,2023-01-20,3,5,1,2,0,2,780c99d13537370f63c03feeb1343bed9d98a4f9,257631519.0,https://www.semanticscholar.org/paper/780c99d13537370f63c03feeb1343bed9d98a4f9,,2023.0,28.0,94.0,2.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '12386833', 'name': 'Wenxiang Jiao'}, {'authorId': '2144328160', 'name': 'Wenxuan Wang'}, {'authorId': '2161306685', 'name': 'Jen-tse Huang'}, {'authorId': '48631170', 'name': 'Xing Wang'}, {'authorId': '2909321', 'name': 'Zhaopeng Tu'}]",['Tencent'],['China'],2023-01
2301.08984,Youshan Miao,"Zhiqi Lin, Youshan Miao, Guodong Liu, Xiaoxiang Shi, Quanlu Zhang, Fan
  Yang, Saeed Maleki, Yi Zhu, Xu Cao, Cheng Li, Mao Yang, Lintao Zhang, Lidong
  Zhou",SuperScaler: Supporting Flexible DNN Parallelization via a Unified Abstraction,,,,,cs.DC cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the growing model size, deep neural networks (DNN) are increasingly trained over massive GPU accelerators, which demands a proper parallelization plan that transforms a DNN model into fine-grained tasks and then schedules them to GPUs for execution. Due to the large search space, the contemporary parallelization plan generators often rely on empirical rules that couple transformation and scheduling, and fall short in exploring more flexible schedules that yield better memory usage and compute efficiency. This tension can be exacerbated by the emerging models with increasing complexity in their structure and model size. SuperScaler is a system that facilitates the design and generation of highly flexible parallelization plans. It formulates the plan design and generation into three sequential phases explicitly: model transformation, space-time scheduling, and data dependency preserving. Such a principled approach decouples multiple seemingly intertwined factors and enables the composition of highly flexible parallelization plans. As a result, SuperScaler can not only generate empirical parallelization plans, but also construct new plans that achieve up to 3.5X speedup compared to state-of-the-art solutions like DeepSpeed, Megatron and Alpa, for emerging DNN models like Swin-Transformer and AlphaFold2, as well as well-optimized models like GPT-3. ","[{'version': 'v1', 'created': 'Sat, 21 Jan 2023 17:47:55 GMT'}]",2023-01-24,"[['Lin', 'Zhiqi', ''], ['Miao', 'Youshan', ''], ['Liu', 'Guodong', ''], ['Shi', 'Xiaoxiang', ''], ['Zhang', 'Quanlu', ''], ['Yang', 'Fan', ''], ['Maleki', 'Saeed', ''], ['Zhu', 'Yi', ''], ['Cao', 'Xu', ''], ['Li', 'Cheng', ''], ['Yang', 'Mao', ''], ['Zhang', 'Lintao', ''], ['Zhou', 'Lidong', '']]",0,1,2023-01-21,1,13,2,1,0,1,f6ba3fab410cf2182fde171beeeec57bffa3e90b,256105230.0,https://www.semanticscholar.org/paper/f6ba3fab410cf2182fde171beeeec57bffa3e90b,arXiv.org,2023.0,60.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108651040', 'name': 'Zhiqi Lin'}, {'authorId': '11009920', 'name': 'Youshan Miao'}, {'authorId': '2158131767', 'name': 'Guodong Liu'}, {'authorId': '144120884', 'name': 'Xiaoxiang Shi'}, {'authorId': '3123382', 'name': 'Quanlu Zhang'}, {'authorId': '145338263', 'name': 'Fan Yang'}, {'authorId': '144834015', 'name': 'Saeed Maleki'}, {'authorId': '2117913672', 'name': 'Yi Zhu'}, {'authorId': '2125517683', 'name': 'Xu Cao'}, {'authorId': '15060444', 'name': 'Cheng-Wu Li'}, {'authorId': '2168609907', 'name': 'Mao Yang'}, {'authorId': '1978393184', 'name': 'Lintao Zhang'}, {'authorId': '2143359114', 'name': 'Lidong Zhou'}]","['Shanghai Jiao Tong University', 'University of Science and Technology of China', 'Microsoft']","['China', 'United States']",2023-01
2301.09043,Yihong Dong,"Yihong Dong, Jiazheng Ding, Xue Jiang, Ge Li, Zhuo Li, and Zhi Jin",CodeScore: Evaluating Code Generation by Learning Code Execution,,,,,cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A proper code evaluation metric (CEM) profoundly impacts the evolution of code generation, which is an important research field in NLP and software engineering. Prevailing match-based CEMs (e.g., BLEU, Accuracy, and CodeBLEU) suffer from two significant drawbacks. 1. They primarily measure the surface differences between codes without considering their functional equivalence. However, functional equivalence is pivotal in evaluating the effectiveness of code generation, as different codes can perform identical operations. 2. They are predominantly designed for the Ref-only input format. However, code evaluation necessitates versatility in input formats. Aside from Ref-only, there are NL-only and Ref\&NL formats, which existing match-based CEMs cannot effectively accommodate. In this paper, we propose CodeScore, a large language model (LLM)-based CEM, which estimates the functional correctness of generated code on three input types. To acquire CodeScore, we present UniCE, a unified code generation learning framework, for LLMs to learn code execution (i.e., learning PassRatio and Executability of generated code) with unified input. Extensive experimental results on multiple code evaluation datasets demonstrate that CodeScore absolutely improves up to 58.87% correlation with functional correctness compared to other CEMs, achieves state-of-the-art performance, and effectively handles three input formats. ","[{'version': 'v1', 'created': 'Sun, 22 Jan 2023 02:59:59 GMT'}, {'version': 'v2', 'created': 'Fri, 29 Sep 2023 21:20:30 GMT'}]",2023-10-03,"[['Dong', 'Yihong', ''], ['Ding', 'Jiazheng', ''], ['Jiang', 'Xue', ''], ['Li', 'Ge', ''], ['Li', 'Zhuo', ''], ['Jin', 'Zhi', '']]",0,0,2023-01-22,2,6,1,0,0,0,a95817df43516f5ff9cfc447209def1fcd569afc,256105296.0,https://www.semanticscholar.org/paper/a95817df43516f5ff9cfc447209def1fcd569afc,arXiv.org,2023.0,56.0,10.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '26845858', 'name': 'Yihong Dong'}, {'authorId': '2111185602', 'name': 'J. Ding'}, {'authorId': '2199808863', 'name': 'Xue Jiang'}, {'authorId': '2118393672', 'name': 'Zhuo Li'}, {'authorId': '2154591375', 'name': 'Ge Li'}, {'authorId': '2152843753', 'name': 'Zhi Jin'}]",['Peking University'],['China'],2023-01
2301.09919,Nico Daheim,"Jakub Macina, Nico Daheim, Lingzhi Wang, Tanmay Sinha, Manu Kapur,
  Iryna Gurevych, Mrinmaya Sachan",Opportunities and Challenges in Neural Dialog Tutoring,"EACL 2023 (main conference, camera-ready)",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Designing dialog tutors has been challenging as it involves modeling the diverse and complex pedagogical strategies employed by human tutors. Although there have been significant recent advances in neural conversational systems using large language models (LLMs) and growth in available dialog corpora, dialog tutoring has largely remained unaffected by these advances. In this paper, we rigorously analyze various generative language models on two dialog tutoring datasets for language learning using automatic and human evaluations to understand the new opportunities brought by these advances as well as the challenges we must overcome to build models that would be usable in real educational settings. We find that although current approaches can model tutoring in constrained learning scenarios when the number of concepts to be taught and possible teacher strategies are small, they perform poorly in less constrained scenarios. Our human quality evaluation shows that both models and ground-truth annotations exhibit low performance in terms of equitable tutoring, which measures learning opportunities for students and how engaging the dialog is. To understand the behavior of our models in a real tutoring setting, we conduct a user study using expert annotators and find a significantly large number of model reasoning errors in 45% of conversations. Finally, we connect our findings to outline future work. ","[{'version': 'v1', 'created': 'Tue, 24 Jan 2023 11:00:17 GMT'}, {'version': 'v2', 'created': 'Mon, 27 Mar 2023 19:13:35 GMT'}]",2023-03-29,"[['Macina', 'Jakub', ''], ['Daheim', 'Nico', ''], ['Wang', 'Lingzhi', ''], ['Sinha', 'Tanmay', ''], ['Kapur', 'Manu', ''], ['Gurevych', 'Iryna', ''], ['Sachan', 'Mrinmaya', '']]",0,0,2023-01-24,2,7,2,0,0,0,076d56635311fb75168dc11a094bf7d70e4178a9,256194136.0,https://www.semanticscholar.org/paper/076d56635311fb75168dc11a094bf7d70e4178a9,Conference of the European Chapter of the Association for Computational Linguistics,2023.0,68.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '23126830', 'name': 'Jakub Macina'}, {'authorId': '2048028927', 'name': 'Nico Daheim'}, {'authorId': '2151976144', 'name': 'Lingzhi Wang'}, {'authorId': '145679048', 'name': 'Tanmay Sinha'}, {'authorId': '2465316', 'name': 'Manu Kapur'}, {'authorId': '1730400', 'name': 'Iryna Gurevych'}, {'authorId': '2790926', 'name': 'Mrinmaya Sachan'}]","['Technical University of Darmstadt', 'Chinese University of Hong Kong', 'ETH Zurich', 'Qu & Co. (Netherlands)']","['China', 'Germany', 'Netherlands', 'Switzerland']",2023-01
2301.10919,Yizhao Jin,"Xiulei Song, Yizhao Jin, Greg Slabaugh, Simon Lucas",Joint action loss for proximal policy optimization,,,,,cs.LG cs.AI,http://creativecommons.org/licenses/by/4.0/,"  PPO (Proximal Policy Optimization) is a state-of-the-art policy gradient algorithm that has been successfully applied to complex computer games such as Dota 2 and Honor of Kings. In these environments, an agent makes compound actions consisting of multiple sub-actions. PPO uses clipping to restrict policy updates. Although clipping is simple and effective, it is not efficient in its sample use. For compound actions, most PPO implementations consider the joint probability (density) of sub-actions, which means that if the ratio of a sample (state compound-action pair) exceeds the range, the gradient the sample produces is zero. Instead, for each sub-action we calculate the loss separately, which is less prone to clipping during updates thereby making better use of samples. Further, we propose a multi-action mixed loss that combines joint and separate probabilities. We perform experiments in Gym-$\mu$RTS and MuJoCo. Our hybrid model improves performance by more than 50\% in different MuJoCo environments compared to OpenAI's PPO benchmark results. And in Gym-$\mu$RTS, we find the sub-action loss outperforms the standard PPO approach, especially when the clip range is large. Our findings suggest this method can better balance the use-efficiency and quality of samples. ","[{'version': 'v1', 'created': 'Thu, 26 Jan 2023 03:42:29 GMT'}]",2023-01-27,"[['Song', 'Xiulei', ''], ['Jin', 'Yizhao', ''], ['Slabaugh', 'Greg', ''], ['Lucas', 'Simon', '']]",0,0,2023-01-26,1,4,2,0,0,0,b1511724ac3cc24c44a925a345072d82c443e777,256274786.0,https://www.semanticscholar.org/paper/b1511724ac3cc24c44a925a345072d82c443e777,arXiv.org,2023.0,11.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2202783965', 'name': 'Xiulei Song'}, {'authorId': '2193091169', 'name': 'Yi-Fan Jin'}, {'authorId': '7622560', 'name': 'G. Slabaugh'}, {'authorId': '145815031', 'name': 'S. Lucas'}]","['Queen Mary University of London', 'JumpW AI Group Shanghai,China']","['China', 'United Kingdom']",2023-01
2302.00288,Hao Yu,"Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai
  Liang, Ying Li, Tao Xie, Qianxiang Wang",CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models,,,,,cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Code generation models based on the pre-training and fine-tuning paradigm have been increasingly attempted by both academia and industry, resulting in well-known industrial models such as Codex, CodeGen, and PanGu-Coder. To validate the performance of these models, multiple existing benchmarks (e.g., AiXBench and HumanEval) are proposed, including only cases of generating a standalone function, i.e., a function that invokes or accesses only built-in functions and standard libraries. However, standalone functions constitute only about 30\% of functions from real open-source projects. To assess a model's performance for pragmatic code generation (i.e., code generation for real settings of open source or proprietary code), in this paper, we propose a benchmark named CoderEval of pragmatic code generation with generative pre-trained models. Compared with the widely-used HumanEval benchmark from OpenAI, CoderEval can be used to assess the performance of models against pragmatic code generation beyond just generating standalone functions. Through the evaluation of three public available models (CodeGen, PanGu-Coder, and Codex) on CoderEval, we analyze and discuss the current progress and future directions of pragmatic code generation with a generative pre-trained model. ","[{'version': 'v1', 'created': 'Wed, 1 Feb 2023 07:39:28 GMT'}]",2023-02-02,"[['Yu', 'Hao', ''], ['Shen', 'Bo', ''], ['Ran', 'Dezhi', ''], ['Zhang', 'Jiaxin', ''], ['Zhang', 'Qi', ''], ['Ma', 'Yuchi', ''], ['Liang', 'Guangtai', ''], ['Li', 'Ying', ''], ['Xie', 'Tao', ''], ['Wang', 'Qianxiang', '']]",0,1,2023-02-01,1,10,1,2,1,1,6ab8aca8f631f42760a86cc614dfd7208b3fe58e,256459413.0,https://www.semanticscholar.org/paper/6ab8aca8f631f42760a86cc614dfd7208b3fe58e,arXiv.org,2023.0,39.0,16.0,3.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2110752255', 'name': 'Hao Yu'}, {'authorId': '2089966950', 'name': 'Bo Shen'}, {'authorId': '2038503437', 'name': 'Dezhi Ran'}, {'authorId': None, 'name': 'Jiaxin Zhang'}, {'authorId': '2145906426', 'name': 'Qi Zhang'}, {'authorId': '2135929661', 'name': 'Yu Ma'}, {'authorId': '2084524', 'name': 'Guangtai Liang'}, {'authorId': '2172444921', 'name': 'Ying Li'}, {'authorId': '2057038049', 'name': 'Tao Xie'}, {'authorId': '7417844', 'name': 'Qianxiang Wang'}]","['Peking University', 'Huawei Technologies (China)']",['China'],2023-02
2302.04023,Yejin Bang,"Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su,
  Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do,
  Yan Xu, Pascale Fung","A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",52 pages,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++ on machine translation, in a multi-turn ""prompt engineering"" fashion. We also release codebase for evaluation set extraction. ","[{'version': 'v1', 'created': 'Wed, 8 Feb 2023 12:35:34 GMT'}, {'version': 'v2', 'created': 'Tue, 28 Feb 2023 15:20:21 GMT'}]",2023-03-01,"[['Bang', 'Yejin', ''], ['Cahyawijaya', 'Samuel', ''], ['Lee', 'Nayeon', ''], ['Dai', 'Wenliang', ''], ['Su', 'Dan', ''], ['Wilie', 'Bryan', ''], ['Lovenia', 'Holy', ''], ['Ji', 'Ziwei', ''], ['Yu', 'Tiezheng', ''], ['Chung', 'Willy', ''], ['Do', 'Quyet V.', ''], ['Xu', 'Yan', ''], ['Fung', 'Pascale', '']]",1,1,2023-02-08,2,13,2,1,0,1,bf8491bef353df126e2306ad2fe4b898697b906a,256662612.0,https://www.semanticscholar.org/paper/bf8491bef353df126e2306ad2fe4b898697b906a,arXiv.org,2023.0,28.0,447.0,32.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '23672613', 'name': 'Yejin Bang'}, {'authorId': '66986482', 'name': 'Samuel Cahyawijaya'}, {'authorId': '40221187', 'name': 'Nayeon Lee'}, {'authorId': '47653392', 'name': 'Wenliang Dai'}, {'authorId': '144610224', 'name': 'Dan Su'}, {'authorId': '150048491', 'name': 'Bryan Wilie'}, {'authorId': '116344405', 'name': 'Holy Lovenia'}, {'authorId': '3391272', 'name': 'Ziwei Ji'}, {'authorId': '1660855299', 'name': 'Tiezheng Yu'}, {'authorId': '2160716372', 'name': 'Willy Chung'}, {'authorId': '2187874252', 'name': 'Quyet V. Do'}, {'authorId': '98271906', 'name': 'Yan Xu'}, {'authorId': '2057151752', 'name': 'Pascale Fung'}]","['Equal Contribution.', 'Hong Kong University of Science and Technology']",['China'],2023-02
2302.06476,Chengwei Qin,"Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro
  Yasunaga, Diyi Yang",Is ChatGPT a General-Purpose Natural Language Processing Task Solver?,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-sa/4.0/,"  Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies. ","[{'version': 'v1', 'created': 'Wed, 8 Feb 2023 09:44:51 GMT'}, {'version': 'v2', 'created': 'Wed, 15 Feb 2023 17:46:20 GMT'}]",2023-02-16,"[['Qin', 'Chengwei', ''], ['Zhang', 'Aston', ''], ['Zhang', 'Zhuosheng', ''], ['Chen', 'Jiaao', ''], ['Yasunaga', 'Michihiro', ''], ['Yang', 'Diyi', '']]",1,1,2023-02-08,2,6,2,1,0,1,873a581320d928249609d3c07229d5af182a379c,256827430.0,https://www.semanticscholar.org/paper/873a581320d928249609d3c07229d5af182a379c,arXiv.org,2023.0,88.0,265.0,18.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2084609980', 'name': 'Chengwei Qin'}, {'authorId': '2085709', 'name': 'Aston Zhang'}, {'authorId': '3322871', 'name': 'Zhuosheng Zhang'}, {'authorId': '47739850', 'name': 'Jiaao Chen'}, {'authorId': '19168196', 'name': 'Michihiro Yasunaga'}, {'authorId': '2143919864', 'name': 'Diyi Yang'}]","['Stanford University', 'Shanghai Jiao Tong University', 'Nanyang Technological University']","['China', 'United States', 'Singapore']",2023-02
2302.07257,Sheng Wang,"Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, Dinggang Shen",ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models,,,,,cs.CV eess.IV,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) have recently demonstrated their potential in clinical applications, providing valuable medical knowledge and advice. For example, a large dialog LLM like ChatGPT has successfully passed part of the US medical licensing exam. However, LLMs currently have difficulty processing images, making it challenging to interpret information from medical images, which are rich in information that supports clinical decisions. On the other hand, computer-aided diagnosis (CAD) networks for medical images have seen significant success in the medical field by using advanced deep-learning algorithms to support clinical decision-making. This paper presents a method for integrating LLMs into medical-image CAD networks. The proposed framework uses LLMs to enhance the output of multiple CAD networks, such as diagnosis networks, lesion segmentation networks, and report generation networks, by summarizing and reorganizing the information presented in natural language text format. The goal is to merge the strengths of LLMs' medical domain knowledge and logical reasoning with the vision understanding capability of existing medical-image CAD models to create a more user-friendly and understandable system for patients compared to conventional CAD systems. In the future, LLM's medical knowledge can be also used to improve the performance of vision-based medical-image CAD models. ","[{'version': 'v1', 'created': 'Tue, 14 Feb 2023 18:54:06 GMT'}]",2023-02-15,"[['Wang', 'Sheng', ''], ['Zhao', 'Zihao', ''], ['Ouyang', 'Xi', ''], ['Wang', 'Qian', ''], ['Shen', 'Dinggang', '']]",1,1,2023-02-14,1,5,2,1,0,1,5ef821267fa68d3231ed8135ff8ec09f25bb1398,256846858.0,https://www.semanticscholar.org/paper/5ef821267fa68d3231ed8135ff8ec09f25bb1398,arXiv.org,2023.0,36.0,62.0,4.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2151487856', 'name': 'Sheng Wang'}, {'authorId': '15594254', 'name': 'Zihao Zhao'}, {'authorId': '2056468197', 'name': 'Xi Ouyang'}, {'authorId': '2215061296', 'name': 'Qian Wang'}, {'authorId': '2150038187', 'name': 'Dinggang Shen'}]","['United Imaging Intelligence Disease Classifier', 'Shanghai Jiao Tong University', 'ShanghaiTech University']",['China'],2023-02
2302.08722,Yizhe Zhang,Yizhe Zhang and Danny Z. Chen,GPT4MIA: Utilizing Generative Pre-trained Transformer (GPT-3) as A Plug-and-Play Transductive Model for Medical Image Analysis,"Version 3: Added appendix with more results and visualizations.
  Questions and suggestions are welcome",,,,cs.CV cs.AI cs.LG cs.MM,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we propose a novel approach (called GPT4MIA) that utilizes Generative Pre-trained Transformer (GPT) as a plug-and-play transductive inference tool for medical image analysis (MIA). We provide theoretical analysis on why a large pre-trained language model such as GPT-3 can be used as a plug-and-play transductive inference model for MIA. At the methodological level, we develop several technical treatments to improve the efficiency and effectiveness of GPT4MIA, including better prompt structure design, sample selection, and prompt ordering of representative samples/features. We present two concrete use cases (with workflow) of GPT4MIA: (1) detecting prediction errors and (2) improving prediction accuracy, working in conjecture with well-established vision-based models for image classification (e.g., ResNet). Experiments validate that our proposed method is effective for these two tasks. We further discuss the opportunities and challenges in utilizing Transformer-based large language models for broader MIA applications. ","[{'version': 'v1', 'created': 'Fri, 17 Feb 2023 06:33:06 GMT'}, {'version': 'v2', 'created': 'Sat, 4 Mar 2023 06:45:27 GMT'}, {'version': 'v3', 'created': 'Tue, 21 Mar 2023 12:59:20 GMT'}]",2023-03-22,"[['Zhang', 'Yizhe', ''], ['Chen', 'Danny Z.', '']]",0,1,2023-02-17,3,2,4,1,0,1,9021c16bca0e9e8b98607e352722b0ff6561e8fd,257364761.0,https://www.semanticscholar.org/paper/9021c16bca0e9e8b98607e352722b0ff6561e8fd,,2023.0,8.0,0.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2000095290', 'name': 'Yizhe Zhang'}, {'authorId': '2132585133', 'name': 'Da Chen'}]","['Nanjing University of Science and Technology', 'University of Notre Dame']","['China', 'United States']",2023-02
2302.09185,Albert Lu,"Albert Lu, Hongxin Zhang, Yanzhe Zhang, Xuezhi Wang, Diyi Yang",Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints,"27 pages, 13 figures, 11 tables, to be published in EACL 2023
  Findings",,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  The limits of open-ended generative models are unclear, yet increasingly important. What causes them to succeed and what causes them to fail? In this paper, we take a prompt-centric approach to analyzing and bounding the abilities of open-ended generative models. We present a generic methodology of analysis with two challenging prompt constraint types: structural and stylistic. These constraint types are categorized into a set of well-defined constraints that are analyzable by a single prompt. We then systematically create a diverse set of simple, natural, and useful prompts to robustly analyze each individual constraint. Using the GPT-3 text-davinci-002 model as a case study, we generate outputs from our collection of prompts and analyze the model's generative failures. We also show the generalizability of our proposed method on other large models like BLOOM and OPT. Our results and our in-context mitigation strategies reveal open challenges for future research. We have publicly released our code at https://github.com/SALT-NLP/Bound-Cap-LLM. ","[{'version': 'v1', 'created': 'Fri, 17 Feb 2023 23:30:28 GMT'}]",2023-02-21,"[['Lu', 'Albert', ''], ['Zhang', 'Hongxin', ''], ['Zhang', 'Yanzhe', ''], ['Wang', 'Xuezhi', ''], ['Yang', 'Diyi', '']]",0,1,2023-02-17,1,5,3,3,2,1,3f83582c08a62e5bd02398fafc93f7eaf1e4b84e,257039031.0,https://www.semanticscholar.org/paper/3f83582c08a62e5bd02398fafc93f7eaf1e4b84e,Findings,2023.0,36.0,8.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2208657585', 'name': 'Albert Lu'}, {'authorId': '2118083343', 'name': 'Hongxin Zhang'}, {'authorId': '2121290295', 'name': 'Yanzhe Zhang'}, {'authorId': '1524732527', 'name': 'Xuezhi Wang'}, {'authorId': '2143919864', 'name': 'Diyi Yang'}]","['Stanford University', 'Google', 'Georgia Institute of Technology', 'Shanghai Jiao Tong University']","['China', 'United States']",2023-02
2302.09268,Qihuang Zhong,"Qihuang Zhong, Liang Ding, Keqin Peng, Juhua Liu, Bo Du, Li Shen,
  Yibing Zhan and Dacheng Tao",Bag of Tricks for Effective Language Model Pretraining and Downstream Adaptation: A Case Study on GLUE,"Technical report. arXiv admin note: text overlap with
  arXiv:2212.01853",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This technical report briefly describes our JDExplore d-team's submission Vega v1 on the General Language Understanding Evaluation (GLUE) leaderboard, where GLUE is a collection of nine natural language understanding tasks, including question answering, linguistic acceptability, sentiment analysis, text similarity, paraphrase detection, and natural language inference. [Method] We investigate several effective strategies and choose their best combination setting as the training recipes. As for model structure, we employ the vanilla Transformer with disentangled attention as the basic block encoder. For self-supervised training, we employ the representative denoising objective (i.e., replaced token detection) in phase 1 and combine the contrastive objective (i.e., sentence embedding contrastive learning) with it in phase 2. During fine-tuning, several advanced techniques such as transductive fine-tuning, self-calibrated fine-tuning, and adversarial fine-tuning are adopted. [Results] According to our submission record (Jan. 2022), with our optimized pretraining and fine-tuning strategies, our 1.3 billion model sets new state-of-the-art on 4/9 tasks, achieving the best average score of 91.3. Encouragingly, our Vega v1 is the first to exceed powerful human performance on the two challenging tasks, i.e., SST-2 and WNLI. We believe our empirically successful recipe with a bag of tricks could shed new light on developing efficient discriminative large language models. ","[{'version': 'v1', 'created': 'Sat, 18 Feb 2023 09:26:35 GMT'}]",2023-02-21,"[['Zhong', 'Qihuang', ''], ['Ding', 'Liang', ''], ['Peng', 'Keqin', ''], ['Liu', 'Juhua', ''], ['Du', 'Bo', ''], ['Shen', 'Li', ''], ['Zhan', 'Yibing', ''], ['Tao', 'Dacheng', '']]",0,0,2023-02-18,1,8,1,0,0,0,1d4ecc8eabd7d5ac86a53988dcffbf81dd270e23,257038296.0,https://www.semanticscholar.org/paper/1d4ecc8eabd7d5ac86a53988dcffbf81dd270e23,arXiv.org,2023.0,52.0,9.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2114810150', 'name': 'Qihuang Zhong'}, {'authorId': '46573238', 'name': 'Liang Ding'}, {'authorId': '2065263152', 'name': 'Keqin Peng'}, {'authorId': '46701032', 'name': 'Juhua Liu'}, {'authorId': '2142452296', 'name': 'Bo Du'}, {'authorId': '2144035454', 'name': 'Li Shen'}, {'authorId': '1895813', 'name': 'Yibing Zhan'}, {'authorId': '2140448089', 'name': 'Dacheng Tao'}]","['Wuhan University', 'Beihang University', 'Jingdong']",['China'],2023-02
2302.09419,Ce Zhou,"Ce Zhou (1), Qian Li (2), Chen Li (2), Jun Yu (3), Yixin Liu (3),
  Guangjing Wang (1), Kai Zhang (3), Cheng Ji (2), Qiben Yan (1), Lifang He
  (3), Hao Peng (2), Jianxin Li (2), Jia Wu (4), Ziwei Liu (5), Pengtao Xie
  (6), Caiming Xiong (7), Jian Pei (8), Philip S. Yu (9), Lichao Sun (3) ((1)
  Michigan State University, (2) Beihang University, (3) Lehigh University, (4)
  Macquarie University, (5) Nanyang Technological University, (6) University of
  California San Diego, (7) Salesforce AI Research, (8) Duke University, (9)
  University of Illinois at Chicago)",A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT,"99 pages, 16 figures",,,,cs.AI cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of recent research advancements, challenges, and opportunities for PFMs in text, image, graph, as well as other data modalities. The review covers the basic components and existing pretraining methods used in natural language processing, computer vision, and graph learning. Additionally, it explores advanced PFMs used for different data modalities and unified PFMs that consider data quality and quantity. The review also discusses research related to the fundamentals of PFMs, such as model efficiency and compression, security, and privacy. Finally, the study provides key implications, future research directions, challenges, and open problems in the field of PFMs. Overall, this survey aims to shed light on the research of the PFMs on scalability, security, logical reasoning ability, cross-domain learning ability, and the user-friendly interactive ability for artificial general intelligence. ","[{'version': 'v1', 'created': 'Sat, 18 Feb 2023 20:51:09 GMT'}, {'version': 'v2', 'created': 'Thu, 30 Mar 2023 14:44:09 GMT'}, {'version': 'v3', 'created': 'Mon, 1 May 2023 07:48:05 GMT'}]",2023-05-02,"[['Zhou', 'Ce', ''], ['Li', 'Qian', ''], ['Li', 'Chen', ''], ['Yu', 'Jun', ''], ['Liu', 'Yixin', ''], ['Wang', 'Guangjing', ''], ['Zhang', 'Kai', ''], ['Ji', 'Cheng', ''], ['Yan', 'Qiben', ''], ['He', 'Lifang', ''], ['Peng', 'Hao', ''], ['Li', 'Jianxin', ''], ['Wu', 'Jia', ''], ['Liu', 'Ziwei', ''], ['Xie', 'Pengtao', ''], ['Xiong', 'Caiming', ''], ['Pei', 'Jian', ''], ['Yu', 'Philip S.', ''], ['Sun', 'Lichao', '']]",1,1,2023-02-18,3,19,3,2,0,2,3599a236f285af48782fc30b1341d13ec7320735,257039063.0,https://www.semanticscholar.org/paper/3599a236f285af48782fc30b1341d13ec7320735,arXiv.org,2023.0,0.0,161.0,8.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2187669795', 'name': 'Ce Zhou'}, {'authorId': '2117126771', 'name': 'Qian Li'}, {'authorId': '2116521405', 'name': 'Chen Li'}, {'authorId': None, 'name': 'Jun Yu'}, {'authorId': None, 'name': 'Yixin Liu'}, {'authorId': '2152582885', 'name': 'Guan Wang'}, {'authorId': '2158520914', 'name': 'Kaichao Zhang'}, {'authorId': '2052296239', 'name': 'Cheng Ji'}, {'authorId': '2072779826', 'name': 'Qi Yan'}, {'authorId': '40901820', 'name': 'Lifang He'}, {'authorId': '49349645', 'name': 'Hao Peng'}, {'authorId': '47785906', 'name': 'Jianxin Li'}, {'authorId': '2144139161', 'name': 'Jia Wu'}, {'authorId': '2145254462', 'name': 'Ziwei Liu'}, {'authorId': '40526720', 'name': 'P. Xie'}, {'authorId': '2054594326', 'name': 'Caiming Xiong'}, {'authorId': '2143960747', 'name': 'Jian Pei'}, {'authorId': '152297693', 'name': 'Philip S. Yu'}, {'authorId': '2208907959', 'name': 'Lichao Sun Michigan State University'}, {'authorId': '88729424', 'name': 'B. University'}, {'authorId': '103217915', 'name': 'Lehigh University'}, {'authorId': '2095706829', 'name': 'M. University'}, {'authorId': '88740224', 'name': 'Nanyang Technological University'}, {'authorId': '102788302', 'name': 'University of California at San Diego'}, {'authorId': '102879328', 'name': 'D. University'}, {'authorId': '6106131', 'name': 'U. Chicago'}, {'authorId': '2208653278', 'name': 'Salesforce AI Research'}]","['Michigan State University', 'University of California, Riverside', 'Macquarie University', 'Lehigh University', 'Salesforce AI Research,', 'Duke University', 'Beihang University', 'University of Illinois at Chicago', 'Nanyang Technological University']","['China', 'United States', 'Singapore', 'Australia']",2023-02
2302.09432,Dakuan Lu,"Dakuan Lu, Hengkui Wu, Jiaqing Liang, Yipei Xu, Qianyu He, Yipeng
  Geng, Mengkun Han, Yingsi Xin, Yanghua Xiao","BBT-Fin: Comprehensive Construction of Chinese Financial Domain Pre-trained Language Model, Corpus and Benchmark",Changed author order,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  To advance Chinese financial natural language processing (NLP), we introduce BBT-FinT5, a new Chinese financial pre-training language model based on the T5 model. To support this effort, we have built BBT-FinCorpus, a large-scale financial corpus with approximately 300GB of raw text from four different sources. In general domain NLP, comprehensive benchmarks like GLUE and SuperGLUE have driven significant advancements in language model pre-training by enabling head-to-head comparisons among models. Drawing inspiration from these benchmarks, we propose BBT-CFLEB, a Chinese Financial Language understanding and generation Evaluation Benchmark, which includes six datasets covering both understanding and generation tasks. Our aim is to facilitate research in the development of NLP within the Chinese financial domain. Our model, corpus and benchmark are released at https://github.com/ssymmetry/BBT-FinCUGE-Applications. Our work belongs to the Big Bang Transformer (BBT), a large-scale pre-trained language model project. ","[{'version': 'v1', 'created': 'Sat, 18 Feb 2023 22:20:37 GMT'}, {'version': 'v2', 'created': 'Sun, 26 Feb 2023 10:50:09 GMT'}]",2023-02-28,"[['Lu', 'Dakuan', ''], ['Wu', 'Hengkui', ''], ['Liang', 'Jiaqing', ''], ['Xu', 'Yipei', ''], ['He', 'Qianyu', ''], ['Geng', 'Yipeng', ''], ['Han', 'Mengkun', ''], ['Xin', 'Yingsi', ''], ['Xiao', 'Yanghua', '']]",0,0,2023-02-18,2,9,1,1,1,0,aafae4730b1add0b3e243e011db9ac87428f83cd,257038067.0,https://www.semanticscholar.org/paper/aafae4730b1add0b3e243e011db9ac87428f83cd,arXiv.org,2023.0,34.0,12.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2208761923', 'name': 'Dakuan Lu'}, {'authorId': '3366523', 'name': 'Jiaqing Liang'}, {'authorId': '3456174', 'name': 'Yipei Xu'}, {'authorId': '2152880833', 'name': 'Qi He'}, {'authorId': '1989726390', 'name': 'Yipeng Geng'}, {'authorId': '2208796386', 'name': 'Mengkun Han'}, {'authorId': '2147223611', 'name': 'Ying Xin'}, {'authorId': '2208906049', 'name': 'Hengkui Wu'}, {'authorId': '2116642640', 'name': 'Yanghua Xiao'}]","['SuperSymmetry Technologies', 'Fudan University']",['China'],2023-02
2302.09582,Dan Zhang,"Ming Li, Yusheng Su, Hsiu-Yuan Huang, Jiali Cheng, Xin Hu, Xinmiao
  Zhang, Huadong Wang, Yujia Qin, Xiaozhi Wang, Zhiyuan Liu, Dan Zhang",Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference,"39 pages, 13 figures, 2 tables, fix formatting errors",,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Understanding how language supports emotion inference remains a topic of debate in emotion science. The present study investigated whether language-derived emotion-concept knowledge would causally support emotion inference by manipulating the language-specific knowledge representations in large language models. Using the prompt technique, 14 attributes of emotion concepts were found to be represented by distinct artificial neuron populations. By manipulating these attribute-related neurons, the majority of the emotion inference tasks showed performance deterioration compared to random manipulations. The attribute-specific performance deterioration was related to the importance of different attributes in human mental space. Our findings provide causal evidence in support of a language-based mechanism for emotion inference and highlight the contributions of emotion-concept knowledge. ","[{'version': 'v1', 'created': 'Sun, 19 Feb 2023 14:21:33 GMT'}, {'version': 'v2', 'created': 'Tue, 21 Feb 2023 07:28:04 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Jul 2023 09:04:14 GMT'}, {'version': 'v4', 'created': 'Mon, 21 Aug 2023 09:44:19 GMT'}]",2023-08-22,"[['Li', 'Ming', ''], ['Su', 'Yusheng', ''], ['Huang', 'Hsiu-Yuan', ''], ['Cheng', 'Jiali', ''], ['Hu', 'Xin', ''], ['Zhang', 'Xinmiao', ''], ['Wang', 'Huadong', ''], ['Qin', 'Yujia', ''], ['Wang', 'Xiaozhi', ''], ['Liu', 'Zhiyuan', ''], ['Zhang', 'Dan', '']]",0,0,2023-02-19,4,11,2,0,0,0,25e48d36c34d958c89244953501638cfcb7a2839,259833437.0,https://www.semanticscholar.org/paper/25e48d36c34d958c89244953501638cfcb7a2839,,2023.0,133.0,1.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Psychology', 'source': 's2-fos-model'}]","[{'authorId': '2222963088', 'name': 'Ming Li'}, {'authorId': '48576745', 'name': 'Yusheng Su'}, {'authorId': '2209072917', 'name': 'Hsiu-Yuan Huang'}, {'authorId': '2112798028', 'name': 'Jialing Cheng'}, {'authorId': '2110048706', 'name': 'Xin Hu'}, {'authorId': '2208908966', 'name': 'Xinmiao Zhang'}, {'authorId': '2155242767', 'name': 'Huadong Wang'}, {'authorId': '50625437', 'name': 'Yujia Qin'}, {'authorId': '48631777', 'name': 'Xiaozhi Wang'}, {'authorId': '46270592', 'name': 'Zhi-Yun Liu'}, {'authorId': '2122440589', 'name': 'Dan Zhang'}]","['University of Massachusetts Lowell', 'Tsinghua University', 'University of Pittsburgh', 'University of Science and Technology Beijing']","['China', 'United States']",2023-02
2302.10198,Qihuang Zhong,"Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du and Dacheng Tao",Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT,"Work in progress. Added results of advanced prompting strategies,
  e.g., CoT. (19 pages)",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, ChatGPT has attracted great attention, as it can generate fluent and high-quality responses to human inquiries. Several prior studies have shown that ChatGPT attains remarkable generation ability compared with existing models. However, the quantitative analysis of ChatGPT's understanding ability has been given little attention. In this report, we explore the understanding ability of ChatGPT by evaluating it on the most popular GLUE benchmark, and comparing it with 4 representative fine-tuned BERT-style models. We find that: 1) ChatGPT falls short in handling paraphrase and similarity tasks; 2) ChatGPT outperforms all BERT models on inference tasks by a large margin; 3) ChatGPT achieves comparable performance compared with BERT on sentiment analysis and question-answering tasks. Additionally, by combining some advanced prompting strategies, we show that the understanding ability of ChatGPT can be further improved. ","[{'version': 'v1', 'created': 'Sun, 19 Feb 2023 12:29:33 GMT'}, {'version': 'v2', 'created': 'Thu, 2 Mar 2023 14:33:12 GMT'}]",2023-03-03,"[['Zhong', 'Qihuang', ''], ['Ding', 'Liang', ''], ['Liu', 'Juhua', ''], ['Du', 'Bo', ''], ['Tao', 'Dacheng', '']]",1,1,2023-02-19,2,5,1,1,0,1,6839816cd3ab194dfa3ffe3066cc35d099820e00,257050251.0,https://www.semanticscholar.org/paper/6839816cd3ab194dfa3ffe3066cc35d099820e00,arXiv.org,2023.0,44.0,85.0,9.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2114810150', 'name': 'Qihuang Zhong'}, {'authorId': '46573238', 'name': 'Liang Ding'}, {'authorId': '46701032', 'name': 'Juhua Liu'}, {'authorId': '2064619959', 'name': 'Bo Du'}, {'authorId': '2140448089', 'name': 'Dacheng Tao'}]","['Wuhan University', 'University of Sydney', 'Jingdong']","['China', 'Australia']",2023-02
2302.10205,Xiang Wei,"Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen
  Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, Yong Jiang, and
  Wenjuan Han",Zero-Shot Information Extraction via Chatting with ChatGPT,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Zero-shot information extraction (IE) aims to build IE systems from the unannotated text. It is challenging due to involving little human intervention. Challenging but worthwhile, zero-shot IE reduces the time and effort that data labeling takes. Recent efforts on large language models (LLMs, e.g., GPT-3, ChatGPT) show promising performance on zero-shot settings, thus inspiring us to explore prompt-based methods. In this work, we ask whether strong IE models can be constructed by directly prompting LLMs. Specifically, we transform the zero-shot IE task into a multi-turn question-answering problem with a two-stage framework (ChatIE). With the power of ChatGPT, we extensively evaluate our framework on three IE tasks: entity-relation triple extract, named entity recognition, and event extraction. Empirical results on six datasets across two languages show that ChatIE achieves impressive performance and even surpasses some full-shot models on several datasets (e.g., NYT11-HRL). We believe that our work could shed light on building IE models with limited resources. ","[{'version': 'v1', 'created': 'Mon, 20 Feb 2023 12:57:12 GMT'}]",2023-02-22,"[['Wei', 'Xiang', ''], ['Cui', 'Xingyu', ''], ['Cheng', 'Ning', ''], ['Wang', 'Xiaobin', ''], ['Zhang', 'Xin', ''], ['Huang', 'Shen', ''], ['Xie', 'Pengjun', ''], ['Xu', 'Jinan', ''], ['Chen', 'Yufeng', ''], ['Zhang', 'Meishan', ''], ['Jiang', 'Yong', ''], ['Han', 'Wenjuan', '']]",1,1,2023-02-20,1,12,1,2,0,2,f4cba0db34aa0c389cec267ca1f3ba5255ea2645,257050669.0,https://www.semanticscholar.org/paper/f4cba0db34aa0c389cec267ca1f3ba5255ea2645,arXiv.org,2023.0,32.0,79.0,6.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2115494116', 'name': 'Xiang Wei'}, {'authorId': '2149528155', 'name': 'Xingyu Cui'}, {'authorId': '2208958032', 'name': 'Ning Cheng'}, {'authorId': '2108114693', 'name': 'Xiaobin Wang'}, {'authorId': '2149173726', 'name': 'Xin Zhang'}, {'authorId': '2186268584', 'name': 'Shen Huang'}, {'authorId': '35930962', 'name': 'Pengjun Xie'}, {'authorId': '2310092', 'name': 'Jinan Xu'}, {'authorId': '47559028', 'name': 'Yufeng Chen'}, {'authorId': '2117849151', 'name': 'Meishan Zhang'}, {'authorId': '50262192', 'name': 'Yong Jiang'}, {'authorId': '144836032', 'name': 'Wenjuan Han'}]","['Alibaba', 'Beijing Jiaotong University']",['China'],2023-02
2302.11957,Jipeng Qiang,Yutao Feng and Jipeng Qiang and Yun Li and Yunhao Yuan and Yi Zhu,Sentence Simplification via Large Language Models,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Sentence Simplification aims to rephrase complex sentences into simpler sentences while retaining original meaning. Large Language models (LLMs) have demonstrated the ability to perform a variety of natural language processing tasks. However, it is not yet known whether LLMs can be served as a high-quality sentence simplification system. In this work, we empirically analyze the zero-/few-shot learning ability of LLMs by evaluating them on a number of benchmark test sets. Experimental results show LLMs outperform state-of-the-art sentence simplification methods, and are judged to be on a par with human annotators. ","[{'version': 'v1', 'created': 'Thu, 23 Feb 2023 12:11:58 GMT'}]",2023-02-24,"[['Feng', 'Yutao', ''], ['Qiang', 'Jipeng', ''], ['Li', 'Yun', ''], ['Yuan', 'Yunhao', ''], ['Zhu', 'Yi', '']]",0,0,2023-02-23,1,5,2,0,0,0,6328c11081558487a4029388e9a5ebb45194b1b7,257102624.0,https://www.semanticscholar.org/paper/6328c11081558487a4029388e9a5ebb45194b1b7,arXiv.org,2023.0,36.0,7.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2150671370', 'name': 'Yutao Feng'}, {'authorId': '35051203', 'name': 'Jipeng Qiang'}, {'authorId': '2117014265', 'name': 'Yun Li'}, {'authorId': '2998154', 'name': 'Yunhao Yuan'}, {'authorId': '2143400684', 'name': 'Yi Zhu'}]",['Yangzhou University'],['China'],2023-02
2302.11978,Shengnan An,"Shengnan An, Zeqi Lin, Bei Chen, Qiang Fu, Nanning Zheng, Jian-Guang
  Lou",Does Deep Learning Learn to Abstract? A Systematic Probing Framework,ICLR 2023,,,,cs.LG cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Abstraction is a desirable capability for deep learning models, which means to induce abstract concepts from concrete instances and flexibly apply them beyond the learning context. At the same time, there is a lack of clear understanding about both the presence and further characteristics of this capability in deep learning models. In this paper, we introduce a systematic probing framework to explore the abstraction capability of deep learning models from a transferability perspective. A set of controlled experiments are conducted based on this framework, providing strong evidence that two probed pre-trained language models (PLMs), T5 and GPT2, have the abstraction capability. We also conduct in-depth analysis, thus shedding further light: (1) the whole training phase exhibits a ""memorize-then-abstract"" two-stage process; (2) the learned abstract concepts are gathered in a few middle-layer attention heads, rather than being evenly distributed throughout the model; (3) the probed abstraction capabilities exhibit robustness against concept mutations, and are more robust to low-level/source-side mutations than high-level/target-side ones; (4) generic pre-training is critical to the emergence of abstraction capability, and PLMs exhibit better abstraction with larger model sizes and data scales. ","[{'version': 'v1', 'created': 'Thu, 23 Feb 2023 12:50:02 GMT'}]",2023-02-24,"[['An', 'Shengnan', ''], ['Lin', 'Zeqi', ''], ['Chen', 'Bei', ''], ['Fu', 'Qiang', ''], ['Zheng', 'Nanning', ''], ['Lou', 'Jian-Guang', '']]",0,1,2023-02-23,1,6,2,2,2,0,2ae807688d5bae0a7331992793be066b93d7655f,257102348.0,https://www.semanticscholar.org/paper/2ae807688d5bae0a7331992793be066b93d7655f,International Conference on Learning Representations,2023.0,69.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2119217081', 'name': 'Shengnan An'}, {'authorId': '2284174', 'name': 'Zeqi Lin'}, {'authorId': '143876723', 'name': 'B. Chen'}, {'authorId': '2113771309', 'name': 'Qiang Fu'}, {'authorId': '2144620206', 'name': 'Nanning Zheng'}, {'authorId': '153249455', 'name': 'Jian-Guang Lou'}]","[""Xi'an Jiaotong University"", 'Microsoft']","['China', 'United States']",2023-02
2302.12095,Jindong Wang,"Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong
  Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, Binxin Jiao, Yue Zhang,
  Xing Xie",On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective,"Highlighted paper at ICLR 2023 workshop on Trustworthy and Reliable
  Large-Scale Machine Learning Models; code is at:
  https://github.com/microsoft/robustlearn; more works:
  https://llm-eval.github.io/",,,,cs.AI cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  ChatGPT is a recent chatbot service released by OpenAI and is receiving increasing attention over the past few months. While evaluations of various aspects of ChatGPT have been done, its robustness, i.e., the performance to unexpected inputs, is still unclear to the public. Robustness is of particular concern in responsible AI, especially for safety-critical applications. In this paper, we conduct a thorough evaluation of the robustness of ChatGPT from the adversarial and out-of-distribution (OOD) perspective. To do so, we employ the AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart review and DDXPlus medical diagnosis datasets for OOD evaluation. We select several popular foundation models as baselines. Results show that ChatGPT shows consistent advantages on most adversarial and OOD classification and translation tasks. However, the absolute performance is far from perfection, which suggests that adversarial and OOD robustness remains a significant threat to foundation models. Moreover, ChatGPT shows astounding performance in understanding dialogue-related texts and we find that it tends to provide informal suggestions for medical tasks instead of definitive answers. Finally, we present in-depth discussions of possible research directions. ","[{'version': 'v1', 'created': 'Wed, 22 Feb 2023 11:01:20 GMT'}, {'version': 'v2', 'created': 'Mon, 27 Feb 2023 02:13:38 GMT'}, {'version': 'v3', 'created': 'Thu, 2 Mar 2023 08:33:04 GMT'}, {'version': 'v4', 'created': 'Wed, 29 Mar 2023 14:21:51 GMT'}, {'version': 'v5', 'created': 'Tue, 29 Aug 2023 05:34:25 GMT'}]",2023-08-30,"[['Wang', 'Jindong', ''], ['Hu', 'Xixu', ''], ['Hou', 'Wenxin', ''], ['Chen', 'Hao', ''], ['Zheng', 'Runkai', ''], ['Wang', 'Yidong', ''], ['Yang', 'Linyi', ''], ['Huang', 'Haojun', ''], ['Ye', 'Wei', ''], ['Geng', 'Xiubo', ''], ['Jiao', 'Binxin', ''], ['Zhang', 'Yue', ''], ['Xie', 'Xing', '']]",1,1,2023-02-22,5,13,3,1,0,1,5c7353fac22a8fdc43fc2f5c006b5d6902c47e75,257102461.0,https://www.semanticscholar.org/paper/5c7353fac22a8fdc43fc2f5c006b5d6902c47e75,arXiv.org,2023.0,74.0,86.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1519290245', 'name': 'Jindong Wang'}, {'authorId': '2186046594', 'name': 'Xixu Hu'}, {'authorId': '32216189', 'name': 'Wenxin Hou'}, {'authorId': '2051536212', 'name': 'Hao Chen'}, {'authorId': '2008039497', 'name': 'Runkai Zheng'}, {'authorId': '2108024273', 'name': 'Yidong Wang'}, {'authorId': '2145500840', 'name': 'Linyi Yang'}, {'authorId': None, 'name': 'Haojun Huang'}, {'authorId': '2147205193', 'name': 'Weirong Ye'}, {'authorId': '2442662', 'name': 'Xiubo Geng'}, {'authorId': '24128606', 'name': 'Binxing Jiao'}, {'authorId': '2211964951', 'name': 'Yue Zhang'}, {'authorId': '1576441343', 'name': 'Xingxu Xie'}]","['Westlake University', 'Chinese University of Hong Kong, Shenzhen', 'City University of Hong Kong', 'Carnegie Mellon University', 'Microsoft']","['China', 'United States']",2023-02
2302.12246,Shizhe Diao,"Shizhe Diao, Pengcheng Wang, Yong Lin, Tong Zhang",Active Prompting with Chain-of-Thought for Large Language Models,"20 pages, 3 figures, 11 tables",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving state-of-the-art on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationship demonstrate the effectiveness of our method. Our code will be available at https://github.com/shizhediao/active-prompt. ","[{'version': 'v1', 'created': 'Thu, 23 Feb 2023 18:58:59 GMT'}, {'version': 'v2', 'created': 'Sun, 26 Feb 2023 15:18:50 GMT'}, {'version': 'v3', 'created': 'Tue, 23 May 2023 15:43:28 GMT'}]",2023-05-24,"[['Diao', 'Shizhe', ''], ['Wang', 'Pengcheng', ''], ['Lin', 'Yong', ''], ['Zhang', 'Tong', '']]",0,0,2023-02-23,3,4,1,0,0,0,3fc3460c4554a28e489a0ea6ef067b79b7d301d9,257102707.0,https://www.semanticscholar.org/paper/3fc3460c4554a28e489a0ea6ef067b79b7d301d9,arXiv.org,2023.0,77.0,40.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '50826757', 'name': 'Shizhe Diao'}, {'authorId': '2108818768', 'name': 'Pengcheng Wang'}, {'authorId': '2154487280', 'name': 'Yong Lin'}, {'authorId': '2146324423', 'name': 'Tong Zhang'}]","['University of Toronto', 'Hong Kong University of Science and Technology', 'University of Hong Kong']","['Canada', 'China', 'Hong Kong']",2023-02
2302.12343,Denis Jered McInerney,"Denis Jered McInerney, Geoffrey Young, Jan-Willem van de Meent, Byron
  C. Wallace",CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have yielded fast and dramatic progress in NLP, and now offer strong few- and zero-shot capabilities on new tasks, reducing the need for annotation. This is especially exciting for the medical domain, in which supervision is often scant and expensive. At the same time, model predictions are rarely so accurate that they can be trusted blindly. Clinicians therefore tend to favor ""interpretable"" classifiers over opaque LLMs. For example, risk prediction tools are often linear models defined over manually crafted predictors that must be laboriously extracted from EHRs. We propose CHiLL (Crafting High-Level Latents), which uses LLMs to permit natural language specification of high-level features for linear models via zero-shot feature extraction using expert-composed queries. This approach has the promise to empower physicians to use their domain expertise to craft features which are clinically meaningful for a downstream task of interest, without having to manually extract these from raw EHR (as often done now). We are motivated by a real-world risk prediction task, but as a reproducible proxy, we use MIMIC-III and MIMIC-CXR data and standard predictive tasks (e.g., 30-day readmission) to evaluate our approach. We find that linear models using automatically extracted features are comparably performant to models using reference features, and provide greater interpretability than linear models using ""Bag-of-Words"" features. We verify that learned feature weights align well with clinical expectations. ","[{'version': 'v1', 'created': 'Thu, 23 Feb 2023 21:23:06 GMT'}]",2023-02-27,"[['McInerney', 'Denis Jered', ''], ['Young', 'Geoffrey', ''], ['van de Meent', 'Jan-Willem', ''], ['Wallace', 'Byron C.', '']]",0,0,2023-02-23,1,4,3,0,0,0,d78eb600f987334b051d0a2ba69c72f9f01849ea,257205986.0,https://www.semanticscholar.org/paper/d78eb600f987334b051d0a2ba69c72f9f01849ea,arXiv.org,2023.0,23.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1621628526', 'name': 'Denis Jered McInerney'}, {'authorId': '2106234673', 'name': 'Geoffrey S. Young'}, {'authorId': '2086966519', 'name': 'J.-W. van de Meent'}, {'authorId': '1912476', 'name': 'Byron C. Wallace'}]","['University of Amsterdam', 'Northeastern University', ""Brigham and Women's Hospital""]","['China', 'United States', 'Netherlands']",2023-02
2302.12822,Shizhe Diao,"KaShun Shum, Shizhe Diao, Tong Zhang",Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data,"22 pages, 4 figures, 13 tables",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Chain-of-thought prompting (CoT) advances the reasoning abilities of large language models (LLMs) and achieves superior performance in arithmetic, commonsense, and symbolic reasoning tasks. However, most CoT studies rely on carefully designed human-annotated rational chains to prompt the language model, which poses challenges for real-world applications where labeled training data is available without human-annotated rational chains. This creates barriers to applications of CoT prompting to these general tasks. This paper proposes a new strategy, Automate-CoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought), that can bypass human engineering of CoTs by automatically augmenting rational chains from a small labeled dataset, and then pruning low-quality chains to construct a candidate pool of machine-generated rationale chains based on the labels. Finally, it selects the optimal combination of several rationale chains from the pool for CoT prompting by employing a variance-reduced policy gradient strategy to estimate the significance of each example in a black-box language model. Automate-CoT enables a quick adaptation of the CoT technique to different tasks. Experimental results demonstrate the effectiveness of our method, where state-of-the-art results are achieved on arithmetic reasoning (+2.7\%), commonsense reasoning (+3.4\%), symbolic reasoning (+3.2\%), and non-reasoning tasks (+2.5\%). Our code will be available at https://github.com/shizhediao/automate-cot. ","[{'version': 'v1', 'created': 'Fri, 24 Feb 2023 18:58:06 GMT'}]",2023-02-27,"[['Shum', 'KaShun', ''], ['Diao', 'Shizhe', ''], ['Zhang', 'Tong', '']]",0,0,2023-02-24,1,3,1,0,0,0,1358f90705b05cdb20ebe6799b02196205e7e9f0,257205763.0,https://www.semanticscholar.org/paper/1358f90705b05cdb20ebe6799b02196205e7e9f0,arXiv.org,2023.0,72.0,29.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2121340452', 'name': 'Kashun Shum'}, {'authorId': '50826757', 'name': 'Shizhe Diao'}, {'authorId': '2146324423', 'name': 'Tong Zhang'}]",['Hong Kong University of Science and Technology'],['China'],2023-02
2302.13136,Rui Wang,"Rui Wang, Pengyu Cheng, Ricardo Henao",Toward Fairness in Text Generation via Mutual Information Minimization based on Importance Sampling,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pretrained language models (PLMs), such as GPT2, have achieved remarkable empirical performance in text generation tasks. However, pretrained on large-scale natural language corpora, the generated text from PLMs may exhibit social bias against disadvantaged demographic groups. To improve the fairness of PLMs in text generation, we propose to minimize the mutual information between the semantics in the generated text sentences and their demographic polarity, i.e., the demographic group to which the sentence is referring. In this way, the mentioning of a demographic group (e.g., male or female) is encouraged to be independent from how it is described in the generated text, thus effectively alleviating the social bias. Moreover, we propose to efficiently estimate the upper bound of the above mutual information via importance sampling, leveraging a natural language corpus. We also propose a distillation mechanism that preserves the language modeling ability of the PLMs after debiasing. Empirical results on real-world benchmarks demonstrate that the proposed method yields superior performance in term of both fairness and language modeling ability. ","[{'version': 'v1', 'created': 'Sat, 25 Feb 2023 18:29:02 GMT'}]",2023-02-28,"[['Wang', 'Rui', ''], ['Cheng', 'Pengyu', ''], ['Henao', 'Ricardo', '']]",0,1,2023-02-25,1,3,2,1,1,0,812867453701f0929579e481bd15dfb0ebee1d7e,257220117.0,https://www.semanticscholar.org/paper/812867453701f0929579e481bd15dfb0ebee1d7e,International Conference on Artificial Intelligence and Statistics,2023.0,26.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '39077217', 'name': 'Rui Wang'}, {'authorId': '144533942', 'name': 'Pengyu Cheng'}, {'authorId': '145153424', 'name': 'Ricardo Henao'}]","['Duke University', 'Tencent']","['China', 'United States']",2023-02
2302.13137,Shangding Gu,"Shangding Gu, Alap Kshirsagar, Yali Du, Guang Chen, Jan Peters, Alois
  Knoll",A Human-Centered Safe Robot Reinforcement Learning Framework with Interactive Behaviors,,,,,cs.RO cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deployment of reinforcement learning algorithms for robotics applications in the real world requires ensuring the safety of the robot and its environment. Safe robot reinforcement learning (SRRL) is a crucial step towards achieving human-robot coexistence. In this paper, we envision a human-centered SRRL framework consisting of three stages: safe exploration, safety value alignment, and safe collaboration. We examine the research gaps in these areas and propose to leverage interactive behaviors for SRRL. Interactive behaviors enable bi-directional information transfer between humans and robots, such as conversational robot ChatGPT. We argue that interactive behaviors need further attention from the SRRL community. We discuss four open challenges related to the robustness, efficiency, transparency, and adaptability of SRRL with interactive behaviors. ","[{'version': 'v1', 'created': 'Sat, 25 Feb 2023 18:29:32 GMT'}, {'version': 'v2', 'created': 'Thu, 2 Mar 2023 12:41:01 GMT'}]",2023-03-03,"[['Gu', 'Shangding', ''], ['Kshirsagar', 'Alap', ''], ['Du', 'Yali', ''], ['Chen', 'Guang', ''], ['Peters', 'Jan', ''], ['Knoll', 'Alois', '']]",1,1,2023-02-25,2,6,2,1,0,1,f173a8652eeda3fa3ede5cdf5a982ae9c9eff0b7,257219601.0,https://www.semanticscholar.org/paper/f173a8652eeda3fa3ede5cdf5a982ae9c9eff0b7,Frontiers in Neurorobotics,2023.0,87.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1381852002', 'name': 'Shangding Gu'}, {'authorId': '2145217956', 'name': 'Alap Kshirsagar'}, {'authorId': '1390662136', 'name': 'Yali Du'}, {'authorId': '143930563', 'name': 'Guang Chen'}, {'authorId': '47796324', 'name': 'Yaodong Yang'}, {'authorId': '145197867', 'name': 'Jan Peters'}, {'authorId': '2159266611', 'name': 'Alois Knoll'}]","['Technical University of Munich', ""King's College London"", 'Tongji University', 'Technical University of Darmstadt']","['Germany', 'China', 'United Kingdom']",2023-02
2302.13939,Rui-Jie Zhu,"Rui-Jie Zhu, Qihang Zhao, Guoqi Li, Jason K. Eshraghian",SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks,,,,,cs.CL cs.LG cs.NE,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language. We achieve this by modifying the transformer block to replace multi-head self attention to reduce quadratic computational complexity O(N^2) to linear complexity O(N) with increasing sequence length. Input tokens are instead streamed in sequentially to our attention mechanism (as with typical SNNs). Our preliminary experiments show that SpikeGPT remains competitive with non-spiking models on tested benchmarks, while maintaining 20x fewer operations when processed on neuromorphic hardware that can leverage sparse, event-driven activations. ","[{'version': 'v1', 'created': 'Mon, 27 Feb 2023 16:43:04 GMT'}, {'version': 'v2', 'created': 'Tue, 28 Feb 2023 06:28:43 GMT'}, {'version': 'v3', 'created': 'Mon, 26 Jun 2023 02:38:07 GMT'}, {'version': 'v4', 'created': 'Tue, 27 Jun 2023 02:55:23 GMT'}]",2023-06-28,"[['Zhu', 'Rui-Jie', ''], ['Zhao', 'Qihang', ''], ['Li', 'Guoqi', ''], ['Eshraghian', 'Jason K.', '']]",0,1,2023-02-27,4,4,3,0,0,0,9f52317ea9c5a6804b978987ff2a6557f98b5b2c,257219477.0,https://www.semanticscholar.org/paper/9f52317ea9c5a6804b978987ff2a6557f98b5b2c,arXiv.org,2023.0,61.0,23.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '144649570', 'name': 'Rui Zhu'}, {'authorId': '2110483969', 'name': 'Qihang Zhao'}, {'authorId': '3444950', 'name': 'J. Eshraghian'}]","['University of California, Santa Cruz', 'Chinese Academy of Sciences', 'Kuaishou Technology Co. Ltd']","['China', 'United States']",2023-02
2302.14003,Mehdi Fatemi,"Meng Cao and Mehdi Fatemi and Jackie Chi Kit Cheung and Samira
  Shabanian",Systematic Rectification of Language Models via Dead-end Analysis,"The Eleventh International Conference on Learning Representations,
  ICLR'23",ICLR 2023,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  With adversarial or otherwise normal prompts, existing large language models (LLM) can be pushed to generate toxic discourses. One way to reduce the risk of LLMs generating undesired discourses is to alter the training of the LLM. This can be very restrictive due to demanding computation requirements. Other methods rely on rule-based or prompt-based token elimination, which are limited as they dismiss future tokens and the overall meaning of the complete discourse. Here, we center detoxification on the probability that the finished discourse is ultimately considered toxic. That is, at each point, we advise against token selections proportional to how likely a finished text from this point will be toxic. To this end, we formally extend the dead-end theory from the recent reinforcement learning (RL) literature to also cover uncertain outcomes. Our approach, called rectification, utilizes a separate but significantly smaller model for detoxification, which can be applied to diverse LLMs as long as they share the same vocabulary. Importantly, our method does not require access to the internal representations of the LLM, but only the token probability distribution at each decoding step. This is crucial as many LLMs today are hosted in servers and only accessible through APIs. When applied to various LLMs, including GPT-3, our approach significantly improves the generated discourse compared to the base LLMs and other techniques in terms of both the overall language and detoxification performance. ","[{'version': 'v1', 'created': 'Mon, 27 Feb 2023 17:47:53 GMT'}]",2023-02-28,"[['Cao', 'Meng', ''], ['Fatemi', 'Mehdi', ''], ['Cheung', 'Jackie Chi Kit', ''], ['Shabanian', 'Samira', '']]",0,1,2023-02-27,1,4,3,1,0,1,da5fcb26c830663b79c9aa1c550ae62e7725fcad,257219472.0,https://www.semanticscholar.org/paper/da5fcb26c830663b79c9aa1c550ae62e7725fcad,International Conference on Learning Representations,2023.0,52.0,4.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2057072442', 'name': 'Mengyao Cao'}, {'authorId': '35189291', 'name': 'Mehdi Fatemi'}, {'authorId': '3159752', 'name': 'J. Cheung'}, {'authorId': '3197429', 'name': 'S. Shabanian'}]","['University of Milan', 'Microsoft']","['China', 'India', 'Italy']",2023-02
2302.14229,Jiaan Wang,"Jiaan Wang, Yunlong Liang, Fandong Meng, Beiqi Zou, Zhixu Li, Jianfeng
  Qu, Jie Zhou",Zero-Shot Cross-Lingual Summarization via Large Language Models,"Technical Report, 12 pages",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Given a document in a source language, cross-lingual summarization (CLS) aims to generate a summary in a different target language. Recently, the emergence of Large Language Models (LLMs), such as GPT-3.5, ChatGPT and GPT-4, has attracted wide attention from the computational linguistics community. However, it is not yet known the performance of LLMs on CLS. In this report, we empirically use various prompts to guide LLMs to perform zero-shot CLS from different paradigms (i.e., end-to-end and pipeline), and provide a preliminary evaluation on the generated summaries. We find that ChatGPT and GPT-4 originally prefer to produce lengthy summaries with detailed information. These two LLMs can further balance informativeness and conciseness with the help of an interactive prompt, significantly improving their CLS performance. Experimental results on three widely-used CLS datasets show that GPT-4 achieves state-of-the-art zero-shot CLS performance, and performs competitively compared with the fine-tuned mBART-50. Moreover, we also find some multi-lingual and bilingual LLMs (i.e., BLOOMZ, ChatGLM-6B, Vicuna-13B and ChatYuan) have limited zero-shot CLS ability. Due to the composite nature of CLS, which requires models to perform summarization and translation simultaneously, accomplishing this task in a zero-shot manner is even a challenge for LLMs. Therefore, we sincerely hope and recommend future LLM research could use CLS as a testbed. ","[{'version': 'v1', 'created': 'Tue, 28 Feb 2023 01:27:37 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Apr 2023 09:27:37 GMT'}, {'version': 'v3', 'created': 'Mon, 3 Jul 2023 14:11:38 GMT'}]",2023-07-04,"[['Wang', 'Jiaan', ''], ['Liang', 'Yunlong', ''], ['Meng', 'Fandong', ''], ['Zou', 'Beiqi', ''], ['Li', 'Zhixu', ''], ['Qu', 'Jianfeng', ''], ['Zhou', 'Jie', '']]",1,1,2023-02-28,3,7,2,6,3,3,90e0a949da276b66d13920a185a6e35042337518,257985556.0,https://www.semanticscholar.org/paper/90e0a949da276b66d13920a185a6e35042337518,,2023.0,69.0,3.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2118328782', 'name': 'Jiaan Wang'}, {'authorId': '3389712', 'name': 'Yunlong Liang'}, {'authorId': '33427918', 'name': 'Fandong Meng'}, {'authorId': '2151794700', 'name': 'Beiqi Zou'}, {'authorId': '115419489', 'name': 'Zhixu Li'}, {'authorId': '2069479032', 'name': 'Jianfeng Qu'}, {'authorId': '48128428', 'name': 'Jie Zhou'}]","['Princeton University', 'Beijing Jiaotong University', 'Tencent', 'Soochow University', 'Fudan University']","['China', 'United States']",2023-02
2302.14401,Daniel Zhang-Li,"Jing Zhang, Xiaokang Zhang, Daniel Zhang-Li, Jifan Yu, Zijun Yao,
  Zeyao Ma, Yiqi Xu, Haohua Wang, Xiaohan Zhang, Nianyi Lin, Sunrui Lu, Juanzi
  Li, Jie Tang",GLM-Dialog: Noise-tolerant Pre-training for Knowledge-grounded Dialogue Generation,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present GLM-Dialog, a large-scale language model (LLM) with 10B parameters capable of knowledge-grounded conversation in Chinese using a search engine to access the Internet knowledge. GLM-Dialog offers a series of applicable techniques for exploiting various external knowledge including both helpful and noisy knowledge, enabling the creation of robust knowledge-grounded dialogue LLMs with limited proper datasets. To evaluate the GLM-Dialog more fairly, we also propose a novel evaluation method to allow humans to converse with multiple deployed bots simultaneously and compare their performance implicitly instead of explicitly rating using multidimensional metrics.Comprehensive evaluations from automatic to human perspective demonstrate the advantages of GLM-Dialog comparing with existing open source Chinese dialogue models. We release both the model checkpoint and source code, and also deploy it as a WeChat application to interact with users. We offer our evaluation platform online in an effort to prompt the development of open source models and reliable dialogue evaluation systems. The additional easy-to-use toolkit that consists of short text entity linking, query generation, and helpful knowledge classification is also released to enable diverse applications. All the source code is available on Github. ","[{'version': 'v1', 'created': 'Tue, 28 Feb 2023 08:35:28 GMT'}]",2023-03-01,"[['Zhang', 'Jing', ''], ['Zhang', 'Xiaokang', ''], ['Zhang-Li', 'Daniel', ''], ['Yu', 'Jifan', ''], ['Yao', 'Zijun', ''], ['Ma', 'Zeyao', ''], ['Xu', 'Yiqi', ''], ['Wang', 'Haohua', ''], ['Zhang', 'Xiaohan', ''], ['Lin', 'Nianyi', ''], ['Lu', 'Sunrui', ''], ['Li', 'Juanzi', ''], ['Tang', 'Jie', '']]",0,0,2023-02-28,1,13,2,1,1,0,0a4441d155374e5853c738b346368e5da8193b79,257232712.0,https://www.semanticscholar.org/paper/0a4441d155374e5853c738b346368e5da8193b79,Knowledge Discovery and Data Mining,2023.0,41.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1519070643', 'name': 'Jing Zhang'}, {'authorId': '2108046717', 'name': 'Xiaokang Zhang'}, {'authorId': '2165225735', 'name': 'Daniel Zhang-li'}, {'authorId': '2116034394', 'name': 'Jifan Yu'}, {'authorId': '1423719712', 'name': 'Zijun Yao'}, {'authorId': '2221699934', 'name': 'Zeyao Ma'}, {'authorId': '6898202', 'name': 'Yixing Xu'}, {'authorId': '2167667666', 'name': 'Haohua Wang'}, {'authorId': '2181337399', 'name': 'Xiaohan Zhang'}, {'authorId': '2210119346', 'name': 'Nianyi Lin'}, {'authorId': '2210133801', 'name': 'Sunrui Lu'}, {'authorId': '2116822735', 'name': 'Juan Li'}, {'authorId': '2148911975', 'name': 'Jie Tang'}]","['https:', 'Tsinghua University', 'Renmin University of China', 'China University of Geosciences']",['China'],2023-02
2303.00293,Xuanting Chen,"Xuanting Chen, Junjie Ye, Can Zu, Nuo Xu, Rui Zheng, Minlong Peng, Jie
  Zhou, Tao Gui, Qi Zhang, Xuanjing Huang",How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The GPT-3.5 models have demonstrated impressive performance in various Natural Language Processing (NLP) tasks, showcasing their strong understanding and reasoning capabilities. However, their robustness and abilities to handle various complexities of the open world have yet to be explored, which is especially crucial in assessing the stability of models and is a key aspect of trustworthy AI. In this study, we perform a comprehensive experimental analysis of GPT-3.5, exploring its robustness using 21 datasets (about 116K test samples) with 66 text transformations from TextFlint that cover 9 popular Natural Language Understanding (NLU) tasks. Our findings indicate that while GPT-3.5 outperforms existing fine-tuned models on some tasks, it still encounters significant robustness degradation, such as its average performance dropping by up to 35.74\% and 43.59\% in natural language inference and sentiment analysis tasks, respectively. We also show that GPT-3.5 faces some specific robustness challenges, including robustness instability, prompt sensitivity, and number sensitivity. These insights are valuable for understanding its limitations and guiding future research in addressing these challenges to enhance GPT-3.5's overall performance and generalization abilities. ","[{'version': 'v1', 'created': 'Wed, 1 Mar 2023 07:39:01 GMT'}]",2023-03-02,"[['Chen', 'Xuanting', ''], ['Ye', 'Junjie', ''], ['Zu', 'Can', ''], ['Xu', 'Nuo', ''], ['Zheng', 'Rui', ''], ['Peng', 'Minlong', ''], ['Zhou', 'Jie', ''], ['Gui', 'Tao', ''], ['Zhang', 'Qi', ''], ['Huang', 'Xuanjing', '']]",0,1,2023-03-01,1,10,1,1,0,1,1f040c3a8d49f8e54169a0e07013692c7d58de4b,257255121.0,https://www.semanticscholar.org/paper/1f040c3a8d49f8e54169a0e07013692c7d58de4b,arXiv.org,2023.0,57.0,27.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2168731359', 'name': 'Xuanting Chen'}, {'authorId': '2143616977', 'name': 'Junjie Ye'}, {'authorId': '15190018', 'name': 'Can Zu'}, {'authorId': '2072805812', 'name': 'Nuo Xu'}, {'authorId': '2058585152', 'name': 'Rui Zheng'}, {'authorId': '24859244', 'name': 'Minlong Peng'}, {'authorId': '2119617269', 'name': 'Jie Zhou'}, {'authorId': '2067331064', 'name': 'Tao Gui'}, {'authorId': '47835189', 'name': 'Qi Zhang'}, {'authorId': '1790227', 'name': 'Xuanjing Huang'}]",['Fudan University'],['China'],2023-03
2303.00446,Yang Yuan,Yang Yuan,Succinct Representations for Concepts,,,,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Foundation models like chatGPT have demonstrated remarkable performance on various tasks. However, for many questions, they may produce false answers that look accurate. How do we train the model to precisely understand the concepts? In this paper, we introduce succinct representations of concepts based on category theory. Such representation yields concept-wise invariance properties under various tasks, resulting a new learning algorithm that can provably and accurately learn complex concepts or fix misconceptions. Moreover, by recursively expanding the succinct representations, one can generate a hierarchical decomposition, and manually verify the concept by individually examining each part inside the decomposition. ","[{'version': 'v1', 'created': 'Wed, 1 Mar 2023 12:11:23 GMT'}]",2023-03-02,"[['Yuan', 'Yang', '']]",1,1,2023-03-01,1,1,1,1,0,1,822f19d7e8f853e0cbf0feea06833c12a127bcda,257255286.0,https://www.semanticscholar.org/paper/822f19d7e8f853e0cbf0feea06833c12a127bcda,arXiv.org,2023.0,8.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2116945612', 'name': 'Yang Yuan'}]","['Shanghai Artificial Intelligence Laboratory', 'Tsinghua University', 'Shanghai Qi Zhi Institute']",['China'],2023-03
2303.02868,Xiaonan Nie,"Xiaonan Nie, Yi Liu, Fangcheng Fu, Jinbao Xue, Dian Jiao, Xupeng Miao,
  Yangyu Tao, Bin Cui",Angel-PTM: A Scalable and Economical Large-scale Pre-training System in Tencent,,,,,cs.LG cs.DC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent years have witnessed the unprecedented achievements of large-scale pre-trained models, especially the Transformer models. Many products and services in Tencent Inc., such as WeChat, QQ, and Tencent Advertisement, have been opted in to gain the power of pre-trained models. In this work, we present Angel-PTM, a productive deep learning system designed for pre-training and fine-tuning Transformer models. Angel-PTM can train extremely large-scale models with hierarchical memory efficiently. The key designs of Angel-PTM are the fine-grained memory management via the Page abstraction and a unified scheduling method that coordinate the computations, data movements, and communications. Furthermore, Angel-PTM supports extreme model scaling with SSD storage and implements the lock-free updating mechanism to address the SSD I/O bandwidth bottlenecks. Experimental results demonstrate that Angel-PTM outperforms existing systems by up to 114.8% in terms of maximum model scale as well as up to 88.9% in terms of training throughput. Additionally, experiments on GPT3-175B and T5-MoE-1.2T models utilizing hundreds of GPUs verify the strong scalability of Angel-PTM. ","[{'version': 'v1', 'created': 'Mon, 6 Mar 2023 03:36:26 GMT'}]",2023-03-07,"[['Nie', 'Xiaonan', ''], ['Liu', 'Yi', ''], ['Fu', 'Fangcheng', ''], ['Xue', 'Jinbao', ''], ['Jiao', 'Dian', ''], ['Miao', 'Xupeng', ''], ['Tao', 'Yangyu', ''], ['Cui', 'Bin', '']]",0,1,2023-03-06,1,8,2,2,1,1,d7e00702bbb5a0cccc97033f0405b634ae9e2d3c,257365347.0,https://www.semanticscholar.org/paper/d7e00702bbb5a0cccc97033f0405b634ae9e2d3c,Proceedings of the VLDB Endowment,2023.0,69.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2113588952', 'name': 'Xiaonan Nie'}, {'authorId': '2231085434', 'name': 'Yi Liu'}, {'authorId': '46182701', 'name': 'Fangcheng Fu'}, {'authorId': '2067731583', 'name': 'J. Xue'}, {'authorId': '2210796850', 'name': 'Dian Jiao'}, {'authorId': '1720763480', 'name': 'Xupeng Miao'}, {'authorId': '2105710451', 'name': 'Yangyu Tao'}, {'authorId': '2143385941', 'name': 'Bin Cui'}]","['Peking University', 'Carnegie Mellon University', 'Tencent']","['China', 'United States']",2023-03
2303.02913,Zhenyu Wu,"Zhenyu Wu, YaoXiang Wang, Jiacheng Ye, Jiangtao Feng, Jingjing Xu, Yu
  Qiao, Zhiyong Wu",OpenICL: An Open-Source Framework for In-context Learning,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In recent years, In-context Learning (ICL) has gained increasing attention and emerged as the new paradigm for large language model (LLM) evaluation. Unlike traditional fine-tuning methods, ICL instead adapts the pre-trained models to unseen tasks without any parameter updates. However, the implementation of ICL is sophisticated due to the diverse retrieval and inference methods involved, as well as the varying pre-processing requirements for different models, datasets, and tasks. A unified and flexible framework for ICL is urgently needed to ease the implementation of the aforementioned components. To facilitate ICL research, we introduce OpenICL, an open-source toolkit for ICL and LLM evaluation. OpenICL is research-friendly with a highly flexible architecture that users can easily combine different components to suit their needs. It also provides various state-of-the-art retrieval and inference methods to streamline the process of adapting ICL to cutting-edge research. The effectiveness of OpenICL has been validated on a wide range of NLP tasks, including classification, QA, machine translation, and semantic parsing. As a side-product, we found OpenICL to be an efficient yet robust tool for LLMs evaluation. OpenICL is released at https://github.com/Shark-NLP/OpenICL ","[{'version': 'v1', 'created': 'Mon, 6 Mar 2023 06:20:25 GMT'}]",2023-03-07,"[['Wu', 'Zhenyu', ''], ['Wang', 'YaoXiang', ''], ['Ye', 'Jiacheng', ''], ['Feng', 'Jiangtao', ''], ['Xu', 'Jingjing', ''], ['Qiao', 'Yu', ''], ['Wu', 'Zhiyong', '']]",0,0,2023-03-06,1,7,1,0,0,0,a55dfc1482c9fa32859d1e8e8c5813f5a22982cc,257365525.0,https://www.semanticscholar.org/paper/a55dfc1482c9fa32859d1e8e8c5813f5a22982cc,Annual Meeting of the Association for Computational Linguistics,2023.0,58.0,14.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2156372999', 'name': 'Zhenyu Wu'}, {'authorId': '2197373943', 'name': 'Yaoxiang Wang'}, {'authorId': '65846898', 'name': 'Jiacheng Ye'}, {'authorId': '2093485', 'name': 'Jiangtao Feng'}, {'authorId': '47883405', 'name': 'Jingjing Xu'}, {'authorId': '2151691771', 'name': 'Yu Qiao'}, {'authorId': '150358371', 'name': 'Zhiyong Wu'}]",['East China Normal University'],['China'],2023-03
2303.03032,Wei Li,"Wei Li, Linchao Zhu, Longyin Wen, Yi Yang",DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training,"Accepted by ICLR 2023. Code is available at
  https://github.com/dhg-wei/DeCap",,,,cs.CV cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large-scale pre-trained multi-modal models (e.g., CLIP) demonstrate strong zero-shot transfer capability in many discriminative tasks. Their adaptation to zero-shot image-conditioned text generation tasks has drawn increasing interest. Prior arts approach to zero-shot captioning by either utilizing the existing large language models (e.g., GPT-2) or pre-training the encoder-decoder network in an end-to-end manner. In this work, we propose a simple framework, named DeCap, for zero-shot captioning. We introduce a lightweight visual-aware language decoder. This decoder is both data-efficient and computation-efficient: 1) it only requires the text data for training, easing the burden on the collection of paired data. 2) it does not require end-to-end training. When trained with text-only data, the decoder takes the text embedding extracted from the off-the-shelf CLIP encoder as a prefix embedding. The challenge is that the decoder is trained on the text corpus but at the inference stage, it needs to generate captions based on visual inputs. The modality gap issue is widely observed in multi-modal contrastive models that prevents us from directly taking the visual embedding as the prefix embedding. We propose a training-free mechanism to reduce the modality gap. We project the visual embedding into the CLIP text embedding space, while the projected embedding retains the information of the visual input. Taking the projected embedding as the prefix embedding, the decoder generates high-quality descriptions that match the visual input. The experiments show that DeCap outperforms other zero-shot captioning methods and unpaired captioning methods on the typical image captioning benchmarks, i.e., MSCOCO and NoCaps. ","[{'version': 'v1', 'created': 'Mon, 6 Mar 2023 11:02:47 GMT'}]",2023-03-07,"[['Li', 'Wei', ''], ['Zhu', 'Linchao', ''], ['Wen', 'Longyin', ''], ['Yang', 'Yi', '']]",0,1,2023-03-06,1,4,3,1,1,0,a20e7e5e4338644d24145e71a9b89100af87e5d0,257365203.0,https://www.semanticscholar.org/paper/a20e7e5e4338644d24145e71a9b89100af87e5d0,International Conference on Learning Representations,2023.0,63.0,18.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '153021098', 'name': 'Wei Li'}, {'authorId': '2948393', 'name': 'Linchao Zhu'}, {'authorId': '39774417', 'name': 'Longyin Wen'}, {'authorId': '2143684866', 'name': 'Yi Yang'}]","['ByteDance', 'Zhejiang University']","['China', 'United States']",2023-03
2303.03751,Zhiwei Tang,"Zhiwei Tang, Dmitry Rybin, Tsung-Hui Chang",Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles,,,,,cs.LG cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  In this paper, we focus on a novel optimization problem in which the objective function is a black-box and can only be evaluated through a ranking oracle. This problem is common in real-world applications, particularly in cases where the function is assessed by human judges. Reinforcement Learning with Human Feedback (RLHF) is a prominent example of such an application, which is adopted by the recent works \cite{ouyang2022training,liu2023languages,chatgpt,bai2022training} to improve the quality of Large Language Models (LLMs) with human guidance. We propose ZO-RankSGD, a first-of-its-kind zeroth-order optimization algorithm, to solve this optimization problem with a theoretical guarantee. Specifically, our algorithm employs a new rank-based random estimator for the descent direction and is proven to converge to a stationary point. ZO-RankSGD can also be directly applied to the policy search problem in reinforcement learning when only a ranking oracle of the episode reward is available. This makes ZO-RankSGD a promising alternative to existing RLHF methods, as it optimizes in an online fashion and thus can work without any pre-collected data. Furthermore, we demonstrate the effectiveness of ZO-RankSGD in a novel application: improving the quality of images generated by a diffusion generative model with human ranking feedback. Throughout experiments, we found that ZO-RankSGD can significantly enhance the detail of generated images with only a few rounds of human feedback. Overall, our work advances the field of zeroth-order optimization by addressing the problem of optimizing functions with only ranking feedback, and offers an effective approach for aligning human and machine intentions in a wide range of domains. Our code is released here \url{https://github.com/TZW1998/Taming-Stable-Diffusion-with-Human-Ranking-Feedback}. ","[{'version': 'v1', 'created': 'Tue, 7 Mar 2023 09:20:43 GMT'}]",2023-03-08,"[['Tang', 'Zhiwei', ''], ['Rybin', 'Dmitry', ''], ['Chang', 'Tsung-Hui', '']]",1,1,2023-03-07,1,3,2,1,0,1,2044ab82dcb2c11ef660bd51d40130fe182f98d3,257378374.0,https://www.semanticscholar.org/paper/2044ab82dcb2c11ef660bd51d40130fe182f98d3,arXiv.org,2023.0,39.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2153431166', 'name': 'Zhiwei Tang'}, {'authorId': '2130575992', 'name': 'Dmitry Rybin'}, {'authorId': '2678812', 'name': 'Tsung-Hui Chang'}]","['Chinese University of Hong Kong, Shenzhen']",['China'],2023-03
2303.03836,Jun Gao,"Jun Gao, Huan Zhao, Changlong Yu, Ruifeng Xu",Exploring the Feasibility of ChatGPT for Event Extraction,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Event extraction is a fundamental task in natural language processing that involves identifying and extracting information about events mentioned in text. However, it is a challenging task due to the lack of annotated data, which is expensive and time-consuming to obtain. The emergence of large language models (LLMs) such as ChatGPT provides an opportunity to solve language tasks with simple prompts without the need for task-specific datasets and fine-tuning. While ChatGPT has demonstrated impressive results in tasks like machine translation, text summarization, and question answering, it presents challenges when used for complex tasks like event extraction. Unlike other tasks, event extraction requires the model to be provided with a complex set of instructions defining all event types and their schemas. To explore the feasibility of ChatGPT for event extraction and the challenges it poses, we conducted a series of experiments. Our results show that ChatGPT has, on average, only 51.04% of the performance of a task-specific model such as EEQA in long-tail and complex scenarios. Our usability testing experiments indicate that ChatGPT is not robust enough, and continuous refinement of the prompt does not lead to stable performance improvements, which can result in a poor user experience. Besides, ChatGPT is highly sensitive to different prompt styles. ","[{'version': 'v1', 'created': 'Tue, 7 Mar 2023 12:03:58 GMT'}, {'version': 'v2', 'created': 'Thu, 9 Mar 2023 17:33:31 GMT'}]",2023-03-10,"[['Gao', 'Jun', ''], ['Zhao', 'Huan', ''], ['Yu', 'Changlong', ''], ['Xu', 'Ruifeng', '']]",1,1,2023-03-07,2,4,1,1,0,1,5a8f34df779227a5cb1f573349556ef479d4359a,257378207.0,https://www.semanticscholar.org/paper/5a8f34df779227a5cb1f573349556ef479d4359a,arXiv.org,2023.0,11.0,34.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1741104', 'name': 'Jun Gao'}, {'authorId': '46430770', 'name': 'Huan Zhao'}, {'authorId': '49778308', 'name': 'Changlong Yu'}, {'authorId': '2115804042', 'name': 'Ruifeng Xu'}]","['Harbin Institute of Technology', 'Hong Kong University of Science and Technology']",['China'],2023-03
2303.03915,Paulo Villegas,"Hugo Lauren\c{c}on, Lucile Saulnier, Thomas Wang, Christopher Akiki,
  Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou,
  Eduardo Gonz\'alez Ponferrada, Huu Nguyen, J\""org Frohberg, Mario
  \v{S}a\v{s}ko, Quentin Lhoest, Angelina McMillan-Major, Gerard Dupont, Stella
  Biderman, Anna Rogers, Loubna Ben allal, Francesco De Toni, Giada Pistilli,
  Olivier Nguyen, Somaieh Nikpoor, Maraim Masoud, Pierre Colombo, Javier de la
  Rosa, Paulo Villegas, Tristan Thrush, Shayne Longpre, Sebastian Nagel, Leon
  Weber, Manuel Mu\~noz, Jian Zhu, Daniel Van Strien, Zaid Alyafeai, Khalid
  Almubarak, Minh Chien Vu, Itziar Gonzalez-Dios, Aitor Soroa, Kyle Lo, Manan
  Dey, Pedro Ortiz Suarez, Aaron Gokaslan, Shamik Bose, David Adelani, Long
  Phan, Hieu Tran, Ian Yu, Suhas Pai, Jenny Chim, Violette Lepercq, Suzana
  Ilic, Margaret Mitchell, Sasha Alexandra Luccioni, Yacine Jernite",The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset,"NeurIPS 2022, Datasets and Benchmarks Track",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings. The BigScience workshop, a 1-year international and multidisciplinary initiative, was formed with the goal of researching and training large language models as a values-driven undertaking, putting issues of ethics, harm, and governance in the foreground. This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM) language model. We further release a large initial subset of the corpus and analyses thereof, and hope to empower large-scale monolingual and multilingual modeling projects with both the data and the processing tools, as well as stimulate research around this large multilingual corpus. ","[{'version': 'v1', 'created': 'Tue, 7 Mar 2023 14:25:44 GMT'}]",2023-03-08,"[['Laurenon', 'Hugo', ''], ['Saulnier', 'Lucile', ''], ['Wang', 'Thomas', ''], ['Akiki', 'Christopher', ''], ['del Moral', 'Albert Villanova', ''], ['Scao', 'Teven Le', ''], ['Von Werra', 'Leandro', ''], ['Mou', 'Chenghao', ''], ['Ponferrada', 'Eduardo Gonzlez', ''], ['Nguyen', 'Huu', ''], ['Frohberg', 'Jrg', ''], ['ako', 'Mario', ''], ['Lhoest', 'Quentin', ''], ['McMillan-Major', 'Angelina', ''], ['Dupont', 'Gerard', ''], ['Biderman', 'Stella', ''], ['Rogers', 'Anna', ''], ['allal', 'Loubna Ben', ''], ['De Toni', 'Francesco', ''], ['Pistilli', 'Giada', ''], ['Nguyen', 'Olivier', ''], ['Nikpoor', 'Somaieh', ''], ['Masoud', 'Maraim', ''], ['Colombo', 'Pierre', ''], ['de la Rosa', 'Javier', ''], ['Villegas', 'Paulo', ''], ['Thrush', 'Tristan', ''], ['Longpre', 'Shayne', ''], ['Nagel', 'Sebastian', ''], ['Weber', 'Leon', ''], ['Muoz', 'Manuel', ''], ['Zhu', 'Jian', ''], ['Van Strien', 'Daniel', ''], ['Alyafeai', 'Zaid', ''], ['Almubarak', 'Khalid', ''], ['Vu', 'Minh Chien', ''], ['Gonzalez-Dios', 'Itziar', ''], ['Soroa', 'Aitor', ''], ['Lo', 'Kyle', ''], ['Dey', 'Manan', ''], ['Suarez', 'Pedro Ortiz', ''], ['Gokaslan', 'Aaron', ''], ['Bose', 'Shamik', ''], ['Adelani', 'David', ''], ['Phan', 'Long', ''], ['Tran', 'Hieu', ''], ['Yu', 'Ian', ''], ['Pai', 'Suhas', ''], ['Chim', 'Jenny', ''], ['Lepercq', 'Violette', ''], ['Ilic', 'Suzana', ''], ['Mitchell', 'Margaret', ''], ['Luccioni', 'Sasha Alexandra', ''], ['Jernite', 'Yacine', '']]",0,0,2023-03-07,1,54,2,1,1,0,16c64f74ce0e6a59b0709c0d8e66596a5bc08ed6,257378329.0,https://www.semanticscholar.org/paper/16c64f74ce0e6a59b0709c0d8e66596a5bc08ed6,Neural Information Processing Systems,2023.0,136.0,76.0,8.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2172404846', 'name': 'Hugo Laurenccon'}, {'authorId': '2113836860', 'name': 'Lucile Saulnier'}, {'authorId': '2135734748', 'name': 'Thomas Wang'}, {'authorId': '2003696840', 'name': 'Christopher Akiki'}, {'authorId': '46219923', 'name': 'Albert Villanova del Moral'}, {'authorId': '1379806208', 'name': 'Teven Le Scao'}, {'authorId': '51128119', 'name': 'Leandro von Werra'}, {'authorId': '35966970', 'name': 'Chenghao Mou'}, {'authorId': '79512668', 'name': 'E. G. Ponferrada'}, {'authorId': '2168170616', 'name': 'Huu Nguyen'}, {'authorId': '2146695800', 'name': 'Jorg Frohberg'}, {'authorId': '2125821515', 'name': 'Mario vSavsko'}, {'authorId': '2113836945', 'name': 'Quentin Lhoest'}, {'authorId': '1584940075', 'name': 'Angelina McMillan-Major'}, {'authorId': '13656138', 'name': 'Grard Dupont'}, {'authorId': '103476203', 'name': 'Stella Rose Biderman'}, {'authorId': '145046059', 'name': 'Anna Rogers'}, {'authorId': '2190281230', 'name': 'Loubna Ben Allal'}, {'authorId': '2067891070', 'name': 'F. Toni'}, {'authorId': '2158858559', 'name': 'Giada Pistilli'}, {'authorId': '2089233725', 'name': 'Olivier Nguyen'}, {'authorId': '2099315138', 'name': 'So-maieh Nikpoor'}, {'authorId': '153528116', 'name': 'Maraim Masoud'}, {'authorId': '46985469', 'name': 'Pierre Colombo'}, {'authorId': '144979591', 'name': 'Javier de la Rosa'}, {'authorId': '2176184659', 'name': 'Paulo Villegas'}, {'authorId': '1500242049', 'name': 'Tristan Thrush'}, {'authorId': '29909347', 'name': 'S. Longpre'}, {'authorId': '47351277', 'name': 'Sebastian Nagel'}, {'authorId': '20308468', 'name': 'Leon Weber'}, {'authorId': '2067766446', 'name': 'M. Muoz'}, {'authorId': '144549416', 'name': 'Jian Zhu'}, {'authorId': '71075073', 'name': 'Daniel Alexander van Strien'}, {'authorId': '25098419', 'name': 'Zaid Alyafeai'}, {'authorId': '90615055', 'name': 'Khalid Almubarak'}, {'authorId': '1484109150', 'name': 'Minh Chien Vu'}, {'authorId': '1404791152', 'name': 'Itziar Gonzalez-Dios'}, {'authorId': '2078619062', 'name': 'Aitor Soroa Etxabe'}, {'authorId': '46258841', 'name': 'Kyle Lo'}, {'authorId': '1879591269', 'name': 'Manan Dey'}, {'authorId': '147846651', 'name': 'Pedro Ortiz Suarez'}, {'authorId': '8278433', 'name': 'Aaron Gokaslan'}, {'authorId': '2795685', 'name': 'Shamik Bose'}, {'authorId': '2518906', 'name': 'David Ifeoluwa Adelani'}, {'authorId': '2066589762', 'name': 'Long Phan'}, {'authorId': '2057078797', 'name': 'H. Tran'}, {'authorId': '52141399', 'name': 'I. Yu'}, {'authorId': '2053516473', 'name': 'S. Pai'}, {'authorId': '2164872258', 'name': 'Jenny Chim'}, {'authorId': '2190280574', 'name': 'Violette Lepercq'}, {'authorId': '2066663276', 'name': 'Suzana Ilic'}, {'authorId': '49501003', 'name': 'Margaret Mitchell'}, {'authorId': '2165225321', 'name': 'Sasha Luccioni'}, {'authorId': '2262249', 'name': 'Yacine Jernite'}]","['King Fahd University of Petroleum and Minerals', 'Western University', 'Ferrum College', 'Humboldt-Universitt zu Berlin', 'University of Copenhagen', 'Queen Mary University of London', 'Puer University', 'University of MichiganAnn Arbor', 'University of Washington', 'Leipzig University']","['Germany', 'Saudi Arabia', 'Denmark', 'Canada', 'United States', 'United Kingdom', 'China']",2023-03
2303.04048,Jiaan Wang,"Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi,
  Zhixu Li, Jinan Xu, Jianfeng Qu, Jie Zhou",Is ChatGPT a Good NLG Evaluator? A Preliminary Study,"Technical Report, 11 pages",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, the emergence of ChatGPT has attracted wide attention from the computational linguistics community. Many prior studies have shown that ChatGPT achieves remarkable performance on various NLP tasks in terms of automatic evaluation metrics. However, the ability of ChatGPT to serve as an evaluation metric is still underexplored. Considering assessing the quality of natural language generation (NLG) models is an arduous task and NLG metrics notoriously show their poor correlation with human judgments, we wonder whether ChatGPT is a good NLG evaluation metric. In this report, we provide a preliminary meta-evaluation on ChatGPT to show its reliability as an NLG metric. In detail, we regard ChatGPT as a human evaluator and give task-specific (e.g., summarization) and aspect-specific (e.g., relevance) instruction to prompt ChatGPT to evaluate the generated results of NLG models. We conduct experiments on five NLG meta-evaluation datasets (including summarization, story generation and data-to-text tasks). Experimental results show that compared with previous automatic metrics, ChatGPT achieves state-of-the-art or competitive correlation with human judgments in most cases. In addition, we find that the effectiveness of the ChatGPT evaluator might be influenced by the creation method of the meta-evaluation datasets. For the meta-evaluation datasets which are created greatly depending on the reference and thus are biased, the ChatGPT evaluator might lose its effectiveness. We hope our preliminary study could prompt the emergence of a general-purposed reliable NLG metric. ","[{'version': 'v1', 'created': 'Tue, 7 Mar 2023 16:57:20 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Apr 2023 16:15:20 GMT'}]",2023-04-26,"[['Wang', 'Jiaan', ''], ['Liang', 'Yunlong', ''], ['Meng', 'Fandong', ''], ['Sun', 'Zengkui', ''], ['Shi', 'Haoxiang', ''], ['Li', 'Zhixu', ''], ['Xu', 'Jinan', ''], ['Qu', 'Jianfeng', ''], ['Zhou', 'Jie', '']]",1,1,2023-03-07,2,9,2,1,0,1,8221f1597000543432b7021ca79dbc51a7a63f9c,257378627.0,https://www.semanticscholar.org/paper/8221f1597000543432b7021ca79dbc51a7a63f9c,arXiv.org,2023.0,51.0,105.0,16.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2118328782', 'name': 'Jiaan Wang'}, {'authorId': '3389712', 'name': 'Yunlong Liang'}, {'authorId': '33427918', 'name': 'Fandong Meng'}, {'authorId': '89997416', 'name': 'Haoxiang Shi'}, {'authorId': '115419489', 'name': 'Zhixu Li'}, {'authorId': '2310092', 'name': 'Jinan Xu'}, {'authorId': '2069479032', 'name': 'Jianfeng Qu'}, {'authorId': '48128428', 'name': 'Jie Zhou'}]","['Waseda University', 'Beijing Jiaotong University', 'Tencent', 'Soochow University', 'Fudan University']","['China', 'Japan']",2023-03
2303.04671,Chenfei Wu,"Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang,
  Nan Duan","Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  ChatGPT is attracting a cross-field interest as it provides a language interface with remarkable conversational competency and reasoning capabilities across many domains. However, since ChatGPT is trained with languages, it is currently not capable of processing or generating images from the visual world. At the same time, Visual Foundation Models, such as Visual Transformers or Stable Diffusion, although showing great visual understanding and generation capabilities, they are only experts on specific tasks with one-round fixed inputs and outputs. To this end, We build a system called \textbf{Visual ChatGPT}, incorporating different Visual Foundation Models, to enable the user to interact with ChatGPT by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps. 3) providing feedback and asking for corrected results. We design a series of prompts to inject the visual model information into ChatGPT, considering models of multiple inputs/outputs and models that require visual feedback. Experiments show that Visual ChatGPT opens the door to investigating the visual roles of ChatGPT with the help of Visual Foundation Models. Our system is publicly available at \url{https://github.com/microsoft/visual-chatgpt}. ","[{'version': 'v1', 'created': 'Wed, 8 Mar 2023 15:50:02 GMT'}]",2023-03-09,"[['Wu', 'Chenfei', ''], ['Yin', 'Shengming', ''], ['Qi', 'Weizhen', ''], ['Wang', 'Xiaodong', ''], ['Tang', 'Zecheng', ''], ['Duan', 'Nan', '']]",1,1,2023-03-08,1,6,1,1,0,1,af997821231898a5f8d0fd78dad4eec526acabe5,257404891.0,https://www.semanticscholar.org/paper/af997821231898a5f8d0fd78dad4eec526acabe5,arXiv.org,2023.0,62.0,225.0,32.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Art', 'source': 's2-fos-model'}]","[{'authorId': '2151101534', 'name': 'Chenfei Wu'}, {'authorId': '2333305', 'name': 'Sheng-Kai Yin'}, {'authorId': '15629561', 'name': 'Weizhen Qi'}, {'authorId': '2211071131', 'name': 'Xiaodong Wang'}, {'authorId': '1576234850', 'name': 'Zecheng Tang'}, {'authorId': '2072609829', 'name': 'Nan Duan'}]",['Microsoft'],['China'],2023-03
2303.04998,Ning Liao,"Ning Liao, Bowen Shi, Min Cao, Xiaopeng Zhang, Qi Tian, Junchi Yan",Rethinking Visual Prompt Learning as Masked Visual Token Modeling,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Prompt learning has achieved great success in efficiently exploiting large-scale pre-trained models in natural language processing (NLP). It reformulates the downstream tasks as the generative pre-training ones, thus narrowing down the gap between them and improving the performance stably. However, when transferring it to the vision area, current visual prompt learning methods are all designed on discriminative pre-trained models, and there is also a lack of careful design to unify the forms of pre-training and downstream tasks. To explore prompt learning on the generative pre-trained visual model as well as keeping the task consistency, we propose Visual Prompt learning as masked visual Token Modeling (VPTM) to transform the downstream visual classification into the pre-trained masked visual token prediction. In addition, we develop the prototypical verbalizer for mapping the predicted visual token with implicit semantics to explicit downstream labels. To our best knowledge, VPTM is the first visual prompt method on the generative pre-trained visual model, and the first to achieve consistency between pre-training and downstream visual classification by task reformulation. Experiments show that VPTM outperforms other visual prompt methods and achieves excellent efficiency. Moreover, the task consistency of VPTM contributes to the robustness against prompt location, prompt length and prototype dimension, and could be deployed uniformly. ","[{'version': 'v1', 'created': 'Thu, 9 Mar 2023 02:43:10 GMT'}]",2023-03-10,"[['Liao', 'Ning', ''], ['Shi', 'Bowen', ''], ['Cao', 'Min', ''], ['Zhang', 'Xiaopeng', ''], ['Tian', 'Qi', ''], ['Yan', 'Junchi', '']]",0,1,2023-03-09,1,6,1,0,0,0,30dc93fea950a86cf8adab20a9b2034544fd41e8,257427299.0,https://www.semanticscholar.org/paper/30dc93fea950a86cf8adab20a9b2034544fd41e8,arXiv.org,2023.0,64.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2123643949', 'name': 'Ning Liao'}, {'authorId': '1633532182', 'name': 'Bowen Shi'}, {'authorId': '2182292060', 'name': 'Min Cao'}, {'authorId': '2108250420', 'name': 'Xiaopeng Zhang'}, {'authorId': '2106415186', 'name': 'Qi Tian'}, {'authorId': '3063894', 'name': 'Junchi Yan'}]","['Shanghai Jiao Tong University', 'Huawei Technologies (China)', 'Soochow University']",['China'],2023-03
2303.05063,Lei Wang,"Jiabang He, Lei Wang, Yi Hu, Ning Liu, Hui Liu, Xing Xu, and Heng Tao
  Shen",ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction,ICCV 2023. Code is available at https://github.com/MAEHCM/ICL-D3IE,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs), such as GPT-3 and ChatGPT, have demonstrated remarkable results in various natural language processing (NLP) tasks with in-context learning, which involves inference based on a few demonstration examples. Despite their successes in NLP tasks, no investigation has been conducted to assess the ability of LLMs to perform document information extraction (DIE) using in-context learning. Applying LLMs to DIE poses two challenges: the modality and task gap. To this end, we propose a simple but effective in-context learning framework called ICL-D3IE, which enables LLMs to perform DIE with different types of demonstration examples. Specifically, we extract the most difficult and distinct segments from hard training documents as hard demonstrations for benefiting all test instances. We design demonstrations describing relationships that enable LLMs to understand positional relationships. We introduce formatting demonstrations for easy answer extraction. Additionally, the framework improves diverse demonstrations by updating them iteratively. Our experiments on three widely used benchmark datasets demonstrate that the ICL-D3IE framework enables Davinci-003/ChatGPT to achieve superior performance when compared to previous pre-trained methods fine-tuned with full training in both the in-distribution (ID) setting and in the out-of-distribution (OOD) setting. Code is available at https://github.com/MAEHCM/ICL-D3IE. ","[{'version': 'v1', 'created': 'Thu, 9 Mar 2023 06:24:50 GMT'}, {'version': 'v2', 'created': 'Sun, 26 Mar 2023 11:56:34 GMT'}, {'version': 'v3', 'created': 'Fri, 14 Jul 2023 06:06:06 GMT'}, {'version': 'v4', 'created': 'Mon, 21 Aug 2023 03:57:18 GMT'}]",2023-08-22,"[['He', 'Jiabang', ''], ['Wang', 'Lei', ''], ['Hu', 'Yi', ''], ['Liu', 'Ning', ''], ['Liu', 'Hui', ''], ['Xu', 'Xing', ''], ['Shen', 'Heng Tao', '']]",1,1,2023-03-09,4,7,1,2,0,2,197022486b2e2584302bd9b6442e44d15bf3e351,257427632.0,https://www.semanticscholar.org/paper/197022486b2e2584302bd9b6442e44d15bf3e351,arXiv.org,2023.0,62.0,12.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2216205261', 'name': 'Jiabang He'}, {'authorId': '145131956', 'name': 'Lei Wang'}, {'authorId': '2149297924', 'name': 'Yingpeng Hu'}, {'authorId': '2100091244', 'name': 'Ning Liu'}, {'authorId': '2144318134', 'name': 'Hui-juan Liu'}, {'authorId': '2051281615', 'name': 'Xingdong Xu'}, {'authorId': '2148452614', 'name': 'Hengtao Shen'}]","['Beijing Forestry University', 'Center for Future Media & School of Computer Science and Engineering, University of Electronic Science and Technology of China', 'Beijing Rongda Technology Co., Ltd.', 'Singapore Management University']","['China', 'Singapore']",2023-03
2303.05771,Jie Zhu,"Jie Zhu, Lingwei Li, Li Yang, Xiaoxiao Ma, Chun Zuo",Automating Method Naming with Context-Aware Prompt-Tuning,Accepted by ICPC-2023,,,,cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Method names are crucial to program comprehension and maintenance. Recently, many approaches have been proposed to automatically recommend method names and detect inconsistent names. Despite promising, their results are still sub-optimal considering the three following drawbacks: 1) These models are mostly trained from scratch, learning two different objectives simultaneously. The misalignment between two objectives will negatively affect training efficiency and model performance. 2) The enclosing class context is not fully exploited, making it difficult to learn the abstract function of the method. 3) Current method name consistency checking methods follow a generate-then-compare process, which restricts the accuracy as they highly rely on the quality of generated names and face difficulty measuring the semantic consistency.   In this paper, we propose an approach named AUMENA to AUtomate MEthod NAming tasks with context-aware prompt-tuning. Unlike existing deep learning based approaches, our model first learns the contextualized representation(i.e., class attributes) of PL and NL through the pre-training model, then fully exploits the capacity and knowledge of large language model with prompt-tuning to precisely detect inconsistent method names and recommend more accurate names. To better identify semantically consistent names, we model the method name consistency checking task as a two-class classification problem, avoiding the limitation of previous similarity-based consistency checking approaches. The experimental results reflect that AUMENA scores 68.6%, 72.0%, 73.6%, 84.7% on four datasets of method name recommendation, surpassing the state-of-the-art baseline by 8.5%, 18.4%, 11.0%, 12.0%, respectively. And our approach scores 80.8% accuracy on method name consistency checking, reaching an 5.5% outperformance. All data and trained models are publicly available. ","[{'version': 'v1', 'created': 'Fri, 10 Mar 2023 08:14:13 GMT'}]",2023-03-13,"[['Zhu', 'Jie', ''], ['Li', 'Lingwei', ''], ['Yang', 'Li', ''], ['Ma', 'Xiaoxiao', ''], ['Zuo', 'Chun', '']]",0,0,2023-03-10,1,5,1,0,0,0,46cc69aed9bd8138ec9e2128671e6516bbb55b39,257482398.0,https://www.semanticscholar.org/paper/46cc69aed9bd8138ec9e2128671e6516bbb55b39,IEEE International Conference on Program Comprehension,2023.0,44.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2155335913', 'name': 'Jie Zhu'}, {'authorId': '2145687795', 'name': 'Ling-jie Li'}, {'authorId': '2153204688', 'name': 'Li Yang'}, {'authorId': '2115722180', 'name': 'Xiaoxiao Ma'}, {'authorId': '31841234', 'name': 'Chun Zuo'}]","['University of Chinese Academy of Sciences', 'Sinosoft Company Limited Beijing, China', 'Chinese Academy of Sciences']",['China'],2023-03
2303.05983,Zhiwei Zhang,"Zhiwei Zhang, Yuliang Liu",Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation,35 pages,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by/4.0/,"  The recent success of ChatGPT and GPT-4 has drawn widespread attention to multimodal dialogue systems. However, the academia community lacks a dataset that can validate the multimodal generation capabilities of Visual Language Models (VLMs) in textual-visual chat tasks. In this paper, we construct two new multimodal datasets: the synthetic CLEVR-ATVC dataset (620K) and the manually pictured Fruit-ATVC dataset (50K), both featuring visual and text-based inputs and outputs. Additionally, to enable the multimodal system to reject human requests (i.e., demonstrate accountability), as in language-based ChatGPT conversations, we develop and incorporate specific rules into the datasets as supervisory signals. This allows the trained VLM to provide a yes or no answer after visual and textual reasoning, accompanied by a language explanation as to why the human instruction cannot be excuted. In our method, we propose a two-state training procedure to train the image auto-encoder and auto-regressive transformer from scratch. The first state involves a discrete variational autoencoder (dVAE) to compress each image into short tokens, which are then concatenated with text tokens as a single data stream to be fed into the decoder-based transformer for generating visual re-creation and textual feedback in the second state. We provide comprehensive analyses of experimental results in terms of re-created image quality, answer accuracy, and the model behavior when faced with uncertainty and imperfect user queries. We hope our explorations and findings contribute valuable insights regarding the accountability of textual-visual generative models. ","[{'version': 'v1', 'created': 'Fri, 10 Mar 2023 15:35:11 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Jun 2023 16:03:55 GMT'}]",2023-06-16,"[['Zhang', 'Zhiwei', ''], ['Liu', 'Yuliang', '']]",1,1,2023-03-10,2,2,2,2,0,2,53df959bcf6499c45e316086a96a624389a39a52,259164940.0,https://www.semanticscholar.org/paper/53df959bcf6499c45e316086a96a624389a39a52,,2023.0,122.0,0.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '103010593', 'name': 'Zhiwei Zhang'}, {'authorId': '2108353180', 'name': 'Yuliang Liu'}]","['HyperEvol Lab', 'Huazhong University of Science and Technology', 'Chinese University of Hong Kong', 'Centre for Perceptual and Interactive Intelligence']",['China'],2023-03
2303.06338,Zhen Wang,"Zhen Wang, Jun Xiao, Yueting Zhuang, Fei Gao, Jian Shao, Long Chen",Learning Combinatorial Prompts for Universal Controllable Image Captioning,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Controllable Image Captioning (CIC) -- generating natural language descriptions about images under the guidance of given control signals -- is one of the most promising directions towards next-generation captioning systems. Till now, various kinds of control signals for CIC have been proposed, ranging from content-related control to structure-related control. However, due to the format and target gaps of different control signals, all existing CIC works (or architectures) only focus on one certain control signal, and overlook the human-like combinatorial ability. By ``combinatorial"", we mean that our humans can easily meet multiple needs (or constraints) simultaneously when generating descriptions. To this end, we propose a novel prompt-based framework for CIC by learning Combinatorial Prompts, dubbed as ComPro. Specifically, we directly utilize a pretrained language model GPT-2 as our language model, which can help to bridge the gap between different signal-specific CIC architectures. Then, we reformulate the CIC as a prompt-guide sentence generation problem, and propose a new lightweight prompt generation network to generate the combinatorial prompts for different kinds of control signals. For different control signals, we further design a new mask attention mechanism to realize the prompt-based CIC. Due to its simplicity, our ComPro can be further extended to more kinds of combined control signals by concatenating these prompts. Extensive experiments on two prevalent CIC benchmarks have verified the effectiveness and efficiency of our ComPro on both single and combined control signals. ","[{'version': 'v1', 'created': 'Sat, 11 Mar 2023 07:53:15 GMT'}, {'version': 'v2', 'created': 'Sun, 21 May 2023 05:57:16 GMT'}, {'version': 'v3', 'created': 'Wed, 2 Aug 2023 12:56:53 GMT'}]",2023-08-03,"[['Wang', 'Zhen', ''], ['Xiao', 'Jun', ''], ['Zhuang', 'Yueting', ''], ['Gao', 'Fei', ''], ['Shao', 'Jian', ''], ['Chen', 'Long', '']]",0,1,2023-03-11,3,6,1,1,1,0,dcff56b6412911bda0b4a4dbd98cfd2fa0a400bc,257496768.0,https://www.semanticscholar.org/paper/dcff56b6412911bda0b4a4dbd98cfd2fa0a400bc,arXiv.org,2023.0,62.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '48708405', 'name': 'Zhen Wang'}, {'authorId': '2143491421', 'name': 'Jun Xiao'}, {'authorId': '2146071116', 'name': 'Lei Chen'}, {'authorId': '144516510', 'name': 'Fei Gao'}, {'authorId': '2177137677', 'name': 'Jian Shao'}, {'authorId': '2145116668', 'name': 'Long Chen'}]","['Hong Kong University of Science and Technology', 'Zhejiang University of Technology', 'Zhejiang University']",['China'],2023-03
2303.06573,Kelong Mao,"Kelong Mao, Zhicheng Dou, Haonan Chen, Fengran Mo, Hongjin Qian",Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search,Work in progress,,,,cs.IR,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we present a prompting framework called LLMCS that leverages large language models, such as code-davinci-002 of GPT-3, to perform few-shot conversational query rewriting for conversational search. We explore three prompting methods to generate multiple query rewrites and hypothetical responses, and propose aggregating them into an integrated representation that can robustly represent the user's real contextual search intent. Experimental results on two conversational search datasets, including CAst-19 and CAsT-20, show that our approach achieves significant improvements in search effectiveness over existing baselines and manual rewrites. Notably, LLMCS can significantly outperform the state-of-the-art baselines by up to +5.9\% and +32.9\% w.r.t. NDCG@3 on CAsT-19 and CAsT-20, highlighting the vast potential of large language models for conversational search. Our code will be released at https://github.com/kyriemao/LLMCS. ","[{'version': 'v1', 'created': 'Sun, 12 Mar 2023 05:08:16 GMT'}]",2023-03-14,"[['Mao', 'Kelong', ''], ['Dou', 'Zhicheng', ''], ['Chen', 'Haonan', ''], ['Mo', 'Fengran', ''], ['Qian', 'Hongjin', '']]",0,1,2023-03-12,1,5,1,1,0,1,e4e744cc96da7987a072571fc3817f040d456566,257495903.0,https://www.semanticscholar.org/paper/e4e744cc96da7987a072571fc3817f040d456566,arXiv.org,2023.0,49.0,11.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1580228663', 'name': 'Kelong Mao'}, {'authorId': '1897235', 'name': 'Zhicheng Dou'}, {'authorId': '2035571489', 'name': 'Haonan Chen'}, {'authorId': '2007643794', 'name': 'Fengran Mo'}, {'authorId': '1972030827', 'name': 'Hongjin Qian'}]","[""Ministry of Education of the People's Republic of China"", 'Institute of Computing Technology', 'Universit de Montral', 'Renmin University of China']","['Canada', 'China']",2023-03
2303.07263,Alexey Svyatkovskiy,"Matthew Jin, Syed Shahriar, Michele Tufano, Xin Shi, Shuai Lu, Neel
  Sundaresan, Alexey Svyatkovskiy",InferFix: End-to-End Program Repair with LLMs,,,,,cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Software development life cycle is profoundly influenced by bugs: their introduction, identification, and eventual resolution account for a significant portion of software cost. This has motivated software engineering researchers and practitioners to propose different approaches for automating the identification and repair of software defects. Large language models have been adapted to the program repair task through few-shot demonstration learning and instruction prompting, treating this as an infilling task. However, these models have only focused on learning general bug-fixing patterns for uncategorized bugs mined from public repositories. In this paper, we propose InferFix: a transformer-based program repair framework paired with a state-of-the-art static analyzer to fix critical security and performance bugs. InferFix combines a Retriever -- transformer encoder model pretrained via contrastive learning objective, which aims at searching for semantically equivalent bugs and corresponding fixes; and a Generator -- a large language model (Codex Cushman) finetuned on supervised bug-fix data with prompts augmented via bug type annotations and semantically similar fixes retrieved from an external non-parametric memory. To train and evaluate our approach, we curated InferredBugs, a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories. Our evaluation demonstrates that InferFix outperforms strong LLM baselines, with a top-1 accuracy of 65.6% for generating fixes in C# and 76.8% in Java. We discuss the deployment of InferFix alongside Infer at Microsoft which offers an end-to-end solution for detection, classification, and localization of bugs, as well as fixing and validation of candidate patches, integrated in the continuous integration pipeline to automate the software development workflow. ","[{'version': 'v1', 'created': 'Mon, 13 Mar 2023 16:42:47 GMT'}]",2023-03-14,"[['Jin', 'Matthew', ''], ['Shahriar', 'Syed', ''], ['Tufano', 'Michele', ''], ['Shi', 'Xin', ''], ['Lu', 'Shuai', ''], ['Sundaresan', 'Neel', ''], ['Svyatkovskiy', 'Alexey', '']]",0,0,2023-03-13,1,7,1,1,0,1,34d24b2d9f116f8f652c112d4ac924afcf11bd0d,257495997.0,https://www.semanticscholar.org/paper/34d24b2d9f116f8f652c112d4ac924afcf11bd0d,arXiv.org,2023.0,34.0,13.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2072904201', 'name': 'Ma Jin'}, {'authorId': '2211425830', 'name': 'Syed Shahriar'}, {'authorId': '40626221', 'name': 'Michele Tufano'}, {'authorId': '2159544304', 'name': 'Xin Shi'}, {'authorId': '2115338656', 'name': 'Shuai Lu'}, {'authorId': '145507437', 'name': 'Neel Sundaresan'}, {'authorId': '2061625488', 'name': 'Alexey Svyatkovskiy'}]","['University of California, Los Angeles', 'Microsoft Research Beijing, China Neel Sundaresan Microsoft Redmond, WA, USA', ""Conference'17, July 2017, Washington, DC, USA"", 'Microsoft']","['China', 'United States']",2023-03
2303.08014,Zhenguang Cai,"Zhenguang G. Cai, David A. Haslett, Xufeng Duan, Shuqi Wang, Martin J.
  Pickering",Does ChatGPT resemble humans in language use?,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) and LLM-driven chatbots such as ChatGPT have shown remarkable capacities in comprehending and producing language. However, their internal workings remain a black box in cognitive terms, and it is unclear whether LLMs and chatbots can develop humanlike characteristics in language use. Cognitive scientists have devised many experiments that probe, and have made great progress in explaining, how people process language. We subjected ChatGPT to 12 of these experiments, pre-registered and with 1,000 runs per experiment. In 10 of them, ChatGPT replicated the human pattern of language use. It associated unfamiliar words with different meanings depending on their forms, continued to access recently encountered meanings of ambiguous words, reused recent sentence structures, reinterpreted implausible sentences that were likely to have been corrupted by noise, glossed over errors, drew reasonable inferences, associated causality with different discourse entities according to verb semantics, and accessed different meanings and retrieved different words depending on the identity of its interlocutor. However, unlike humans, it did not prefer using shorter words to convey less informative content and it did not use context to disambiguate syntactic ambiguities. We discuss how these convergences and divergences may occur in the transformer architecture. Overall, these experiments demonstrate that LLM-driven chatbots like ChatGPT are capable of mimicking human language processing to a great extent, and that they have the potential to provide insights into how people learn and use language. ","[{'version': 'v1', 'created': 'Fri, 10 Mar 2023 10:47:59 GMT'}]",2023-03-15,"[['Cai', 'Zhenguang G.', ''], ['Haslett', 'David A.', ''], ['Duan', 'Xufeng', ''], ['Wang', 'Shuqi', ''], ['Pickering', 'Martin J.', '']]",1,1,2023-03-10,1,5,1,1,0,1,4a5826475b3705615c1b18c8310387b1de5020f3,257505538.0,https://www.semanticscholar.org/paper/4a5826475b3705615c1b18c8310387b1de5020f3,arXiv.org,2023.0,63.0,13.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Psychology', 'source': 's2-fos-model'}]","[{'authorId': '3374857', 'name': 'Zhenguang G Cai'}, {'authorId': '2146210312', 'name': 'David A Haslett'}, {'authorId': '1996176827', 'name': 'Xufeng Duan'}, {'authorId': '2211477584', 'name': 'Shuqi Wang'}, {'authorId': '2425749', 'name': 'M. Pickering'}]","['University of Edinburgh', 'Chinese University of Hong Kong']","['China', 'United Kingdom']",2023-03
2303.08601,Hengran Zhang,"Yequan Wang, Hengran Zhang, Aixin Sun, Xuying Meng",GCRE-GPT: A Generative Model for Comparative Relation Extraction,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Given comparative text, comparative relation extraction aims to extract two targets (\eg two cameras) in comparison and the aspect they are compared for (\eg image quality). The extracted comparative relations form the basis of further opinion analysis.Existing solutions formulate this task as a sequence labeling task, to extract targets and aspects. However, they cannot directly extract comparative relation(s) from text. In this paper, we show that comparative relations can be directly extracted with high accuracy, by generative model. Based on GPT-2, we propose a Generation-based Comparative Relation Extractor (GCRE-GPT). Experiment results show that \modelname achieves state-of-the-art accuracy on two datasets. ","[{'version': 'v1', 'created': 'Wed, 15 Mar 2023 13:15:22 GMT'}]",2023-03-16,"[['Wang', 'Yequan', ''], ['Zhang', 'Hengran', ''], ['Sun', 'Aixin', ''], ['Meng', 'Xuying', '']]",0,1,2023-03-15,1,4,1,1,1,0,c596fdfb92c086b00807a3b0848a27b4796b3e06,257532399.0,https://www.semanticscholar.org/paper/c596fdfb92c086b00807a3b0848a27b4796b3e06,arXiv.org,2023.0,14.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2119256294', 'name': 'Yequan Wang'}, {'authorId': '2204780059', 'name': 'Hengran Zhang'}, {'authorId': '1735962', 'name': 'Aixin Sun'}, {'authorId': '3134123', 'name': 'Xuying Meng'}]","['Beijing Academy of Artificial Intelligence', 'University of Chinese Academy of Sciences', 'Chinese Academy of Sciences', 'Nanyang Technological University']","['China', 'Singapore']",2023-03
2303.09136,Zhongxiang Sun,Zhongxiang Sun,A Short Survey of Viewing Large Language Models in Legal Aspect,8 pages,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) have transformed many fields, including natural language processing, computer vision, and reinforcement learning. These models have also made a significant impact in the field of law, where they are being increasingly utilized to automate various legal tasks, such as legal judgement prediction, legal document analysis, and legal document writing. However, the integration of LLMs into the legal field has also raised several legal problems, including privacy concerns, bias, and explainability. In this survey, we explore the integration of LLMs into the field of law. We discuss the various applications of LLMs in legal tasks, examine the legal challenges that arise from their use, and explore the data resources that can be used to specialize LLMs in the legal domain. Finally, we discuss several promising directions and conclude this paper. By doing so, we hope to provide an overview of the current state of LLMs in law and highlight the potential benefits and challenges of their integration. ","[{'version': 'v1', 'created': 'Thu, 16 Mar 2023 08:01:22 GMT'}]",2023-03-17,"[['Sun', 'Zhongxiang', '']]",0,0,2023-03-16,1,1,1,0,0,0,9fe9af7cf3d54b707a7be3c53ce94b77dcc3bae5,257557504.0,https://www.semanticscholar.org/paper/9fe9af7cf3d54b707a7be3c53ce94b77dcc3bae5,arXiv.org,2023.0,25.0,17.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Law', 'source': 's2-fos-model'}]","[{'authorId': '2109820280', 'name': 'ZhongXiang Sun'}]",['Renmin University of China'],['China'],2023-03
2303.09184,Gaochen Dong,"Gaochen Dong, Wei Chen",Block-wise Bit-Compression of Transformer-based Models,Need to add figures and adjust languages to improve readability,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the popularity of the recent Transformer-based models represented by BERT, GPT-3 and ChatGPT, there has been state-of-the-art performance in a range of natural language processing tasks. However, the massive computations, huge memory footprint, and thus high latency of Transformer-based models is an inevitable challenge for the cloud with high real-time requirement. To tackle the issue, we propose BBCT, a method of block-wise bit-compression for transformer without retraining. Our method achieves more fine-grained compression of the whole transformer, including embedding, matrix multiplication, GELU, softmax, layer normalization, and all the intermediate results. As a case, we compress an efficient BERT with the method of BBCT. Our benchmark test results on General Language Understanding Evaluation (GLUE) show that BBCT can achieve less than 1% accuracy drop in most tasks. ","[{'version': 'v1', 'created': 'Thu, 16 Mar 2023 09:53:57 GMT'}, {'version': 'v2', 'created': 'Sat, 1 Apr 2023 12:50:29 GMT'}]",2023-04-04,"[['Dong', 'Gaochen', ''], ['Chen', 'Wei', '']]",1,1,2023-03-16,2,2,2,2,0,2,9891d1b67c92f283aa283c27e87d0676c068a818,257557845.0,https://www.semanticscholar.org/paper/9891d1b67c92f283aa283c27e87d0676c068a818,arXiv.org,2023.0,24.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2211733371', 'name': 'Gaochen Dong'}, {'authorId': '2154939788', 'name': 'W. Chen'}]",['China Iron and Steel Research Institute Group'],['China'],2023-03
2303.09618,Shu Zhang,"Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu,
  Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong and Ran
  Xu",HIVE: Harnessing Human Feedback for Instructional Visual Editing,,,,,cs.CV cs.AI cs.CL cs.HC cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Incorporating human feedback has been shown to be crucial to align text generated by large language models to human preferences. We hypothesize that state-of-the-art instructional image editing models, where outputs are generated based on an input image and an editing instruction, could similarly benefit from human feedback, as their outputs may not adhere to the correct instructions and preferences of users. In this paper, we present a novel framework to harness human feedback for instructional visual editing (HIVE). Specifically, we collect human feedback on the edited images and learn a reward function to capture the underlying user preferences. We then introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward. Besides, to mitigate the bias brought by the limitation of data, we contribute a new 1M training dataset, a 3.6K reward dataset for rewards learning, and a 1K evaluation dataset to boost the performance of instructional image editing. We conduct extensive empirical experiments quantitatively and qualitatively, showing that HIVE is favored over previous state-of-the-art instructional image editing approaches by a large margin. ","[{'version': 'v1', 'created': 'Thu, 16 Mar 2023 19:47:41 GMT'}]",2023-03-20,"[['Zhang', 'Shu', ''], ['Yang', 'Xinyi', ''], ['Feng', 'Yihao', ''], ['Qin', 'Can', ''], ['Chen', 'Chia-Chih', ''], ['Yu', 'Ning', ''], ['Chen', 'Zeyuan', ''], ['Wang', 'Huan', ''], ['Savarese', 'Silvio', ''], ['Ermon', 'Stefano', ''], ['Xiong', 'Caiming', ''], ['Xu', 'Ran', '']]",0,0,2023-03-16,1,12,5,0,0,0,372bc41602bbd21f192305775f0a58de9880e454,257622925.0,https://www.semanticscholar.org/paper/372bc41602bbd21f192305775f0a58de9880e454,arXiv.org,2023.0,58.0,13.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108088736', 'name': 'Shu Zhang'}, {'authorId': '2150441485', 'name': 'Xinyi Yang'}, {'authorId': '22758695', 'name': 'Yihao Feng'}, {'authorId': '12282768', 'name': 'Can Qin'}, {'authorId': '2211960332', 'name': 'Chia-Chih Chen'}, {'authorId': '2211974297', 'name': 'Ning Yu'}, {'authorId': '5478513', 'name': 'Zeyuan Chen'}, {'authorId': '46507194', 'name': 'Haiquan Wang'}, {'authorId': '1702137', 'name': 'S. Savarese'}, {'authorId': '2490652', 'name': 'Stefano Ermon'}, {'authorId': '2054594326', 'name': 'Caiming Xiong'}, {'authorId': '2115800155', 'name': 'Ran Xu'}]","['Stanford University', 'Salesforce AI Research,', 'Northeastern University']","['China', 'United States']",2023-03
2303.09769,Weilai Xiang,"Weilai Xiang, Hongyu Yang, Di Huang, Yunhong Wang",Denoising Diffusion Autoencoders are Unified Self-supervised Learners,ICCV 2023 Oral,,,,cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Inspired by recent advances in diffusion models, which are reminiscent of denoising autoencoders, we investigate whether they can acquire discriminative representations for classification via generative pre-training. This paper shows that the networks in diffusion models, namely denoising diffusion autoencoders (DDAE), are unified self-supervised learners: by pre-training on unconditional image generation, DDAE has already learned strongly linear-separable representations within its intermediate layers without auxiliary encoders, thus making diffusion pre-training emerge as a general approach for generative-and-discriminative dual learning. To validate this, we conduct linear probe and fine-tuning evaluations. Our diffusion-based approach achieves 95.9% and 50.0% linear evaluation accuracies on CIFAR-10 and Tiny-ImageNet, respectively, and is comparable to contrastive learning and masked autoencoders for the first time. Transfer learning from ImageNet also confirms the suitability of DDAE for Vision Transformers, suggesting the potential to scale DDAEs as unified foundation models. Code is available at github.com/FutureXiang/ddae. ","[{'version': 'v1', 'created': 'Fri, 17 Mar 2023 04:20:47 GMT'}, {'version': 'v2', 'created': 'Sat, 19 Aug 2023 11:12:29 GMT'}]",2023-08-22,"[['Xiang', 'Weilai', ''], ['Yang', 'Hongyu', ''], ['Huang', 'Di', ''], ['Wang', 'Yunhong', '']]",0,1,2023-03-17,2,4,2,0,0,0,df4b6713abfe226d06099d7749f8b47903ac087b,257623083.0,https://www.semanticscholar.org/paper/df4b6713abfe226d06099d7749f8b47903ac087b,arXiv.org,2023.0,74.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '3449051', 'name': 'Weilai Xiang'}, {'authorId': '2118616566', 'name': 'Hongyu Yang'}, {'authorId': '40119164', 'name': 'Di Huang'}, {'authorId': '2108702972', 'name': 'Yunhong Wang'}]",['Beihang University'],['China'],2023-03
2303.09813,Chaofan Ma,"Chaofan Ma, Yuhuan Yang, Chen Ju, Fei Zhang, Jinxiang Liu, Yu Wang, Ya
  Zhang, Yanfeng Wang",DiffusionSeg: Adapting Diffusion Towards Unsupervised Object Discovery,,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Learning from a large corpus of data, pre-trained models have achieved impressive progress nowadays. As popular generative pre-training, diffusion models capture both low-level visual knowledge and high-level semantic relations. In this paper, we propose to exploit such knowledgeable diffusion models for mainstream discriminative tasks, i.e., unsupervised object discovery: saliency segmentation and object localization. However, the challenges exist as there is one structural difference between generative and discriminative models, which limits the direct use. Besides, the lack of explicitly labeled data significantly limits performance in unsupervised settings. To tackle these issues, we introduce DiffusionSeg, one novel synthesis-exploitation framework containing two-stage strategies. To alleviate data insufficiency, we synthesize abundant images, and propose a novel training-free AttentionCut to obtain masks in the first synthesis stage. In the second exploitation stage, to bridge the structural gap, we use the inversion technique, to map the given image back to diffusion features. These features can be directly used by downstream architectures. Extensive experiments and ablation studies demonstrate the superiority of adapting diffusion for unsupervised object discovery. ","[{'version': 'v1', 'created': 'Fri, 17 Mar 2023 07:47:55 GMT'}]",2023-03-20,"[['Ma', 'Chaofan', ''], ['Yang', 'Yuhuan', ''], ['Ju', 'Chen', ''], ['Zhang', 'Fei', ''], ['Liu', 'Jinxiang', ''], ['Wang', 'Yu', ''], ['Zhang', 'Ya', ''], ['Wang', 'Yanfeng', '']]",0,1,2023-03-17,1,8,2,0,0,0,aef10182026764b376256ece8e4381a34f808980,257622897.0,https://www.semanticscholar.org/paper/aef10182026764b376256ece8e4381a34f808980,arXiv.org,2023.0,86.0,10.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2124034665', 'name': 'Chao Ma'}, {'authorId': '2108773876', 'name': 'Yu-Hao Yang'}, {'authorId': '2059175825', 'name': 'Chen Ju'}, {'authorId': '2211893297', 'name': 'Feifan Zhang'}, {'authorId': '2047214830', 'name': 'Jinxian Liu'}, {'authorId': '39583918', 'name': 'Yu Wang'}, {'authorId': '46868037', 'name': 'Ya Zhang'}, {'authorId': '2108846176', 'name': 'Yanfeng Wang'}]","['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University']",['China'],2023-03
2303.10089,Weiyi Zhang,"Weiyi Zhang, Yushi Guo, Liting Niu, Peijun Li, Chun Zhang, Zeyu Wan,
  Jiaxiang Yan, Fasih Ud Din Farrukh, Debing Zhang",LP-SLAM: Language-Perceptive RGB-D SLAM system based on Large Language Model,"12 pages, 16 figures",,,,cs.RO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Simultaneous localization and mapping (SLAM) is a critical technology that enables autonomous robots to be aware of their surrounding environment. With the development of deep learning, SLAM systems can achieve a higher level of perception of the environment, including the semantic and text levels. However, current works are limited in their ability to achieve a natural-language level of perception of the world. To address this limitation, we propose LP-SLAM, the first language-perceptive SLAM system that leverages large language models (LLMs). LP-SLAM has two major features: (a) it can detect text in the scene and determine whether it represents a landmark to be stored during the tracking and mapping phase, and (b) it can understand natural language input from humans and provide guidance based on the generated map. We illustrated three usages of the LLM in the system including text cluster, landmark judgment, and natural language navigation. Our proposed system represents an advancement in the field of LLMs based SLAM and opens up new possibilities for autonomous robots to interact with their environment in a more natural and intuitive way. ","[{'version': 'v1', 'created': 'Fri, 17 Mar 2023 16:12:22 GMT'}]",2023-03-20,"[['Zhang', 'Weiyi', ''], ['Guo', 'Yushi', ''], ['Niu', 'Liting', ''], ['Li', 'Peijun', ''], ['Zhang', 'Chun', ''], ['Wan', 'Zeyu', ''], ['Yan', 'Jiaxiang', ''], ['Farrukh', 'Fasih Ud Din', ''], ['Zhang', 'Debing', '']]",0,0,2023-03-17,1,9,1,0,0,0,dc0a86154c37f3aca69a3aaa2e3c4b6df8839f2a,257622860.0,https://www.semanticscholar.org/paper/dc0a86154c37f3aca69a3aaa2e3c4b6df8839f2a,arXiv.org,2023.0,42.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2155279739', 'name': 'Weiyi Zhang'}, {'authorId': '2211939217', 'name': 'Yushi Guo'}, {'authorId': '2099808971', 'name': 'L. Niu'}, {'authorId': '2212058969', 'name': 'Peijun Li'}, {'authorId': '2193340663', 'name': 'Chun Zhang'}, {'authorId': '2048971209', 'name': 'Zeyu Wan'}, {'authorId': '2211928077', 'name': 'Jiaxiang Yan'}, {'authorId': '145667829', 'name': 'F. Farrukh'}, {'authorId': '2145943273', 'name': 'Debing Zhang'}]","['University of Edinburgh', 'Tsinghua University', 'School of Integrated Circuits,']","['China', 'United Kingdom']",2023-03
2303.11032,Xiang Li,"Zhengliang Liu, Xiaowei Yu, Lu Zhang, Zihao Wu, Chao Cao, Haixing Dai,
  Lin Zhao, Wei Liu, Dinggang Shen, Quanzheng Li, Tianming Liu, Dajiang Zhu,
  Xiang Li",DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4,,,,,cs.CL cs.CY,http://creativecommons.org/licenses/by/4.0/,"  The digitization of healthcare has facilitated the sharing and re-using of medical data but has also raised concerns about confidentiality and privacy. HIPAA (Health Insurance Portability and Accountability Act) mandates removing re-identifying information before the dissemination of medical records. Thus, effective and efficient solutions for de-identifying medical data, especially those in free-text forms, are highly needed. While various computer-assisted de-identification methods, including both rule-based and learning-based, have been developed and used in prior practice, such solutions still lack generalizability or need to be fine-tuned according to different scenarios, significantly imposing restrictions in wider use. The advancement of large language models (LLM), such as ChatGPT and GPT-4, have shown great potential in processing text data in the medical domain with zero-shot in-context learning, especially in the task of privacy protection, as these models can identify confidential information by their powerful named entity recognition (NER) capability. In this work, we developed a novel GPT4-enabled de-identification framework (""DeID-GPT"") to automatically identify and remove the identifying information. Compared to existing commonly used medical text data de-identification methods, our developed DeID-GPT showed the highest accuracy and remarkable reliability in masking private information from the unstructured medical text while preserving the original structure and meaning of the text. This study is one of the earliest to utilize ChatGPT and GPT-4 for medical text data processing and de-identification, which provides insights for further research and solution development on the use of LLMs such as ChatGPT/GPT-4 in healthcare. Codes and benchmarking data information are available at https://github.com/yhydhx/ChatGPT-API. ","[{'version': 'v1', 'created': 'Mon, 20 Mar 2023 11:34:37 GMT'}]",2023-03-21,"[['Liu', 'Zhengliang', ''], ['Yu', 'Xiaowei', ''], ['Zhang', 'Lu', ''], ['Wu', 'Zihao', ''], ['Cao', 'Chao', ''], ['Dai', 'Haixing', ''], ['Zhao', 'Lin', ''], ['Liu', 'Wei', ''], ['Shen', 'Dinggang', ''], ['Li', 'Quanzheng', ''], ['Liu', 'Tianming', ''], ['Zhu', 'Dajiang', ''], ['Li', 'Xiang', '']]",1,1,2023-03-20,1,13,2,2,0,2,cff26bda86237d113ed01c812ad8bedd0afbe070,257632030.0,https://www.semanticscholar.org/paper/cff26bda86237d113ed01c812ad8bedd0afbe070,arXiv.org,2023.0,95.0,55.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '70995262', 'name': 'Zheng-Long Liu'}, {'authorId': '2139760758', 'name': 'Xiao-Xing Yu'}, {'authorId': '2156146823', 'name': 'Lu Zhang'}, {'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '5762440', 'name': 'Chao-Yang Cao'}, {'authorId': '29944950', 'name': 'Haixing Dai'}, {'authorId': '2111641126', 'name': 'Lin Zhao'}, {'authorId': '46641573', 'name': 'W. Liu'}, {'authorId': '2150038187', 'name': 'Dinggang Shen'}, {'authorId': '1762919', 'name': 'Quanzheng Li'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}, {'authorId': '2181182', 'name': 'Dajiang Zhu'}, {'authorId': '113075935', 'name': 'Xiang Li'}]","['Harvard Medical School', 'Lehigh University', 'ShanghaiTech University', 'Shanghai United Imaging Intelligence Co., Ltd., Shanghai, China;', 'Mayo Clinic Hospital', 'Shanghai Clinical Research Center', 'Massachusetts General Hospital', 'University of Georgia', 'The University of Texas at Arlington']","['China', 'United States']",2023-03
2303.11366,Noah Shinn,"Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik
  Narasimhan, Shunyu Yao",Reflexion: Language Agents with Verbal Reinforcement Learning,v3 contains additional citations,,,,cs.AI cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance. ","[{'version': 'v1', 'created': 'Mon, 20 Mar 2023 18:08:50 GMT'}, {'version': 'v2', 'created': 'Sun, 21 May 2023 06:20:36 GMT'}, {'version': 'v3', 'created': 'Sat, 10 Jun 2023 04:32:30 GMT'}]",2023-06-13,"[['Shinn', 'Noah', ''], ['Cassano', 'Federico', ''], ['Labash', 'Beck', ''], ['Gopinath', 'Ashwin', ''], ['Narasimhan', 'Karthik', ''], ['Yao', 'Shunyu', '']]",0,1,2023-03-20,3,6,3,1,0,1,0671fd553dd670a4e820553a974bc48040ba0819,258833055.0,https://www.semanticscholar.org/paper/0671fd553dd670a4e820553a974bc48040ba0819,,2023.0,35.0,123.0,17.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2212367248', 'name': 'Noah Shinn'}, {'authorId': '2197063831', 'name': 'Federico Cassano'}, {'authorId': '2212367414', 'name': 'Beck Labash'}, {'authorId': '2162047785', 'name': 'A. Gopinath'}, {'authorId': '144958935', 'name': 'Karthik Narasimhan'}, {'authorId': '47188964', 'name': 'Shunyu Yao'}]","['Massachusetts Institute of Technology', 'Princeton University', 'Northeastern University']","['China', 'United States']",2023-03
2303.11568,Jianing Qiu,"Jianing Qiu, Lin Li, Jiankai Sun, Jiachuan Peng, Peilun Shi, Ruiyang
  Zhang, Yinzhao Dong, Kyle Lam, Frank P.-W. Lo, Bo Xiao, Wu Yuan, Ningli Wang,
  Dong Xu, Benny Lo","Large AI Models in Health Informatics: Applications, Challenges, and the Future","This article has been accepted for publication in IEEE Journal of
  Biomedical and Health Informatics","JBHI, 2023",10.1109/JBHI.2023.3316750,,cs.AI cs.CY,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Large AI models, or foundation models, are models recently emerging with massive scales both parameter-wise and data-wise, the magnitudes of which can reach beyond billions. Once pretrained, large AI models demonstrate impressive performance in various downstream tasks. A prime example is ChatGPT, whose capability has compelled people's imagination about the far-reaching influence that large AI models can have and their potential to transform different domains of our lives. In health informatics, the advent of large AI models has brought new paradigms for the design of methodologies. The scale of multi-modal data in the biomedical and health domain has been ever-expanding especially since the community embraced the era of deep learning, which provides the ground to develop, validate, and advance large AI models for breakthroughs in health-related areas. This article presents a comprehensive review of large AI models, from background to their applications. We identify seven key sectors in which large AI models are applicable and might have substantial influence, including 1) bioinformatics; 2) medical diagnosis; 3) medical imaging; 4) medical informatics; 5) medical education; 6) public health; and 7) medical robotics. We examine their challenges, followed by a critical discussion about potential future directions and pitfalls of large AI models in transforming the field of health informatics. ","[{'version': 'v1', 'created': 'Tue, 21 Mar 2023 03:28:33 GMT'}, {'version': 'v2', 'created': 'Sun, 24 Sep 2023 06:09:58 GMT'}]",2023-09-26,"[['Qiu', 'Jianing', ''], ['Li', 'Lin', ''], ['Sun', 'Jiankai', ''], ['Peng', 'Jiachuan', ''], ['Shi', 'Peilun', ''], ['Zhang', 'Ruiyang', ''], ['Dong', 'Yinzhao', ''], ['Lam', 'Kyle', ''], ['Lo', 'Frank P. -W.', ''], ['Xiao', 'Bo', ''], ['Yuan', 'Wu', ''], ['Wang', 'Ningli', ''], ['Xu', 'Dong', ''], ['Lo', 'Benny', '']]",1,1,2023-03-21,2,14,2,1,0,1,23684a07517870cffd1f97fafbaae16ba22bd2b7,257636930.0,https://www.semanticscholar.org/paper/23684a07517870cffd1f97fafbaae16ba22bd2b7,IEEE journal of biomedical and health informatics,2023.0,345.0,23.0,0.0,True,"['Computer Science', 'Medicine']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Medicine', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '9720064', 'name': 'Jianing Qiu'}, {'authorId': '47681623', 'name': 'Lin Li'}, {'authorId': '2282025', 'name': 'Jiankai Sun'}, {'authorId': '2182888388', 'name': 'Jiachuan Peng'}, {'authorId': '2182698296', 'name': 'Peilun Shi'}, {'authorId': '144142354', 'name': 'Rui Zhang'}, {'authorId': '97436309', 'name': 'Yinzhao Dong'}, {'authorId': '2212174676', 'name': 'Kyle Lam'}, {'authorId': '7281521', 'name': 'F. P. Lo'}, {'authorId': '2212367961', 'name': 'Bo Xiao'}, {'authorId': '2212037281', 'name': 'Wu Yuan'}, {'authorId': '2212008585', 'name': 'Dong Xu'}, {'authorId': '2106084676', 'name': 'Benny P. L. Lo'}]","[""King's College London"", 'Precision Robotics (Hong Kong) Ltd., Hong Kong SAR', 'Imperial College London', 'Capital Medical University', 'Stanford University', 'Chinese University of Hong Kong', 'Hong Kong Polytechnic University', 'University of Missouri', 'University of Hong Kong', 'Large AI Models in Health Informatics']","['China', 'United States', 'United Kingdom', 'Hong Kong']",2023-03
2303.11717,Chaoning Zhang,"Chaoning Zhang, Chenshuang Zhang, Sheng Zheng, Yu Qiao, Chenghao Li,
  Mengchun Zhang, Sumit Kumar Dam, Chu Myaet Thwal, Ye Lin Tun, Le Luang Huy,
  Donguk kim, Sung-Ho Bae, Lik-Hang Lee, Yang Yang, Heng Tao Shen, In So Kweon,
  Choong Seon Hong",A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?,"56 pages, 548 citations",,,,cs.AI cs.CV cs.LG cs.MM,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has made headlines everywhere because of its ability to analyze and create text, images, and beyond. With such overwhelming media coverage, it is almost impossible for us to miss the opportunity to glimpse AIGC from a certain angle. In the era of AI transitioning from pure analysis to creation, it is worth noting that ChatGPT, with its most recent language model GPT-4, is just a tool out of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many people are wondering about its limits: can GPT-5 (or other future GPT variants) help ChatGPT unify all AIGC tasks for diversified content creation? Toward answering this question, a comprehensive review of existing AIGC tasks is needed. As such, our work comes to fill this gap promptly by offering a first look at AIGC, ranging from its techniques to applications. Modern generative AI relies on various technical foundations, ranging from model architecture and self-supervised pretraining to generative modeling methods (like GAN and diffusion models). After introducing the fundamental techniques, this work focuses on the technological development of various AIGC tasks based on their output type, including text, images, videos, 3D content, etc., which depicts the full potential of ChatGPT's future. Moreover, we summarize their significant applications in some mainstream industries, such as education and creativity content. Finally, we discuss the challenges currently faced and present an outlook on how generative AI might evolve in the near future. ","[{'version': 'v1', 'created': 'Tue, 21 Mar 2023 10:09:47 GMT'}]",2023-03-22,"[['Zhang', 'Chaoning', ''], ['Zhang', 'Chenshuang', ''], ['Zheng', 'Sheng', ''], ['Qiao', 'Yu', ''], ['Li', 'Chenghao', ''], ['Zhang', 'Mengchun', ''], ['Dam', 'Sumit Kumar', ''], ['Thwal', 'Chu Myaet', ''], ['Tun', 'Ye Lin', ''], ['Huy', 'Le Luang', ''], ['kim', 'Donguk', ''], ['Bae', 'Sung-Ho', ''], ['Lee', 'Lik-Hang', ''], ['Yang', 'Yang', ''], ['Shen', 'Heng Tao', ''], ['Kweon', 'In So', ''], ['Hong', 'Choong Seon', '']]",1,1,2023-03-21,1,17,4,2,0,2,1b492746ee3a304a13950cad1a59861b9ee44645,257636561.0,https://www.semanticscholar.org/paper/1b492746ee3a304a13950cad1a59861b9ee44645,arXiv.org,2023.0,533.0,67.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '31044159', 'name': 'Chaoning Zhang'}, {'authorId': '48934876', 'name': 'Chenshuang Zhang'}, {'authorId': '2211960167', 'name': 'Sheng Zheng'}, {'authorId': '2209185953', 'name': 'Yu Qiao'}, {'authorId': '2128662588', 'name': 'Chenghao Li'}, {'authorId': '50495602', 'name': 'Mengchun Zhang'}, {'authorId': '1642047032', 'name': 'S. Dam'}, {'authorId': '2053849768', 'name': 'Chu Myaet Thwal'}, {'authorId': '49935726', 'name': 'Ye Lin Tun'}, {'authorId': '2212370613', 'name': 'Le Luang Huy'}, {'authorId': '2212043758', 'name': 'Donguk kim'}, {'authorId': '40547898', 'name': 'S. Bae'}, {'authorId': '2188647130', 'name': 'Lik-Hang Lee'}, {'authorId': '2152283122', 'name': 'Yang Yang'}, {'authorId': '1724393', 'name': 'Heng Tao Shen'}, {'authorId': '145017151', 'name': 'In-So Kweon'}, {'authorId': '2159807650', 'name': 'Choong-Seon Hong'}]","['Xidian University', 'Beijing Institute of Technology', 'Hong Kong Polytechnic University', 'Korea Advanced Institute of Science and Technology', 'Kyung Hee University']","['South Korea', 'China', 'Hong Kong']",2023-03
2303.11812,Siyi Cao,"Tongquan Zhou, Siyi Cao, Siruo Zhou, Yao Zhang, Aijing He",Chinese Intermediate English Learners outdid ChatGPT in deep cohesion: Evidence from English narrative writing,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  ChatGPT is a publicly available chatbot that can quickly generate texts on given topics, but it is unknown whether the chatbot is really superior to human writers in all aspects of writing and whether its writing quality can be prominently improved on the basis of updating commands. Consequently, this study compared the writing performance on a narrative topic by ChatGPT and Chinese intermediate English (CIE) learners so as to reveal the chatbot's advantage and disadvantage in writing. The data were analyzed in terms of five discourse components using Coh-Metrix (a special instrument for analyzing language discourses), and the results revealed that ChatGPT performed better than human writers in narrativity, word concreteness, and referential cohesion, but worse in syntactic simplicity and deep cohesion in its initial version. After more revision commands were updated, while the resulting version was facilitated in syntactic simplicity, yet it is still lagged far behind CIE learners' writing in deep cohesion. In addition, the correlation analysis of the discourse components suggests that narrativity was correlated with referential cohesion in both ChatGPT and human writers, but the correlations varied within each group. ","[{'version': 'v1', 'created': 'Tue, 21 Mar 2023 12:55:54 GMT'}]",2023-03-22,"[['Zhou', 'Tongquan', ''], ['Cao', 'Siyi', ''], ['Zhou', 'Siruo', ''], ['Zhang', 'Yao', ''], ['He', 'Aijing', '']]",1,1,2023-03-21,1,5,1,1,0,1,cb755845a2cc9502fc5c6f0656900cac5dd54db7,257636859.0,https://www.semanticscholar.org/paper/cb755845a2cc9502fc5c6f0656900cac5dd54db7,System (Linkping),2023.0,49.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '1752793691', 'name': 'Tongquan Zhou'}, {'authorId': '102312299', 'name': 'Siyi Cao'}, {'authorId': '2174817394', 'name': 'Siruo Zhou'}, {'authorId': '2118391047', 'name': 'Yao Zhang'}, {'authorId': '2212085283', 'name': 'Aijing He'}]","['Guangzhou University', 'Southeast University', 'Nanjing University of Posts and Telecommunications']",['China'],2023-03
2303.12135,Xin Jin,"Xin Jin, Yuchen Wang",Understand Legal Documents with Contextualized Large Language Models,SemEval 2023,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The growth of pending legal cases in populous countries, such as India, has become a major issue. Developing effective techniques to process and understand legal documents is extremely useful in resolving this problem. In this paper, we present our systems for SemEval-2023 Task 6: understanding legal texts (Modi et al., 2023). Specifically, we first develop the Legal-BERT-HSLN model that considers the comprehensive context information in both intra- and inter-sentence levels to predict rhetorical roles (subtask A) and then train a Legal-LUKE model, which is legal-contextualized and entity-aware, to recognize legal entities (subtask B). Our evaluations demonstrate that our designed models are more accurate than baselines, e.g., with an up to 15.0% better F1 score in subtask B. We achieved notable performance in the task leaderboard, e.g., 0.834 micro F1 score, and ranked No.5 out of 27 teams in subtask A. ","[{'version': 'v1', 'created': 'Tue, 21 Mar 2023 18:48:11 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Jun 2023 03:25:37 GMT'}, {'version': 'v3', 'created': 'Sun, 2 Jul 2023 22:51:52 GMT'}, {'version': 'v4', 'created': 'Wed, 19 Jul 2023 05:30:31 GMT'}]",2023-07-20,"[['Jin', 'Xin', ''], ['Wang', 'Yuchen', '']]",0,0,2023-03-21,4,2,1,0,0,0,77f1fd9d5d0deaaba2ae9b2b7d0f8df5b5762268,257663840.0,https://www.semanticscholar.org/paper/77f1fd9d5d0deaaba2ae9b2b7d0f8df5b5762268,arXiv.org,2023.0,41.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2149170954', 'name': 'Xin Jin'}, {'authorId': '2108898835', 'name': 'Yuchen Wang'}]","['The Ohio State University', 'Northwestern Polytechnical University']","['China', 'United States']",2023-03
2303.13001,Mingyang Song,"Mingyang Song, Haiyun Jiang, Shuming Shi, Songfang Yao, Shilong Lu, Yi
  Feng, Huafeng Liu, Liping Jing",Is ChatGPT A Good Keyphrase Generator? A Preliminary Study,"Technical Report, 7 pages",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The emergence of ChatGPT has recently garnered significant attention from the computational linguistics community. To demonstrate its capabilities as a keyphrase generator, we conduct a preliminary evaluation of ChatGPT for the keyphrase generation task. We evaluate its performance in various aspects, including keyphrase generation prompts, keyphrase generation diversity, multi-domain keyphrase generation, and long document understanding. Our evaluation is based on six benchmark datasets, and we adopt the prompt suggested by OpenAI while extending it to six candidate prompts. We find that ChatGPT performs exceptionally well on all six candidate prompts, with minor performance differences observed across the datasets. Based on our findings, we conclude that ChatGPT has great potential for keyphrase generation. Moreover, we discover that ChatGPT still faces challenges when it comes to generating absent keyphrases. Meanwhile, in the final section, we also present some limitations and future expansions of this report. ","[{'version': 'v1', 'created': 'Thu, 23 Mar 2023 02:50:38 GMT'}]",2023-03-24,"[['Song', 'Mingyang', ''], ['Jiang', 'Haiyun', ''], ['Shi', 'Shuming', ''], ['Yao', 'Songfang', ''], ['Lu', 'Shilong', ''], ['Feng', 'Yi', ''], ['Liu', 'Huafeng', ''], ['Jing', 'Liping', '']]",1,1,2023-03-23,1,8,1,1,0,1,d36267e688225d976d19e0f892918fdd86d40a56,257687477.0,https://www.semanticscholar.org/paper/d36267e688225d976d19e0f892918fdd86d40a56,arXiv.org,2023.0,29.0,7.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '3358418', 'name': 'M. Song'}, {'authorId': '48579460', 'name': 'Haiyun Jiang'}, {'authorId': '2072684668', 'name': 'Shuming Shi'}, {'authorId': '2212022945', 'name': 'Songfang Yao'}, {'authorId': '2212057371', 'name': 'Shilong Lu'}, {'authorId': '2134046786', 'name': 'Yi Feng'}, {'authorId': '2145489099', 'name': 'Huafeng Liu'}, {'authorId': '144889532', 'name': 'L. Jing'}]","['Tencent', 'Beijing Jiaotong University']",['China'],2023-03
2303.13013,Nan Gao,"Nan Gao, Zeyu Zhao, Zhi Zeng, Shuwu Zhang, Dongdong Weng",GesGPT: Speech Gesture Synthesis With Text Parsing from GPT,,,,,cs.CL cs.CV cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Gesture synthesis has gained significant attention as a critical research area, focusing on producing contextually appropriate and natural gestures corresponding to speech or textual input. Although deep learning-based approaches have achieved remarkable progress, they often overlook the rich semantic information present in the text, leading to less expressive and meaningful gestures. We propose GesGPT, a novel approach to gesture generation that leverages the semantic analysis capabilities of Large Language Models (LLMs), such as GPT. By capitalizing on the strengths of LLMs for text analysis, we design prompts to extract gesture-related information from textual input. Our method entails developing prompt principles that transform gesture generation into an intention classification problem based on GPT, and utilizing a curated gesture library and integration module to produce semantically rich co-speech gestures. Experimental results demonstrate that GesGPT effectively generates contextually appropriate and expressive gestures, offering a new perspective on semantic co-speech gesture generation. ","[{'version': 'v1', 'created': 'Thu, 23 Mar 2023 03:30:30 GMT'}]",2023-03-24,"[['Gao', 'Nan', ''], ['Zhao', 'Zeyu', ''], ['Zeng', 'Zhi', ''], ['Zhang', 'Shuwu', ''], ['Weng', 'Dongdong', '']]",0,1,2023-03-23,1,5,3,0,0,0,6b78c356579c6bd9a1ef0cf4516af9030a0f1299,257687239.0,https://www.semanticscholar.org/paper/6b78c356579c6bd9a1ef0cf4516af9030a0f1299,arXiv.org,2023.0,37.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2165382686', 'name': 'Nan Gao'}, {'authorId': '2186400109', 'name': 'Zeyu Zhao'}, {'authorId': '46490648', 'name': 'Zhi Zeng'}, {'authorId': '48691302', 'name': 'Shuwu Zhang'}, {'authorId': '1744702', 'name': 'Dongdong Weng'}]","['Beijing University of Posts and Telecommunications', 'University of Chinese Academy of Sciences', 'Beijing Institute of Technology', 'Chinese Academy of Sciences']",['China'],2023-03
2303.13217,Huan Ma,"Huan Ma, Changqing Zhang, Yatao Bian, Lemao Liu, Zhirui Zhang, Peilin
  Zhao, Shu Zhang, Huazhu Fu, Qinghua Hu, Bingzhe Wu",Fairness-guided Few-shot Prompting for Large Language Models,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large language models have demonstrated surprising ability to perform in-context learning, i.e., these models can be directly applied to solve numerous downstream tasks by conditioning on a prompt constructed by a few input-output examples. However, prior research has shown that in-context learning can suffer from high instability due to variations in training examples, example order, and prompt formats. Therefore, the construction of an appropriate prompt is essential for improving the performance of in-context learning. In this paper, we revisit this problem from the view of predictive bias. Specifically, we introduce a metric to evaluate the predictive bias of a fixed prompt against labels or a given attributes. Then we empirically show that prompts with higher bias always lead to unsatisfactory predictive quality. Based on this observation, we propose a novel search strategy based on the greedy search to identify the near-optimal prompt for improving the performance of in-context learning. We perform comprehensive experiments with state-of-the-art mainstream models such as GPT-3 on various downstream tasks. Our results indicate that our method can enhance the model's in-context learning performance in an effective and interpretable manner. ","[{'version': 'v1', 'created': 'Thu, 23 Mar 2023 12:28:25 GMT'}, {'version': 'v2', 'created': 'Sat, 25 Mar 2023 04:56:38 GMT'}, {'version': 'v3', 'created': 'Fri, 31 Mar 2023 06:11:06 GMT'}]",2023-04-03,"[['Ma', 'Huan', ''], ['Zhang', 'Changqing', ''], ['Bian', 'Yatao', ''], ['Liu', 'Lemao', ''], ['Zhang', 'Zhirui', ''], ['Zhao', 'Peilin', ''], ['Zhang', 'Shu', ''], ['Fu', 'Huazhu', ''], ['Hu', 'Qinghua', ''], ['Wu', 'Bingzhe', '']]",0,1,2023-03-23,3,10,2,1,0,1,3436ff7a1dd4c6547ba78968d3eec2545a6dccb9,257687840.0,https://www.semanticscholar.org/paper/3436ff7a1dd4c6547ba78968d3eec2545a6dccb9,arXiv.org,2023.0,21.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2110817620', 'name': 'Huan Ma'}, {'authorId': '144704038', 'name': 'Changqing Zhang'}, {'authorId': '2419616', 'name': 'Yatao Bian'}, {'authorId': '2978364', 'name': 'Lemao Liu'}, {'authorId': '4947404', 'name': 'Zhirui Zhang'}, {'authorId': '144259957', 'name': 'P. Zhao'}, {'authorId': '2108088736', 'name': 'Shu Zhang'}, {'authorId': '1929093', 'name': 'H. Fu'}, {'authorId': '2113360928', 'name': 'Qinghua Hu'}, {'authorId': '2152564746', 'name': 'Bing Wu'}]","['Institute of High Performance Computing', 'Tencent', 'Tianjin University']","['China', 'Singapore']",2023-03
2303.13547,Aiwei Liu,"Aiwei Liu, Xuming Hu, Lijie Wen, Philip S. Yu",A comprehensive evaluation of ChatGPT's zero-shot Text-to-SQL capability,"6 pages, 1 figures",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  This paper presents the first comprehensive analysis of ChatGPT's Text-to-SQL ability. Given the recent emergence of large-scale conversational language model ChatGPT and its impressive capabilities in both conversational abilities and code generation, we sought to evaluate its Text-to-SQL performance. We conducted experiments on 12 benchmark datasets with different languages, settings, or scenarios, and the results demonstrate that ChatGPT has strong text-to-SQL abilities. Although there is still a gap from the current state-of-the-art (SOTA) model performance, considering that the experiment was conducted in a zero-shot scenario, ChatGPT's performance is still impressive. Notably, in the ADVETA (RPL) scenario, the zero-shot ChatGPT even outperforms the SOTA model that requires fine-tuning on the Spider dataset by 4.1\%, demonstrating its potential for use in practical applications. To support further research in related fields, we have made the data generated by ChatGPT publicly available at https://github.com/THU-BPM/chatgpt-sql. ","[{'version': 'v1', 'created': 'Sun, 12 Mar 2023 04:22:01 GMT'}]",2023-03-27,"[['Liu', 'Aiwei', ''], ['Hu', 'Xuming', ''], ['Wen', 'Lijie', ''], ['Yu', 'Philip S.', '']]",1,1,2023-03-12,1,4,2,1,0,1,6034a1ed44de01ffc18cd2265223ed2bf1d216cd,257757019.0,https://www.semanticscholar.org/paper/6034a1ed44de01ffc18cd2265223ed2bf1d216cd,arXiv.org,2023.0,30.0,51.0,11.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '10017193', 'name': 'Aiwei Liu'}, {'authorId': '2109906988', 'name': 'Xuming Hu'}, {'authorId': '2114092431', 'name': 'Lijie Wen'}, {'authorId': '2191036692', 'name': 'Philip S. Yu'}]","['University of Illinois at Chicago', 'Tsinghua University']","['China', 'United States']",2023-03
2303.13648,Wenxuan Wang,"Haoran Wu, Wenxuan Wang, Yuxuan Wan, Wenxiang Jiao, Michael Lyu",ChatGPT or Grammarly? Evaluating ChatGPT on Grammatical Error Correction Benchmark,Working in progress,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  ChatGPT is a cutting-edge artificial intelligence language model developed by OpenAI, which has attracted a lot of attention due to its surprisingly strong ability in answering follow-up questions. In this report, we aim to evaluate ChatGPT on the Grammatical Error Correction(GEC) task, and compare it with commercial GEC product (e.g., Grammarly) and state-of-the-art models (e.g., GECToR). By testing on the CoNLL2014 benchmark dataset, we find that ChatGPT performs not as well as those baselines in terms of the automatic evaluation metrics (e.g., $F_{0.5}$ score), particularly on long sentences. We inspect the outputs and find that ChatGPT goes beyond one-by-one corrections. Specifically, it prefers to change the surface expression of certain phrases or sentence structure while maintaining grammatical correctness. Human evaluation quantitatively confirms this and suggests that ChatGPT produces less under-correction or mis-correction issues but more over-corrections. These results demonstrate that ChatGPT is severely under-estimated by the automatic evaluation metrics and could be a promising tool for GEC. ","[{'version': 'v1', 'created': 'Wed, 15 Mar 2023 00:35:50 GMT'}]",2023-03-27,"[['Wu', 'Haoran', ''], ['Wang', 'Wenxuan', ''], ['Wan', 'Yuxuan', ''], ['Jiao', 'Wenxiang', ''], ['Lyu', 'Michael', '']]",1,1,2023-03-15,1,5,1,1,0,1,0244dd4ce41b7a1ad441185e0a2f48433e976cfc,257757296.0,https://www.semanticscholar.org/paper/0244dd4ce41b7a1ad441185e0a2f48433e976cfc,arXiv.org,2023.0,20.0,43.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1664776313', 'name': 'Hao Wu'}, {'authorId': '2144328160', 'name': 'Wenxuan Wang'}, {'authorId': '2167583580', 'name': 'Yuxuan Wan'}, {'authorId': '12386833', 'name': 'Wenxiang Jiao'}, {'authorId': '2146840128', 'name': 'Michael R. Lyu'}]","['Tencent', 'Chinese University of Hong Kong']",['China'],2023-03
2303.13809,Liang Ding,"Qingyu Lu, Baopu Qiu, Liang Ding, Liping Xie, Dacheng Tao",Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT,,,,,cs.CL,http://creativecommons.org/publicdomain/zero/1.0/,"  Generative large language models (LLMs), e.g., ChatGPT, have demonstrated remarkable proficiency across several NLP tasks such as machine translation, question answering, text summarization, and natural language understanding. Recent research has shown that utilizing ChatGPT for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but performs poorly at the segment level. To further improve the performance of LLMs on MT quality assessment, we conducted an investigation into several prompting methods. Our results indicate that by combining Chain-of-Thoughts and Error Analysis, a new prompting method called \textbf{\texttt{Error Analysis Prompting}}, LLMs like ChatGPT can \textit{generate human-like MT evaluations at both the system and segment level}. Additionally, we discovered some limitations of ChatGPT as an MT evaluator, such as unstable scoring and biases when provided with multiple translations in a single query. Our findings aim to provide a preliminary experience for appropriately evaluating translation quality on ChatGPT while offering a variety of tricks in designing prompts for in-context learning. We anticipate that this report will shed new light on advancing the field of translation evaluation with LLMs by enhancing both the accuracy and reliability of metrics. The project can be found in \url{https://github.com/Coldmist-Lu/ErrorAnalysis_Prompt}. ","[{'version': 'v1', 'created': 'Fri, 24 Mar 2023 05:05:03 GMT'}]",2023-03-27,"[['Lu', 'Qingyu', ''], ['Qiu', 'Baopu', ''], ['Ding', 'Liang', ''], ['Xie', 'Liping', ''], ['Tao', 'Dacheng', '']]",1,1,2023-03-24,1,5,1,1,0,1,8bc313e04cbd39847eb50b22af0a698ff2971a35,257756967.0,https://www.semanticscholar.org/paper/8bc313e04cbd39847eb50b22af0a698ff2971a35,arXiv.org,2023.0,51.0,29.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2117523622', 'name': 'Qingyu Lu'}, {'authorId': '2136339354', 'name': 'Baopu Qiu'}, {'authorId': '46573238', 'name': 'Liang Ding'}, {'authorId': '153164376', 'name': 'Liping Xie'}, {'authorId': '2140448089', 'name': 'Dacheng Tao'}]","['Southeast University', 'Nanjing University', 'Microsoft', 'Jingdong']","['China', 'United States']",2023-03
2303.13856,Pengyuan Zhou,Pengyuan Zhou,Unleashing ChatGPT on the Metaverse: Savior or Destroyer?,"13 pages, 19 figures",,,,cs.HC cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  The incorporation of artificial intelligence (AI) technology, and in particular natural language processing (NLP), is becoming increasingly vital for the development of immersive and interactive metaverse experiences. One such artificial intelligence tool that is gaining traction in the metaverse is ChatGPT, a large language model trained by OpenAI. The article delves into the pros and cons of utilizing ChatGPT for metaverse-based education, entertainment, personalization, and support. Dynamic and personalized experiences are possible with this technology, but there are also legitimate privacy, bias, and ethical issues to consider. This article aims to help readers understand the possible influence of ChatGPT on the metaverse and how it may be used to effectively create a more immersive and engaging virtual environment by evaluating these opportunities and obstacles. ","[{'version': 'v1', 'created': 'Fri, 24 Mar 2023 08:35:37 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Apr 2023 08:46:21 GMT'}]",2023-04-13,"[['Zhou', 'Pengyuan', '']]",1,1,2023-03-24,2,1,2,1,0,1,10189066c40d4db2c67f305f7edb1dd9235ad108,257757364.0,https://www.semanticscholar.org/paper/10189066c40d4db2c67f305f7edb1dd9235ad108,arXiv.org,2023.0,14.0,10.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '72050450', 'name': 'Pengyuan Zhou'}]","['University of Science and Technology of China', 'Figure 1: Incorporating ChatGPT to Metaverse.']",['China'],2023-03
2303.13873,Yongwei Chen,"Rui Chen, Yongwei Chen, Ningxin Jiao, Kui Jia",Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation,Accepted by ICCV 2023. Project page: https://fantasia3d.github.io/,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Automatic 3D content creation has achieved rapid progress recently due to the availability of pre-trained, large language models and image diffusion models, forming the emerging topic of text-to-3D content creation. Existing text-to-3D methods commonly use implicit scene representations, which couple the geometry and appearance via volume rendering and are suboptimal in terms of recovering finer geometries and achieving photorealistic rendering; consequently, they are less effective for generating high-quality 3D assets. In this work, we propose a new method of Fantasia3D for high-quality text-to-3D content creation. Key to Fantasia3D is the disentangled modeling and learning of geometry and appearance. For geometry learning, we rely on a hybrid scene representation, and propose to encode surface normal extracted from the representation as the input of the image diffusion model. For appearance modeling, we introduce the spatially varying bidirectional reflectance distribution function (BRDF) into the text-to-3D task, and learn the surface material for photorealistic rendering of the generated surface. Our disentangled framework is more compatible with popular graphics engines, supporting relighting, editing, and physical simulation of the generated 3D assets. We conduct thorough experiments that show the advantages of our method over existing ones under different text-to-3D task settings. Project page and source codes: https://fantasia3d.github.io/. ","[{'version': 'v1', 'created': 'Fri, 24 Mar 2023 09:30:09 GMT'}, {'version': 'v2', 'created': 'Tue, 28 Mar 2023 14:18:56 GMT'}, {'version': 'v3', 'created': 'Wed, 27 Sep 2023 10:35:49 GMT'}]",2023-09-28,"[['Chen', 'Rui', ''], ['Chen', 'Yongwei', ''], ['Jiao', 'Ningxin', ''], ['Jia', 'Kui', '']]",0,0,2023-03-24,3,4,2,0,0,0,0cbb518c364067200476a51e5ce7476a4f582770,257757213.0,https://www.semanticscholar.org/paper/0cbb518c364067200476a51e5ce7476a4f582770,arXiv.org,2023.0,44.0,68.0,18.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1562383795', 'name': 'Rui Chen'}, {'authorId': '7377960', 'name': 'Y. Chen'}, {'authorId': '2133887272', 'name': 'Ningxin Jiao'}, {'authorId': '2066492431', 'name': 'K. Jia'}]",['South China University of Technology'],['China'],2023-03
2303.14325,Xukun Zhou,"Xukun Zhou, Jiwei Li, Tianwei Zhang, Lingjuan Lyu, Muqiao Yang, Jun He",Backdoor Attacks with Input-unique Triggers in NLP,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Backdoor attack aims at inducing neural models to make incorrect predictions for poison data while keeping predictions on the clean dataset unchanged, which creates a considerable threat to current natural language processing (NLP) systems. Existing backdoor attacking systems face two severe issues:firstly, most backdoor triggers follow a uniform and usually input-independent pattern, e.g., insertion of specific trigger words, synonym replacement. This significantly hinders the stealthiness of the attacking model, leading the trained backdoor model being easily identified as malicious by model probes. Secondly, trigger-inserted poisoned sentences are usually disfluent, ungrammatical, or even change the semantic meaning from the original sentence, making them being easily filtered in the pre-processing stage. To resolve these two issues, in this paper, we propose an input-unique backdoor attack(NURA), where we generate backdoor triggers unique to inputs. IDBA generates context-related triggers by continuing writing the input with a language model like GPT2. The generated sentence is used as the backdoor trigger. This strategy not only creates input-unique backdoor triggers, but also preserves the semantics of the original input, simultaneously resolving the two issues above. Experimental results show that the IDBA attack is effective for attack and difficult to defend: it achieves high attack success rate across all the widely applied benchmarks, while is immune to existing defending methods. In addition, it is able to generate fluent, grammatical, and diverse backdoor inputs, which can hardly be recognized through human inspection. ","[{'version': 'v1', 'created': 'Sat, 25 Mar 2023 01:41:54 GMT'}]",2023-03-28,"[['Zhou', 'Xukun', ''], ['Li', 'Jiwei', ''], ['Zhang', 'Tianwei', ''], ['Lyu', 'Lingjuan', ''], ['Yang', 'Muqiao', ''], ['He', 'Jun', '']]",0,1,2023-03-25,1,6,1,1,1,0,02396a82642ca3a3c7e653e764cea193beaf88ba,257767146.0,https://www.semanticscholar.org/paper/02396a82642ca3a3c7e653e764cea193beaf88ba,arXiv.org,2023.0,57.0,3.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2212241158', 'name': 'Xukun Zhou'}, {'authorId': '2172372802', 'name': 'Jiwei Li'}, {'authorId': '2146331573', 'name': 'Tianwei Zhang'}, {'authorId': '3366777', 'name': 'L. Lyu'}, {'authorId': '8724126', 'name': 'Muqiao Yang'}, {'authorId': '2109932574', 'name': 'Jun He'}]","['National Youth Policy Institute', 'Carnegie Mellon University', 'Shandong University', 'Nanyang Technological University', 'Renmin University of China']","['South Korea', 'United States', 'China', 'Singapore']",2023-03
2303.14524,Tao Sheng,"Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, Jiawei
  Zhang",Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System,,,,,cs.IR cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) have demonstrated their significant potential to be applied for addressing various application tasks. However, traditional recommender systems continue to face great challenges such as poor interactivity and explainability, which actually also hinder their broad deployment in real-world systems. To address these limitations, this paper proposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender System) that innovatively augments LLMs for building conversational recommender systems by converting user profiles and historical interactions into prompts. Chat-Rec is demonstrated to be effective in learning user preferences and establishing connections between users and products through in-context learning, which also makes the recommendation process more interactive and explainable. What's more, within the Chat-Rec framework, user's preferences can transfer to different products for cross-domain recommendations, and prompt-based injection of information into LLMs can also handle the cold-start scenarios with new items. In our experiments, Chat-Rec effectively improve the results of top-k recommendations and performs better in zero-shot rating prediction task. Chat-Rec offers a novel approach to improving recommender systems and presents new practical scenarios for the implementation of AIGC (AI generated content) in recommender system studies. ","[{'version': 'v1', 'created': 'Sat, 25 Mar 2023 17:37:43 GMT'}, {'version': 'v2', 'created': 'Tue, 4 Apr 2023 03:51:27 GMT'}]",2023-04-05,"[['Gao', 'Yunfan', ''], ['Sheng', 'Tao', ''], ['Xiang', 'Youlin', ''], ['Xiong', 'Yun', ''], ['Wang', 'Haofen', ''], ['Zhang', 'Jiawei', '']]",1,1,2023-03-25,2,6,3,1,0,1,0cfdd655100055f234fd23ebecd915504b8e00e3,257766541.0,https://www.semanticscholar.org/paper/0cfdd655100055f234fd23ebecd915504b8e00e3,arXiv.org,2023.0,28.0,62.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1390781327', 'name': 'Yunfan Gao'}, {'authorId': '2169937292', 'name': 'Tao Sheng'}, {'authorId': '2152318443', 'name': 'Youlin Xiang'}, {'authorId': '2212411539', 'name': 'Yun Xiong'}, {'authorId': '21606013', 'name': 'Haofen Wang'}, {'authorId': '2144138716', 'name': 'Jiawei Zhang'}]","['Fudan University', 'Tongji University', 'University of California, Davis']","['China', 'United States']",2023-03
2303.14624,Hongyang Du,"Jiacheng Wang, Hongyang Du, Dusit Niyato, Zehui Xiong, Jiawen Kang,
  Shiwen Mao, and Xuemin (Sherman) Shen",Guiding AI-Generated Digital Content with Wireless Perception,,,,,cs.AI cs.HC cs.NI,http://creativecommons.org/licenses/by/4.0/,"  Recent advances in artificial intelligence (AI), coupled with a surge in training data, have led to the widespread use of AI for digital content generation, with ChatGPT serving as a representative example. Despite the increased efficiency and diversity, the inherent instability of AI models poses a persistent challenge in guiding these models to produce the desired content for users. In this paper, we introduce an integration of wireless perception (WP) with AI-generated content (AIGC) and propose a unified WP-AIGC framework to improve the quality of digital content production. The framework employs a novel multi-scale perception technology to read user's posture, which is difficult to describe accurately in words, and transmits it to the AIGC model as skeleton images. Based on these images and user's service requirements, the AIGC model generates corresponding digital content. Since the production process imposes the user's posture as a constraint on the AIGC model, it makes the generated content more aligned with the user's requirements. Additionally, WP-AIGC can also accept user's feedback, allowing adjustment of computing resources at edge server to improve service quality. Experiments results verify the effectiveness of the WP-AIGC framework, highlighting its potential as a novel approach for guiding AI models in the accurate generation of digital content. ","[{'version': 'v1', 'created': 'Sun, 26 Mar 2023 04:39:03 GMT'}]",2023-03-28,"[['Wang', 'Jiacheng', '', 'Sherman'], ['Du', 'Hongyang', '', 'Sherman'], ['Niyato', 'Dusit', '', 'Sherman'], ['Xiong', 'Zehui', '', 'Sherman'], ['Kang', 'Jiawen', '', 'Sherman'], ['Mao', 'Shiwen', '', 'Sherman'], ['Xuemin', '', '', 'Sherman'], ['Shen', '', '']]",1,1,2023-03-26,1,8,3,1,0,1,618fc40c8574668e0807d49fa8871433e073d961,257766435.0,https://www.semanticscholar.org/paper/618fc40c8574668e0807d49fa8871433e073d961,arXiv.org,2023.0,15.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2110175408', 'name': 'Jiacheng Wang'}, {'authorId': '2175043468', 'name': 'Hongyang Du'}, {'authorId': '1713586', 'name': 'D. Niyato'}, {'authorId': '2943819', 'name': 'Zehui Xiong'}, {'authorId': '145993626', 'name': 'Jiawen Kang'}, {'authorId': '2056609621', 'name': 'Shiwen Mao'}, {'authorId': '2158524414', 'name': 'X. Shen'}]","['Singapore University of Technology and Design', 'Auburn University', 'Nanyang Technological University', 'Guangdong University of Technology']","['China', 'United States', 'Singapore']",2023-03
2303.14701,Shuai Ma,"Guangming Shi, Dahua Gao, Shuai Ma, Minxi Yang, Yong Xiao, and Xuemei
  Xie",Mathematical Characterization of Signal Semantics and Rethinking of the Mathematical Theory of Information,,,,,eess.SP,http://creativecommons.org/licenses/by/4.0/,"  Shannon information theory is established based on probability and bits, and the communication technology based on this theory realizes the information age. The original goal of Shannon's information theory is to describe and transmit information content. However, due to information is related to cognition, and cognition is considered to be subjective, Shannon information theory is to describe and transmit information-bearing signals. With the development of the information age to the intelligent age, the traditional signal-oriented processing needs to be upgraded to content-oriented processing. For example, chat generative pre-trained transformer (ChatGPT) has initially realized the content processing capability based on massive data. For many years, researchers have been searching for the answer to what the information content in the signal is, because only when the information content is mathematically and accurately described can information-based machines be truly intelligent. This paper starts from rethinking the essence of the basic concepts of the information, such as semantics, meaning, information and knowledge, presents the mathematical characterization of the information content, investigate the relationship between them, studies the transformation from Shannon's signal information theory to semantic information theory, and therefore proposes a content-oriented semantic communication framework. Furthermore, we propose semantic decomposition and composition scheme to achieve conversion between complex and simple semantics. Finally, we verify the proposed characterization of information-related concepts by implementing evolvable knowledge-based semantic recognition. ","[{'version': 'v1', 'created': 'Sun, 26 Mar 2023 12:08:27 GMT'}]",2023-03-28,"[['Shi', 'Guangming', ''], ['Gao', 'Dahua', ''], ['Ma', 'Shuai', ''], ['Yang', 'Minxi', ''], ['Xiao', 'Yong', ''], ['Xie', 'Xuemei', '']]",1,1,2023-03-26,1,6,1,1,0,1,a8994f03c257bc79784b67fb48c3955220914add,257766489.0,https://www.semanticscholar.org/paper/a8994f03c257bc79784b67fb48c3955220914add,,2023.0,45.0,0.0,0.0,False,['Engineering'],"[{'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '35367337', 'name': 'Guangming Shi'}, {'authorId': '1775883', 'name': 'Dahua Gao'}, {'authorId': '2212254496', 'name': 'Shuai Ma'}, {'authorId': '122694430', 'name': 'Minxi Yang'}, {'authorId': '2153508144', 'name': 'Yong Xiao'}, {'authorId': '2033687', 'name': 'Xuemei Xie'}]","['Huazhong University of Science and Technology', 'Xidian University', 'Peng Cheng Laboratory']",['China'],2023-03
2303.14704,Guorun Wang,"Guorun Wang, Jun Yang, Yaoru Sun",Task-oriented Memory-efficient Pruning-Adapter,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The Outstanding performance and growing size of Large Language Models has led to increased attention in parameter efficient learning. The two predominant approaches are Adapters and Pruning. Adapters are to freeze the model and give it a new weight matrix on the side, which can significantly reduce the time and memory of training, but the cost is that the evaluation and testing will increase the time and memory consumption. Pruning is to cut off some weight and re-distribute the remaining weight, which sacrifices the complexity of training at the cost of extremely high memory and training time, making the cost of evaluation and testing relatively low. So efficiency of training and inference can't be obtained in the same time. In this work, we propose a task-oriented Pruning-Adapter method that achieve a high memory efficiency of training and memory, and speeds up training time and ensures no significant decrease in accuracy in GLUE tasks, achieving training and inference efficiency at the same time. ","[{'version': 'v1', 'created': 'Sun, 26 Mar 2023 12:18:00 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Apr 2023 03:44:38 GMT'}]",2023-04-07,"[['Wang', 'Guorun', ''], ['Yang', 'Jun', ''], ['Sun', 'Yaoru', '']]",0,0,2023-03-26,2,3,2,0,0,0,ba13829566e3f6c6732e9e0e0948fc81f71a04a3,257766832.0,https://www.semanticscholar.org/paper/ba13829566e3f6c6732e9e0e0948fc81f71a04a3,arXiv.org,2023.0,30.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2209608291', 'name': 'Guorun Wang'}, {'authorId': None, 'name': 'Qingqing Cao'}, {'authorId': '2146156715', 'name': 'Jun Yang'}, {'authorId': '48186778', 'name': 'Yaoru Sun'}]",['Tongji University'],['China'],2023-03
2303.14742,Baochang Ma,"Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang,
  Baochang Ma, Xiangang Li",Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The success of ChatGPT has recently attracted numerous efforts to replicate it, with instruction-tuning strategies being a key factor in achieving remarkable results. Instruction-tuning not only significantly enhances the model's performance and generalization but also makes the model's generated results more consistent with human speech patterns. However current research rarely studies the impact of different amounts of instruction data on model performance, especially in the real-world use cases. In this paper we explore the performance of large language models based on instruction tuning across different scales of instruction data. An evaluation dataset consisting of 12 major online use cases is constructed in the experiment. With Bloomz-7B1-mt as the base model, the results show that 1) merely increasing the amount of instruction data leads to continuous improvement in tasks such as open-ended generation, 2) in tasks such as math and code, the model performance curve remains quite flat while increasing data size. We further analyze the possible causes of these phenomena and propose potential future research directions such as effectively selecting high-quality training data, scaling base models and training methods specialized for hard tasks. We will release our training and evaluation datasets, as well as model checkpoints. ","[{'version': 'v1', 'created': 'Sun, 26 Mar 2023 14:49:37 GMT'}]",2023-03-28,"[['Ji', 'Yunjie', ''], ['Deng', 'Yong', ''], ['Gong', 'Yan', ''], ['Peng', 'Yiping', ''], ['Niu', 'Qiang', ''], ['Zhang', 'Lei', ''], ['Ma', 'Baochang', ''], ['Li', 'Xiangang', '']]",1,1,2023-03-26,1,8,1,2,1,1,8fc90497d9043fdf35e71302b7c2e79bb907144f,257766844.0,https://www.semanticscholar.org/paper/8fc90497d9043fdf35e71302b7c2e79bb907144f,arXiv.org,2023.0,48.0,40.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '24780166', 'name': 'Yunjie Ji'}, {'authorId': '2111213765', 'name': 'Yang Deng'}, {'authorId': '2211736448', 'name': 'Yan Gong'}, {'authorId': '2111015160', 'name': 'Yiping Peng'}, {'authorId': '2212854899', 'name': 'Qiang Niu'}, {'authorId': '1720539', 'name': 'L. Zhang'}, {'authorId': '152358488', 'name': 'Baochang Ma'}, {'authorId': '1898780', 'name': 'Xiangang Li'}]",['Peking University'],['China'],2023-03
2303.14956,Xuanfan Ni,"Xuanfan Ni, Piji Li and Huayang Li",Unified Text Structuralization with Instruction-tuned Language Models,"13 pages, 5 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text structuralization is one of the important fields of natural language processing (NLP) consists of information extraction (IE) and structure formalization. However, current studies of text structuralization suffer from a shortage of manually annotated high-quality datasets from different domains and languages, which require specialized professional knowledge. In addition, most IE methods are designed for a specific type of structured data, e.g., entities, relations, and events, making them hard to generalize to others. In this work, we propose a simple and efficient approach to instruct large language model (LLM) to extract a variety of structures from texts. More concretely, we add a prefix and a suffix instruction to indicate the desired IE task and structure type, respectively, before feeding the text into a LLM. Experiments on two LLMs show that this approach can enable language models to perform comparable with other state-of-the-art methods on datasets of a variety of languages and knowledge, and can generalize to other IE sub-tasks via changing the content of instruction. Another benefit of our approach is that it can help researchers to build datasets in low-source and domain-specific scenarios, e.g., fields in finance and law, with low cost. ","[{'version': 'v1', 'created': 'Mon, 27 Mar 2023 07:39:05 GMT'}, {'version': 'v2', 'created': 'Thu, 30 Mar 2023 13:41:54 GMT'}]",2023-03-31,"[['Ni', 'Xuanfan', ''], ['Li', 'Piji', ''], ['Li', 'Huayang', '']]",0,0,2023-03-27,2,3,1,0,0,0,a938fcc877d181e3d9a64e5de0a1421ef461bb58,257766560.0,https://www.semanticscholar.org/paper/a938fcc877d181e3d9a64e5de0a1421ef461bb58,arXiv.org,2023.0,53.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2212474955', 'name': 'Xuanfan Ni'}, {'authorId': '2193560', 'name': 'Piji Li'}]","['Nanjing University of Aeronautics and Astronautics', 'Nara Institute of Science and Technology']","['China', 'Japan']",2023-03
2303.15078,Ning Wu,"Ning Wu, Ming Gong, Linjun Shou, Shining Liang, Daxin Jiang",Large Language Models are Diverse Role-Players for Summarization Evaluation,NLPCC 2023,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Text summarization has a wide range of applications in many scenarios. The evaluation of the quality of the generated text is a complex problem. A big challenge to language evaluation is that there is a clear divergence between existing metrics and human evaluation. A document summary's quality can be assessed by human annotators on various criteria, both objective ones like grammar and correctness, and subjective ones like informativeness, succinctness, and appeal. Most of the automatic evaluation methods like BLUE/ROUGE may be not able to adequately capture the above dimensions. In this paper, we propose a new evaluation framework based on LLMs, which provides a comprehensive evaluation framework by comparing generated text and reference text from both objective and subjective aspects. First, we propose to model objective and subjective dimensions of generated text based on roleplayers prompting mechanism. Furthermore, we introduce a context-based prompting mechanism that is able to generate dynamic roleplayer profiles based on input context. Finally, we design a multi-roleplayer prompting technology based on batch prompting and integrate multiple outputs into the final evaluation results. Experimental results on three real datasets for summarization show that our model is highly competitive and has a very high consistency with human annotators. ","[{'version': 'v1', 'created': 'Mon, 27 Mar 2023 10:40:59 GMT'}, {'version': 'v2', 'created': 'Tue, 28 Mar 2023 15:25:19 GMT'}, {'version': 'v3', 'created': 'Tue, 19 Sep 2023 10:07:55 GMT'}]",2023-09-20,"[['Wu', 'Ning', ''], ['Gong', 'Ming', ''], ['Shou', 'Linjun', ''], ['Liang', 'Shining', ''], ['Jiang', 'Daxin', '']]",0,0,2023-03-27,3,5,1,0,0,0,4a4f81543a424ab2494cfb0fb49e0e47329b5cb6,257767249.0,https://www.semanticscholar.org/paper/4a4f81543a424ab2494cfb0fb49e0e47329b5cb6,Natural Language Processing and Chinese Computing,2023.0,25.0,11.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2068345080', 'name': 'Ning Wu'}, {'authorId': '50175330', 'name': 'Ming Gong'}, {'authorId': '24962156', 'name': 'Linjun Shou'}, {'authorId': '31314451', 'name': 'Shining Liang'}, {'authorId': '71790825', 'name': 'Daxin Jiang'}]",['Microsoft'],['China'],2023-03
2303.15587,Wenshi Gu,Wenshi Gu,Linguistically Informed ChatGPT Prompts to Enhance Japanese-Chinese Machine Translation: A Case Study on Attributive Clauses,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the field of Japanese-Chinese translation linguistics, the issue of correctly translating attributive clauses has persistently proven to be challenging. Present-day machine translation tools often fail to accurately translate attributive clauses from Japanese to Chinese. In light of this, this paper investigates the linguistic problem underlying such difficulties, namely how does the semantic role of the modified noun affect the selection of translation patterns for attributive clauses, from a linguistic perspective. To ad-dress these difficulties, a pre-edit scheme is proposed, which aims to enhance the accuracy of translation. Furthermore, we propose a novel two-step prompt strategy, which combines this pre-edit scheme with ChatGPT, currently the most widely used large language model. This prompt strategy is capable of optimizing translation input in zero-shot scenarios and has been demonstrated to improve the average translation accuracy score by over 35%. ","[{'version': 'v1', 'created': 'Mon, 27 Mar 2023 20:33:40 GMT'}]",2023-03-29,"[['Gu', 'Wenshi', '']]",1,1,2023-03-27,1,1,1,1,0,1,3a180e993a9e25d6b6a02ce8bbb6510755df47bc,257771853.0,https://www.semanticscholar.org/paper/3a180e993a9e25d6b6a02ce8bbb6510755df47bc,arXiv.org,2023.0,9.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '2212648998', 'name': 'Wenshi Gu'}]",['Beihang University'],['China'],2023-03
2303.15772,Rishi Bommasani,"Rishi Bommasani and Dilara Soylu and Thomas I. Liao and Kathleen A.
  Creel and Percy Liang",Ecosystem Graphs: The Social Footprint of Foundation Models,"Authored by the Center for Research on Foundation Models (CRFM) at
  the Stanford Institute for Human-Centered Artificial Intelligence (HAI).
  Ecosystem Graphs available at https://crfm.stanford.edu/ecosystem-graphs/",,,,cs.LG cs.AI cs.CY,http://creativecommons.org/licenses/by/4.0/,"  Foundation models (e.g. ChatGPT, StableDiffusion) pervasively influence society, warranting immediate social attention. While the models themselves garner much attention, to accurately characterize their impact, we must consider the broader sociotechnical ecosystem. We propose Ecosystem Graphs as a documentation framework to transparently centralize knowledge of this ecosystem. Ecosystem Graphs is composed of assets (datasets, models, applications) linked together by dependencies that indicate technical (e.g. how Bing relies on GPT-4) and social (e.g. how Microsoft relies on OpenAI) relationships. To supplement the graph structure, each asset is further enriched with fine-grained metadata (e.g. the license or training emissions). We document the ecosystem extensively at https://crfm.stanford.edu/ecosystem-graphs/. As of March 16, 2023, we annotate 262 assets (64 datasets, 128 models, 70 applications) from 63 organizations linked by 356 dependencies. We show Ecosystem Graphs functions as a powerful abstraction and interface for achieving the minimum transparency required to address myriad use cases. Therefore, we envision Ecosystem Graphs will be a community-maintained resource that provides value to stakeholders spanning AI researchers, industry professionals, social scientists, auditors and policymakers. ","[{'version': 'v1', 'created': 'Tue, 28 Mar 2023 07:18:29 GMT'}]",2023-03-29,"[['Bommasani', 'Rishi', ''], ['Soylu', 'Dilara', ''], ['Liao', 'Thomas I.', ''], ['Creel', 'Kathleen A.', ''], ['Liang', 'Percy', '']]",1,1,2023-03-28,1,5,3,2,0,2,8ed7c9ba7cdb33e816135381ca502ace649c7985,257771875.0,https://www.semanticscholar.org/paper/8ed7c9ba7cdb33e816135381ca502ace649c7985,arXiv.org,2023.0,110.0,9.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '150272855', 'name': 'Rishi Bommasani'}, {'authorId': '1914569491', 'name': 'Dilara Soylu'}, {'authorId': '145636331', 'name': 'Thomas Liao'}, {'authorId': '1383066965', 'name': 'Kathleen A. Creel'}, {'authorId': '145419642', 'name': 'Percy Liang'}]","['Northeastern University', 'Stanford University']","['China', 'United States']",2023-03
2303.16129,Hongyang Du,"Minrui Xu, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Shiwen
  Mao, Zhu Han, Abbas Jamalipour, Dong In Kim, Xuemin (Sherman) Shen, Victor C.
  M. Leung, and H. Vincent Poor",Unleashing the Power of Edge-Cloud Generative AI in Mobile Networks: A Survey of AIGC Services,,,,,cs.NI,http://creativecommons.org/licenses/by/4.0/,"  Artificial Intelligence-Generated Content (AIGC) is an automated method for generating, manipulating, and modifying valuable and diverse data using AI algorithms creatively. This survey paper focuses on the deployment of AIGC applications, e.g., ChatGPT and Dall-E, at mobile edge networks, namely mobile AIGC networks, that provide personalized and customized AIGC services in real time while maintaining user privacy. We begin by introducing the background and fundamentals of generative models and the lifecycle of AIGC services at mobile AIGC networks, which includes data collection, training, finetuning, inference, and product management. We then discuss the collaborative cloud-edge-mobile infrastructure and technologies required to support AIGC services and enable users to access AIGC at mobile edge networks. Furthermore, we explore AIGCdriven creative applications and use cases for mobile AIGC networks. Additionally, we discuss the implementation, security, and privacy challenges of deploying mobile AIGC networks. Finally, we highlight some future research directions and open issues for the full realization of mobile AIGC networks. ","[{'version': 'v1', 'created': 'Tue, 28 Mar 2023 16:52:05 GMT'}, {'version': 'v2', 'created': 'Wed, 29 Mar 2023 16:21:20 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Jul 2023 05:27:51 GMT'}]",2023-07-20,"[['Xu', 'Minrui', '', 'Sherman'], ['Du', 'Hongyang', '', 'Sherman'], ['Niyato', 'Dusit', '', 'Sherman'], ['Kang', 'Jiawen', '', 'Sherman'], ['Xiong', 'Zehui', '', 'Sherman'], ['Mao', 'Shiwen', '', 'Sherman'], ['Han', 'Zhu', '', 'Sherman'], ['Jamalipour', 'Abbas', '', 'Sherman'], ['Kim', 'Dong In', '', 'Sherman'], ['Xuemin', '', '', 'Sherman'], ['Shen', '', ''], ['Leung', 'Victor C. M.', ''], ['Poor', 'H. Vincent', '']]",1,1,2023-03-28,3,13,1,1,0,1,f8e77bd3d573d0daee0744443c65c40e3b5dc10f,257771737.0,https://www.semanticscholar.org/paper/f8e77bd3d573d0daee0744443c65c40e3b5dc10f,arXiv.org,2023.0,306.0,23.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1454018677', 'name': 'Minrui Xu'}, {'authorId': '2175043468', 'name': 'Hongyang Du'}, {'authorId': '1713586', 'name': 'D. Niyato'}, {'authorId': '145993626', 'name': 'Jiawen Kang'}, {'authorId': '2943819', 'name': 'Zehui Xiong'}, {'authorId': '2056609621', 'name': 'Shiwen Mao'}, {'authorId': '2155438325', 'name': 'Zhu Han'}, {'authorId': '1725427', 'name': 'A. Jamalipour'}, {'authorId': '2155182631', 'name': 'Dong In Kim'}, {'authorId': '2158524414', 'name': 'X. Shen'}, {'authorId': '2144885207', 'name': 'Victor C. M. Leung'}, {'authorId': '2113229282', 'name': 'H. Poor'}]","['University of Houston', 'University of Waterloo', 'University of British Columbia', 'Singapore University of Technology and Design', 'University of Sydney', 'Princeton University', 'Auburn University', 'Shenzhen University', 'Nanyang Technological University', 'Guangdong University of Technology', 'Kyung Hee University', 'Sungkyunkwan University']","['Singapore', 'Canada', 'South Korea', 'United States', 'China', 'Australia']",2023-03
2303.16199,Renrui Zhang,"Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei
  Hu, Shilin Yan, Pan Lu, Hongsheng Li, Yu Qiao",LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention,Code is available at https://github.com/OpenGVLab/LLaMA-Adapter,,,,cs.CV cs.AI cs.CL cs.LG cs.MM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the word tokens at higher transformer layers. Then, a zero-initialized attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter can generate high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, our approach can be simply extended to multi-modal instructions for learning image-conditioned LLaMA model, which achieves superior reasoning performance on ScienceQA and COCO Caption benchmarks. Furthermore, we also evaluate the zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on traditional vision and language tasks, demonstrating the superior generalization capacity of our approach. Code is released at https://github.com/OpenGVLab/LLaMA-Adapter. ","[{'version': 'v1', 'created': 'Tue, 28 Mar 2023 17:59:12 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Jun 2023 17:31:32 GMT'}]",2023-06-16,"[['Zhang', 'Renrui', ''], ['Han', 'Jiaming', ''], ['Liu', 'Chris', ''], ['Gao', 'Peng', ''], ['Zhou', 'Aojun', ''], ['Hu', 'Xiangfei', ''], ['Yan', 'Shilin', ''], ['Lu', 'Pan', ''], ['Li', 'Hongsheng', ''], ['Qiao', 'Yu', '']]",0,0,2023-03-28,2,10,5,2,1,1,a757999ed260d7bc45484dc6b4456bf33fe6f679,257771811.0,https://www.semanticscholar.org/paper/a757999ed260d7bc45484dc6b4456bf33fe6f679,arXiv.org,2023.0,101.0,157.0,12.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2115713503', 'name': 'Renrui Zhang'}, {'authorId': '150147382', 'name': 'Jiaming Han'}, {'authorId': '9548994', 'name': 'Aojun Zhou'}, {'authorId': '2193028455', 'name': 'Xiangfei Hu'}, {'authorId': '2210554979', 'name': 'Shilin Yan'}, {'authorId': '2887562', 'name': 'Pan Lu'}, {'authorId': '47893312', 'name': 'Hongsheng Li'}, {'authorId': '144740494', 'name': 'Peng Gao'}, {'authorId': '2059129841', 'name': 'Y. Qiao'}]","['Shanghai Artificial Intelligence Laboratory', 'University of California, Los Angeles', 'Chinese University of Hong Kong']","['China', 'United States']",2023-03
2303.16421,Ning Bian,"Ning Bian, Xianpei Han, Le Sun, Hongyu Lin, Yaojie Lu, Ben He",ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) such as ChatGPT and GPT-4 have made significant progress in NLP. However, their ability to memorize, represent, and leverage commonsense knowledge has been a well-known pain point for LLMs. It remains unclear that: (1) Can GPTs effectively answer commonsense questions? (2) Are GPTs knowledgeable in commonsense? (3) Are GPTs aware of the underlying commonsense knowledge for answering a specific question? (4) Can GPTs effectively leverage commonsense for answering questions? To evaluate the above commonsense problems, we conduct a series of experiments to evaluate ChatGPT's commonsense abilities, and the experimental results show that: (1) GPTs can achieve good QA accuracy in commonsense tasks, while they still struggle with certain types of knowledge. (2) ChatGPT is knowledgeable, and can accurately generate most of the commonsense knowledge using knowledge prompts. (3) Despite its knowledge, ChatGPT is an inexperienced commonsense problem solver, which cannot precisely identify the needed commonsense knowledge for answering a specific question, i.e., ChatGPT does not precisely know what commonsense knowledge is required to answer a question. The above findings raise the need to investigate better mechanisms for utilizing commonsense knowledge in LLMs, such as instruction following, better commonsense guidance, etc. ","[{'version': 'v1', 'created': 'Wed, 29 Mar 2023 03:05:43 GMT'}]",2023-03-30,"[['Bian', 'Ning', ''], ['Han', 'Xianpei', ''], ['Sun', 'Le', ''], ['Lin', 'Hongyu', ''], ['Lu', 'Yaojie', ''], ['He', 'Ben', '']]",1,1,2023-03-29,1,6,1,2,0,2,4d7571441f507f39133209e8afa7ad088da2199c,257804619.0,https://www.semanticscholar.org/paper/4d7571441f507f39133209e8afa7ad088da2199c,arXiv.org,2023.0,39.0,32.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Education', 'source': 's2-fos-model'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '72657087', 'name': 'Ning Bian'}, {'authorId': '2118233348', 'name': 'Xianpei Han'}, {'authorId': '2110832778', 'name': 'Le Sun'}, {'authorId': '2116455765', 'name': 'Hongyu Lin'}, {'authorId': '1831434', 'name': 'Yaojie Lu'}, {'authorId': '2046814249', 'name': 'Ben He'}]","['University of Chinese Academy of Sciences', 'Chinese Academy of Sciences']",['China'],2023-03
2303.16563,Zongqing Lu,"Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao
  Dong, Zongqing Lu",Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks,19 pages,,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We study building a multi-task agent in Minecraft. Without human demonstrations, solving long-horizon tasks in this open-ended environment with reinforcement learning (RL) is extremely sample inefficient. To tackle the challenge, we decompose solving Minecraft tasks into learning basic skills and planning over the skills. We propose three types of fine-grained basic skills in Minecraft, and use RL with intrinsic rewards to accomplish basic skills with high success rates. For skill planning, we use Large Language Models to find the relationships between skills and build a skill graph in advance. When the agent is solving a task, our skill search algorithm walks on the skill graph and generates the proper skill plans for the agent. In experiments, our method accomplishes 24 diverse Minecraft tasks, where many tasks require sequentially executing for more than 10 skills. Our method outperforms baselines in most tasks by a large margin. The project's website and code can be found at https://sites.google.com/view/plan4mc. ","[{'version': 'v1', 'created': 'Wed, 29 Mar 2023 09:45:50 GMT'}]",2023-03-30,"[['Yuan', 'Haoqi', ''], ['Zhang', 'Chi', ''], ['Wang', 'Hongcheng', ''], ['Xie', 'Feiyang', ''], ['Cai', 'Penglin', ''], ['Dong', 'Hao', ''], ['Lu', 'Zongqing', '']]",0,0,2023-03-29,1,7,2,0,0,0,2d1ad38d83a5b8a6bb47630972ada82b62ea4aac,257805102.0,https://www.semanticscholar.org/paper/2d1ad38d83a5b8a6bb47630972ada82b62ea4aac,arXiv.org,2023.0,32.0,17.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1429192914', 'name': 'Haoqi Yuan'}, {'authorId': '2115811509', 'name': 'Chi Zhang'}, {'authorId': '2157834018', 'name': 'Hongchen Wang'}, {'authorId': '2152182031', 'name': 'Feiyang Xie'}, {'authorId': '2139688739', 'name': 'Penglin Cai'}, {'authorId': '47866340', 'name': 'Hao Dong'}, {'authorId': '2265693', 'name': 'Zongqing Lu'}]","['Peking University', 'Beijing Academy of Artificial Intelligence']",['China'],2023-03
2303.16854,Xingwei He,"Xingwei He, Zhenghao Lin, Yeyun Gong, A-Long Jin, Hang Zhang, Chen
  Lin, Jian Jiao, Siu Ming Yiu, Nan Duan, Weizhu Chen",AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many natural language processing (NLP) tasks rely on labeled data to train machine learning models to achieve high performance. However, data annotation can be a time-consuming and expensive process, especially when the task involves a large amount of data or requires specialized domains. Recently, GPT-3.5 series models have demonstrated remarkable few-shot and zero-shot ability across various NLP tasks. In this paper, we first claim that large language models (LLMs), such as GPT-3.5, can serve as an excellent crowdsourced annotator by providing them with sufficient guidance and demonstrated examples. To make LLMs to be better annotators, we propose a two-step approach, 'explain-then-annotate'. To be more precise, we begin by creating prompts for every demonstrated example, which we subsequently utilize to prompt a LLM to provide an explanation for why the specific ground truth answer/label was chosen for that particular example. Following this, we construct the few-shot chain-of-thought prompt with the self-generated explanation and employ it to annotate the unlabeled data. We conduct experiments on three tasks, including user input and keyword relevance assessment, BoolQ and WiC. The annotation results from GPT-3.5 surpasses those from crowdsourced annotation for user input and keyword relevance assessment. Additionally, for the other two tasks, GPT-3.5 achieves results that are comparable to those obtained through crowdsourced annotation. ","[{'version': 'v1', 'created': 'Wed, 29 Mar 2023 17:03:21 GMT'}]",2023-03-30,"[['He', 'Xingwei', ''], ['Lin', 'Zhenghao', ''], ['Gong', 'Yeyun', ''], ['Jin', 'A-Long', ''], ['Zhang', 'Hang', ''], ['Lin', 'Chen', ''], ['Jiao', 'Jian', ''], ['Yiu', 'Siu Ming', ''], ['Duan', 'Nan', ''], ['Chen', 'Weizhu', '']]",0,1,2023-03-29,1,10,1,1,0,1,70da4fb798a86cbe8cad96c27ced0415885bbd9d,257805087.0,https://www.semanticscholar.org/paper/70da4fb798a86cbe8cad96c27ced0415885bbd9d,arXiv.org,2023.0,32.0,35.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1754500', 'name': 'Xingwei He'}, {'authorId': '31113759', 'name': 'Zheng-Wen Lin'}, {'authorId': '2171182', 'name': 'Yeyun Gong'}, {'authorId': '15796861', 'name': 'Alex Jin'}, {'authorId': '2119077859', 'name': 'Hang Zhang'}, {'authorId': '2186278677', 'name': 'Chen Lin'}, {'authorId': '2143968416', 'name': 'Jian Jiao'}, {'authorId': '2184000509', 'name': 'S. Yiu'}, {'authorId': '46429989', 'name': 'Nan Duan'}, {'authorId': '2109136147', 'name': 'Weizhu Chen'}]","['Xiamen University', 'Microsoft Research Asia, 4 Microsoft', 'Microsoft', 'University of Hong Kong']","['China', 'United States', 'Hong Kong']",2023-03
2303.16894,Ziyu Guo,"Zoey Guo, Yiwen Tang, Ray Zhang, Dong Wang, Zhigang Wang, Bin Zhao,
  Xuelong Li",ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance,Accepted by ICCV 2023,,,,cs.CV cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Understanding 3D scenes from multi-view inputs has been proven to alleviate the view discrepancy issue in 3D visual grounding. However, existing methods normally neglect the view cues embedded in the text modality and fail to weigh the relative importance of different views. In this paper, we propose ViewRefer, a multi-view framework for 3D visual grounding exploring how to grasp the view knowledge from both text and 3D modalities. For the text branch, ViewRefer leverages the diverse linguistic knowledge of large-scale language models, e.g., GPT, to expand a single grounding text to multiple geometry-consistent descriptions. Meanwhile, in the 3D modality, a transformer fusion module with inter-view attention is introduced to boost the interaction of objects across views. On top of that, we further present a set of learnable multi-view prototypes, which memorize scene-agnostic knowledge for different views, and enhance the framework from two perspectives: a view-guided attention module for more robust text features, and a view-guided scoring strategy during the final prediction. With our designed paradigm, ViewRefer achieves superior performance on three benchmarks and surpasses the second-best by +2.8%, +1.5%, and +1.35% on Sr3D, Nr3D, and ScanRefer. ","[{'version': 'v1', 'created': 'Wed, 29 Mar 2023 17:59:10 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Apr 2023 17:09:05 GMT'}, {'version': 'v3', 'created': 'Thu, 24 Aug 2023 18:45:43 GMT'}]",2023-08-28,"[['Guo', 'Zoey', ''], ['Tang', 'Yiwen', ''], ['Zhang', 'Ray', ''], ['Wang', 'Dong', ''], ['Wang', 'Zhigang', ''], ['Zhao', 'Bin', ''], ['Li', 'Xuelong', '']]",0,1,2023-03-29,3,7,3,0,0,0,380a4d6873736427886cd00ed9c137abe97b8da9,257804752.0,https://www.semanticscholar.org/paper/380a4d6873736427886cd00ed9c137abe97b8da9,arXiv.org,2023.0,66.0,8.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '145490494', 'name': 'Ziyu Guo'}, {'authorId': '2193267071', 'name': 'Yiwen Tang'}, {'authorId': '2115713503', 'name': 'Renrui Zhang'}, {'authorId': '2152692487', 'name': 'Dong Wang'}, {'authorId': '2145078379', 'name': 'Zhigang Wang'}, {'authorId': '143946808', 'name': 'Bin Zhao'}, {'authorId': '2192821449', 'name': 'Xuelong Li'}]","['Shanghai Artificial Intelligence Laboratory', 'Chinese University of Hong Kong', 'Northwestern Polytechnical University']",['China'],2023-03
2303.17466,Yong Cao,"Yong Cao, Li Zhou, Seolhwa Lee, Laura Cabello, Min Chen, Daniel
  Hershcovich",Assessing Cross-Cultural Alignment between ChatGPT and Human Societies: An Empirical Study,C3NLP@EACL 2023,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The recent release of ChatGPT has garnered widespread recognition for its exceptional ability to generate human-like responses in dialogue. Given its usage by users from various nations and its training on a vast multilingual corpus that incorporates diverse cultural and societal norms, it is crucial to evaluate its effectiveness in cultural adaptation. In this paper, we investigate the underlying cultural background of ChatGPT by analyzing its responses to questions designed to quantify human cultural differences. Our findings suggest that, when prompted with American context, ChatGPT exhibits a strong alignment with American culture, but it adapts less effectively to other cultural contexts. Furthermore, by using different prompts to probe the model, we show that English prompts reduce the variance in model responses, flattening out cultural differences and biasing them towards American culture. This study provides valuable insights into the cultural implications of ChatGPT and highlights the necessity of greater diversity and cultural awareness in language technologies. ","[{'version': 'v1', 'created': 'Thu, 30 Mar 2023 15:43:39 GMT'}, {'version': 'v2', 'created': 'Fri, 31 Mar 2023 15:02:48 GMT'}]",2023-04-03,"[['Cao', 'Yong', ''], ['Zhou', 'Li', ''], ['Lee', 'Seolhwa', ''], ['Cabello', 'Laura', ''], ['Chen', 'Min', ''], ['Hershcovich', 'Daniel', '']]",1,1,2023-03-30,2,6,1,1,0,1,ca94c924d8a3b77a2bd5b16ffc03b8723bce9c1f,257833897.0,https://www.semanticscholar.org/paper/ca94c924d8a3b77a2bd5b16ffc03b8723bce9c1f,C3NLP,2023.0,45.0,21.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Sociology', 'source': 's2-fos-model'}]","[{'authorId': '2112402733', 'name': 'Yong Cao'}, {'authorId': '2116635488', 'name': 'Li Zhou'}, {'authorId': '2132579230', 'name': 'Seolhwa Lee'}, {'authorId': '2092471782', 'name': 'Laura Cabello'}, {'authorId': '2108557855', 'name': 'Min Chen'}, {'authorId': '2064295987', 'name': 'Daniel Hershcovich'}]","['South China University of Technology', 'University of Copenhagen', 'Xidian University', 'Technical University of Darmstadt', 'Huazhong University of Science and Technology']","['China', 'Germany', 'Denmark']",2023-03
2303.18184,Kai Huang,"Kai Huang, Zhengzi Xu, Su Yang, Hongyu Sun, Xuejun Li, Zheng Yan,
  Yuqing Zhang",A Survey on Automated Program Repair Techniques,This paper's earlier version was submitted to CSUR in August 2022,,,,cs.SE,http://creativecommons.org/licenses/by/4.0/,"  With the rapid development and large-scale popularity of program software, modern society increasingly relies on software systems. However, the problems exposed by software have also come to the fore. Software defect has become an important factor troubling developers. In this context, Automated Program Repair (APR) techniques have emerged, aiming to automatically fix software defect problems and reduce manual debugging work. In particular, benefiting from the advances in deep learning, numerous learning-based APR techniques have emerged in recent years, which also bring new opportunities for APR research. To give researchers a quick overview of APR techniques' complete development and future opportunities, we revisit the evolution of APR techniques and discuss in depth the latest advances in APR research. In this paper, the development of APR techniques is introduced in terms of four different patch generation schemes: search-based, constraint-based, template-based, and learning-based. Moreover, we propose a uniform set of criteria to review and compare each APR tool, summarize the advantages and disadvantages of APR techniques, and discuss the current state of APR development. Furthermore, we introduce the research on the related technical areas of APR that have also provided a strong motivation to advance APR development. Finally, we analyze current challenges and future directions, especially highlighting the critical opportunities that large language models bring to APR research. ","[{'version': 'v1', 'created': 'Fri, 31 Mar 2023 16:28:37 GMT'}, {'version': 'v2', 'created': 'Wed, 5 Apr 2023 14:39:04 GMT'}, {'version': 'v3', 'created': 'Sat, 13 May 2023 14:15:26 GMT'}]",2023-05-16,"[['Huang', 'Kai', ''], ['Xu', 'Zhengzi', ''], ['Yang', 'Su', ''], ['Sun', 'Hongyu', ''], ['Li', 'Xuejun', ''], ['Yan', 'Zheng', ''], ['Zhang', 'Yuqing', '']]",0,0,2023-03-31,3,7,1,0,0,0,11f0edf4a08144fdfdb9560014c0971f2f8680f3,257900782.0,https://www.semanticscholar.org/paper/11f0edf4a08144fdfdb9560014c0971f2f8680f3,arXiv.org,2023.0,161.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2112769603', 'name': 'Kai Huang'}, {'authorId': '3430657', 'name': 'Zhengzi Xu'}, {'authorId': '50591872', 'name': 'Su Yang'}, {'authorId': '35365620', 'name': 'Hongyu Sun'}, {'authorId': '2108782130', 'name': 'Xuejun Li'}, {'authorId': '2152531701', 'name': 'Zheng Yan'}, {'authorId': '2108445005', 'name': 'Yuqing Zhang'}]","['China and Hainan University, Haikou, China.', 'University of Chinese Academy of Sciences', 'Xidian University', 'Aalto University', 'Xuejun Li,', 'Hainan University', 'Nanyang Technological University']","['China', 'Singapore', 'Finland']",2023-03
2304.00723,Yi Chen,"Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, Ruifeng Xu",Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Evaluating the quality of generated text is a challenging task in NLP, due to the inherent complexity and diversity of text. Recently, large language models (LLMs) have garnered significant attention due to their impressive performance in various tasks. Therefore, we present this paper to investigate the effectiveness of LLMs, especially ChatGPT, and explore ways to optimize their use in assessing text quality. We compared three kinds of reference-free evaluation methods. The experimental results prove that ChatGPT is capable of evaluating text quality effectively from various perspectives without reference and demonstrates superior performance than most existing automatic metrics. In particular, the Explicit Score, which utilizes ChatGPT to generate a numeric score measuring text quality, is the most effective and reliable method among the three exploited approaches. However, directly comparing the quality of two texts may lead to suboptimal results. We believe this paper will provide valuable insights for evaluating text quality with LLMs and have released the used data. ","[{'version': 'v1', 'created': 'Mon, 3 Apr 2023 05:29:58 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Apr 2023 12:46:17 GMT'}, {'version': 'v3', 'created': 'Mon, 18 Sep 2023 03:52:18 GMT'}]",2023-09-19,"[['Chen', 'Yi', ''], ['Wang', 'Rui', ''], ['Jiang', 'Haiyun', ''], ['Shi', 'Shuming', ''], ['Xu', 'Ruifeng', '']]",1,1,2023-04-03,3,5,1,1,0,1,fa7805c7ad42610b89d07353cb3600f3ecaf2c2f,261881796.0,https://www.semanticscholar.org/paper/fa7805c7ad42610b89d07353cb3600f3ecaf2c2f,,2023.0,40.0,2.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2165302640', 'name': 'Yi Chen'}, {'authorId': '2151036536', 'name': 'Rui Wang'}, {'authorId': '48579460', 'name': 'Haiyun Jiang'}, {'authorId': '2239383048', 'name': 'Shuming Shi'}, {'authorId': '1753529', 'name': 'Ruifeng Xu'}]","['Harbin Institute of Technology', 'Peng Cheng Laboratory', 'https://github.com/MilkWhite/LLMs_for_ Reference_Free_Text_Quality_Evaluation', 'Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies']",['China'],2023-04
2304.01083,Quanshi Zhang,"Wen Shen, Lei Cheng, Yuxiao Yang, Mingjie Li, Quanshi Zhang",Can the Inference Logic of Large Language Models be Disentangled into Symbolic Concepts?,,,,,cs.CL cs.AI cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we explain the inference logic of large language models (LLMs) as a set of symbolic concepts. Many recent studies have discovered that traditional DNNs usually encode sparse symbolic concepts. However, because an LLM has much more parameters than traditional DNNs, whether the LLM also encodes sparse symbolic concepts is still an open problem. Therefore, in this paper, we propose to disentangle the inference score of LLMs for dialogue tasks into a small number of symbolic concepts. We verify that we can use those sparse concepts to well estimate all inference scores of the LLM on all arbitrarily masking states of the input sentence. We also evaluate the transferability of concepts encoded by an LLM and verify that symbolic concepts usually exhibit high transferability across similar input sentences. More crucially, those symbolic concepts can be used to explain the exact reasons accountable for the LLM's prediction errors. ","[{'version': 'v1', 'created': 'Mon, 3 Apr 2023 15:39:35 GMT'}]",2023-04-04,"[['Shen', 'Wen', ''], ['Cheng', 'Lei', ''], ['Yang', 'Yuxiao', ''], ['Li', 'Mingjie', ''], ['Zhang', 'Quanshi', '']]",0,0,2023-04-03,1,5,4,0,0,0,b7ffb8a43545938cb073c686202b4a39c3a78b4d,257913345.0,https://www.semanticscholar.org/paper/b7ffb8a43545938cb073c686202b4a39c3a78b4d,arXiv.org,2023.0,20.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2149662127', 'name': 'Wen Shen'}, {'authorId': '2213709625', 'name': 'Lei Cheng'}, {'authorId': '2163283977', 'name': 'Yuxiao Yang'}, {'authorId': '1720849297', 'name': 'Mingjie Li'}, {'authorId': '22063226', 'name': 'Quanshi Zhang'}]",['Shanghai Jiao Tong University'],['China'],2023-04
2304.01089,Zhihang Yuan,"Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang
  Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, Bingzhe Wu",RPTQ: Reorder-based Post-training Quantization for Large Language Models,18 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large-scale language models (LLMs) have demonstrated impressive performance, but their deployment presents challenges due to their significant memory usage. This issue can be alleviated through quantization. In this paper, we identify that the challenge in quantizing activations in LLMs arises from varying ranges across channels, rather than solely the presence of outliers. To address this challenge, we introduce a quantization method called RPTQ, which utilizes a reorder-based approach. By rearranging the channels and quantizing them in clusters, RPTQ effectively mitigates the impact of range differences between channels. To minimize the overhead of the reorder operation, we fuse it into the layer norm operation and weights in linear layers. In our experiments, RPTQ achieved a significant breakthrough by utilizing 3-bit activation in LLMs for the first time, resulting in a substantial reduction in memory usage. For instance, quantizing OPT-175b can lead to a memory consumption reduction of up to 80%. ","[{'version': 'v1', 'created': 'Mon, 3 Apr 2023 15:46:15 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Apr 2023 15:51:17 GMT'}, {'version': 'v3', 'created': 'Tue, 25 Apr 2023 06:29:00 GMT'}, {'version': 'v4', 'created': 'Wed, 17 May 2023 10:07:33 GMT'}]",2023-05-18,"[['Yuan', 'Zhihang', ''], ['Niu', 'Lin', ''], ['Liu', 'Jiawei', ''], ['Liu', 'Wenyu', ''], ['Wang', 'Xinggang', ''], ['Shang', 'Yuzhang', ''], ['Sun', 'Guangyu', ''], ['Wu', 'Qiang', ''], ['Wu', 'Jiaxiang', ''], ['Wu', 'Bingzhe', '']]",0,0,2023-04-03,4,10,1,1,1,0,2a44c6b7f291f625314a82ba3131e605009fd533,257913374.0,https://www.semanticscholar.org/paper/2a44c6b7f291f625314a82ba3131e605009fd533,arXiv.org,2023.0,44.0,18.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '50314671', 'name': 'Zhihang Yuan'}, {'authorId': '2195695596', 'name': 'Lin Niu'}, {'authorId': '2085077', 'name': 'Jia-Wen Liu'}, {'authorId': '2109194747', 'name': 'Wenyu Liu'}, {'authorId': '2443233', 'name': 'Xinggang Wang'}, {'authorId': '2125633957', 'name': 'Yuzhang Shang'}, {'authorId': '2113638448', 'name': 'Guangyu Sun'}, {'authorId': '2152359101', 'name': 'Qiang Wu'}, {'authorId': '2109159128', 'name': 'Jiaxiang Wu'}, {'authorId': '27055880', 'name': 'Bingzhe Wu'}]","['Hanoi Open University', 'Huazhong University of Science and Technology', 'Tencent', 'Peking University']","['China', 'Vietnam']",2023-04
2304.01097,Sheng Wang,"Honglin Xiong, Sheng Wang, Yitao Zhu, Zihao Zhao, Yuxiao Liu, Linlin
  Huang, Qian Wang, Dinggang Shen",DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The recent progress of large language models (LLMs), including ChatGPT and GPT-4, in comprehending and responding to human instructions has been remarkable. Nevertheless, these models typically perform better in English and have not been explicitly trained for the medical domain, resulting in suboptimal precision in diagnoses, drug recommendations, and other medical advice. Additionally, training and deploying a dialogue model is still believed to be impossible for hospitals, hindering the promotion of LLMs. To tackle these challenges, we have collected databases of medical dialogues in Chinese with ChatGPT's help and adopted several techniques to train an easy-deploy LLM. Remarkably, we were able to fine-tune the ChatGLM-6B on a single A100 80G in 13 hours, which means having a healthcare-purpose LLM can be very affordable. DoctorGLM is currently an early-stage engineering attempt and contain various mistakes. We are sharing it with the broader community to invite feedback and suggestions to improve its healthcare-focused capabilities: https://github.com/xionghonglin/DoctorGLM. ","[{'version': 'v1', 'created': 'Mon, 3 Apr 2023 15:57:51 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Apr 2023 17:06:29 GMT'}]",2023-04-18,"[['Xiong', 'Honglin', ''], ['Wang', 'Sheng', ''], ['Zhu', 'Yitao', ''], ['Zhao', 'Zihao', ''], ['Liu', 'Yuxiao', ''], ['Huang', 'Linlin', ''], ['Wang', 'Qian', ''], ['Shen', 'Dinggang', '']]",1,1,2023-04-03,2,8,1,3,1,2,bce55193d9a887ad00774a9134df08cd521a85ae,257912795.0,https://www.semanticscholar.org/paper/bce55193d9a887ad00774a9134df08cd521a85ae,arXiv.org,2023.0,16.0,38.0,6.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Medicine', 'source': 's2-fos-model'}]","[{'authorId': '2004876731', 'name': 'Honglin Xiong'}, {'authorId': '2151487856', 'name': 'Sheng Wang'}, {'authorId': '2212244316', 'name': 'Yitao Zhu'}, {'authorId': '2156163661', 'name': 'Zihao Zhao'}, {'authorId': '2213045865', 'name': 'Yuxiao Liu'}, {'authorId': '2215211443', 'name': 'Linlin Huang'}, {'authorId': '2215061296', 'name': 'Qian Wang'}, {'authorId': '2150038187', 'name': 'Dinggang Shen'}]","['United Imaging Intelligence', 'Shanghai Jiao Tong University', 'Fudan University', 'ShanghaiTech University']",['China'],2023-04
2304.01196,Canwen Xu,Canwen Xu and Daya Guo and Nan Duan and Julian McAuley,Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data,Baize v2,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Chat models, such as ChatGPT, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field. We propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself. Subsequently, we employ parameter-efficient tuning to enhance LLaMA, an open-source large language model. The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. Furthermore, we propose a new technique called Self-Distill with Feedback, to further improve the performance of the Baize models with feedback from ChatGPT. The Baize models and data are released for research purposes only at https://github.com/project-baize/baize-chatbot. An online demo is also available at https://huggingface.co/spaces/project-baize/chat-with-baize. ","[{'version': 'v1', 'created': 'Mon, 3 Apr 2023 17:59:09 GMT'}, {'version': 'v2', 'created': 'Tue, 4 Apr 2023 08:34:16 GMT'}, {'version': 'v3', 'created': 'Tue, 23 May 2023 19:40:03 GMT'}]",2023-05-25,"[['Xu', 'Canwen', ''], ['Guo', 'Daya', ''], ['Duan', 'Nan', ''], ['McAuley', 'Julian', '']]",1,1,2023-04-03,3,4,2,2,1,1,42e741e0be43954ae684d14333e4074f4d0ae961,257912848.0,https://www.semanticscholar.org/paper/42e741e0be43954ae684d14333e4074f4d0ae961,arXiv.org,2023.0,26.0,103.0,12.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '66247317', 'name': 'Canwen Xu'}, {'authorId': '51223794', 'name': 'Daya Guo'}, {'authorId': '46429989', 'name': 'Nan Duan'}, {'authorId': '35660011', 'name': 'Julian McAuley'}]","['Sun Yat-sen University', 'Microsoft', 'University of California, San Diego']","['China', 'United States']",2023-04
2304.01433,Cliff Young,"Norman P. Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan,
  Lifeng Nai, Nishant Patil, Suvinay Subramanian, Andy Swing, Brian Towles,
  Cliff Young, Xiang Zhou, Zongwei Zhou, and David Patterson",TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings,"15 pages; 16 figures; to be published at ISCA 2023 (the International
  Symposium on Computer Architecture)",,,,cs.AR cs.AI cs.LG cs.PF,http://creativecommons.org/licenses/by/4.0/,"  In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are <5% of system cost and <3% of system power. Each TPU v4 includes SparseCores, dataflow processors that accelerate models that rely on embeddings by 5x-7x yet use only 5% of die area and power. Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves performance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chips and thus ~10x faster overall, which along with OCS flexibility helps large language models. For similar sized systems, it is ~4.3x-4.5x faster than the Graphcore IPU Bow and is 1.2x-1.7x faster and uses 1.3x-1.9x less power than the Nvidia A100. TPU v4s inside the energy-optimized warehouse scale computers of Google Cloud use ~3x less energy and produce ~20x less CO2e than contemporary DSAs in a typical on-premise data center. ","[{'version': 'v1', 'created': 'Tue, 4 Apr 2023 00:52:46 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Apr 2023 14:50:57 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Apr 2023 22:25:51 GMT'}]",2023-04-24,"[['Jouppi', 'Norman P.', ''], ['Kurian', 'George', ''], ['Li', 'Sheng', ''], ['Ma', 'Peter', ''], ['Nagarajan', 'Rahul', ''], ['Nai', 'Lifeng', ''], ['Patil', 'Nishant', ''], ['Subramanian', 'Suvinay', ''], ['Swing', 'Andy', ''], ['Towles', 'Brian', ''], ['Young', 'Cliff', ''], ['Zhou', 'Xiang', ''], ['Zhou', 'Zongwei', ''], ['Patterson', 'David', '']]",0,0,2023-04-04,3,14,4,0,0,0,7c25adf2ddb35df05a61c697da97efb8583d77df,257921908.0,https://www.semanticscholar.org/paper/7c25adf2ddb35df05a61c697da97efb8583d77df,International Symposium on Computer Architecture,2023.0,65.0,44.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1715454', 'name': 'N. Jouppi'}, {'authorId': '1753079661', 'name': 'George Kurian'}, {'authorId': '2153701529', 'name': 'Sheng Li'}, {'authorId': '49735130', 'name': 'Peter C. Ma'}, {'authorId': '1395811464', 'name': 'R. Nagarajan'}, {'authorId': '2144577', 'name': 'Lifeng Nai'}, {'authorId': '2056800684', 'name': 'Nishant Patil'}, {'authorId': '1929462', 'name': 'Suvinay Subramanian'}, {'authorId': '1394189636', 'name': 'Andy Swing'}, {'authorId': '1762455', 'name': 'Brian Towles'}, {'authorId': '39660914', 'name': 'C. Young'}, {'authorId': '50177639', 'name': 'Xiaoping Zhou'}, {'authorId': '2109465503', 'name': 'Zongwei Zhou'}, {'authorId': '2052996328', 'name': 'David A. Patterson'}]","['ISCA Technologies (United States)', 'Google', 'General Hospital of Guangzhou Military Command']","['China', 'United States']",2023-04
2304.01483,Gaochen Dong,"Gaochen Dong, Wei Chen",Blockwise Compression of Transformer-based Models without Retraining,"6 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:2303.09184",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transformer-based models, exemplified by GPT-3, ChatGPT, and GPT-4, have recently garnered considerable attention in both academia and industry due to their promising performance in general language tasks. Nevertheless, these models typically involve computationally encoding processes, and in some cases, decoding processes as well, both of which are fundamentally large-scale matrix multiplication. These operations bring the inevitable challenges of massive computation resources and huge memory footprint, usually requiring at least 10^23 FLOPs and hundreds of gigabytes, respectively. A common method to address this issue is to reduce the computational and memory requirements by applying layerwise quantization to the transformer, replacing the usual fp32 data type with a low-bit equivalent. Unfortunately, this method often leads to decreased model accuracy and necessitates time-consuming retraining. Such retraining not only requires fine-tuning skills but also substantial computational resources, posing challenges for users. To specifically tackle these issues, we propose BCT, a framework of blockwise compression for transformers without retraining, aiming to facilitate model deployment. Unlike layerwise compression methods, BCT achieves finer compression of the entire transformer by operating blockwise. This method mitigates data distribution deviation caused by quantization, eliminating the requirement for retraining. BCT effectively compresses all components of the model, including but not limited to the embedding, matrix multiplication, GELU, Softmax, layer normalization, and intermediate results. In a case study, an efficient model is compressed by BCT achieving up to 7.988x compression. Subsequently, we also evaluate it on several General Language Understanding Evaluation (GLUE) datasets. ","[{'version': 'v1', 'created': 'Tue, 4 Apr 2023 02:55:40 GMT'}, {'version': 'v2', 'created': 'Sun, 17 Sep 2023 22:47:50 GMT'}]",2023-09-19,"[['Dong', 'Gaochen', ''], ['Chen', 'Wei', '']]",1,1,2023-04-04,2,2,2,3,0,3,4f72c61ad7f5f6379b26083fc7da52598334c503,257921638.0,https://www.semanticscholar.org/paper/4f72c61ad7f5f6379b26083fc7da52598334c503,arXiv.org,2023.0,14.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2211733371', 'name': 'Gaochen Dong'}, {'authorId': '2154939788', 'name': 'W. Chen'}]",['China Iron and Steel Research Institute Group'],['China'],2023-04
2304.01746,Runzhe Zhan,"Tao Fang, Shu Yang, Kaixin Lan, Derek F. Wong, Jinpeng Hu, Lidia S.
  Chao, Yue Zhang",Is ChatGPT a Highly Fluent Grammatical Error Correction System? A Comprehensive Evaluation,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  ChatGPT, a large-scale language model based on the advanced GPT-3.5 architecture, has shown remarkable potential in various Natural Language Processing (NLP) tasks. However, there is currently a dearth of comprehensive study exploring its potential in the area of Grammatical Error Correction (GEC). To showcase its capabilities in GEC, we design zero-shot chain-of-thought (CoT) and few-shot CoT settings using in-context learning for ChatGPT. Our evaluation involves assessing ChatGPT's performance on five official test sets in three different languages, along with three document-level GEC test sets in English. Our experimental results and human evaluations demonstrate that ChatGPT has excellent error detection capabilities and can freely correct errors to make the corrected sentences very fluent, possibly due to its over-correction tendencies and not adhering to the principle of minimal edits. Additionally, its performance in non-English and low-resource settings highlights its potential in multilingual GEC tasks. However, further analysis of various types of errors at the document-level has shown that ChatGPT cannot effectively correct agreement, coreference, tense errors across sentences, and cross-sentence boundary errors. ","[{'version': 'v1', 'created': 'Tue, 4 Apr 2023 12:33:40 GMT'}]",2023-04-05,"[['Fang', 'Tao', ''], ['Yang', 'Shu', ''], ['Lan', 'Kaixin', ''], ['Wong', 'Derek F.', ''], ['Hu', 'Jinpeng', ''], ['Chao', 'Lidia S.', ''], ['Zhang', 'Yue', '']]",1,1,2023-04-04,1,7,1,2,0,2,93d6fa92d60938b5bd0e405e159832b91332f169,257921532.0,https://www.semanticscholar.org/paper/93d6fa92d60938b5bd0e405e159832b91332f169,arXiv.org,2023.0,34.0,31.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2213476432', 'name': 'Tao Fang'}, {'authorId': '2155665129', 'name': 'Shu Yang'}, {'authorId': '2187727550', 'name': 'Kaixin Lan'}, {'authorId': '1758353', 'name': 'Derek F. Wong'}, {'authorId': '49268110', 'name': 'Jinpeng Hu'}, {'authorId': '1774304', 'name': 'Lidia S. Chao'}, {'authorId': '1895977079', 'name': 'Yue Zhang'}]","['Chinese University of Hong Kong, Shenzhen', 'Soochow University', 'University of Macau']","['China', 'Macao']",2023-04
2304.01771,Fangzhen Lin,Fangzhen Lin and Ziyi Shou and Chengcai Chen,Using Language Models For Knowledge Acquisition in Natural Language Reasoning Problems,,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  For a natural language problem that requires some non-trivial reasoning to solve, there are at least two ways to do it using a large language model (LLM). One is to ask it to solve it directly. The other is to use it to extract the facts from the problem text and then use a theorem prover to solve it. In this note, we compare the two methods using ChatGPT and GPT4 on a series of logic word puzzles, and conclude that the latter is the right approach. ","[{'version': 'v1', 'created': 'Tue, 4 Apr 2023 13:01:48 GMT'}]",2023-04-05,"[['Lin', 'Fangzhen', ''], ['Shou', 'Ziyi', ''], ['Chen', 'Chengcai', '']]",1,1,2023-04-04,1,3,1,2,0,2,c83e93b519acec9f9588dfb43a0d97fbc1222267,257921224.0,https://www.semanticscholar.org/paper/c83e93b519acec9f9588dfb43a0d97fbc1222267,arXiv.org,2023.0,20.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Philosophy', 'source': 's2-fos-model'}]","[{'authorId': '144125597', 'name': 'Fangzhen Lin'}, {'authorId': '2061325345', 'name': 'Ziyi Shou'}, {'authorId': '2423802', 'name': 'Chengcai Chen'}]","['Hong Kong University of Science and Technology', 'Xiaoi Robot Inc., Shanghai, China']",['China'],2023-04
2304.01852,Yiheng Liu,"Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang,
  Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, Zihao Wu, Lin
  Zhao, Dajiang Zhu, Xiang Li, Ning Qiang, Dingang Shen, Tianming Liu, Bao Ge",Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models,"21 pages, 4 figures, accepted by Meta-Radiology",Meta-Radiology (2023)100017,10.1016/j.metrad.2023.100017,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  This paper presents a comprehensive survey of ChatGPT-related (GPT-3.5 and GPT-4) research, state-of-the-art large language models (LLM) from the GPT series, and their prospective applications across diverse domains. Indeed, key innovations such as large-scale pre-training that captures knowledge across the entire world wide web, instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability and performance. We performed an in-depth analysis of 194 relevant papers on arXiv, encompassing trend analysis, word cloud representation, and distribution analysis across various application domains. The findings reveal a significant and increasing interest in ChatGPT-related research, predominantly centered on direct natural language processing applications, while also demonstrating considerable potential in areas ranging from education and history to mathematics, medicine, and physics. This study endeavors to furnish insights into ChatGPT's capabilities, potential implications, ethical concerns, and offer direction for future advancements in this field. ","[{'version': 'v1', 'created': 'Tue, 4 Apr 2023 15:01:06 GMT'}, {'version': 'v2', 'created': 'Sat, 8 Apr 2023 14:42:40 GMT'}, {'version': 'v3', 'created': 'Thu, 11 May 2023 03:50:53 GMT'}, {'version': 'v4', 'created': 'Tue, 22 Aug 2023 03:18:43 GMT'}]",2023-08-25,"[['Liu', 'Yiheng', ''], ['Han', 'Tianle', ''], ['Ma', 'Siyuan', ''], ['Zhang', 'Jiayue', ''], ['Yang', 'Yuanyuan', ''], ['Tian', 'Jiaming', ''], ['He', 'Hao', ''], ['Li', 'Antong', ''], ['He', 'Mengshen', ''], ['Liu', 'Zhengliang', ''], ['Wu', 'Zihao', ''], ['Zhao', 'Lin', ''], ['Zhu', 'Dajiang', ''], ['Li', 'Xiang', ''], ['Qiang', 'Ning', ''], ['Shen', 'Dingang', ''], ['Liu', 'Tianming', ''], ['Ge', 'Bao', '']]",1,1,2023-04-04,4,18,1,3,0,3,51a0bba0c5fb4257e843040615bb23f712fed4e6,257921533.0,https://www.semanticscholar.org/paper/51a0bba0c5fb4257e843040615bb23f712fed4e6,Meta-Radiology,2023.0,113.0,82.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2107995766', 'name': 'Yi-Hsien Liu'}, {'authorId': '2184719751', 'name': 'Tianle Han'}, {'authorId': '143791100', 'name': 'Siyuan Ma'}, {'authorId': '2144133280', 'name': 'Jia-Yu Zhang'}, {'authorId': '1911009053', 'name': 'Yuanyu Yang'}, {'authorId': '2213753130', 'name': 'Jiaming Tian'}, {'authorId': '2155082967', 'name': 'Haoyang He'}, {'authorId': '2141520266', 'name': 'Antong Li'}, {'authorId': '2165762495', 'name': 'Mengshen He'}, {'authorId': '2145977326', 'name': 'Zheng Liu'}, {'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '2181182', 'name': 'Dajiang Zhu'}, {'authorId': '2144438902', 'name': 'Xiang Li'}, {'authorId': '1500422299', 'name': 'Ning Qiang'}, {'authorId': '2212881824', 'name': 'Dingang Shen'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}, {'authorId': '144691205', 'name': 'Bao Ge'}]","[""Xi'an Jiaotong University"", 'ShanghaiTech University', 'Shanghai Clinical Research Center', 'Massachusetts General Hospital', 'Shaanxi Normal University', 'United Imaging', 'University of Georgia', 'The University of Texas at Arlington']","['China', 'United States']",2023-04
2304.01933,Lei Wang,"Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei
  Lee, Lidong Bing, Xing Xu, Soujanya Poria",LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models,"Technical Report. The code of our framework can be found at
  https://github.com/AGI-Edgerunners/LLM-Adapters. We will keep all of the code
  open-source and continue to update the framework with new adapters, LLMs, and
  tasks",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The success of large language models (LLMs), like GPT-3 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by fine-tuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire LLMs while achieving comparable or even better performance. To enable further research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks. The framework includes state-of-the-art open-access LLMs such as LLaMA, BLOOM, OPT, and GPT-J, as well as widely used adapters such as Series adapter, Parallel adapter, and LoRA. The framework is designed to be research-friendly, efficient, modular, and extendable, allowing the integration of new adapters and the evaluation of them with new and larger-scale LLMs. Furthermore, to evaluate the effectiveness of adapters in LLMs-Adapters, we conduct experiments on six math reasoning datasets. The results demonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few extra trainable parameters yields comparable, and in some cases superior, performance to that of powerful LLMs (175B) in zero-shot inference on simple math reasoning datasets. Overall, we provide a promising framework for fine-tuning large LLMs on downstream tasks. We believe the proposed LLMs-Adapters will advance adapter-based PEFT research, facilitate the deployment of research pipelines, and enable practical applications to real-world systems. ","[{'version': 'v1', 'created': 'Tue, 4 Apr 2023 16:31:37 GMT'}, {'version': 'v2', 'created': 'Mon, 8 May 2023 03:41:00 GMT'}]",2023-05-09,"[['Hu', 'Zhiqiang', ''], ['Lan', 'Yihuai', ''], ['Wang', 'Lei', ''], ['Xu', 'Wanyu', ''], ['Lim', 'Ee-Peng', ''], ['Lee', 'Roy Ka-Wei', ''], ['Bing', 'Lidong', ''], ['Xu', 'Xing', ''], ['Poria', 'Soujanya', '']]",1,1,2023-04-04,2,9,1,6,3,3,bdb68c5e2369633b20e733774ac66eb4600c34d1,257921386.0,https://www.semanticscholar.org/paper/bdb68c5e2369633b20e733774ac66eb4600c34d1,arXiv.org,2023.0,55.0,32.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1557412457', 'name': 'Zhiqiang Hu'}, {'authorId': '2150277971', 'name': 'Yihuai Lan'}, {'authorId': '145131956', 'name': 'Lei Wang'}, {'authorId': '2143418409', 'name': 'Wanyu Xu'}, {'authorId': '2212836814', 'name': 'Ee-Peng Lim'}, {'authorId': '38656724', 'name': 'R. Lee'}, {'authorId': '1996394', 'name': 'Lidong Bing'}, {'authorId': '1746416', 'name': 'Soujanya Poria'}]","['Alibaba', 'Singapore University of Technology and Design', 'Xidian University', 'Singapore Management University', 'Southwest Jiaotong University']","['China', 'Singapore']",2023-04
2304.01964,Aditi Mishra,"Aditi Mishra, Utkarsh Soni, Anjana Arunkumar, Jinbin Huang, Bum Chul
  Kwon, Chris Bryan","PromptAid: Prompt Exploration, Perturbation, Testing and Iteration using Visual Analytics for Large Language Models",,,,,cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have gained widespread popularity due to their ability to perform ad-hoc Natural Language Processing (NLP) tasks with a simple natural language prompt. Part of the appeal for LLMs is their approachability to the general public, including individuals with no prior technical experience in NLP techniques. However, natural language prompts can vary significantly in terms of their linguistic structure, context, and other semantics. Modifying one or more of these aspects can result in significant differences in task performance. Non-expert users may find it challenging to identify the changes needed to improve a prompt, especially when they lack domain-specific knowledge and lack appropriate feedback. To address this challenge, we present PromptAid, a visual analytics system designed to interactively create, refine, and test prompts through exploration, perturbation, testing, and iteration. PromptAid uses multiple, coordinated visualizations which allow users to improve prompts by using the three strategies: keyword perturbations, paraphrasing perturbations, and obtaining the best set of in-context few-shot examples. PromptAid was designed through an iterative prototyping process involving NLP experts and was evaluated through quantitative and qualitative assessments for LLMs. Our findings indicate that PromptAid helps users to iterate over prompt template alterations with less cognitive overhead, generate diverse prompts with help of recommendations, and analyze the performance of the generated prompts while surpassing existing state-of-the-art prompting interfaces in performance. ","[{'version': 'v1', 'created': 'Tue, 4 Apr 2023 17:14:54 GMT'}, {'version': 'v2', 'created': 'Sat, 8 Apr 2023 16:25:10 GMT'}]",2023-04-11,"[['Mishra', 'Aditi', ''], ['Soni', 'Utkarsh', ''], ['Arunkumar', 'Anjana', ''], ['Huang', 'Jinbin', ''], ['Kwon', 'Bum Chul', ''], ['Bryan', 'Chris', '']]",0,0,2023-04-04,2,6,1,0,0,0,a2c8d1c5470435176185bf891c76711a9b44808a,257921397.0,https://www.semanticscholar.org/paper/a2c8d1c5470435176185bf891c76711a9b44808a,arXiv.org,2023.0,43.0,10.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2004558290', 'name': 'Aditi Mishra'}, {'authorId': '39181352', 'name': 'Utkarsh Soni'}, {'authorId': '1667817604', 'name': 'Anjana Arunkumar'}, {'authorId': '8669194', 'name': 'Jinbin Huang'}, {'authorId': '145276140', 'name': 'B. Kwon'}, {'authorId': '143922308', 'name': 'Chris Bryan'}]","['Arizona State University', 'IBM Research - China']","['China', 'United States']",2023-04
2304.02015,Zheng Yuan,"Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang",How well do Large Language Models perform in Arithmetic tasks?,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large language models have emerged abilities including chain-of-thought to answer math word problems step by step. Solving math word problems not only requires abilities to disassemble problems via chain-of-thought but also needs to calculate arithmetic expressions correctly for each step. To the best of our knowledge, there is no work to focus on evaluating the arithmetic ability of large language models. In this work, we propose an arithmetic dataset MATH 401 to test the latest large language models including GPT-4, ChatGPT, InstrctGPT, Galactica, and LLaMA with various arithmetic expressions and provide a detailed analysis of the ability of large language models. MATH 401 and evaluation codes are released at \url{https://github.com/GanjinZero/math401-llm}. ","[{'version': 'v1', 'created': 'Thu, 16 Mar 2023 09:28:15 GMT'}]",2023-04-06,"[['Yuan', 'Zheng', ''], ['Yuan', 'Hongyi', ''], ['Tan', 'Chuanqi', ''], ['Wang', 'Wei', ''], ['Huang', 'Songfang', '']]",1,1,2023-03-16,1,5,2,3,1,2,817e52b815560f95171d8fa60f78dd965e885a65,257952500.0,https://www.semanticscholar.org/paper/817e52b815560f95171d8fa60f78dd965e885a65,arXiv.org,2023.0,35.0,33.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2112340945', 'name': 'Zheng Yuan'}, {'authorId': '2114128654', 'name': 'Hongyi Yuan'}, {'authorId': '2111727840', 'name': 'Chuanqi Tan'}, {'authorId': '2203795932', 'name': 'Wei Wang'}, {'authorId': '2410938', 'name': 'Songfang Huang'}]",['Tsinghua University'],['China'],2023-03
2304.02195,Sungmin Kang,"Sungmin Kang, Bei Chen, Shin Yoo, Jian-Guang Lou",Explainable Automated Debugging via Large Language Model-driven Scientific Debugging,,,,,cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Automated debugging techniques have the potential to reduce developer effort in debugging, and have matured enough to be adopted by industry. However, one critical issue with existing techniques is that, while developers want rationales for the provided automatic debugging results, existing techniques are ill-suited to provide them, as their deduction process differs significantly from that of human developers. Inspired by the way developers interact with code when debugging, we propose Automated Scientific Debugging (AutoSD), a technique that given buggy code and a bug-revealing test, prompts large language models to automatically generate hypotheses, uses debuggers to actively interact with buggy code, and thus automatically reach conclusions prior to patch generation. By aligning the reasoning of automated debugging more closely with that of human developers, we aim to produce intelligible explanations of how a specific patch has been generated, with the hope that the explanation will lead to more efficient and accurate developer decisions. Our empirical analysis on three program repair benchmarks shows that AutoSD performs competitively with other program repair baselines, and that it can indicate when it is confident in its results. Furthermore, we perform a human study with 20 participants, including six professional developers, to evaluate the utility of explanations from AutoSD. Participants with access to explanations could judge patch correctness in roughly the same time as those without, but their accuracy improved for five out of six real-world bugs studied: 70% of participants answered that they wanted explanations when using repair tools, while 55% answered that they were satisfied with the Scientific Debugging presentation. ","[{'version': 'v1', 'created': 'Wed, 5 Apr 2023 02:34:10 GMT'}]",2023-04-06,"[['Kang', 'Sungmin', ''], ['Chen', 'Bei', ''], ['Yoo', 'Shin', ''], ['Lou', 'Jian-Guang', '']]",0,0,2023-04-05,1,4,1,0,0,0,e74dee333546d9ad1a611a18fb0d5fa82980c006,257952253.0,https://www.semanticscholar.org/paper/e74dee333546d9ad1a611a18fb0d5fa82980c006,arXiv.org,2023.0,45.0,11.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2115515341', 'name': 'Sungmin Kang'}, {'authorId': '143876723', 'name': 'B. Chen'}, {'authorId': '143828721', 'name': 'S. Yoo'}, {'authorId': '153249455', 'name': 'Jian-Guang Lou'}]","['Korea Advanced Institute of Science and Technology', 'Universit de Toulouse', 'Microsoft']","['China', 'South Korea', 'France']",2023-04
2304.02426,Wenxiang Jiao,"Wenxiang Jiao, Jen-tse Huang, Wenxuan Wang, Xing Wang, Shuming Shi and
  Zhaopeng Tu",ParroT: Translating During Chat Using Large Language Models,fixed some typos and added missing comet scores,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) like ChatGPT and GPT-4 have exhibited remarkable abilities on a wide range of natural language processing (NLP) tasks, including various machine translation abilities accomplished during chat. However, these models are only accessible through restricted APIs, which creates barriers to new research and advancements in the field. Therefore, we propose the $\mathbf{ParroT}$ framework to enhance and regulate the translation abilities during chat based on open-sourced LLMs (i.e., LLaMA-7b, BLOOMZ-7b-mt) and human written translation and evaluation data. Specifically, ParroT reformulates translation data into the instruction-following style, and introduces a ""$\mathbf{Hint}$"" field for incorporating extra requirements to regulate the translation process. Accordingly, we propose three instruction types for finetuning ParroT models, including translation instruction, contrastive instruction, and error-guided instruction. We can finetune either the full models or partial parameters via low rank adaptation (LoRA). Experiments on Flores subsets and WMT22 test sets suggest that translation instruction improves the translation performance of vanilla LLMs significantly while error-guided instruction can lead to a further improvement, which demonstrates the importance of learning from low-quality translations annotated by human. Meanwhile, the ParroT models can also preserve the ability on general tasks with the Alpaca multi-task dataset involved in finetuning. Please refer to our Github project for more implementation details: https://github.com/wxjiao/ParroT ","[{'version': 'v1', 'created': 'Wed, 5 Apr 2023 13:12:00 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Apr 2023 01:07:36 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Apr 2023 01:35:28 GMT'}, {'version': 'v4', 'created': 'Tue, 9 May 2023 03:53:05 GMT'}]",2023-05-10,"[['Jiao', 'Wenxiang', ''], ['Huang', 'Jen-tse', ''], ['Wang', 'Wenxuan', ''], ['Wang', 'Xing', ''], ['Shi', 'Shuming', ''], ['Tu', 'Zhaopeng', '']]",1,1,2023-04-05,4,6,1,5,2,3,41e384ba2ebb9ca65057fb5df42dff332b609b9e,257952594.0,https://www.semanticscholar.org/paper/41e384ba2ebb9ca65057fb5df42dff332b609b9e,arXiv.org,2023.0,37.0,14.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '12386833', 'name': 'Wenxiang Jiao'}, {'authorId': '2161306685', 'name': 'Jen-tse Huang'}, {'authorId': '2144328160', 'name': 'Wenxuan Wang'}, {'authorId': '2610876', 'name': 'Zhiwei He'}, {'authorId': '48631170', 'name': 'Xing Wang'}, {'authorId': '2072684668', 'name': 'Shuming Shi'}, {'authorId': '2909321', 'name': 'Zhaopeng Tu'}]","['Shanghai Jiao Tong University', 'Chinese University of Hong Kong', 'Tsinghua Shenzhen International Graduate School', 'Tencent']",['China'],2023-04
2304.02554,Mingqi Gao,"Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, Xiaojun
  Wan",Human-like Summarization Evaluation with ChatGPT,"9 pages, 5 figures, in process",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Evaluating text summarization is a challenging problem, and existing evaluation metrics are far from satisfactory. In this study, we explored ChatGPT's ability to perform human-like summarization evaluation using four human evaluation methods on five datasets. We found that ChatGPT was able to complete annotations relatively smoothly using Likert scale scoring, pairwise comparison, Pyramid, and binary factuality evaluation. Additionally, it outperformed commonly used automatic evaluation metrics on some datasets. Furthermore, we discussed the impact of different prompts, compared its performance with that of human evaluation, and analyzed the generated explanations and invalid responses. ","[{'version': 'v1', 'created': 'Wed, 5 Apr 2023 16:17:32 GMT'}]",2023-04-06,"[['Gao', 'Mingqi', ''], ['Ruan', 'Jie', ''], ['Sun', 'Renliang', ''], ['Yin', 'Xunjian', ''], ['Yang', 'Shiping', ''], ['Wan', 'Xiaojun', '']]",1,1,2023-04-05,1,6,1,1,0,1,82e440220e29d6c2c5866f9cb40e522ca0c8a22d,257952492.0,https://www.semanticscholar.org/paper/82e440220e29d6c2c5866f9cb40e522ca0c8a22d,arXiv.org,2023.0,21.0,38.0,6.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '82340188', 'name': 'Mingqi Gao'}, {'authorId': '51300695', 'name': 'Jie Ruan'}, {'authorId': '2068172988', 'name': 'Renliang Sun'}, {'authorId': '2165272941', 'name': 'Xunjian Yin'}, {'authorId': '2189419845', 'name': 'Shiping Yang'}, {'authorId': '117908148', 'name': 'Xiaojun Wan'}]",['Peking University'],['China'],2023-04
2304.02697,Zehua Zeng,Zehua Zeng and Hongwu Du,Revolutionizing Single Cell Analysis: The Power of Large Language Models for Cell Type Annotation,"5 pages, 1 figures",,,,q-bio.GN cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  In recent years, single cell RNA sequencing has become a widely used technique to study cellular diversity and function. However, accurately annotating cell types from single cell data has been a challenging task, as it requires extensive knowledge of cell biology and gene function. The emergence of large language models such as ChatGPT and New Bing in 2023 has revolutionized this process by integrating the scientific literature and providing accurate annotations of cell types. This breakthrough enables researchers to conduct literature reviews more efficiently and accurately, and can potentially uncover new insights into cell type annotation. By using ChatGPT to annotate single cell data, we can relate rare cell type to their function and reveal specific differentiation trajectories of cell subtypes that were previously overlooked. This can have important applications in understanding cancer progression, mammalian development, and stem cell differentiation, and can potentially lead to the discovery of key cells that interrupt the differentiation pathway and solve key problems in the life sciences. Overall, the future of cell type annotation in single cell data looks promising and the Large Language model will be an important milestone in the history of single cell analysis. ","[{'version': 'v1', 'created': 'Wed, 5 Apr 2023 18:45:54 GMT'}]",2023-04-07,"[['Zeng', 'Zehua', ''], ['Du', 'Hongwu', '']]",1,1,2023-04-05,1,2,2,1,0,1,6462281c1c57bd85e11159ed39d57626185cab9b,257985103.0,https://www.semanticscholar.org/paper/6462281c1c57bd85e11159ed39d57626185cab9b,arXiv.org,2023.0,14.0,1.0,0.0,True,"['Computer Science', 'Biology']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Biology', 'source': 'external'}, {'category': 'Biology', 'source': 's2-fos-model'}]","[{'authorId': '49512105', 'name': 'Zehua Zeng'}, {'authorId': '2105634373', 'name': 'Hongwu Du'}]","['China. Prof. Hongwu Du,', 'University of Science and Technology Beijing', 'North China Electric Power University']",['China'],2023-04
2304.03022,Chen Li,"Chen Li, Yixiao Ge, Jiayong Mao, Dian Li, Ying Shan",TagGPT: Large Language Models are Zero-shot Multimodal Taggers,"13 pages, 6 figures",,,,cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Tags are pivotal in facilitating the effective distribution of multimedia content in various applications in the contemporary Internet era, such as search engines and recommendation systems. Recently, large language models (LLMs) have demonstrated impressive capabilities across a wide range of tasks. In this work, we propose TagGPT, a fully automated system capable of tag extraction and multimodal tagging in a completely zero-shot fashion. Our core insight is that, through elaborate prompt engineering, LLMs are able to extract and reason about proper tags given textual clues of multimodal data, e.g., OCR, ASR, title, etc. Specifically, to automatically build a high-quality tag set that reflects user intent and interests for a specific application, TagGPT predicts large-scale candidate tags from a series of raw data via prompting LLMs, filtered with frequency and semantics. Given a new entity that needs tagging for distribution, TagGPT introduces two alternative options for zero-shot tagging, i.e., a generative method with late semantic matching with the tag set, and another selective method with early matching in prompts. It is well noticed that TagGPT provides a system-level solution based on a modular framework equipped with a pre-trained LLM (GPT-3.5 used here) and a sentence embedding model (SimCSE used here), which can be seamlessly replaced with any more advanced one you want. TagGPT is applicable for various modalities of data in modern social media and showcases strong generalization ability to a wide range of applications. We evaluate TagGPT on publicly available datasets, i.e., Kuaishou and Food.com, and demonstrate the effectiveness of TagGPT compared to existing hashtags and off-the-shelf taggers. Project page: https://github.com/TencentARC/TagGPT. ","[{'version': 'v1', 'created': 'Thu, 6 Apr 2023 12:17:46 GMT'}]",2023-04-07,"[['Li', 'Chen', ''], ['Ge', 'Yixiao', ''], ['Mao', 'Jiayong', ''], ['Li', 'Dian', ''], ['Shan', 'Ying', '']]",0,1,2023-04-06,1,5,1,1,0,1,4895d443c36bd136a818be2db34442354ba408d1,257984964.0,https://www.semanticscholar.org/paper/4895d443c36bd136a818be2db34442354ba408d1,arXiv.org,2023.0,23.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '40475614', 'name': 'Chen Li'}, {'authorId': '152988335', 'name': 'Yixiao Ge'}, {'authorId': '2213513905', 'name': 'Jiayong Mao'}, {'authorId': '2151495740', 'name': 'Dian Li'}, {'authorId': '1387190008', 'name': 'Ying Shan'}]","['Tencent', 'Proton Collaborative Group']","['China', 'United States']",2023-04
2304.03087,Liwen Jing,"Bowen Zhang, Xianghua Fu, Daijun Ding, Hu Huang, Yangyang Li, Liwen
  Jing",Investigating Chain-of-thought with ChatGPT for Stance Detection on Social Media,arXiv admin note: text overlap with arXiv:2212.14548,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Stance detection predicts attitudes towards targets in texts and has gained attention with the rise of social media. Traditional approaches include conventional machine learning, early deep neural networks, and pre-trained fine-tuning models. However, with the evolution of very large pre-trained language models (VLPLMs) like ChatGPT (GPT-3.5), traditional methods face deployment challenges. The parameter-free Chain-of-Thought (CoT) approach, not requiring backpropagation training, has emerged as a promising alternative. This paper examines CoT's effectiveness in stance detection tasks, demonstrating its superior accuracy and discussing associated challenges. ","[{'version': 'v1', 'created': 'Thu, 6 Apr 2023 14:12:02 GMT'}]",2023-04-07,"[['Zhang', 'Bowen', ''], ['Fu', 'Xianghua', ''], ['Ding', 'Daijun', ''], ['Huang', 'Hu', ''], ['Li', 'Yangyang', ''], ['Jing', 'Liwen', '']]",1,1,2023-04-06,1,6,1,2,0,2,7cda7d64bd6bb3797bfcbf10dd7467e413ac2a74,257985456.0,https://www.semanticscholar.org/paper/7cda7d64bd6bb3797bfcbf10dd7467e413ac2a74,arXiv.org,2023.0,22.0,7.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': None, 'name': 'Bowen Zhang'}, {'authorId': '7780595', 'name': 'Xianghua Fu'}, {'authorId': '2199013050', 'name': 'Daijun Ding'}, {'authorId': '2139736576', 'name': 'Hutchin Huang'}, {'authorId': '2154900837', 'name': 'Yangyang Li'}, {'authorId': '2147240380', 'name': 'Liwen Jing'}]","['Shenzhen Technology University', 'University of Science and Technology of China', 'Academy of Cyber, Beijing, China', 'Shenzhen University']",['China'],2023-04
2304.03439,Hanmeng Liu,"Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, Yue Zhang",Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Harnessing logical reasoning ability is a comprehensive natural language understanding endeavor. With the release of Generative Pretrained Transformer 4 (GPT-4), highlighted as ""advanced"" at reasoning tasks, we are eager to learn the GPT-4 performance on various logical reasoning tasks. This report analyses multiple logical reasoning datasets, with popular benchmarks like LogiQA and ReClor, and newly-released datasets like AR-LSAT. We test the multi-choice reading comprehension and natural language inference tasks with benchmarks requiring logical reasoning. We further construct a logical reasoning out-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4. We also make a performance comparison between ChatGPT and GPT-4. Experiment results show that ChatGPT performs significantly better than the RoBERTa fine-tuning method on most logical reasoning benchmarks. With early access to the GPT-4 API we are able to conduct intense experiments on the GPT-4 model. The results show GPT-4 yields even higher performance on most logical reasoning datasets. Among benchmarks, ChatGPT and GPT-4 do relatively well on well-known datasets like LogiQA and ReClor. However, the performance drops significantly when handling newly released and out-of-distribution datasets. Logical reasoning remains challenging for ChatGPT and GPT-4, especially on out-of-distribution and natural language inference datasets. We release the prompt-style logical reasoning datasets as a benchmark suite and name it LogiEval. ","[{'version': 'v1', 'created': 'Fri, 7 Apr 2023 01:37:45 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Apr 2023 15:25:44 GMT'}, {'version': 'v3', 'created': 'Fri, 5 May 2023 07:24:48 GMT'}]",2023-05-08,"[['Liu', 'Hanmeng', ''], ['Ning', 'Ruoxi', ''], ['Teng', 'Zhiyang', ''], ['Liu', 'Jian', ''], ['Zhou', 'Qiji', ''], ['Zhang', 'Yue', '']]",1,1,2023-04-07,3,6,2,2,0,2,85cc48276c69924d3e92ddb38facb7d92be9a4a6,258041354.0,https://www.semanticscholar.org/paper/85cc48276c69924d3e92ddb38facb7d92be9a4a6,arXiv.org,2023.0,25.0,80.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2118960911', 'name': 'Hanmeng Liu'}, {'authorId': '30819687', 'name': 'Ruoxi Ning'}, {'authorId': '2272668', 'name': 'Zhiyang Teng'}, {'authorId': '2150168584', 'name': 'Jian Liu'}, {'authorId': '92758499', 'name': 'Qiji Zhou'}, {'authorId': '2145915465', 'name': 'Yuexin Zhang'}]","['Nanyang Technological University', 'Westlake University', 'Fudan University', 'Zhejiang University']","['China', 'Singapore']",2023-04
2304.03442,Joon Sung Park,"Joon Sung Park, Joseph C. O'Brien, Carrie J. Cai, Meredith Ringel
  Morris, Percy Liang, Michael S. Bernstein",Generative Agents: Interactive Simulacra of Human Behavior,,,,,cs.HC cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior. ","[{'version': 'v1', 'created': 'Fri, 7 Apr 2023 01:55:19 GMT'}, {'version': 'v2', 'created': 'Sun, 6 Aug 2023 00:21:19 GMT'}]",2023-08-08,"[['Park', 'Joon Sung', ''], [""O'Brien"", 'Joseph C.', ''], ['Cai', 'Carrie J.', ''], ['Morris', 'Meredith Ringel', ''], ['Liang', 'Percy', ''], ['Bernstein', 'Michael S.', '']]",0,0,2023-04-07,2,6,3,0,0,0,5278a8eb2ba2429d4029745caf4e661080073c81,258040990.0,https://www.semanticscholar.org/paper/5278a8eb2ba2429d4029745caf4e661080073c81,ACM Symposium on User Interface Software and Technology,2023.0,112.0,276.0,22.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2116649486', 'name': 'J. Park'}, {'authorId': '2213764034', 'name': ""Joseph C. O'Brien""}, {'authorId': '145941081', 'name': 'Carrie J. Cai'}, {'authorId': '144844426', 'name': 'M. Morris'}, {'authorId': '145419642', 'name': 'Percy Liang'}, {'authorId': '145879842', 'name': 'Michael S. Bernstein'}]","['Stanford University', 'Google', 'Nanjing University of Information Science and Technology']","['China', 'United States']",2023-04
2304.03516,Xinyu Lin,"Wenjie Wang, Xinyu Lin, Fuli Feng, Xiangnan He, and Tat-Seng Chua",Generative Recommendation: Towards Next-generation Recommender Paradigm,,,,,cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recommender systems typically retrieve items from an item corpus for personalized recommendations. However, such a retrieval-based recommender paradigm faces two limitations: 1) the human-generated items in the corpus might fail to satisfy the users' diverse information needs, and 2) users usually adjust the recommendations via passive and inefficient feedback such as clicks. Nowadays, AI-Generated Content (AIGC) has revealed significant success across various domains, offering the potential to overcome these limitations: 1) generative AI can produce personalized items to meet users' specific information needs, and 2) the newly emerged ChatGPT significantly facilitates users to express information needs more precisely via natural language instructions. In this light, the boom of AIGC points the way towards the next-generation recommender paradigm with two new objectives: 1) generating personalized content through generative AI, and 2) integrating user instructions to guide content generation.   To this end, we propose a novel Generative Recommender paradigm named GeneRec, which adopts an AI generator to personalize content generation and leverages user instructions to acquire users' information needs. Specifically, we pre-process users' instructions and traditional feedback (e.g., clicks) via an instructor to output the generation guidance. Given the guidance, we instantiate the AI generator through an AI editor and an AI creator to repurpose existing items and create new items, respectively. Eventually, GeneRec can perform content retrieval, repurposing, and creation to meet users' information needs. Besides, to ensure the trustworthiness of the generated items, we emphasize various fidelity checks such as authenticity and legality checks. Lastly, we study the feasibility of implementing the AI editor and AI creator on micro-video generation, showing promising results. ","[{'version': 'v1', 'created': 'Fri, 7 Apr 2023 07:20:16 GMT'}]",2023-04-10,"[['Wang', 'Wenjie', ''], ['Lin', 'Xinyu', ''], ['Feng', 'Fuli', ''], ['He', 'Xiangnan', ''], ['Chua', 'Tat-Seng', '']]",1,1,2023-04-07,1,5,1,1,0,1,6159549f986c63e160a678feef2130a2a4b93feb,258041206.0,https://www.semanticscholar.org/paper/6159549f986c63e160a678feef2130a2a4b93feb,arXiv.org,2023.0,57.0,24.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2117833732', 'name': 'Wenjie Wang'}, {'authorId': '2192203811', 'name': 'Xinyu Lin'}, {'authorId': '2163400298', 'name': 'Fuli Feng'}, {'authorId': '7792071', 'name': 'Xiangnan He'}, {'authorId': '143779329', 'name': 'Tat-seng Chua'}]","['National University of Singapore', 'University of Science and Technology of China']","['China', 'Singapore']",2023-04
2304.03531,Shirong Ma,"Shulin Huang, Shirong Ma, Yangning Li, Yinghui Li, Hai-Tao Zheng, Yong
  Jiang and Hong-Gee Kim",From Retrieval to Generation: Efficient and Effective Entity Set Expansion,,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Entity Set Expansion (ESE) is a critical task aiming to expand entities of the target semantic class described by a small seed entity set. Most existing ESE methods are retrieval-based frameworks that need to extract the contextual features of entities and calculate the similarity between seed entities and candidate entities. To achieve the two purposes, they should iteratively traverse the corpus and the entity vocabulary provided in the datasets, resulting in poor efficiency and scalability. The experimental results indicate that the time consumed by the retrieval-based ESE methods increases linearly with entity vocabulary and corpus size. In this paper, we firstly propose a generative ESE framework, Generative Entity Set Expansion (GenExpan), which utilizes a generative pre-trained language model to accomplish ESE task. Specifically, a prefix tree is employed to guarantee the validity of entity generation, and automatically generated class names are adopted to guide the model to generate target entities. Moreover, we propose Knowledge Calibration and Generative Ranking to further bridge the gap between generic knowledge of the language model and the goal of ESE task. Experiments on publicly available datasets show that GenExpan is efficient and effective. For efficiency, expansion time consumed by GenExpan is independent of entity vocabulary and corpus size, and GenExpan achieves an average 600% speedup compared to strong baselines. For expansion performance, our framework outperforms previous state-of-the-art ESE methods. ","[{'version': 'v1', 'created': 'Fri, 7 Apr 2023 08:09:50 GMT'}, {'version': 'v2', 'created': 'Thu, 10 Aug 2023 10:52:39 GMT'}]",2023-08-11,"[['Huang', 'Shulin', ''], ['Ma', 'Shirong', ''], ['Li', 'Yangning', ''], ['Li', 'Yinghui', ''], ['Zheng', 'Hai-Tao', ''], ['Jiang', 'Yong', ''], ['Kim', 'Hong-Gee', '']]",0,1,2023-04-07,2,7,2,0,0,0,d43835183d0c441bd26b2286062bcaab511182d7,258041140.0,https://www.semanticscholar.org/paper/d43835183d0c441bd26b2286062bcaab511182d7,arXiv.org,2023.0,60.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2177124783', 'name': 'Shulin Huang'}, {'authorId': '2118867306', 'name': 'Shirong Ma'}, {'authorId': '98177814', 'name': 'Y. Li'}, {'authorId': '2110503552', 'name': 'Yinghui Li'}, {'authorId': '16215052', 'name': 'Haitao Zheng'}, {'authorId': '50262192', 'name': 'Yong Jiang'}]","['Alibaba', 'Tsinghua University', 'Sun Yat-sen University', 'Peng Cheng Laboratory']",['China'],2023-04
2304.03728,Hongyin Luo,"Tianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell,
  Thomas Hartvigsen, Xixin Wu, Danny Fox, Helen Meng, James Glass",Interpretable Unified Language Checking,10 + 5 pages,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Despite recent concerns about undesirable behaviors generated by large language models (LLMs), including non-factual, biased, and hateful language, we find LLMs are inherent multi-task language checkers based on their latent representations of natural and social knowledge. We present an interpretable, unified, language checking (UniLC) method for both human and machine-generated language that aims to check if language input is factual and fair. While fairness and fact-checking tasks have been handled separately with dedicated models, we find that LLMs can achieve high performance on a combination of fact-checking, stereotype detection, and hate speech detection tasks with a simple, few-shot, unified set of prompts. With the ``1/2-shot'' multi-task language checking method proposed in this work, the GPT3.5-turbo model outperforms fully supervised baselines on several language tasks. The simple approach and results suggest that based on strong latent knowledge representations, an LLM can be an adaptive and explainable tool for detecting misinformation, stereotypes, and hate speech. ","[{'version': 'v1', 'created': 'Fri, 7 Apr 2023 16:47:49 GMT'}]",2023-04-10,"[['Zhang', 'Tianhua', ''], ['Luo', 'Hongyin', ''], ['Chuang', 'Yung-Sung', ''], ['Fang', 'Wei', ''], ['Gaitskell', 'Luc', ''], ['Hartvigsen', 'Thomas', ''], ['Wu', 'Xixin', ''], ['Fox', 'Danny', ''], ['Meng', 'Helen', ''], ['Glass', 'James', '']]",0,1,2023-04-07,1,10,1,1,0,1,3fcc8cb68488cfdfe4c52b81f27a236352fe5582,258041307.0,https://www.semanticscholar.org/paper/3fcc8cb68488cfdfe4c52b81f27a236352fe5582,arXiv.org,2023.0,48.0,12.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2146333115', 'name': 'Tianhua Zhang'}, {'authorId': '1944274', 'name': 'Hongyin Luo'}, {'authorId': '2475831', 'name': 'Yung-Sung Chuang'}, {'authorId': '2117161925', 'name': 'Wei Fang'}, {'authorId': '2214001322', 'name': 'Luc Gaitskell'}, {'authorId': '32452740', 'name': 'Thomas Hartvigsen'}, {'authorId': '1847260', 'name': 'Xixin Wu'}, {'authorId': '31997718', 'name': 'D. Fox'}, {'authorId': '2057833292', 'name': 'Helen M. Meng'}, {'authorId': '145898106', 'name': 'James R. Glass'}]","['MIT Computer Science and Artificial Intelligence Lab, Cambridge MA, USA', 'Massachusetts Institute of Technology', 'Chinese University of Hong Kong']","['China', 'United States']",2023-04
2304.03946,Xiaonan Nie,"Xiaonan Nie, Xupeng Miao, Zilong Wang, Zichao Yang, Jilong Xue,
  Lingxiao Ma, Gang Cao, Bin Cui",FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement,Accepted by SIGMOD 2023,"Proc. ACM Manag. Data, Vol. 1, No. 1, Article 110. Publication
  date: May 2023",10.1145/3588964,,cs.DC cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the increasing data volume, there is a trend of using large-scale pre-trained models to store the knowledge into an enormous number of model parameters. The training of these models is composed of lots of dense algebras, requiring a huge amount of hardware resources. Recently, sparsely-gated Mixture-of-Experts (MoEs) are becoming more popular and have demonstrated impressive pretraining scalability in various downstream tasks. However, such a sparse conditional computation may not be effective as expected in practical systems due to the routing imbalance and fluctuation problems. Generally, MoEs are becoming a new data analytics paradigm in the data life cycle and suffering from unique challenges at scales, complexities, and granularities never before possible.   In this paper, we propose a novel DNN training framework, FlexMoE, which systematically and transparently address the inefficiency caused by dynamic dataflow. We first present an empirical analysis on the problems and opportunities of training MoE models, which motivates us to overcome the routing imbalance and fluctuation problems by a dynamic expert management and device placement mechanism. Then we introduce a novel scheduling module over the existing DNN runtime to monitor the data flow, make the scheduling plans, and dynamically adjust the model-to-hardware mapping guided by the real-time data traffic. A simple but efficient heuristic algorithm is exploited to dynamically optimize the device placement during training. We have conducted experiments on both NLP models (e.g., BERT and GPT) and vision models (e.g., Swin). And results show FlexMoE can achieve superior performance compared with existing systems on real-world workloads -- FlexMoE outperforms DeepSpeed by 1.70x on average and up to 2.10x, and outperforms FasterMoE by 1.30x on average and up to 1.45x. ","[{'version': 'v1', 'created': 'Sat, 8 Apr 2023 07:34:26 GMT'}]",2023-04-11,"[['Nie', 'Xiaonan', ''], ['Miao', 'Xupeng', ''], ['Wang', 'Zilong', ''], ['Yang', 'Zichao', ''], ['Xue', 'Jilong', ''], ['Ma', 'Lingxiao', ''], ['Cao', 'Gang', ''], ['Cui', 'Bin', '']]",0,1,2023-04-08,1,8,2,0,0,0,dbbc5003af690799fa4fe6330fb795311cde106f,258048524.0,https://www.semanticscholar.org/paper/dbbc5003af690799fa4fe6330fb795311cde106f,Proc. ACM Manag. Data,2023.0,48.0,8.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2113588952', 'name': 'Xiaonan Nie'}, {'authorId': '1720763480', 'name': 'Xupeng Miao'}, {'authorId': '2168091655', 'name': 'Zilong Wang'}, {'authorId': '8387085', 'name': 'Zichao Yang'}, {'authorId': '2870618', 'name': 'Jilong Xue'}, {'authorId': '2492241', 'name': 'Lingxiao Ma'}, {'authorId': '2158415558', 'name': 'Gang-Ming Cao'}, {'authorId': '2143385941', 'name': 'Bin Cui'}]","['Peking University', 'Carnegie Mellon University', 'Microsoft', 'Beijing Academy of Artificial Intelligence']","['China', 'United States']",2023-04
2304.04256,Libo Qin,"Wenbo Pan, Qiguang Chen, Xiao Xu, Wanxiang Che, Libo Qin",A Preliminary Evaluation of ChatGPT for Zero-shot Dialogue Understanding,Technical Report,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Zero-shot dialogue understanding aims to enable dialogue to track the user's needs without any training data, which has gained increasing attention. In this work, we investigate the understanding ability of ChatGPT for zero-shot dialogue understanding tasks including spoken language understanding (SLU) and dialogue state tracking (DST). Experimental results on four popular benchmarks reveal the great potential of ChatGPT for zero-shot dialogue understanding. In addition, extensive analysis shows that ChatGPT benefits from the multi-turn interactive prompt in the DST task but struggles to perform slot filling for SLU. Finally, we summarize several unexpected behaviors of ChatGPT in dialogue understanding tasks, hoping to provide some insights for future research on building zero-shot dialogue understanding systems with Large Language Models (LLMs). ","[{'version': 'v1', 'created': 'Sun, 9 Apr 2023 15:28:36 GMT'}]",2023-04-11,"[['Pan', 'Wenbo', ''], ['Chen', 'Qiguang', ''], ['Xu', 'Xiao', ''], ['Che', 'Wanxiang', ''], ['Qin', 'Libo', '']]",1,1,2023-04-09,1,5,1,1,0,1,8ce0d339896e658b124aee63532f085c55fee225,258049061.0,https://www.semanticscholar.org/paper/8ce0d339896e658b124aee63532f085c55fee225,arXiv.org,2023.0,26.0,11.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2213932237', 'name': 'Wenbo Pan'}, {'authorId': '2133447633', 'name': 'Qiguang Chen'}, {'authorId': '33506639', 'name': 'Xiao Xu'}, {'authorId': '2256319', 'name': 'Wanxiang Che'}, {'authorId': '49169076', 'name': 'Libo Qin'}]",['Harbin Institute of Technology'],['China'],2023-04
2304.04339,Zengzhi Wang,"Zengzhi Wang, Qiming Xie, Zixiang Ding, Yi Feng, Rui Xia",Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study,Technical Report,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, ChatGPT has drawn great attention from both the research community and the public. We are particularly curious about whether it can serve as a universal sentiment analyzer. To this end, in this work, we provide a preliminary evaluation of ChatGPT on the understanding of opinions, sentiments, and emotions contained in the text. Specifically, we evaluate it in four settings, including standard evaluation, polarity shift evaluation, open-domain evaluation, and sentiment inference evaluation. The above evaluation involves 18 benchmark datasets and 5 representative sentiment analysis tasks, and we compare ChatGPT with fine-tuned BERT and corresponding state-of-the-art (SOTA) models on end-task. Moreover, we also conduct human evaluation and present some qualitative case studies to gain a deep comprehension of its sentiment analysis capabilities. ","[{'version': 'v1', 'created': 'Mon, 10 Apr 2023 00:55:59 GMT'}]",2023-04-11,"[['Wang', 'Zengzhi', ''], ['Xie', 'Qiming', ''], ['Ding', 'Zixiang', ''], ['Feng', 'Yi', ''], ['Xia', 'Rui', '']]",1,1,2023-04-10,1,5,2,1,0,1,1aeb3239735e28c7318af096044e48d919ea500b,258048703.0,https://www.semanticscholar.org/paper/1aeb3239735e28c7318af096044e48d919ea500b,arXiv.org,2023.0,56.0,31.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '3196852', 'name': 'Zengzhi Wang'}, {'authorId': '2213862793', 'name': 'Qiming Xie'}, {'authorId': '3465740', 'name': 'Zixiang Ding'}, {'authorId': '2213903745', 'name': 'Yi Feng'}, {'authorId': '1491639587', 'name': 'Rui Xia'}]",['Nanjing University of Science and Technology'],['China'],2023-04
2304.04675,Wenhao Zhu,"Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang,
  Lingpeng Kong, Jiajun Chen, Lei Li",Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have demonstrated remarkable potential in handling multilingual machine translation (MMT). In this paper, we systematically investigate the advantages and challenges of LLMs for MMT by answering two questions: 1) How well do LLMs perform in translating a massive number of languages? 2) Which factors affect LLMs' performance in translation? We evaluate popular LLMs, including XGLM, OPT, BLOOMZ, and ChatGPT, on 102 languages. Our empirical results show that even the best model ChatGPT still lags behind the supervised baseline NLLB in 83.33% of translation directions. Through further analysis, we discover that LLMs exhibit new working patterns when used for MMT. First, prompt semantics can surprisingly be ignored when given in-context exemplars, where LLMs still show strong performance even with unreasonable prompts. Second, cross-lingual exemplars can provide better task instruction for low-resource translation than exemplars in the same language pairs. Third, we observe the overestimated performance of BLOOMZ on dataset Flores-101, indicating the potential risk when using public datasets for evaluation. ","[{'version': 'v1', 'created': 'Mon, 10 Apr 2023 15:51:30 GMT'}, {'version': 'v2', 'created': 'Tue, 2 May 2023 02:23:50 GMT'}]",2023-05-03,"[['Zhu', 'Wenhao', ''], ['Liu', 'Hongyi', ''], ['Dong', 'Qingxiu', ''], ['Xu', 'Jingjing', ''], ['Huang', 'Shujian', ''], ['Kong', 'Lingpeng', ''], ['Chen', 'Jiajun', ''], ['Li', 'Lei', '']]",1,1,2023-04-10,2,8,1,4,3,1,dfd8944d39b378489b878d6e105d040fa0e524db,258048937.0,https://www.semanticscholar.org/paper/dfd8944d39b378489b878d6e105d040fa0e524db,arXiv.org,2023.0,53.0,40.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2131383723', 'name': 'Wenhao Zhu'}, {'authorId': '2115669628', 'name': 'Hongyi Liu'}, {'authorId': '2047143813', 'name': 'Qingxiu Dong'}, {'authorId': '47883405', 'name': 'Jingjing Xu'}, {'authorId': '47648549', 'name': 'Lingpeng Kong'}, {'authorId': '1838162', 'name': 'Jiajun Chen'}, {'authorId': '143900005', 'name': 'Lei Li'}, {'authorId': '2046010', 'name': 'Shujian Huang'}]","['Nanjing University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'Peking University', 'University of Hong Kong', 'University of California, Santa Barbara']","['China', 'United States', 'Hong Kong']",2023-04
2304.05197,Haoran Li,"Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng,
  Yangqiu Song",Multi-step Jailbreaking Privacy Attacks on ChatGPT,Updated with more experiments and improved writing,,,,cs.CL cs.CR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the rapid progress of large language models (LLMs), many downstream NLP tasks can be well solved given appropriate prompts. Though model developers and researchers work hard on dialog safety to avoid generating harmful content from LLMs, it is still challenging to steer AI-generated content (AIGC) for the human good. As powerful LLMs are devouring existing text data from various domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether the private information is included in the training data and what privacy threats can these LLMs and their downstream applications bring. In this paper, we study the privacy threats from OpenAI's ChatGPT and the New Bing enhanced by ChatGPT and show that application-integrated LLMs may cause new privacy threats. To this end, we conduct extensive experiments to support our claims and discuss LLMs' privacy implications. ","[{'version': 'v1', 'created': 'Tue, 11 Apr 2023 13:05:04 GMT'}, {'version': 'v2', 'created': 'Thu, 18 May 2023 17:11:40 GMT'}]",2023-05-19,"[['Li', 'Haoran', ''], ['Guo', 'Dadi', ''], ['Fan', 'Wei', ''], ['Xu', 'Mingshi', ''], ['Huang', 'Jie', ''], ['Meng', 'Fanpu', ''], ['Song', 'Yangqiu', '']]",1,1,2023-04-11,2,7,2,2,0,2,025ca4c125d6ecabc816a56f160e5c992abc76d9,258060250.0,https://www.semanticscholar.org/paper/025ca4c125d6ecabc816a56f160e5c992abc76d9,arXiv.org,2023.0,43.0,81.0,6.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Law', 'source': 's2-fos-model'}]","[{'authorId': '2145539796', 'name': 'Haoran Li'}, {'authorId': '2213872863', 'name': 'Dadi Guo'}, {'authorId': '2113533823', 'name': 'Wei Fan'}, {'authorId': '2153556700', 'name': 'Mingshi Xu'}, {'authorId': '1490651934', 'name': 'Jie Huang'}, {'authorId': '1809614', 'name': 'Yangqiu Song'}]","['Peking University', 'University of Illinois Urbana-Champaign', 'Hong Kong University of Science and Technology', 'University of Notre Dame']","['China', 'United States']",2023-04
2304.05302,Hongyi Yuan,"Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, Fei
  Huang",RRHF: Rank Responses to Align Language Models with Human Feedback without tears,Codes available at https://github.com/GanjinZero/RRHF,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment of large language models with human preferences, significantly enhancing the quality of interactions between humans and these models. InstructGPT implements RLHF through several stages, including Supervised Fine-Tuning (SFT), reward model training, and Proximal Policy Optimization (PPO). PPO, however, is sensitive to hyperparameters and requires a minimum of four models in its standard implementation, which makes it hard to train. In contrast, we propose a novel learning paradigm called RRHF, which scores responses generated by different sampling policies and learns to align them with human preferences through ranking loss. RRHF can efficiently align language model output probabilities with human preferences as robust as fine-tuning and it only needs 1 to 2 models during tuning. In addition, RRHF can be considered an extension of SFT and reward models while being simpler than PPO in terms of coding, model counts, and hyperparameters. The entire alignment process can be accomplished within a single RRHF training session. We evaluate RRHF using LLaMA and Alpaca on Helpful and Harmless data, demonstrating performance comparable to PPO. ","[{'version': 'v1', 'created': 'Tue, 11 Apr 2023 15:53:40 GMT'}, {'version': 'v2', 'created': 'Mon, 22 May 2023 17:27:47 GMT'}]",2023-05-23,"[['Yuan', 'Zheng', ''], ['Yuan', 'Hongyi', ''], ['Tan', 'Chuanqi', ''], ['Wang', 'Wei', ''], ['Huang', 'Songfang', ''], ['Huang', 'Fei', '']]",0,1,2023-04-11,2,6,1,3,1,2,748698bd4387afd08594e0dc8150c2afa210d9ae,258059818.0,https://www.semanticscholar.org/paper/748698bd4387afd08594e0dc8150c2afa210d9ae,arXiv.org,2023.0,58.0,66.0,11.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2112340945', 'name': 'Zheng Yuan'}, {'authorId': '2114128654', 'name': 'Hongyi Yuan'}, {'authorId': '2111727840', 'name': 'Chuanqi Tan'}, {'authorId': '2203795932', 'name': 'Wei Wang'}, {'authorId': '2410938', 'name': 'Songfang Huang'}, {'authorId': '51012694', 'name': 'Feiran Huang'}]",['Tsinghua University'],['China'],2023-04
2304.05351,Qianqian Xie,"Qianqian Xie, Weiguang Han, Yanzhao Lai, Min Peng, Jimin Huang",The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges,13 pages,,,,cs.CL cs.LG q-fin.ST,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, large language models (LLMs) like ChatGPT have demonstrated remarkable performance across a variety of natural language processing tasks. However, their effectiveness in the financial domain, specifically in predicting stock market movements, remains to be explored. In this paper, we conduct an extensive zero-shot analysis of ChatGPT's capabilities in multimodal stock movement prediction, on three tweets and historical stock price datasets. Our findings indicate that ChatGPT is a ""Wall Street Neophyte"" with limited success in predicting stock movements, as it underperforms not only state-of-the-art methods but also traditional methods like linear regression using price features. Despite the potential of Chain-of-Thought prompting strategies and the inclusion of tweets, ChatGPT's performance remains subpar. Furthermore, we observe limitations in its explainability and stability, suggesting the need for more specialized training or fine-tuning. This research provides insights into ChatGPT's capabilities and serves as a foundation for future work aimed at improving financial market analysis and prediction by leveraging social media sentiment and historical stock data. ","[{'version': 'v1', 'created': 'Mon, 10 Apr 2023 04:31:00 GMT'}, {'version': 'v2', 'created': 'Fri, 28 Apr 2023 12:06:43 GMT'}]",2023-05-01,"[['Xie', 'Qianqian', ''], ['Han', 'Weiguang', ''], ['Lai', 'Yanzhao', ''], ['Peng', 'Min', ''], ['Huang', 'Jimin', '']]",1,1,2023-04-10,2,5,3,1,0,1,ef4cb88b1635b34af15059567dfdf134f79797aa,258059831.0,https://www.semanticscholar.org/paper/ef4cb88b1635b34af15059567dfdf134f79797aa,arXiv.org,2023.0,26.0,13.0,1.0,True,"['Computer Science', 'Economics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Economics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '145229872', 'name': 'Qianqian Xie'}, {'authorId': '104843747', 'name': 'Weiguang Han'}, {'authorId': '2202405535', 'name': 'Yanzhao Lai'}, {'authorId': '47278503', 'name': 'Min Peng'}, {'authorId': '2555230', 'name': 'Jimin Huang'}]","['Wuhan University', 'Southwest Jiaotong University']",['China'],2023-04
2304.05402,Tony Ma,"Tony Ma, Songze Li, Yisong Xiao, Shunchang Liu",Boosting Cross-task Transferability of Adversarial Patches with Visual Relations,,,,,cs.CV cs.CR cs.LG cs.MM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The transferability of adversarial examples is a crucial aspect of evaluating the robustness of deep learning systems, particularly in black-box scenarios. Although several methods have been proposed to enhance cross-model transferability, little attention has been paid to the transferability of adversarial examples across different tasks. This issue has become increasingly relevant with the emergence of foundational multi-task AI systems such as Visual ChatGPT, rendering the utility of adversarial samples generated by a single task relatively limited. Furthermore, these systems often entail inferential functions beyond mere recognition-like tasks. To address this gap, we propose a novel Visual Relation-based cross-task Adversarial Patch generation method called VRAP, which aims to evaluate the robustness of various visual tasks, especially those involving visual reasoning, such as Visual Question Answering and Image Captioning. VRAP employs scene graphs to combine object recognition-based deception with predicate-based relations elimination, thereby disrupting the visual reasoning information shared among inferential tasks. Our extensive experiments demonstrate that VRAP significantly surpasses previous methods in terms of black-box transferability across diverse visual reasoning tasks. ","[{'version': 'v1', 'created': 'Tue, 11 Apr 2023 11:43:57 GMT'}]",2023-04-13,"[['Ma', 'Tony', ''], ['Li', 'Songze', ''], ['Xiao', 'Yisong', ''], ['Liu', 'Shunchang', '']]",1,1,2023-04-11,1,4,4,1,0,1,6316cbb4f1e7dba5806a3310ec7f89f3571bc3db,258079295.0,https://www.semanticscholar.org/paper/6316cbb4f1e7dba5806a3310ec7f89f3571bc3db,arXiv.org,2023.0,22.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2214154861', 'name': 'Tony Ma'}, {'authorId': '2239461', 'name': 'Songze Li'}, {'authorId': '40033753', 'name': 'Yisong Xiao'}, {'authorId': '2107947584', 'name': 'Shunchang Liu'}]","['Beihang University', 'Zhongguancun Laboratory']",['China'],2023-04
2304.06488,Chaoning Zhang,"Chaoning Zhang, Chenshuang Zhang, Chenghao Li, Yu Qiao, Sheng Zheng,
  Sumit Kumar Dam, Mengchun Zhang, Jung Uk Kim, Seong Tae Kim, Jinwoo Choi,
  Gyeong-Moon Park, Sung-Ho Bae, Lik-Hang Lee, Pan Hui, In So Kweon, Choong
  Seon Hong","One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era","A Survey on ChatGPT and GPT-4, 29 pages. Feedback is appreciated
  (chaoningzhang1990@gmail.com)",,,,cs.CY cs.AI cs.CL cs.CV cs.LG,http://creativecommons.org/licenses/by-sa/4.0/,"  OpenAI has recently released GPT-4 (a.k.a. ChatGPT plus), which is demonstrated to be one small step for generative AI (GAI), but one giant leap for artificial general intelligence (AGI). Since its official release in November 2022, ChatGPT has quickly attracted numerous users with extensive media coverage. Such unprecedented attention has also motivated numerous researchers to investigate ChatGPT from various aspects. According to Google scholar, there are more than 500 articles with ChatGPT in their titles or mentioning it in their abstracts. Considering this, a review is urgently needed, and our work fills this gap. Overall, this work is the first to survey ChatGPT with a comprehensive review of its underlying technology, applications, and challenges. Moreover, we present an outlook on how ChatGPT might evolve to realize general-purpose AIGC (a.k.a. AI-generated content), which will be a significant milestone for the development of AGI. ","[{'version': 'v1', 'created': 'Tue, 4 Apr 2023 06:22:09 GMT'}]",2023-04-14,"[['Zhang', 'Chaoning', ''], ['Zhang', 'Chenshuang', ''], ['Li', 'Chenghao', ''], ['Qiao', 'Yu', ''], ['Zheng', 'Sheng', ''], ['Dam', 'Sumit Kumar', ''], ['Zhang', 'Mengchun', ''], ['Kim', 'Jung Uk', ''], ['Kim', 'Seong Tae', ''], ['Choi', 'Jinwoo', ''], ['Park', 'Gyeong-Moon', ''], ['Bae', 'Sung-Ho', ''], ['Lee', 'Lik-Hang', ''], ['Hui', 'Pan', ''], ['Kweon', 'In So', ''], ['Hong', 'Choong Seon', '']]",1,1,2023-04-04,1,16,5,2,0,2,4de290467d903b9977e31b3d4084006789bd6ebd,258108139.0,https://www.semanticscholar.org/paper/4de290467d903b9977e31b3d4084006789bd6ebd,arXiv.org,2023.0,231.0,50.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '31044159', 'name': 'Chaoning Zhang'}, {'authorId': '48934876', 'name': 'Chenshuang Zhang'}, {'authorId': '2128662588', 'name': 'Chenghao Li'}, {'authorId': '2209185953', 'name': 'Yu Qiao'}, {'authorId': '2211960167', 'name': 'Sheng Zheng'}, {'authorId': '1642047032', 'name': 'S. Dam'}, {'authorId': '50495602', 'name': 'Mengchun Zhang'}, {'authorId': '2214125697', 'name': 'Jung Uk Kim'}, {'authorId': '2155490927', 'name': 'Seonghyeon Kim'}, {'authorId': '2200167615', 'name': 'J. Choi'}, {'authorId': '3144955', 'name': 'Gyeong-Moon Park'}, {'authorId': '40547898', 'name': 'S. Bae'}, {'authorId': '41177600', 'name': 'Lik-Hang Lee'}, {'authorId': '2066971474', 'name': 'Pan Hui'}, {'authorId': '145017151', 'name': 'In-So Kweon'}, {'authorId': '2159807650', 'name': 'Choong-Seon Hong'}]","['Hong Kong University of Science and Technology', 'Beijing Institute of Technology', 'Hong Kong Polytechnic University', 'Korea Advanced Institute of Science and Technology', 'Kyung Hee University']","['South Korea', 'China', 'Hong Kong']",2023-04
2304.06767,Hanze Dong,"Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui
  Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, Tong Zhang",RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment,"26 pages, 8 figures",,,,cs.LG cs.AI cs.CL cs.CV stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially serious consequences. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) to address this problem, where generative models are fine-tuned with RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generative models effectively. Utilizing a reward model and a sufficient number of samples, our approach selects the high-quality samples, discarding those that exhibit undesired behavior, and subsequently enhancing the model by fine-tuning on these filtered samples. Our studies show that RAFT can effectively improve the model performance in both reward learning and other automated metrics in both large language models and diffusion models. ","[{'version': 'v1', 'created': 'Thu, 13 Apr 2023 18:22:40 GMT'}, {'version': 'v2', 'created': 'Thu, 25 May 2023 06:27:31 GMT'}, {'version': 'v3', 'created': 'Wed, 30 Aug 2023 01:25:29 GMT'}]",2023-08-31,"[['Dong', 'Hanze', ''], ['Xiong', 'Wei', ''], ['Goyal', 'Deepanshu', ''], ['Zhang', 'Yihan', ''], ['Chow', 'Winnie', ''], ['Pan', 'Rui', ''], ['Diao', 'Shizhe', ''], ['Zhang', 'Jipeng', ''], ['Shum', 'Kashun', ''], ['Zhang', 'Tong', '']]",0,0,2023-04-13,3,10,5,0,0,0,3ab661db57d924f4ff1706e05ac807873ca00e0a,258170300.0,https://www.semanticscholar.org/paper/3ab661db57d924f4ff1706e05ac807873ca00e0a,arXiv.org,2023.0,74.0,51.0,6.0,True,"['Computer Science', 'Mathematics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Mathematics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '35279146', 'name': 'Hanze Dong'}, {'authorId': '49602517', 'name': 'Wei Xiong'}, {'authorId': '46414419', 'name': 'Deepanshu Goyal'}, {'authorId': '2192845956', 'name': 'Rui Pan'}, {'authorId': '50826757', 'name': 'Shizhe Diao'}, {'authorId': '50561049', 'name': 'Jipeng Zhang'}, {'authorId': '2121340452', 'name': 'Kashun Shum'}, {'authorId': '38144094', 'name': 'T. Zhang'}]",['Hong Kong University of Science and Technology'],['China'],2023-04
2304.06962,Chenkai Ma,Chenkai Ma,Prompt Engineering and Calibration for Zero-Shot Commonsense Reasoning,To be published in the ICLR TinyPaper track,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Prompt engineering and calibration make large language models excel at reasoning tasks, including multiple choice commonsense reasoning. From a practical perspective, we investigate and evaluate these strategies on smaller language models. Through experiments on five commonsense reasoning benchmarks, we find that each strategy favors certain models, but their joint effects are mostly negative. ","[{'version': 'v1', 'created': 'Fri, 14 Apr 2023 07:07:42 GMT'}]",2023-04-17,"[['Ma', 'Chenkai', '']]",0,0,2023-04-14,1,1,2,0,0,0,e1d66b33f3ce596c7f34a4cb649709a446a32d6d,258170435.0,https://www.semanticscholar.org/paper/e1d66b33f3ce596c7f34a4cb649709a446a32d6d,Tiny Papers @ ICLR,2023.0,24.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1796243744', 'name': 'Chenkai Ma'}]",['University of Electronic Science and Technology of China'],['China'],2023-04
2304.06975,Haochun Wang,"Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin
  and Ting Liu",HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge,"LLaMA-based Chinese Medical model - HuaTuo. Model, code and training
  data are available at https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs), such as the LLaMA model, have demonstrated their effectiveness in various general-domain natural language processing (NLP) tasks. Nevertheless, LLMs have not yet performed optimally in biomedical domain tasks due to the need for medical expertise in the responses. In response to this challenge, we propose HuaTuo, a LLaMA-based model that has been supervised-fine-tuned with generated QA (Question-Answer) instances. The experimental results demonstrate that HuaTuo generates responses that possess more reliable medical knowledge. Our proposed HuaTuo model is accessible at https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese. ","[{'version': 'v1', 'created': 'Fri, 14 Apr 2023 07:54:17 GMT'}]",2023-04-17,"[['Wang', 'Haochun', ''], ['Liu', 'Chi', ''], ['Xi', 'Nuwa', ''], ['Qiang', 'Zewen', ''], ['Zhao', 'Sendong', ''], ['Qin', 'Bing', ''], ['Liu', 'Ting', '']]",0,0,2023-04-14,1,7,1,1,1,0,302ee27524a717ddc21f332ca634b9211c6ec6aa,258170497.0,https://www.semanticscholar.org/paper/302ee27524a717ddc21f332ca634b9211c6ec6aa,arXiv.org,2023.0,13.0,56.0,12.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2164011959', 'name': 'Hao Wang'}, {'authorId': '117660020', 'name': 'Chi-Liang Liu'}, {'authorId': '2184773728', 'name': 'Nuwa Xi'}, {'authorId': '2214541370', 'name': 'Zewen Qiang'}, {'authorId': '1571183602', 'name': 'Sendong Zhao'}, {'authorId': '2166387643', 'name': 'Bing Qin'}, {'authorId': '2140034231', 'name': 'Ting Liu'}]",['Harbin Institute of Technology'],['China'],2023-04
2304.07061,Hao Wen,"Hao Wen, Hongming Wang, Jiaxuan Liu, Yuanchun Li",DroidBot-GPT: GPT-powered UI Automation for Android,"8 pages, 5 figures",,,,cs.SE cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper introduces DroidBot-GPT, a tool that utilizes GPT-like large language models (LLMs) to automate the interactions with Android mobile applications. Given a natural language description of a desired task, DroidBot-GPT can automatically generate and execute actions that navigate the app to complete the task. It works by translating the app GUI state information and the available actions on the smartphone screen to natural language prompts and asking the LLM to make a choice of actions. Since the LLM is typically trained on a large amount of data including the how-to manuals of diverse software applications, it has the ability to make reasonable choices of actions based on the provided information. We evaluate DroidBot-GPT with a self-created dataset that contains 33 tasks collected from 17 Android applications spanning 10 categories. It can successfully complete 39.39% of the tasks, and the average partial completion progress is about 66.76%. Given the fact that our method is fully unsupervised (no modification required from both the app and the LLM), we believe there is great potential to enhance automation performance with better app development paradigms and/or custom model training. ","[{'version': 'v1', 'created': 'Fri, 14 Apr 2023 11:31:56 GMT'}]",2023-04-17,"[['Wen', 'Hao', ''], ['Wang', 'Hongming', ''], ['Liu', 'Jiaxuan', ''], ['Li', 'Yuanchun', '']]",0,1,2023-04-14,1,4,2,0,0,0,2e4650d4ed613572b243d37eabb9bee33e45ab8f,258170084.0,https://www.semanticscholar.org/paper/2e4650d4ed613572b243d37eabb9bee33e45ab8f,arXiv.org,2023.0,23.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2188861830', 'name': 'Hao Wen'}, {'authorId': '46506498', 'name': 'Hongmin Wang'}, {'authorId': '2224423695', 'name': 'Jiaxuan Liu'}, {'authorId': '2110421643', 'name': 'Yuanchun Li'}]","['Beijing University of Posts and Telecommunications', 'Tsinghua University']",['China'],2023-04
2304.07493,Cong Guo,"Cong Guo, Jiaming Tang, Weiming Hu, Jingwen Leng, Chen Zhang, Fan
  Yang, Yunxin Liu, Minyi Guo, Yuhao Zhu",OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization,ISCA 2023,,10.1145/3579371.3589038,,cs.AR,http://creativecommons.org/licenses/by/4.0/,"  Transformer-based large language models (LLMs) have achieved great success with the growing model size. LLMs' size grows by $240\times$ every two years, which outpaces the hardware progress and makes model inference increasingly costly. Model quantization is a promising approach to mitigate the widening gap between LLM size and hardware capacity. However, the existence of outliers, values with significant magnitudes, in LLMs makes existing quantization methods less effective. Prior outlier-aware quantization schemes adopt sparsity encoding techniques to separate outliers from normal values where the process requires global coordination (e.g., a global sparsity coordination list). This incurs complex encoding/decoding hardware logics and an extra orchestration controller for the computation between outlier and normal values. As such, it is not hardware-efficient and hence only achieves sub-optimal quantization benefits.   We propose OliVe, an algorithm/architecture co-designed solution that adopts an outlier-victim pair (OVP) quantization and handles outlier values locally with low hardware overheads and high performance gains. The key insight of OliVe is that outliers are important while the normal values next to them are not. Thus those normal values (called victims) can be sacrificed to accommodate outliers. This enables a memory-aligned OVP encoding scheme, which can be efficiently integrated to the existing hardware accelerators like systolic array and tensor core. As a result, OliVe-based accelerator surpasses the existing outlier-aware accelerator, GOBO, by 4.5$\times$ speedup and 4.0$\times$ energy reduction, respectively, with a superior model accuracy. ","[{'version': 'v1', 'created': 'Sat, 15 Apr 2023 07:12:05 GMT'}]",2023-04-18,"[['Guo', 'Cong', ''], ['Tang', 'Jiaming', ''], ['Hu', 'Weiming', ''], ['Leng', 'Jingwen', ''], ['Zhang', 'Chen', ''], ['Yang', 'Fan', ''], ['Liu', 'Yunxin', ''], ['Guo', 'Minyi', ''], ['Zhu', 'Yuhao', '']]",0,0,2023-04-15,1,9,1,0,0,0,e92a5332390f0ba94615935541da4da9bed56512,258179335.0,https://www.semanticscholar.org/paper/e92a5332390f0ba94615935541da4da9bed56512,International Symposium on Computer Architecture,2023.0,95.0,6.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2109865181', 'name': 'Cong Guo'}, {'authorId': '2214687479', 'name': 'Jiaming Tang'}, {'authorId': '2214664410', 'name': 'Weiming Hu'}, {'authorId': '1831521', 'name': 'Jingwen Leng'}, {'authorId': '145107889', 'name': 'Chen Zhang'}, {'authorId': '145338263', 'name': 'Fan Yang'}, {'authorId': '2117415805', 'name': 'Yun-Bo Liu'}, {'authorId': '2151671216', 'name': 'Minyi Guo'}, {'authorId': '2155860957', 'name': 'Yuhao Zhu'}]","['ISCA Technologies (United States)', 'Jiaming Tang *', 'Fan Yang', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'Tsinghua University', 'Microsoft', 'Jingwen Leng ', 'University of Rochester', 'Linkping University']","['China', 'United States', 'Sweden']",2023-04
2304.07666,Yikang Liu,"Yikang Liu, Ziyin Zhang, Wanyang Zhang, Shisen Yue, Xiaojing Zhao,
  Xinyuan Cheng, Yiwen Zhang, Hai Hu","ArguGPT: evaluating, understanding and identifying argumentative essays generated by GPT models",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  AI generated content (AIGC) presents considerable challenge to educators around the world. Instructors need to be able to detect such text generated by large language models, either with the naked eye or with the help of some tools. There is also growing need to understand the lexical, syntactic and stylistic features of AIGC. To address these challenges in English language teaching, we first present ArguGPT, a balanced corpus of 4,038 argumentative essays generated by 7 GPT models in response to essay prompts from three sources: (1) in-class or homework exercises, (2) TOEFL and (3) GRE writing tasks. Machine-generated texts are paired with roughly equal number of human-written essays with three score levels matched in essay prompts. We then hire English instructors to distinguish machine essays from human ones. Results show that when first exposed to machine-generated essays, the instructors only have an accuracy of 61% in detecting them. But the number rises to 67% after one round of minimal self-training. Next, we perform linguistic analyses of these essays, which show that machines produce sentences with more complex syntactic structures while human essays tend to be lexically more complex. Finally, we test existing AIGC detectors and build our own detectors using SVMs and RoBERTa. Results suggest that a RoBERTa fine-tuned with the training set of ArguGPT achieves above 90% accuracy in both essay- and sentence-level classification. To the best of our knowledge, this is the first comprehensive analysis of argumentative essays produced by generative large language models. Machine-authored essays in ArguGPT and our models will be made publicly available at https://github.com/huhailinguist/ArguGPT ","[{'version': 'v1', 'created': 'Sun, 16 Apr 2023 01:50:26 GMT'}, {'version': 'v2', 'created': 'Sat, 23 Sep 2023 14:05:58 GMT'}]",2023-09-26,"[['Liu', 'Yikang', ''], ['Zhang', 'Ziyin', ''], ['Zhang', 'Wanyang', ''], ['Yue', 'Shisen', ''], ['Zhao', 'Xiaojing', ''], ['Cheng', 'Xinyuan', ''], ['Zhang', 'Yiwen', ''], ['Hu', 'Hai', '']]",0,1,2023-04-16,2,8,1,0,0,0,d0a93ad1af752853f73d89288e8fafb0c9906801,258179137.0,https://www.semanticscholar.org/paper/d0a93ad1af752853f73d89288e8fafb0c9906801,arXiv.org,2023.0,78.0,16.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '2214607250', 'name': 'Yikang Liu'}, {'authorId': '2116463153', 'name': 'Ziyin Zhang'}, {'authorId': '2214587233', 'name': 'Wanyang Zhang'}, {'authorId': '2214581710', 'name': 'Shisen Yue'}, {'authorId': '2214639123', 'name': 'Xiaojing Zhao'}, {'authorId': '2192057280', 'name': 'Xinyuan Cheng'}, {'authorId': '2108139880', 'name': 'Yiwen Zhang'}, {'authorId': '145309512', 'name': 'Hai Hu'}]",['Shanghai Jiao Tong University'],['China'],2023-04
2304.07778,Liu Chang,"Liu Chang, Wang Dongbo, Zhao Zhixiao, Hu Die, Wu Mengcheng, Lin Litao,
  Shen Si, Li Bin, Liu Jiangfeng, Zhang Hai, Zhao Lianzheng",SikuGPT: A Generative Pre-trained Model for Intelligent Information Processing of Ancient Texts from the Perspective of Digital Humanities,"20 pages,1 figure",,,,cs.CL,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  The rapid advance in artificial intelligence technology has facilitated the prosperity of digital humanities research. Against such backdrop, research methods need to be transformed in the intelligent processing of ancient texts, which is a crucial component of digital humanities research, so as to adapt to new development trends in the wave of AIGC. In this study, we propose a GPT model called SikuGPT based on the corpus of Siku Quanshu. The model's performance in tasks such as intralingual translation and text classification exceeds that of other GPT-type models aimed at processing ancient texts. SikuGPT's ability to process traditional Chinese ancient texts can help promote the organization of ancient information and knowledge services, as well as the international dissemination of Chinese ancient culture. ","[{'version': 'v1', 'created': 'Sun, 16 Apr 2023 13:25:24 GMT'}]",2023-04-18,"[['Chang', 'Liu', ''], ['Dongbo', 'Wang', ''], ['Zhixiao', 'Zhao', ''], ['Die', 'Hu', ''], ['Mengcheng', 'Wu', ''], ['Litao', 'Lin', ''], ['Si', 'Shen', ''], ['Bin', 'Li', ''], ['Jiangfeng', 'Liu', ''], ['Hai', 'Zhang', ''], ['Lianzheng', 'Zhao', '']]",0,1,2023-04-16,1,11,1,0,0,0,a914e3bd68387ebf993145782124b0f6c45d4d77,258179714.0,https://www.semanticscholar.org/paper/a914e3bd68387ebf993145782124b0f6c45d4d77,arXiv.org,2023.0,35.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2118483828', 'name': 'Chang Liu'}, {'authorId': '47858962', 'name': 'Dongbo Wang'}, {'authorId': '2214963769', 'name': 'Zhixiao Zhao'}, {'authorId': '2228651805', 'name': 'Die Hu'}, {'authorId': '2215108670', 'name': 'Mengcheng Wu'}, {'authorId': '113530810', 'name': 'Litao Lin'}, {'authorId': '152909152', 'name': 'Si Shen'}, {'authorId': '2185909341', 'name': 'Bin Li'}, {'authorId': '2229187676', 'name': 'Jiangfeng Liu'}, {'authorId': '2208129904', 'name': 'Hai Zhang'}, {'authorId': '2230193620', 'name': 'Lianzheng Zhao'}]","['Nanjing University of Science and Technology', 'Nanjing Agricultural University', 'China Pharmaceutical University', 'Nanjing Normal University']",['China'],2023-04
2304.07854,Baochang Ma,"Yunjie Ji, Yan Gong, Yong Deng, Yiping Peng, Qiang Niu, Baochang Ma,
  Xiangang Li",Towards Better Instruction Following Language Models for Chinese: Investigating the Impact of Training Data and Evaluation,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, significant public efforts have been directed towards developing low-cost models with capabilities akin to ChatGPT, thereby fostering the growth of open-source conversational models. However, there remains a scarcity of comprehensive and in-depth evaluations of these models' performance. In this study, we examine the influence of training data factors, including quantity, quality, and linguistic distribution, on model performance. Our analysis is grounded in several publicly accessible, high-quality instruction datasets, as well as our own Chinese multi-turn conversations. We assess various models using a evaluation set of 1,000 samples, encompassing nine real-world scenarios. Our goal is to supplement manual evaluations with quantitative analyses, offering valuable insights for the continued advancement of open-source chat models. Furthermore, to enhance the performance and training and inference efficiency of models in the Chinese domain, we extend the vocabulary of LLaMA - the model with the closest open-source performance to proprietary language models like GPT-3 - and conduct secondary pre-training on 3.4B Chinese words. We make our model, data, as well as code publicly available. ","[{'version': 'v1', 'created': 'Sun, 16 Apr 2023 18:37:39 GMT'}]",2023-04-18,"[['Ji', 'Yunjie', ''], ['Gong', 'Yan', ''], ['Deng', 'Yong', ''], ['Peng', 'Yiping', ''], ['Niu', 'Qiang', ''], ['Ma', 'Baochang', ''], ['Li', 'Xiangang', '']]",1,1,2023-04-16,1,7,1,3,1,2,a8d740aff768210d21bf30cd83bab156d78a232a,258180415.0,https://www.semanticscholar.org/paper/a8d740aff768210d21bf30cd83bab156d78a232a,arXiv.org,2023.0,58.0,7.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '24780166', 'name': 'Yunjie Ji'}, {'authorId': '2211736448', 'name': 'Yan Gong'}, {'authorId': '2111213765', 'name': 'Yang Deng'}, {'authorId': '2111015160', 'name': 'Yiping Peng'}, {'authorId': '2212854899', 'name': 'Qiang Niu'}, {'authorId': '152358488', 'name': 'Baochang Ma'}, {'authorId': '1898780', 'name': 'Xiangang Li'}]","['Bell (Canada)', 'Beike']","['China', 'Canada']",2023-04
2304.07919,Jiaxin Ge,"Jiaxin Ge, Hongyin Luo, Siyuan Qian, Yulu Gan, Jie Fu, Shanghang Zhang",Chain of Thought Prompt Tuning in Vision Language Models,,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Language-Image Pre-training has demonstrated promising results on zero-shot and few-shot downstream tasks by prompting visual models with natural language prompts. However, most recent studies only use a single prompt for tuning, neglecting the inherent step-to-step cognitive reasoning process that humans conduct in complex task settings, for example, when processing images from unfamiliar domains. Chain of Thought is a simple and effective approximation to human reasoning process and has been proven useful for natural language processing (NLP) tasks. Based on this cognitive intuition, we believe that conducting effective reasoning is also an important problem in visual tasks, and a chain of thought could be a solution to this problem. In this work, we propose a novel chain of thought prompt tuning for vision-language modeling. Extensive experiments show that our method not only generalizes better in image classification tasks, has greater transferability beyond a single dataset, and has stronger domain generalization performance, but also performs much better in imagetext retrieval and visual question answering, which require more reasoning capabilities. We are the first to successfully adapt chain-of-thought prompting that combines visual and textual embeddings. We will release our codes ","[{'version': 'v1', 'created': 'Sun, 16 Apr 2023 23:59:25 GMT'}, {'version': 'v2', 'created': 'Sat, 17 Jun 2023 06:40:27 GMT'}]",2023-06-21,"[['Ge', 'Jiaxin', ''], ['Luo', 'Hongyin', ''], ['Qian', 'Siyuan', ''], ['Gan', 'Yulu', ''], ['Fu', 'Jie', ''], ['Zhang', 'Shanghang', '']]",0,0,2023-04-16,2,6,2,0,0,0,a8680b3419f3cbe6650f72b1023aed0ad0becb9e,258180277.0,https://www.semanticscholar.org/paper/a8680b3419f3cbe6650f72b1023aed0ad0becb9e,,2023.0,46.0,6.0,1.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2214584825', 'name': 'Jiaxin Ge'}, {'authorId': '1944274', 'name': 'Hongyin Luo'}, {'authorId': '1610531198', 'name': 'Siyuan Qian'}, {'authorId': '2142286278', 'name': 'Yulu Gan'}, {'authorId': '2119314983', 'name': 'Jie Fu'}, {'authorId': '2437353', 'name': 'Shanghang Zhang'}]","['China University of Petroleum, Beijing', 'Massachusetts Institute of Technology', 'Peking University']","['China', 'United States']",2023-04
2304.07987,Ruibin Yuan,"Ge Zhang, Yemin Shi, Ruibo Liu, Ruibin Yuan, Yizhi Li, Siwei Dong, Yu
  Shu, Zhaoqun Li, Zekun Wang, Chenghua Lin, Wenhao Huang, Jie Fu",Chinese Open Instruction Generalist: A Preliminary Release,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Instruction tuning is widely recognized as a key technique for building generalist language models, which has attracted the attention of researchers and the public with the release of InstructGPT~\citep{ouyang2022training} and ChatGPT\footnote{\url{https://chat.openai.com/}}. Despite impressive progress in English-oriented large-scale language models (LLMs), it is still under-explored whether English-based foundation LLMs can perform similarly on multilingual tasks compared to English tasks with well-designed instruction tuning and how we can construct the corpora needed for the tuning. To remedy this gap, we propose the project as an attempt to create a Chinese instruction dataset by various methods adapted to the intrinsic characteristics of 4 sub-tasks. We collect around 200k Chinese instruction tuning samples, which have been manually checked to guarantee high quality. We also summarize the existing English and Chinese instruction corpora and briefly describe some potential applications of the newly constructed Chinese instruction corpora. The resulting \textbf{C}hinese \textbf{O}pen \textbf{I}nstruction \textbf{G}eneralist (\textbf{COIG}) corpora are available in Huggingface\footnote{\url{https://huggingface.co/datasets/BAAI/COIG}} and Github\footnote{\url{https://github.com/BAAI-Zlab/COIG}}, and will be continuously updated. ","[{'version': 'v1', 'created': 'Mon, 17 Apr 2023 04:45:06 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Apr 2023 04:46:57 GMT'}, {'version': 'v3', 'created': 'Fri, 21 Apr 2023 03:16:13 GMT'}, {'version': 'v4', 'created': 'Tue, 25 Apr 2023 01:50:19 GMT'}]",2023-04-26,"[['Zhang', 'Ge', ''], ['Shi', 'Yemin', ''], ['Liu', 'Ruibo', ''], ['Yuan', 'Ruibin', ''], ['Li', 'Yizhi', ''], ['Dong', 'Siwei', ''], ['Shu', 'Yu', ''], ['Li', 'Zhaoqun', ''], ['Wang', 'Zekun', ''], ['Lin', 'Chenghua', ''], ['Huang', 'Wenhao', ''], ['Fu', 'Jie', '']]",1,1,2023-04-17,4,12,2,2,0,2,c01e43c65a04d766e429863bdf7cf65b895df20e,258179044.0,https://www.semanticscholar.org/paper/c01e43c65a04d766e429863bdf7cf65b895df20e,arXiv.org,2023.0,52.0,13.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2143853895', 'name': 'Ge Zhang'}, {'authorId': '38179026', 'name': 'Yemin Shi'}, {'authorId': '7247867', 'name': 'Ruibo Liu'}, {'authorId': '2032236274', 'name': 'Ruibin Yuan'}, {'authorId': '2129449392', 'name': 'Yizhi Li'}, {'authorId': '2026180078', 'name': 'Siwei Dong'}, {'authorId': '2066476157', 'name': 'Yu Shu'}, {'authorId': '2214748090', 'name': 'Zhaoqun Li'}, {'authorId': None, 'name': 'Zekun Wang'}, {'authorId': '2268783', 'name': 'Chenghua Lin'}, {'authorId': '1825796760', 'name': 'Wen-Fen Huang'}, {'authorId': '2089772402', 'name': 'Jie Fu'}]","['University of Sheffield', 'Beijing Academy of Artificial Intelligence', 'Zhejiang University', 'Carnegie Mellon University', 'Beihang University', 'Dartmouth College', 'University of MichiganAnn Arbor']","['China', 'United States', 'United Kingdom']",2023-04
2304.07995,Qian Liu,"Qian Liu, Fan Zhou, Zhengbao Jiang, Longxu Dou, Min Lin",From Zero to Hero: Examining the Power of Symbolic Tasks in Instruction Tuning,"Work in Progress. The code is released at
  https://github.com/sail-sg/symbolic-instruction-tuning",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Fine-tuning language models on tasks with instructions has demonstrated potential in facilitating zero-shot generalization to unseen tasks. In this paper, we introduce a straightforward yet effective method for enhancing instruction tuning by employing symbolic tasks. Compared to crowdsourced human tasks or model-generated tasks, symbolic tasks present a unique advantage as they can be easily generated in vast quantities, theoretically providing an infinite supply of high-quality training instances. To explore the potential of symbolic tasks, we carry out an extensive case study on the representative symbolic task of SQL execution. Empirical results on various benchmarks validate that the integration of SQL execution leads to significant improvements in zero-shot scenarios, particularly in table reasoning. Notably, our 3B model surpasses both the 175B GPT-3 and ChatGPT in zero-shot table reasoning across four benchmarks. Furthermore, experimental results on BBH (27 tasks) and MMLU (57 tasks) reveal that language models can be enhanced through symbolic tasks without compromising their generality. We hope that our paper serves as a catalyst, inspiring increased efforts to incorporate symbolic tasks in instruction tuning. ","[{'version': 'v1', 'created': 'Mon, 17 Apr 2023 05:29:42 GMT'}]",2023-04-18,"[['Liu', 'Qian', ''], ['Zhou', 'Fan', ''], ['Jiang', 'Zhengbao', ''], ['Dou', 'Longxu', ''], ['Lin', 'Min', '']]",1,1,2023-04-17,1,5,2,2,0,2,90dd829f3d64dda19092b6e26909803bea5c37c1,258179750.0,https://www.semanticscholar.org/paper/90dd829f3d64dda19092b6e26909803bea5c37c1,arXiv.org,2023.0,65.0,6.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1409707585', 'name': 'Qian Liu'}, {'authorId': '2153433679', 'name': 'Fan Zhou'}, {'authorId': '2669515', 'name': 'Zhengbao Jiang'}, {'authorId': '49093992', 'name': 'Longxu Dou'}, {'authorId': '1491081747', 'name': 'Min Lin'}]","['National University of Singapore', 'Shanghai Jiao Tong University', 'Carnegie Mellon University', 'Applied Research Laboratory at the University of Hawaii']","['China', 'United States', 'Singapore']",2023-04
2304.08085,Xiao Wang,"Xiao Wang, Weikang Zhou, Can Zu, Han Xia, Tianze Chen, Yuansen Zhang,
  Rui Zheng, Junjie Ye, Qi Zhang, Tao Gui, Jihua Kang, Jingsheng Yang, Siyuan
  Li, Chunsai Du",InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models have unlocked strong multi-task capabilities from reading instructive prompts. However, recent studies have shown that existing large models still have difficulty with information extraction tasks. For example, gpt-3.5-turbo achieved an F1 score of 18.22 on the Ontonotes dataset, which is significantly lower than the state-of-the-art performance. In this paper, we propose InstructUIE, a unified information extraction framework based on instruction tuning, which can uniformly model various information extraction tasks and capture the inter-task dependency. To validate the proposed method, we introduce IE INSTRUCTIONS, a benchmark of 32 diverse information extraction datasets in a unified text-to-text format with expert-written instructions. Experimental results demonstrate that our method achieves comparable performance to Bert in supervised settings and significantly outperforms the state-of-the-art and gpt3.5 in zero-shot settings. ","[{'version': 'v1', 'created': 'Mon, 17 Apr 2023 09:00:50 GMT'}]",2023-04-18,"[['Wang', 'Xiao', ''], ['Zhou', 'Weikang', ''], ['Zu', 'Can', ''], ['Xia', 'Han', ''], ['Chen', 'Tianze', ''], ['Zhang', 'Yuansen', ''], ['Zheng', 'Rui', ''], ['Ye', 'Junjie', ''], ['Zhang', 'Qi', ''], ['Gui', 'Tao', ''], ['Kang', 'Jihua', ''], ['Yang', 'Jingsheng', ''], ['Li', 'Siyuan', ''], ['Du', 'Chunsai', '']]",0,1,2023-04-17,1,14,2,1,0,1,bbb2fc6e95d24fb58ab6c25b216b14ac49a32fbe,258179792.0,https://www.semanticscholar.org/paper/bbb2fc6e95d24fb58ab6c25b216b14ac49a32fbe,arXiv.org,2023.0,71.0,32.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2118451107', 'name': 'Xiao Wang'}, {'authorId': '101829183', 'name': 'Wei Zhou'}, {'authorId': '15190018', 'name': 'Can Zu'}, {'authorId': '2214583300', 'name': 'Han Xia'}, {'authorId': '2214644916', 'name': 'Tianze Chen'}, {'authorId': '2213635839', 'name': 'Yuan Zhang'}, {'authorId': '2058585152', 'name': 'Rui Zheng'}, {'authorId': '2143616977', 'name': 'Junjie Ye'}, {'authorId': '47835189', 'name': 'Qi Zhang'}, {'authorId': '2067331064', 'name': 'Tao Gui'}, {'authorId': '2357968', 'name': 'Jihua Kang'}, {'authorId': '145039750', 'name': 'J. Yang'}, {'authorId': '2118155623', 'name': 'Siyuan Li'}, {'authorId': '2214581440', 'name': 'Chunsai Du'}]","['ByteDance', 'Fudan University']",['China'],2023-04
2304.08103,Shaoguang Mao,"Yuzhe Cai, Shaoguang Mao, Wenshan Wu, Zehua Wang, Yaobo Liang, Tao Ge,
  Chenfei Wu, Wang You, Ting Song, Yan Xia, Jonathan Tien, Nan Duan",Low-code LLM: Visual Programming over LLMs,,,,,cs.CL cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Effectively utilizing LLMs for complex tasks is challenging, often involving a time-consuming and uncontrollable prompt engineering process. This paper introduces a novel human-LLM interaction framework, Low-code LLM. It incorporates six types of simple low-code visual programming interactions, all supported by clicking, dragging, or text editing, to achieve more controllable and stable responses. Through visual interaction with a graphical user interface, users can incorporate their ideas into the workflow without writing trivial prompts. The proposed Low-code LLM framework consists of a Planning LLM that designs a structured planning workflow for complex tasks, which can be correspondingly edited and confirmed by users through low-code visual programming operations, and an Executing LLM that generates responses following the user-confirmed workflow. We highlight three advantages of the low-code LLM: controllable generation results, user-friendly human-LLM interaction, and broadly applicable scenarios. We demonstrate its benefits using four typical applications. By introducing this approach, we aim to bridge the gap between humans and LLMs, enabling more effective and efficient utilization of LLMs for complex tasks. Our system will be soon publicly available at LowCodeLLM. ","[{'version': 'v1', 'created': 'Mon, 17 Apr 2023 09:27:40 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Apr 2023 06:42:24 GMT'}]",2023-04-21,"[['Cai', 'Yuzhe', ''], ['Mao', 'Shaoguang', ''], ['Wu', 'Wenshan', ''], ['Wang', 'Zehua', ''], ['Liang', 'Yaobo', ''], ['Ge', 'Tao', ''], ['Wu', 'Chenfei', ''], ['You', 'Wang', ''], ['Song', 'Ting', ''], ['Xia', 'Yan', ''], ['Tien', 'Jonathan', ''], ['Duan', 'Nan', '']]",0,0,2023-04-17,2,12,2,0,0,0,be2b0396de9431bae931642516a1d3e4906329f5,258180418.0,https://www.semanticscholar.org/paper/be2b0396de9431bae931642516a1d3e4906329f5,arXiv.org,2023.0,27.0,11.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '116275985', 'name': 'Yuzhe Cai'}, {'authorId': '35374367', 'name': 'Shaoguang Mao'}, {'authorId': '51198241', 'name': 'Wenshan Wu'}, {'authorId': '2108725194', 'name': 'Zehua Wang'}, {'authorId': '3887469', 'name': 'Yaobo Liang'}, {'authorId': '48741177', 'name': 'Tao Ge'}, {'authorId': '2151101534', 'name': 'Chenfei Wu'}, {'authorId': '2214585980', 'name': 'Wang You'}, {'authorId': '2212903837', 'name': 'Ting Song'}, {'authorId': '2111130689', 'name': 'Yan Xia'}, {'authorId': '2069738994', 'name': 'Jonathan Tien'}, {'authorId': '2072609829', 'name': 'Nan Duan'}]",['Microsoft'],['China'],2023-04
2304.08109,Baochang Ma,"Xianghui Sun, Yunjie Ji, Baochang Ma, Xiangang Li",A Comparative Study between Full-Parameter and LoRA-based Fine-Tuning on Chinese Instruction Data for Instruction Following Large Language Model,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, the instruction-tuning of large language models is a crucial area of research in the field of natural language processing. Due to resource and cost limitations, several researchers have employed parameter-efficient tuning techniques, such as LoRA, for instruction tuning, and have obtained encouraging results In comparison to full-parameter fine-tuning, LoRA-based tuning demonstrates salient benefits in terms of training costs. In this study, we undertook experimental comparisons between full-parameter fine-tuning and LoRA-based tuning methods, utilizing LLaMA as the base model. The experimental results show that the selection of the foundational model, training dataset scale, learnable parameter quantity, and model training cost are all important factors. We hope that the experimental conclusions of this paper can provide inspiration for training large language models, especially in the field of Chinese, and help researchers find a better trade-off strategy between training cost and model performance. To facilitate the reproduction of the paper's results, the dataset, model and code will be released. ","[{'version': 'v1', 'created': 'Mon, 17 Apr 2023 09:36:36 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Apr 2023 03:08:18 GMT'}]",2023-04-19,"[['Sun', 'Xianghui', ''], ['Ji', 'Yunjie', ''], ['Ma', 'Baochang', ''], ['Li', 'Xiangang', '']]",0,0,2023-04-17,2,4,1,1,1,0,f05011f5a485e59896db81c8700c27e7d5c6622f,258179474.0,https://www.semanticscholar.org/paper/f05011f5a485e59896db81c8700c27e7d5c6622f,arXiv.org,2023.0,48.0,7.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '49816188', 'name': 'Xianghui Sun'}, {'authorId': '24780166', 'name': 'Yunjie Ji'}, {'authorId': '152358488', 'name': 'Baochang Ma'}, {'authorId': '1898780', 'name': 'Xiangang Li'}]","['Bell (Canada)', 'Beike']","['China', 'Canada']",2023-04
2304.08191,Jialun Cao,Jialun Cao and Meiziniu Li and Ming Wen and Shing-chi Cheung,"A study on Prompt Design, Advantages and Limitations of ChatGPT for Deep Learning Program Repair",10 pages,,,,cs.SE,http://creativecommons.org/licenses/by/4.0/,"  ChatGPT has revolutionized many research and industrial fields. ChatGPT has shown great potential in software engineering to boost various traditional tasks such as program repair, code understanding, and code generation. However, whether automatic program repair (APR) applies to deep learning (DL) programs is still unknown. DL programs, whose decision logic is not explicitly encoded in the source code, have posed unique challenges to APR. While to repair DL programs, an APR approach needs to not only parse the source code syntactically but also needs to understand the code intention. With the best prior work, the performance of fault localization is still far less than satisfactory (only about 30\%). Therefore, in this paper, we explore ChatGPT's capability for DL program repair by asking three research questions. (1) Can ChatGPT debug DL programs effectively? (2) How can ChatGPT's repair performance be improved by prompting? (3) In which way can dialogue help facilitate the repair? On top of that, we categorize the common aspects useful for prompt design for DL program repair. Also, we propose various prompt templates to facilitate the performance and summarize the advantages and disadvantages of ChatGPT's abilities such as detecting bad code smell, code refactoring, and detecting API misuse/deprecation. ","[{'version': 'v1', 'created': 'Mon, 17 Apr 2023 12:04:17 GMT'}]",2023-04-18,"[['Cao', 'Jialun', ''], ['Li', 'Meiziniu', ''], ['Wen', 'Ming', ''], ['Cheung', 'Shing-chi', '']]",1,1,2023-04-17,1,4,1,1,0,1,c6808575096a6e4f3cbdc5f893384bc5a01cc6f8,258179639.0,https://www.semanticscholar.org/paper/c6808575096a6e4f3cbdc5f893384bc5a01cc6f8,arXiv.org,2023.0,50.0,19.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '51170803', 'name': 'Jialun Cao'}, {'authorId': '2030984767', 'name': 'Meiziniu Li'}, {'authorId': '38784613', 'name': 'Ming Wen'}, {'authorId': '143608708', 'name': 'S. Cheung'}]","['Huazhong University of Science and Technology', 'Hong Kong University of Science and Technology']",['China'],2023-04
2304.08448,Chong Ma,"Chong Ma, Zihao Wu, Jiaqi Wang, Shaochen Xu, Yaonai Wei, Zhengliang
  Liu, Xi Jiang, Lei Guo, Xiaoyan Cai, Shu Zhang, Tuo Zhang, Dajiang Zhu,
  Dinggang Shen, Tianming Liu, Xiang Li",ImpressionGPT: An Iterative Optimizing Framework for Radiology Report Summarization with ChatGPT,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  The 'Impression' section of a radiology report is a critical basis for communication between radiologists and other physicians, and it is typically written by radiologists based on the 'Findings' section. However, writing numerous impressions can be laborious and error-prone for radiologists. Although recent studies have achieved promising results in automatic impression generation using large-scale medical text data for pre-training and fine-tuning pre-trained language models, such models often require substantial amounts of medical text data and have poor generalization performance. While large language models (LLMs) like ChatGPT have shown strong generalization capabilities and performance, their performance in specific domains, such as radiology, remains under-investigated and potentially limited. To address this limitation, we propose ImpressionGPT, which leverages the in-context learning capability of LLMs by constructing dynamic contexts using domain-specific, individualized data. This dynamic prompt approach enables the model to learn contextual knowledge from semantically similar examples from existing data. Additionally, we design an iterative optimization algorithm that performs automatic evaluation on the generated impression results and composes the corresponding instruction prompts to further optimize the model. The proposed ImpressionGPT model achieves state-of-the-art performance on both MIMIC-CXR and OpenI datasets without requiring additional training data or fine-tuning the LLMs. This work presents a paradigm for localizing LLMs that can be applied in a wide range of similar application scenarios, bridging the gap between general-purpose LLMs and the specific language processing needs of various domains. ","[{'version': 'v1', 'created': 'Mon, 17 Apr 2023 17:13:42 GMT'}, {'version': 'v2', 'created': 'Wed, 3 May 2023 08:09:53 GMT'}]",2023-05-04,"[['Ma', 'Chong', ''], ['Wu', 'Zihao', ''], ['Wang', 'Jiaqi', ''], ['Xu', 'Shaochen', ''], ['Wei', 'Yaonai', ''], ['Liu', 'Zhengliang', ''], ['Jiang', 'Xi', ''], ['Guo', 'Lei', ''], ['Cai', 'Xiaoyan', ''], ['Zhang', 'Shu', ''], ['Zhang', 'Tuo', ''], ['Zhu', 'Dajiang', ''], ['Shen', 'Dinggang', ''], ['Liu', 'Tianming', ''], ['Li', 'Xiang', '']]",1,1,2023-04-17,2,15,2,1,0,1,a7f8fd45fbcdd81449cb7a1a6a2b2c18b38f8151,258180358.0,https://www.semanticscholar.org/paper/a7f8fd45fbcdd81449cb7a1a6a2b2c18b38f8151,arXiv.org,2023.0,92.0,38.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2132543537', 'name': 'Chong Ma'}, {'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '2143074042', 'name': 'Jiaqi Wang'}, {'authorId': '2211904452', 'name': 'Shaochen Xu'}, {'authorId': '2163675860', 'name': 'Yaonai Wei'}, {'authorId': '2145977326', 'name': 'Zheng Liu'}, {'authorId': '143621713', 'name': 'Lei Guo'}, {'authorId': '2174965546', 'name': 'Xiaoya Cai'}, {'authorId': '2108086798', 'name': 'Shu Zhang'}, {'authorId': '49104946', 'name': 'Tuo Zhang'}, {'authorId': '2181182', 'name': 'Dajiang Zhu'}, {'authorId': '2150038187', 'name': 'Dinggang Shen'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}, {'authorId': '2144438902', 'name': 'Xiang Li'}]","['University of Electronic Science and Technology of China', 'ShanghaiTech University', 'The University of Texas at Arlington', 'Shanghai Clinical Research Center', 'Massachusetts General Hospital', 'United Imaging', 'University of Georgia', 'Northwestern Polytechnical University']","['China', 'United States']",2023-04
2304.08763,Qianqian Xie,Qianqian Xie and Zheheng Luo and Benyou Wang and Sophia Ananiadou,A Survey for Biomedical Text Summarization: From Pre-trained to Large Language Models,"30 pages, 5 figures",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The exponential growth of biomedical texts such as biomedical literature and electronic health records (EHRs), poses a significant challenge for clinicians and researchers to access clinical information efficiently. To tackle this challenge, biomedical text summarization (BTS) has been proposed as a solution to support clinical information retrieval and management. BTS aims at generating concise summaries that distill key information from single or multiple biomedical documents. In recent years, the rapid advancement of fundamental natural language processing (NLP) techniques, from pre-trained language models (PLMs) to large language models (LLMs), has greatly facilitated the progress of BTS. This growth has led to numerous proposed summarization methods, datasets, and evaluation metrics, raising the need for a comprehensive and up-to-date survey for BTS. In this paper, we present a systematic review of recent advancements in BTS, leveraging cutting-edge NLP techniques from PLMs to LLMs, to help understand the latest progress, challenges, and future directions. We begin by introducing the foundational concepts of BTS, PLMs and LLMs, followed by an in-depth review of available datasets, recent approaches, and evaluation metrics in BTS. We finally discuss existing challenges and promising future directions in the era of LLMs. To facilitate the research community, we line up open resources including available datasets, recent approaches, codes, evaluation metrics, and the leaderboard in a public project: https://github.com/KenZLuo/Biomedical-Text-Summarization-Survey/tree/master. We believe that this survey will be a useful resource to researchers, allowing them to quickly track recent advancements and provide guidelines for future BTS research within the research community. ","[{'version': 'v1', 'created': 'Tue, 18 Apr 2023 06:38:40 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Jul 2023 04:13:17 GMT'}]",2023-07-17,"[['Xie', 'Qianqian', ''], ['Luo', 'Zheheng', ''], ['Wang', 'Benyou', ''], ['Ananiadou', 'Sophia', '']]",0,0,2023-04-18,2,4,1,0,0,0,b3c1fad1f5f8f0213b6d3f3458fa86205a3434f7,259924436.0,https://www.semanticscholar.org/paper/b3c1fad1f5f8f0213b6d3f3458fa86205a3434f7,,2023.0,146.0,1.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '145229872', 'name': 'Qianqian Xie'}, {'authorId': '31689330', 'name': 'Zheheng Luo'}, {'authorId': '2894465', 'name': 'Benyou Wang'}, {'authorId': '1881965', 'name': 'S. Ananiadou'}]","['University of Manchester', 'Chinese University of Hong Kong']","['China', 'United Kingdom']",2023-04
2304.08782,Minrui Xu,"Minrui Xu, Dusit Niyato, Hongliang Zhang, Jiawen Kang, Zehui Xiong,
  Shiwen Mao, Zhu Han",Sparks of GPTs in Edge Intelligence for Metaverse: Caching and Inference for Mobile AIGC Services,,,,,cs.NI,http://creativecommons.org/licenses/by/4.0/,"  Aiming at achieving artificial general intelligence (AGI) for Metaverse, pretrained foundation models (PFMs), e.g., generative pretrained transformers (GPTs), can effectively provide various AI services, such as autonomous driving, digital twins, and AI-generated content (AIGC) for extended reality. With the advantages of low latency and privacy-preserving, serving PFMs of mobile AI services in edge intelligence is a viable solution for caching and executing PFMs on edge servers with limited computing resources and GPU memory. However, PFMs typically consist of billions of parameters that are computation and memory-intensive for edge servers during loading and execution. In this article, we investigate edge PFM serving problems for mobile AIGC services of Metaverse. First, we introduce the fundamentals of PFMs and discuss their characteristic fine-tuning and inference methods in edge intelligence. Then, we propose a novel framework of joint model caching and inference for managing models and allocating resources to satisfy users' requests efficiently. Furthermore, considering the in-context learning ability of PFMs, we propose a new metric to evaluate the freshness and relevance between examples in demonstrations and executing tasks, namely the Age of Context (AoC). Finally, we propose a least context algorithm for managing cached models at edge servers by balancing the tradeoff among latency, energy consumption, and accuracy. ","[{'version': 'v1', 'created': 'Tue, 18 Apr 2023 07:34:36 GMT'}, {'version': 'v2', 'created': 'Fri, 28 Apr 2023 05:15:28 GMT'}]",2023-05-01,"[['Xu', 'Minrui', ''], ['Niyato', 'Dusit', ''], ['Zhang', 'Hongliang', ''], ['Kang', 'Jiawen', ''], ['Xiong', 'Zehui', ''], ['Mao', 'Shiwen', ''], ['Han', 'Zhu', '']]",0,1,2023-04-18,2,7,1,0,0,0,5b4fbc167ac3d0045e362282745d298af63ae664,258186961.0,https://www.semanticscholar.org/paper/5b4fbc167ac3d0045e362282745d298af63ae664,arXiv.org,2023.0,22.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1454018677', 'name': 'Minrui Xu'}, {'authorId': '1713586', 'name': 'D. Niyato'}, {'authorId': '46702608', 'name': 'Hongliang Zhang'}, {'authorId': '145993626', 'name': 'Jiawen Kang'}, {'authorId': '2943819', 'name': 'Zehui Xiong'}, {'authorId': '2056609621', 'name': 'Shiwen Mao'}, {'authorId': '2114874032', 'name': 'Zhu Han'}]","['University of Houston', 'Singapore University of Technology and Design', 'Auburn University', 'Nanyang Technological University', 'Guangdong University of Technology', 'Peking University', 'Kyung Hee University']","['South Korea', 'United States', 'China', 'Singapore']",2023-04
2304.09138,Xiang Li,"Zihao Wu, Lu Zhang, Chao Cao, Xiaowei Yu, Haixing Dai, Chong Ma,
  Zhengliang Liu, Lin Zhao, Gang Li, Wei Liu, Quanzheng Li, Dinggang Shen,
  Xiang Li, Dajiang Zhu, Tianming Liu",Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Recently, ChatGPT and GPT-4 have emerged and gained immense global attention due to their unparalleled performance in language processing. Despite demonstrating impressive capability in various open-domain tasks, their adequacy in highly specific fields like radiology remains untested. Radiology presents unique linguistic phenomena distinct from open-domain data due to its specificity and complexity. Assessing the performance of large language models (LLMs) in such specific domains is crucial not only for a thorough evaluation of their overall performance but also for providing valuable insights into future model design directions: whether model design should be generic or domain-specific. To this end, in this study, we evaluate the performance of ChatGPT/GPT-4 on a radiology NLI task and compare it to other models fine-tuned specifically on task-related data samples. We also conduct a comprehensive investigation on ChatGPT/GPT-4's reasoning ability by introducing varying levels of inference difficulty. Our results show that 1) GPT-4 outperforms ChatGPT in the radiology NLI task; 2) other specifically fine-tuned models require significant amounts of data samples to achieve comparable performance to ChatGPT/GPT-4. These findings demonstrate that constructing a generic model that is capable of solving various tasks across different domains is feasible. ","[{'version': 'v1', 'created': 'Tue, 18 Apr 2023 17:21:48 GMT'}]",2023-04-19,"[['Wu', 'Zihao', ''], ['Zhang', 'Lu', ''], ['Cao', 'Chao', ''], ['Yu', 'Xiaowei', ''], ['Dai', 'Haixing', ''], ['Ma', 'Chong', ''], ['Liu', 'Zhengliang', ''], ['Zhao', 'Lin', ''], ['Li', 'Gang', ''], ['Liu', 'Wei', ''], ['Li', 'Quanzheng', ''], ['Shen', 'Dinggang', ''], ['Li', 'Xiang', ''], ['Zhu', 'Dajiang', ''], ['Liu', 'Tianming', '']]",1,1,2023-04-18,1,15,1,2,0,2,258605dc5b00fe66b72091f947642a554e472aee,258187362.0,https://www.semanticscholar.org/paper/258605dc5b00fe66b72091f947642a554e472aee,arXiv.org,2023.0,49.0,24.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '2156146823', 'name': 'Lu Zhang'}, {'authorId': '5762440', 'name': 'Chao-Yang Cao'}, {'authorId': '2139760758', 'name': 'Xiao-Xing Yu'}, {'authorId': '29944950', 'name': 'Haixing Dai'}, {'authorId': '120688117', 'name': 'Chong-Yi Ma'}, {'authorId': '2145977326', 'name': 'Zheng Liu'}, {'authorId': '2111641126', 'name': 'Lin Zhao'}, {'authorId': '143736626', 'name': 'Gang Li'}, {'authorId': None, 'name': 'Wei Liu'}, {'authorId': '1762919', 'name': 'Quanzheng Li'}, {'authorId': '2150038187', 'name': 'Dinggang Shen'}, {'authorId': '113075935', 'name': 'Xiang Li'}, {'authorId': '2181182', 'name': 'Dajiang Zhu'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}]","['Mayo Clinic', 'University of North Carolina at Chapel Hill', 'ShanghaiTech University', 'The University of Texas at Arlington', 'Shanghai Clinical Research Center', 'Massachusetts General Hospital', 'United Imaging', 'University of Georgia', 'Northwestern Polytechnical University']","['China', 'United States']",2023-04
2304.09145,Xiuying Wei,"Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong,
  Jinyang Guo, Xianglong Liu",Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Quantization of transformer language models faces significant challenges due to the existence of detrimental outliers in activations. We observe that these outliers are asymmetric and concentrated in specific channels. To address this issue, we propose the Outlier Suppression+ framework. First, we introduce channel-wise shifting and scaling operations to eliminate asymmetric presentation and scale down problematic channels. We demonstrate that these operations can be seamlessly migrated into subsequent modules while maintaining equivalence. Second, we quantitatively analyze the optimal values for shifting and scaling, taking into account both the asymmetric property and quantization errors of weights in the next layer. Our lightweight framework can incur minimal performance degradation under static and standard post-training quantization settings. Comprehensive results across various tasks and models reveal that our approach achieves near-floating-point performance on both small models, such as BERT, and large language models (LLMs) including OPTs, BLOOM, and BLOOMZ at 8-bit and 6-bit settings. Furthermore, we establish a new state of the art for 4-bit BERT. ","[{'version': 'v1', 'created': 'Tue, 18 Apr 2023 17:34:23 GMT'}]",2023-04-19,"[['Wei', 'Xiuying', ''], ['Zhang', 'Yunchen', ''], ['Li', 'Yuhang', ''], ['Zhang', 'Xiangguo', ''], ['Gong', 'Ruihao', ''], ['Guo', 'Jinyang', ''], ['Liu', 'Xianglong', '']]",0,0,2023-04-18,1,7,1,2,2,0,f3275146eb973c9726dc550a88cc552f0dfa5ea7,264424348.0,https://www.semanticscholar.org/paper/f3275146eb973c9726dc550a88cc552f0dfa5ea7,,2023.0,69.0,0.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '14457026', 'name': 'Xiuying Wei'}, {'authorId': '2129510271', 'name': 'Yunchen Zhang'}, {'authorId': '2110539305', 'name': 'Yuhang Li'}, {'authorId': '2156004638', 'name': 'Xiangguo Zhang'}, {'authorId': '152217579', 'name': 'Ruihao Gong'}, {'authorId': '2261414086', 'name': 'Jinyang Guo'}, {'authorId': '2261367071', 'name': 'Xianglong Liu'}, {'authorId': '2261282667', 'name': 'Tom Brown'}, {'authorId': '2056658938', 'name': 'Benjamin Mann'}, {'authorId': '2260406867', 'name': 'Nick Ryder'}, {'authorId': '2065894334', 'name': 'Melanie Subbiah'}, {'authorId': '2053807409', 'name': 'Jared Kaplan'}, {'authorId': '6515819', 'name': 'Prafulla Dhariwal'}, {'authorId': '2072676', 'name': 'Arvind Neelakantan'}, {'authorId': '67311962', 'name': 'Pranav Shyam'}, {'authorId': '144864359', 'name': 'Girish Sastry'}, {'authorId': '2261276556', 'name': 'Yaohui Cai'}, {'authorId': '9088433', 'name': 'Z. Yao'}, {'authorId': '2257809664', 'name': 'Zhen Dong'}, {'authorId': '10419477', 'name': 'A. Gholami'}, {'authorId': '1717098', 'name': 'Michael W. Mahoney'}, {'authorId': '2242659602', 'name': 'Kurt Keutzer'}, {'authorId': '2261258756', 'name': 'Hawq'}, {'authorId': '2357931', 'name': 'Steven K. Esser'}, {'authorId': '46571359', 'name': 'J. McKinstry'}, {'authorId': '2064431971', 'name': 'Deepika Bablani'}, {'authorId': '2257291831', 'name': 'Edward J. Hu'}, {'authorId': '2261329866', 'name': 'Yelong Shen'}, {'authorId': '2261258092', 'name': 'Zeyuan Phillip Wallis'}, {'authorId': '2072796341', 'name': 'Qing Jin'}, {'authorId': '2258296012', 'name': 'Jian Ren'}, {'authorId': '98168954', 'name': 'Richard Zhuang'}, {'authorId': '2261258088', 'name': 'Sumant Hanu-mante'}, {'authorId': '2145366406', 'name': 'Zhengang Li'}, {'authorId': '2119225630', 'name': 'Zhiyu Chen'}, {'authorId': '2261356529', 'name': 'Yanzhi Wang'}, {'authorId': '2118048312', 'name': 'Kai-Min Yang'}, {'authorId': '2261258409', 'name': 'Sergey Tulyakov. 2022'}, {'authorId': '2261257494', 'name': 'F8net'}, {'authorId': '2109586102', 'name': 'Sehoon Kim'}]","['SenseTime Research', 'Beihang University', 'cole Polytechnique Fdrale de Lausanne', 'Yale University']","['China', 'United States', 'Switzerland']",2023-04
2304.09487,Lie Tang,"Lie Tang, Xianke Zhou, Min Lu",A GPT-based Approach for Research Article Identification: a Case Study in Artificial Intelligence,"24 pages, 10 figures, 5 tables",,,,cs.DL,http://creativecommons.org/licenses/by/4.0/,"  This study presents a comprehensive approach that addresses the challenges of identification and analysis of research articles in rapidly evolving fields, using the field of Artificial Intelligence (AI) as a case study. By combining search terms related to AI with the advanced language processing capabilities of generative pre-trained transformers (GPT), we developed a highly accurate method for identifying and analyzing AI-related articles in the Web of Science (WoS) database. Our multi-step approach included filtering articles based on WoS citation topics and category, keyword screening, and GPT classification. We evaluated the effectiveness of our method through precision and recall calculations, finding that our combined approach captured around 94% of AI-related articles in the entire WoS corpus with a precision of 90%. Following this, we analyzed the publication volume trends, revealing an increasing degree of interdisciplinarity. We conducted citation analysis on the top countries and institutions and identified common research themes using keyword analysis and GPT. This study demonstrates the potential of our approach as a tool for the accurate identification of scholarly articles, which is also capable of providing insights into the growth, interdisciplinary nature, and key players in a research area. ","[{'version': 'v1', 'created': 'Wed, 19 Apr 2023 08:17:10 GMT'}, {'version': 'v2', 'created': 'Fri, 28 Jul 2023 07:27:59 GMT'}]",2023-07-31,"[['Tang', 'Lie', ''], ['Zhou', 'Xianke', ''], ['Lu', 'Min', '']]",0,1,2023-04-19,2,3,1,0,0,0,0b48503894e6d960b58963b04a7eb4bc4781eed3,260316029.0,https://www.semanticscholar.org/paper/0b48503894e6d960b58963b04a7eb4bc4781eed3,,2023.0,19.0,0.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2214791310', 'name': 'Lie Tang'}, {'authorId': '2215796805', 'name': 'Xian Zhou'}, {'authorId': '2214865316', 'name': 'Min Lu'}]","['Zhejiang Science and Technology Information Institute', 'Zhejiang University']",['China'],2023-04
2304.09513,Xuying Meng,"Xuying Meng, Chungang Lin, Yequan Wang, Yujun Zhang",NetGPT: Generative Pretrained Transformer for Network Traffic,,,,,cs.NI cs.AI cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  All data on the Internet are transferred by network traffic, thus accurately modeling network traffic can help improve network services quality and protect data privacy. Pretrained models for network traffic can utilize large-scale raw data to learn the essential characteristics of network traffic, and generate distinguishable results for input traffic without considering specific downstream tasks. Effective pretrained models can significantly optimize the training efficiency and effectiveness of downstream tasks, such as application classification, attack detection and traffic generation. Despite the great success of pretraining in natural language processing, there is no work in the network field. Considering the diverse demands and characteristics of network traffic and network tasks, it is non-trivial to build a pretrained model for network traffic and we face various challenges, especially the heterogeneous headers and payloads in the multi-pattern network traffic and the different dependencies for contexts of diverse downstream network tasks.   To tackle these challenges, in this paper, we make the first attempt to provide a generative pretrained model NetGPT for both traffic understanding and generation tasks. We propose the multi-pattern network traffic modeling to construct unified text inputs and support both traffic understanding and generation tasks. We further optimize the adaptation effect of the pretrained model to diversified tasks by shuffling header fields, segmenting packets in flows, and incorporating diverse task labels with prompts. With diverse traffic datasets from encrypted software, DNS, private industrial protocols and cryptocurrency mining, expensive experiments demonstrate the effectiveness of our NetGPT in a range of traffic understanding and generation tasks on traffic datasets, and outperform state-of-the-art baselines by a wide margin. ","[{'version': 'v1', 'created': 'Wed, 19 Apr 2023 09:04:30 GMT'}, {'version': 'v2', 'created': 'Wed, 17 May 2023 11:23:35 GMT'}]",2023-05-18,"[['Meng', 'Xuying', ''], ['Lin', 'Chungang', ''], ['Wang', 'Yequan', ''], ['Zhang', 'Yujun', '']]",0,1,2023-04-19,2,4,3,0,0,0,5c40e387373ad2c750f0c7dd01442ee5a6ee3468,258212981.0,https://www.semanticscholar.org/paper/5c40e387373ad2c750f0c7dd01442ee5a6ee3468,arXiv.org,2023.0,25.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '3134123', 'name': 'Xuying Meng'}, {'authorId': '51087548', 'name': 'Chun-Chen Lin'}, {'authorId': '2119256294', 'name': 'Yequan Wang'}, {'authorId': '2198357154', 'name': 'Yujun Zhang'}]","['Beijing Academy of Artificial Intelligence', 'Institute of Computing Technology']",['China'],2023-04
2304.09542,Weiwei Sun,"Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, Zhaochun
  Ren",Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent,,,,,cs.CL cs.IR,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) have demonstrated a remarkable ability to generalize zero-shot to various language-related tasks. This paper focuses on the study of exploring generative LLMs such as ChatGPT and GPT-4 for relevance ranking in Information Retrieval (IR). Surprisingly, our experiments reveal that properly instructed ChatGPT and GPT-4 can deliver competitive, even superior results than supervised methods on popular IR benchmarks. Notably, GPT-4 outperforms the fully fine-tuned monoT5-3B on MS MARCO by an average of 2.7 nDCG on TREC datasets, an average of 2.3 nDCG on eight BEIR datasets, and an average of 2.7 nDCG on ten low-resource languages Mr.TyDi. Subsequently, we delve into the potential for distilling the ranking capabilities of ChatGPT into a specialized model. Our small specialized model that trained on 10K ChatGPT generated data outperforms monoT5 trained on 400K annotated MS MARCO data on BEIR. The code to reproduce our results is available at www.github.com/sunnweiwei/RankGPT ","[{'version': 'v1', 'created': 'Wed, 19 Apr 2023 10:16:03 GMT'}]",2023-04-20,"[['Sun', 'Weiwei', ''], ['Yan', 'Lingyong', ''], ['Ma', 'Xinyu', ''], ['Ren', 'Pengjie', ''], ['Yin', 'Dawei', ''], ['Ren', 'Zhaochun', '']]",1,1,2023-04-19,1,6,2,2,0,2,459c82205d2a27a8542bba7a4d478a8a23be2f5d,258212638.0,https://www.semanticscholar.org/paper/459c82205d2a27a8542bba7a4d478a8a23be2f5d,arXiv.org,2023.0,46.0,55.0,6.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2153198380', 'name': 'Weiwei Sun'}, {'authorId': '1387839383', 'name': 'Lingyong Yan'}, {'authorId': '121875983', 'name': 'Xinyu Ma'}, {'authorId': '1749477', 'name': 'Pengjie Ren'}, {'authorId': '2136400100', 'name': 'Dawei Yin'}, {'authorId': '2780667', 'name': 'Z. Ren'}]","['Shandong University', 'Leiden University', 'Baidu']","['China', 'Netherlands']",2023-04
2304.09582,Weixiang Zhao,"Weixiang Zhao, Yanyan Zhao, Xin Lu, Shilong Wang, Yanpeng Tong, Bing
  Qin",Is ChatGPT Equipped with Emotional Dialogue Capabilities?,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This report presents a study on the emotional dialogue capability of ChatGPT, an advanced language model developed by OpenAI. The study evaluates the performance of ChatGPT on emotional dialogue understanding and generation through a series of experiments on several downstream tasks. Our findings indicate that while ChatGPT's performance on emotional dialogue understanding may still lag behind that of supervised models, it exhibits promising results in generating emotional responses. Furthermore, the study suggests potential avenues for future research directions. ","[{'version': 'v1', 'created': 'Wed, 19 Apr 2023 11:42:40 GMT'}]",2023-04-20,"[['Zhao', 'Weixiang', ''], ['Zhao', 'Yanyan', ''], ['Lu', 'Xin', ''], ['Wang', 'Shilong', ''], ['Tong', 'Yanpeng', ''], ['Qin', 'Bing', '']]",1,1,2023-04-19,1,6,1,1,0,1,616597b6c8cc3d24339d9f16bb4b195624046abe,258212863.0,https://www.semanticscholar.org/paper/616597b6c8cc3d24339d9f16bb4b195624046abe,arXiv.org,2023.0,47.0,18.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Business', 'source': 's2-fos-model'}]","[{'authorId': '29757826', 'name': 'Weixiang Zhao'}, {'authorId': '49339265', 'name': 'Yanyan Zhao'}, {'authorId': '2124828330', 'name': 'Xin Lu'}, {'authorId': '2214828656', 'name': 'Shilong Wang'}, {'authorId': '2214716354', 'name': 'Yanpeng Tong'}, {'authorId': '2203961541', 'name': 'Bing Qin'}]",['Harbin Institute of Technology'],['China'],2023-04
2304.09595,Shengrui Li,"Shengrui Li, Xueting Han, Jing Bai",AdapterGNN: Efficient Delta Tuning Improves Generalization Ability in Graph Neural Networks,,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fine-tuning pre-trained models has recently yielded remarkable performance gains in graph neural networks (GNNs). In addition to pre-training techniques, inspired by the latest work in the natural language fields, more recent work has shifted towards applying effective fine-tuning approaches, such as parameter-efficient tuning (delta tuning). However, given the substantial differences between GNNs and transformer-based models, applying such approaches directly to GNNs proved to be less effective. In this paper, we present a comprehensive comparison of delta tuning techniques for GNNs and propose a novel delta tuning method specifically designed for GNNs, called AdapterGNN. AdapterGNN preserves the knowledge of the large pre-trained model and leverages highly expressive adapters for GNNs, which can adapt to downstream tasks effectively with only a few parameters, while also improving the model's generalization ability on the downstream tasks. Extensive experiments show that AdapterGNN achieves higher evaluation performance (outperforming full fine-tuning by 1.4% and 5.5% in the chemistry and biology domains respectively, with only 5% of its parameters tuned) and lower generalization gaps compared to full fine-tuning. Moreover, we empirically show that a larger GNN model can have a worse generalization ability, which differs from the trend observed in large language models. We have also provided a theoretical justification for delta tuning can improve the generalization ability of GNNs by applying generalization bounds. ","[{'version': 'v1', 'created': 'Wed, 19 Apr 2023 12:00:15 GMT'}]",2023-04-20,"[['Li', 'Shengrui', ''], ['Han', 'Xueting', ''], ['Bai', 'Jing', '']]",0,0,2023-04-19,1,3,1,0,0,0,bb0770be24e49d1c11a61ee4c0cec2730a7256cc,258212699.0,https://www.semanticscholar.org/paper/bb0770be24e49d1c11a61ee4c0cec2730a7256cc,arXiv.org,2023.0,46.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '39541577', 'name': 'Sheng Li'}, {'authorId': '2149224458', 'name': 'Xueting Han'}, {'authorId': '2113829510', 'name': 'Jing Bai'}]","['Tsinghua University', 'Microsoft']",['China'],2023-04
2304.09797,Chuanyang Zheng,"Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li",Progressive-Hint Prompting Improves Reasoning in Large Language Models,Tech Report,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experiments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sample paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% -> 91.9%), GSM8K (92% -> 95.5%), AQuA (76.4% -> 79.9%) and MATH (50.3% -> 53.9%). ","[{'version': 'v1', 'created': 'Wed, 19 Apr 2023 16:29:48 GMT'}, {'version': 'v2', 'created': 'Fri, 5 May 2023 14:33:37 GMT'}, {'version': 'v3', 'created': 'Thu, 11 May 2023 06:53:16 GMT'}, {'version': 'v4', 'created': 'Fri, 19 May 2023 09:05:39 GMT'}, {'version': 'v5', 'created': 'Thu, 10 Aug 2023 03:41:04 GMT'}]",2023-08-11,"[['Zheng', 'Chuanyang', ''], ['Liu', 'Zhengying', ''], ['Xie', 'Enze', ''], ['Li', 'Zhenguo', ''], ['Li', 'Yu', '']]",0,1,2023-04-19,5,5,2,1,0,1,261549439aebdda72b648ecc462448fd24857ac1,258212839.0,https://www.semanticscholar.org/paper/261549439aebdda72b648ecc462448fd24857ac1,arXiv.org,2023.0,66.0,43.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1560302377', 'name': 'Chuanyang Zheng'}, {'authorId': '73785871', 'name': 'Zhengying Liu'}, {'authorId': '41020000', 'name': 'Enze Xie'}, {'authorId': '7718952', 'name': 'Zhenguo Li'}, {'authorId': '2116612471', 'name': 'Yu Li'}]","['Chinese University of Hong Kong', 'Huawei Technologies (China)']",['China'],2023-04
2304.09972,David Adelani,"David Ifeoluwa Adelani, Marek Masiak, Israel Abebe Azime, Jesujoba
  Alabi, Atnafu Lambebo Tonja, Christine Mwase, Odunayo Ogundepo, Bonaventure
  F. P. Dossou, Akintunde Oladipo, Doreen Nixdorf, Chris Chinenye Emezue, sana
  al-azzawi, Blessing Sibanda, Davis David, Lolwethu Ndolela, Jonathan Mukiibi,
  Tunde Ajayi, Tatiana Moteu, Brian Odhiambo, Abraham Owodunni, Nnaemeka
  Obiefuna, Muhidin Mohamed, Shamsuddeen Hassan Muhammad, Teshome Mulugeta
  Ababu, Saheed Abdullahi Salahudeen, Mesay Gemeda Yigezu, Tajuddeen Gwadabe,
  Idris Abdulmumin, Mahlet Taye, Oluwabusayo Awoyomi, Iyanuoluwa Shode,
  Tolulope Adelani, Habiba Abdulganiyu, Abdul-Hakeem Omotayo, Adetola Adeeko,
  Abeeb Afolabi, Anuoluwapo Aremu, Olanrewaju Samuel, Clemencia Siro, Wangari
  Kimotho, Onyekachi Ogbu, Chinedu Mbonu, Chiamaka Chukwuneke, Samuel Fanijo,
  Jessica Ojo, Oyinkansola Awosan, Tadesse Kebede, Toadoum Sari Sakayo, Pamela
  Nyatsine, Freedmore Sidume, Oreen Yousuf, Mardiyyah Oduwole, Tshinu Tshinu,
  Ussen Kimanuka, Thina Diko, Siyanda Nxakama, Sinodos Nigusse, Abdulmejid
  Johar, Shafie Mohamed, Fuad Mire Hassan, Moges Ahmed Mehamed, Evrard Ngabire,
  Jules Jules, Ivan Ssenkungu and Pontus Stenetorp",MasakhaNEWS: News Topic Classification for African languages,Accepted to IJCNLP-AACL 2023 (main conference),,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  African languages are severely under-represented in NLP research due to lack of datasets covering several NLP tasks. While there are individual language specific datasets that are being expanded to different tasks, only a handful of NLP tasks (e.g. named entity recognition and machine translation) have standardized benchmark datasets covering several geographical and typologically-diverse African languages. In this paper, we develop MasakhaNEWS -- a new benchmark dataset for news topic classification covering 16 languages widely spoken in Africa. We provide an evaluation of baseline models by training classical machine learning models and fine-tuning several language models. Furthermore, we explore several alternatives to full fine-tuning of language models that are better suited for zero-shot and few-shot learning such as cross-lingual parameter-efficient fine-tuning (like MAD-X), pattern exploiting training (PET), prompting language models (like ChatGPT), and prompt-free sentence transformer fine-tuning (SetFit and Cohere Embedding API). Our evaluation in zero-shot setting shows the potential of prompting ChatGPT for news topic classification in low-resource African languages, achieving an average performance of 70 F1 points without leveraging additional supervision like MAD-X. In few-shot setting, we show that with as little as 10 examples per label, we achieved more than 90\% (i.e. 86.0 F1 points) of the performance of full supervised training (92.6 F1 points) leveraging the PET approach. ","[{'version': 'v1', 'created': 'Wed, 19 Apr 2023 21:12:23 GMT'}, {'version': 'v2', 'created': 'Wed, 20 Sep 2023 17:14:40 GMT'}]",2023-09-21,"[['Adelani', 'David Ifeoluwa', ''], ['Masiak', 'Marek', ''], ['Azime', 'Israel Abebe', ''], ['Alabi', 'Jesujoba', ''], ['Tonja', 'Atnafu Lambebo', ''], ['Mwase', 'Christine', ''], ['Ogundepo', 'Odunayo', ''], ['Dossou', 'Bonaventure F. P.', ''], ['Oladipo', 'Akintunde', ''], ['Nixdorf', 'Doreen', ''], ['Emezue', 'Chris Chinenye', ''], ['al-azzawi', 'sana', ''], ['Sibanda', 'Blessing', ''], ['David', 'Davis', ''], ['Ndolela', 'Lolwethu', ''], ['Mukiibi', 'Jonathan', ''], ['Ajayi', 'Tunde', ''], ['Moteu', 'Tatiana', ''], ['Odhiambo', 'Brian', ''], ['Owodunni', 'Abraham', ''], ['Obiefuna', 'Nnaemeka', ''], ['Mohamed', 'Muhidin', ''], ['Muhammad', 'Shamsuddeen Hassan', ''], ['Ababu', 'Teshome Mulugeta', ''], ['Salahudeen', 'Saheed Abdullahi', ''], ['Yigezu', 'Mesay Gemeda', ''], ['Gwadabe', 'Tajuddeen', ''], ['Abdulmumin', 'Idris', ''], ['Taye', 'Mahlet', ''], ['Awoyomi', 'Oluwabusayo', ''], ['Shode', 'Iyanuoluwa', ''], ['Adelani', 'Tolulope', ''], ['Abdulganiyu', 'Habiba', ''], ['Omotayo', 'Abdul-Hakeem', ''], ['Adeeko', 'Adetola', ''], ['Afolabi', 'Abeeb', ''], ['Aremu', 'Anuoluwapo', ''], ['Samuel', 'Olanrewaju', ''], ['Siro', 'Clemencia', ''], ['Kimotho', 'Wangari', ''], ['Ogbu', 'Onyekachi', ''], ['Mbonu', 'Chinedu', ''], ['Chukwuneke', 'Chiamaka', ''], ['Fanijo', 'Samuel', ''], ['Ojo', 'Jessica', ''], ['Awosan', 'Oyinkansola', ''], ['Kebede', 'Tadesse', ''], ['Sakayo', 'Toadoum Sari', ''], ['Nyatsine', 'Pamela', ''], ['Sidume', 'Freedmore', ''], ['Yousuf', 'Oreen', ''], ['Oduwole', 'Mardiyyah', ''], ['Tshinu', 'Tshinu', ''], ['Kimanuka', 'Ussen', ''], ['Diko', 'Thina', ''], ['Nxakama', 'Siyanda', ''], ['Nigusse', 'Sinodos', ''], ['Johar', 'Abdulmejid', ''], ['Mohamed', 'Shafie', ''], ['Hassan', 'Fuad Mire', ''], ['Mehamed', 'Moges Ahmed', ''], ['Ngabire', 'Evrard', ''], ['Jules', 'Jules', ''], ['Ssenkungu', 'Ivan', ''], ['Stenetorp', 'Pontus', '']]",1,1,2023-04-19,2,65,1,2,0,2,d847ab7f4109d0a4c640d5ee34b510a76002fddb,258236351.0,https://www.semanticscholar.org/paper/d847ab7f4109d0a4c640d5ee34b510a76002fddb,AfricaNLP,2023.0,68.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2518906', 'name': 'David Ifeoluwa Adelani'}, {'authorId': '2091150316', 'name': 'Marek Masiak'}, {'authorId': '2052358080', 'name': 'Israel Abebe Azime'}, {'authorId': '122367036', 'name': 'Jesujoba Oluwadara Alabi'}, {'authorId': '2148631756', 'name': 'A. Tonja'}, {'authorId': '35882251', 'name': 'Christine Mwase'}, {'authorId': '2166106776', 'name': 'Odunayo Ogundepo'}, {'authorId': '1591111757', 'name': 'Bonaventure F. P. Dossou'}, {'authorId': '2175480812', 'name': 'Akintunde Oladipo'}, {'authorId': '2214804027', 'name': 'Doreen Nixdorf'}, {'authorId': '1591176064', 'name': 'Chris C. Emezue'}, {'authorId': '2094432651', 'name': 'Sana Al-Azzawi'}, {'authorId': '1591123568', 'name': 'Blessing K. Sibanda'}, {'authorId': '2058260775', 'name': 'Davis David'}, {'authorId': '2214814174', 'name': 'Lolwethu Ndolela'}, {'authorId': '1412684911', 'name': 'Jonathan Mukiibi'}, {'authorId': '98725872', 'name': 'T. Ajayi'}, {'authorId': '2188733769', 'name': 'Tatiana Moteu Ngoli'}, {'authorId': '98877434', 'name': 'B. Odhiambo'}, {'authorId': '2188832804', 'name': 'A. Owodunni'}, {'authorId': '2214807035', 'name': 'Nnaemeka C. Obiefuna'}, {'authorId': '7744881', 'name': 'Shamsuddeen Hassan Muhammad'}, {'authorId': '1492068759', 'name': 'S. Abdullahi'}, {'authorId': '2148736370', 'name': 'Mesay Gemeda Yigezu'}, {'authorId': '2352354', 'name': 'T. Gwadabe'}, {'authorId': '1429833598', 'name': 'Idris Abdulmumin'}, {'authorId': '2214807032', 'name': 'Mahlet Taye Bame'}, {'authorId': '2190104797', 'name': 'Oluwabusayo Olufunke Awoyomi'}, {'authorId': '2163094493', 'name': 'Iyanuoluwa Shode'}, {'authorId': '2214806255', 'name': 'T. Adelani'}, {'authorId': '2214814170', 'name': 'Habiba Abdulganiy Kailani'}, {'authorId': '2212894510', 'name': 'Abdul-Hakeem Omotayo'}, {'authorId': '2214804172', 'name': 'Adetola Adeeko'}, {'authorId': '2214809191', 'name': 'Afolabi Abeeb'}, {'authorId': '2056773747', 'name': 'Anuoluwapo Aremu'}, {'authorId': '2164156047', 'name': 'Olanrewaju Samuel'}, {'authorId': '2056776870', 'name': 'Clemencia Siro'}, {'authorId': '2214806253', 'name': 'Wangari Kimotho'}, {'authorId': '2214814462', 'name': 'Onyekachi Raphael Ogbu'}, {'authorId': '2158396538', 'name': 'Chinedu E. Mbonu'}, {'authorId': '73054967', 'name': 'C. Chukwuneke'}, {'authorId': '2214305954', 'name': 'Samuel Fanijo'}, {'authorId': '2214806280', 'name': 'Jessica Ojo'}, {'authorId': '2214814443', 'name': 'Oyinkansola F. Awosan'}, {'authorId': '2214804214', 'name': 'Tadesse Kebede Guge'}, {'authorId': '2214809578', 'name': 'Sakayo Toadoum Sari'}, {'authorId': '2214806244', 'name': 'Pamela Nyatsine'}, {'authorId': '146750684', 'name': 'Freedmore Sidume'}, {'authorId': '2164122749', 'name': 'Oreen Yousuf'}, {'authorId': '2214280653', 'name': 'Mardiyyah Oduwole'}, {'authorId': '2038247196', 'name': 'Ussen Kimanuka'}, {'authorId': '2214809343', 'name': 'Kanda Patrick Tshinu'}, {'authorId': '2214809340', 'name': 'Thina Diko'}, {'authorId': '2214809338', 'name': 'Siyanda Nxakama'}, {'authorId': '2214806574', 'name': 'Abdulmejid Tuni Johar'}, {'authorId': '2214809336', 'name': 'Sinodos Gebre'}, {'authorId': '47302320', 'name': 'Muhidin A. Mohamed'}, {'authorId': '2192964958', 'name': 'Shafie Abdi Mohamed'}, {'authorId': '2034285229', 'name': 'Fuad Mire Hassan'}, {'authorId': '2212794028', 'name': 'Moges Ahmed Mehamed'}, {'authorId': '2214804211', 'name': 'Evrard Ngabire'}, {'authorId': '1918552', 'name': 'Pontus Stenetorp'}]","['Insight Centre for Data Analytics, Ireland,', 'Ahmadu Bello University', 'University of California, Davis', 'Somali National University', 'National Open University of Nigeria', 'Jamhuriya University of Science and Technology', 'Technical University of Munich', 'Kaduna State University', 'Paderborn University', 'University of Waterloo', 'Aston University', 'Portuguese Environment Agency', 'College of Saint Rose', 'Lancaster University', 'Botswana International University of Science and Technology', 'Iowa State University', 'Instituto Politcnico Nacional', 'University College London', 'Lule University of Technology', 'Makerere University', 'Nnamdi Azikiwe University', 'University of Porto', 'Tanzania Data Lab, Tanzania', 'University of Amsterdam', 'Haramaya University', 'Deutschzentrum an der Universitt Burundi, Ethiopia', 'Montclair State University', 'Wuhan University of Technology', 'McGill University', 'University of Rwanda', 'Universidad Nacional de La Plata', 'Saarland University', 'Mila Quebec AI Institute, Canada,', 'Fudan University', 'Dire Dawa University', 'Food Research Institute']","['Germany', 'Netherlands', 'Argentina', 'Sweden', 'United Kingdom', 'Canada', 'Nigeria', 'Ethiopia', 'Mexico', 'Uganda', 'Tanzania', 'Botswana', 'Portugal', 'China', 'Ireland', 'Rwanda', 'Somalia', 'United States', 'Japan']",2023-04
2304.09974,Lalithkumar Seenivasan,"Lalithkumar Seenivasan, Mobarakol Islam, Gokul Kannan and Hongliang
  Ren",SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,"The manuscript is accepted in MICCAI 2023. Code are available at:
  https://github.com/lalithjets/SurgicalGPT",,,,cs.CV cs.AI eess.IV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Advances in GPT-based large language models (LLMs) are revolutionizing natural language processing, exponentially increasing its use across various domains. Incorporating uni-directional attention, these autoregressive LLMs can generate long and coherent paragraphs. However, for visual question answering (VQA) tasks that require both vision and language processing, models with bi-directional attention or models employing fusion techniques are often employed to capture the context of multiple modalities all at once. As GPT does not natively process vision tokens, to exploit the advancements in GPT models for VQA in robotic surgery, we design an end-to-end trainable Language-Vision GPT (LV-GPT) model that expands the GPT2 model to include vision input (image). The proposed LV-GPT incorporates a feature extractor (vision tokenizer) and vision token embedding (token type and pose). Given the limitations of unidirectional attention in GPT models and their ability to generate coherent long paragraphs, we carefully sequence the word tokens before vision tokens, mimicking the human thought process of understanding the question to infer an answer from an image. Quantitatively, we prove that the LV-GPT model outperforms other state-of-the-art VQA models on two publically available surgical-VQA datasets (based on endoscopic vision challenge robotic scene segmentation 2018 and CholecTriplet2021) and on our newly annotated dataset (based on the holistic surgical scene dataset). We further annotate all three datasets to include question-type annotations to allow sub-type analysis. Furthermore, we extensively study and present the effects of token sequencing, token type and pose embedding for vision tokens in the LV-GPT model. ","[{'version': 'v1', 'created': 'Wed, 19 Apr 2023 21:22:52 GMT'}, {'version': 'v2', 'created': 'Sat, 22 Jul 2023 15:43:46 GMT'}]",2023-07-25,"[['Seenivasan', 'Lalithkumar', ''], ['Islam', 'Mobarakol', ''], ['Kannan', 'Gokul', ''], ['Ren', 'Hongliang', '']]",0,1,2023-04-19,2,4,3,1,1,0,95430a76264a9be7d64633e56831c60041fb2948,258236427.0,https://www.semanticscholar.org/paper/95430a76264a9be7d64633e56831c60041fb2948,International Conference on Medical Image Computing and Computer-Assisted Intervention,2023.0,25.0,3.0,0.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1409253593', 'name': 'L. Seenivasan'}, {'authorId': '145481136', 'name': 'Mobarakol Islam'}, {'authorId': '2214813455', 'name': 'Gokul Kannan'}, {'authorId': '1738214662', 'name': 'Hongliang Ren'}]","['University College London', 'National University of Singapore', 'Maulana Azad National Institute of Technology', 'Chinese University of Hong Kong']","['China', 'India', 'United Kingdom', 'Singapore']",2023-04
2304.10145,Yiming Zhu,"Yiming Zhu, Peixian Zhang, Ehsan-Ul Haq, Pan Hui, Gareth Tyson",Can ChatGPT Reproduce Human-Generated Labels? A Study of Social Computing Tasks,,,,,cs.AI cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  The release of ChatGPT has uncovered a range of possibilities whereby large language models (LLMs) can substitute human intelligence. In this paper, we seek to understand whether ChatGPT has the potential to reproduce human-generated label annotations in social computing tasks. Such an achievement could significantly reduce the cost and complexity of social computing research. As such, we use ChatGPT to relabel five seminal datasets covering stance detection (2x), sentiment analysis, hate speech, and bot detection. Our results highlight that ChatGPT does have the potential to handle these data annotation tasks, although a number of challenges remain. ChatGPT obtains an average accuracy 0.609. Performance is highest for the sentiment analysis dataset, with ChatGPT correctly annotating 64.9% of tweets. Yet, we show that performance varies substantially across individual labels. We believe this work can open up new lines of analysis and act as a basis for future research into the exploitation of ChatGPT for human annotation tasks. ","[{'version': 'v1', 'created': 'Thu, 20 Apr 2023 08:08:12 GMT'}, {'version': 'v2', 'created': 'Sat, 22 Apr 2023 08:55:33 GMT'}]",2023-04-25,"[['Zhu', 'Yiming', ''], ['Zhang', 'Peixian', ''], ['Haq', 'Ehsan-Ul', ''], ['Hui', 'Pan', ''], ['Tyson', 'Gareth', '']]",1,1,2023-04-20,2,5,2,1,0,1,2f45e71b2fddccf73430c8ecb1642b577a22610b,258236184.0,https://www.semanticscholar.org/paper/2f45e71b2fddccf73430c8ecb1642b577a22610b,arXiv.org,2023.0,47.0,30.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2128128019', 'name': 'Yiming Zhu'}, {'authorId': '2214837096', 'name': 'Peixian Zhang'}, {'authorId': '144505165', 'name': 'E. Haq'}, {'authorId': '143966169', 'name': 'Pan Hui'}, {'authorId': '2181343328', 'name': 'Gareth Tyson'}]",['Hong Kong University of Science and Technology'],['China'],2023-04
2304.10149,Junling Liu,"Junling Liu, Chao Liu, Peilin Zhou, Renjie Lv, Kang Zhou, Yan Zhang",Is ChatGPT a Good Recommender? A Preliminary Study,,,,,cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recommendation systems have witnessed significant advancements and have been widely used over the past decades. However, most traditional recommendation methods are task-specific and therefore lack efficient generalization ability. Recently, the emergence of ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. Nonetheless, the application of ChatGPT in the recommendation domain has not been thoroughly investigated. In this paper, we employ ChatGPT as a general-purpose recommendation model to explore its potential for transferring extensive linguistic and world knowledge acquired from large-scale corpora to recommendation scenarios. Specifically, we design a set of prompts and evaluate ChatGPT's performance on five recommendation scenarios. Unlike traditional recommendation methods, we do not fine-tune ChatGPT during the entire evaluation process, relying only on the prompts themselves to convert recommendation tasks into natural language tasks. Further, we explore the use of few-shot prompting to inject interaction information that contains user potential interest to help ChatGPT better understand user needs and interests. Comprehensive experimental results on Amazon Beauty dataset show that ChatGPT has achieved promising results in certain tasks and is capable of reaching the baseline level in others. We conduct human evaluations on two explainability-oriented tasks to more accurately evaluate the quality of contents generated by different models. And the human evaluations show ChatGPT can truly understand the provided information and generate clearer and more reasonable results. We hope that our study can inspire researchers to further explore the potential of language models like ChatGPT to improve recommendation performance and contribute to the advancement of the recommendation systems field. ","[{'version': 'v1', 'created': 'Thu, 20 Apr 2023 08:16:07 GMT'}, {'version': 'v2', 'created': 'Mon, 26 Jun 2023 15:43:04 GMT'}]",2023-06-27,"[['Liu', 'Junling', ''], ['Liu', 'Chao', ''], ['Zhou', 'Peilin', ''], ['Lv', 'Renjie', ''], ['Zhou', 'Kang', ''], ['Zhang', 'Yan', '']]",1,1,2023-04-20,2,6,1,1,0,1,ca7bd64d372e3bcb3f4633ca4a20291ff57de3c3,258236609.0,https://www.semanticscholar.org/paper/ca7bd64d372e3bcb3f4633ca4a20291ff57de3c3,arXiv.org,2023.0,72.0,57.0,9.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2109104637', 'name': 'Junling Liu'}, {'authorId': '50557623', 'name': 'Chaoyong Liu'}, {'authorId': '2214809142', 'name': 'Renjie Lv'}, {'authorId': '96746184', 'name': 'Kangdi Zhou'}, {'authorId': '2152822535', 'name': 'Y. Zhang'}]","['Alibaba', 'University of Science and Technology']","['China', 'Yemen']",2023-04
2304.10392,Quyet V. Do,"Tianqing Fang, Quyet V. Do, Sehyun Choi, Weiqi Wang, Yangqiu Song",CKBP v2: An Expert-Annotated Evaluation Set for Commonsense Knowledge Base Population,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Populating Commonsense Knowledge Bases (CSKB) is an important yet hard task in NLP, as it tackles knowledge from external sources with unseen events and entities. Fang et al. (2021a) proposed a CSKB Population benchmark with an evaluation set CKBP v1. However, CKBP v1 adopts crowdsourced annotations that suffer from a substantial fraction of incorrect answers, and the evaluation set is not well-aligned with the external knowledge source as a result of random sampling. In this paper, we introduce CKBP v2, a new high-quality CSKB Population benchmark, which addresses the two mentioned problems by using experts instead of crowd-sourced annotation and by adding diversified adversarial samples to make the evaluation set more representative. We conduct extensive experiments comparing state-of-the-art methods for CSKB Population on the new evaluation set for future research comparisons. Empirical results show that the population task is still challenging, even for large language models (LLM) such as ChatGPT. Codes and data are available at https://github.com/HKUST-KnowComp/CSKB-Population. ","[{'version': 'v1', 'created': 'Thu, 20 Apr 2023 15:27:29 GMT'}]",2023-04-21,"[['Fang', 'Tianqing', ''], ['Do', 'Quyet V.', ''], ['Choi', 'Sehyun', ''], ['Wang', 'Weiqi', ''], ['Song', 'Yangqiu', '']]",1,1,2023-04-20,1,5,1,1,0,1,3270c3054e6a14c5ee483f690ccda7367dd9a556,258236552.0,https://www.semanticscholar.org/paper/3270c3054e6a14c5ee483f690ccda7367dd9a556,arXiv.org,2023.0,54.0,6.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2044202073', 'name': 'Tianqing Fang'}, {'authorId': '2187874252', 'name': 'Quyet V. Do'}, {'authorId': '2127128777', 'name': 'Sehyun Choi'}, {'authorId': '1587728690', 'name': 'Weiqi Wang'}, {'authorId': '1809614', 'name': 'Yangqiu Song'}]",['Hong Kong University of Science and Technology'],['China'],2023-04
2304.10436,Hao Sun,"Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, Minlie Huang",Safety Assessment of Chinese Large Language Models,"Benchmark website: http://coai.cs.tsinghua.edu.cn/leaderboard/ ;
  SafetyPrompts repo: https://github.com/thu-coai/Safety-Prompts",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the rapid popularity of large language models such as ChatGPT and GPT-4, a growing amount of attention is paid to their safety concerns. These models may generate insulting and discriminatory content, reflect incorrect social values, and may be used for malicious purposes such as fraud and dissemination of misleading information. Evaluating and enhancing their safety is particularly essential for the wide application of large language models (LLMs). To further promote the safe deployment of LLMs, we develop a Chinese LLM safety assessment benchmark. Our benchmark explores the comprehensive safety performance of LLMs from two perspectives: 8 kinds of typical safety scenarios and 6 types of more challenging instruction attacks. Our benchmark is based on a straightforward process in which it provides the test prompts and evaluates the safety of the generated responses from the evaluated model. In evaluation, we utilize the LLM's strong evaluation ability and develop it as a safety evaluator by prompting. On top of this benchmark, we conduct safety assessments and analyze 15 LLMs including the OpenAI GPT series and other well-known Chinese LLMs, where we observe some interesting findings. For example, we find that instruction attacks are more likely to expose safety issues of all LLMs. Moreover, to promote the development and deployment of safe, responsible, and ethical AI, we publicly release SafetyPrompts including 100k augmented prompts and responses by LLMs. ","[{'version': 'v1', 'created': 'Thu, 20 Apr 2023 16:27:35 GMT'}]",2023-04-21,"[['Sun', 'Hao', ''], ['Zhang', 'Zhexin', ''], ['Deng', 'Jiawen', ''], ['Cheng', 'Jiale', ''], ['Huang', 'Minlie', '']]",1,1,2023-04-20,1,5,1,2,0,2,59fc49dfd81b92661437eaf7e339c0792ccd8755,258236069.0,https://www.semanticscholar.org/paper/59fc49dfd81b92661437eaf7e339c0792ccd8755,arXiv.org,2023.0,25.0,23.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '144990601', 'name': 'Hao Sun'}, {'authorId': '101371510', 'name': 'Zhexin Zhang'}, {'authorId': '2090444914', 'name': 'Jiawen Deng'}, {'authorId': '2109077637', 'name': 'Jiale Cheng'}, {'authorId': '2196817617', 'name': 'Minlie Huang'}]","['The CoAI group, DCST; Institute for Artificial Intelligence; State Key Lab of Intelligent Technology and Systems;', 'Tsinghua University']",['China'],2023-04
2304.10453,Benyou Wang,"Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming
  Chen, Hongbo Zhang, Juhao Liang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xiang
  Wan, Benyou Wang, Haizhou Li",Phoenix: Democratizing ChatGPT across Languages,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  This paper presents our efforts to democratize ChatGPT across language. We release a large language model ""Phoenix"", achieving competitive performance among open-source English and Chinese models while excelling in languages with limited resources (covering both Latin and non-Latin languages). We believe this work will be beneficial to make ChatGPT more accessible, especially in countries where people cannot use ChatGPT due to restrictions from OpenAI or local goverments. Our data, code, and models are available at https://github.com/FreedomIntelligence/LLMZoo. ","[{'version': 'v1', 'created': 'Thu, 20 Apr 2023 16:50:04 GMT'}]",2023-04-21,"[['Chen', 'Zhihong', ''], ['Jiang', 'Feng', ''], ['Chen', 'Junying', ''], ['Wang', 'Tiannan', ''], ['Yu', 'Fei', ''], ['Chen', 'Guiming', ''], ['Zhang', 'Hongbo', ''], ['Liang', 'Juhao', ''], ['Zhang', 'Chen', ''], ['Zhang', 'Zhiyi', ''], ['Li', 'Jianquan', ''], ['Wan', 'Xiang', ''], ['Wang', 'Benyou', ''], ['Li', 'Haizhou', '']]",1,1,2023-04-20,1,14,2,1,0,1,d2b4b4804479714ad0e406e8ab73fa3e72069216,258236343.0,https://www.semanticscholar.org/paper/d2b4b4804479714ad0e406e8ab73fa3e72069216,arXiv.org,2023.0,16.0,19.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '46843171', 'name': 'Zhihong Chen'}, {'authorId': '2214807050', 'name': 'Feng Jiang'}, {'authorId': '2108170007', 'name': 'Junying Chen'}, {'authorId': '2214867501', 'name': 'Tiannan Wang'}, {'authorId': '40471592', 'name': 'Fei Yu'}, {'authorId': '2214875415', 'name': 'Guiming Chen'}, {'authorId': '2116271777', 'name': 'Hongbo Zhang'}, {'authorId': '2212240432', 'name': 'Juhao Liang'}, {'authorId': None, 'name': 'Chen Zhang'}, {'authorId': '2214794954', 'name': 'Zhiyi Zhang'}, {'authorId': '2130169642', 'name': 'Jianquan Li'}, {'authorId': '2101317304', 'name': 'Xiang Wan'}, {'authorId': '2894465', 'name': 'Benyou Wang'}, {'authorId': '2119251083', 'name': 'Haizhou Li'}]","['Chinese University of Hong Kong, Shenzhen', 'Shenzhen Research Institute of Big Data']",['China'],2023-04
2304.10464,Yiduo Guo,"Yiduo Guo, Yaobo Liang, Chenfei Wu, Wenshan Wu, Dongyan Zhao, Nan Duan",Learning to Program with Natural Language,Work in progress,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) have shown remarkable performance in various basic natural language tasks, which raises hope for achieving Artificial General Intelligence. For completing the complex task, we still need a program for the task first and then ask LLMs to follow the program to generate the specific solution. We propose using natural language as a new programming language to describe task procedures, making them easily understandable to both humans and LLMs. ~The LLM is capable of directly generating natural language programs, but these programs may still contain factual errors or incomplete steps. Therefore, we further propose the Learning to Program (\text{LP}) method to ask LLMs themselves to learn the natural language program based on the training dataset of the complex task first and then use the learned program to guide the inference. Our experiments on the reasoning tasks of five different reasoning types (8 datasets) demonstrate the effectiveness of our approach. Further, our analysis experiment shows that the learned program can be directly used to guide another LLM to improve its performance, which reveals a new transfer learning paradigm. ","[{'version': 'v1', 'created': 'Thu, 20 Apr 2023 17:09:12 GMT'}, {'version': 'v2', 'created': 'Sun, 23 Apr 2023 11:04:30 GMT'}, {'version': 'v3', 'created': 'Mon, 29 May 2023 01:12:18 GMT'}]",2023-05-30,"[['Guo', 'Yiduo', ''], ['Liang', 'Yaobo', ''], ['Wu', 'Chenfei', ''], ['Wu', 'Wenshan', ''], ['Zhao', 'Dongyan', ''], ['Duan', 'Nan', '']]",0,0,2023-04-20,3,6,1,0,0,0,e36e5df3dde73c4e3606cdd4498cfc304a29bf5c,258236649.0,https://www.semanticscholar.org/paper/e36e5df3dde73c4e3606cdd4498cfc304a29bf5c,arXiv.org,2023.0,37.0,9.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2214448244', 'name': 'Yiduo Guo'}, {'authorId': '3887469', 'name': 'Yaobo Liang'}, {'authorId': '2151101534', 'name': 'Chenfei Wu'}, {'authorId': '51198241', 'name': 'Wenshan Wu'}, {'authorId': '144060462', 'name': 'Dongyan Zhao'}, {'authorId': '2072609829', 'name': 'Nan Duan'}]","['Peking University', 'Microsoft']",['China'],2023-04
2304.11076,Xingxuan Li,"Ruochen Zhao, Xingxuan Li, Yew Ken Chia, Bosheng Ding, Lidong Bing",Can ChatGPT-like Generative Models Guarantee Factual Accuracy? On the Mistakes of New Generation Search Engines,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although large conversational AI models such as OpenAI's ChatGPT have demonstrated great potential, we question whether such models can guarantee factual accuracy. Recently, technology companies such as Microsoft and Google have announced new services which aim to combine search engines with conversational AI. However, we have found numerous mistakes in the public demonstrations that suggest we should not easily trust the factual claims of the AI models. Rather than criticizing specific models or companies, we hope to call on researchers and developers to improve AI models' transparency and factual correctness. ","[{'version': 'v1', 'created': 'Fri, 3 Mar 2023 04:27:44 GMT'}]",2023-04-24,"[['Zhao', 'Ruochen', ''], ['Li', 'Xingxuan', ''], ['Chia', 'Yew Ken', ''], ['Ding', 'Bosheng', ''], ['Bing', 'Lidong', '']]",1,1,2023-03-03,1,5,2,1,0,1,5723773df1bf7ea4cfd46a3af7ba6be8c1ce331f,258291855.0,https://www.semanticscholar.org/paper/5723773df1bf7ea4cfd46a3af7ba6be8c1ce331f,arXiv.org,2023.0,23.0,11.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2091437375', 'name': 'Ruochen Zhao'}, {'authorId': '2155447436', 'name': 'Xingxuan Li'}, {'authorId': '2066312627', 'name': 'Yew Ken Chia'}, {'authorId': '2064493724', 'name': 'Bosheng Ding'}, {'authorId': '1996394', 'name': 'Lidong Bing'}]","['Alibaba', 'Nanyang Technological University']","['China', 'Singapore']",2023-03
2304.11107,Xiang Li,"Tianyang Zhong, Yaonai Wei, Li Yang, Zihao Wu, Zhengliang Liu,
  Xiaozheng Wei, Wenjun Li, Junjie Yao, Chong Ma, Xiang Li, Dajiang Zhu, Xi
  Jiang, Junwei Han, Dinggang Shen, Tianming Liu, Tuo Zhang",ChatABL: Abductive Learning via Natural Language Interaction with ChatGPT,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) such as ChatGPT have recently demonstrated significant potential in mathematical abilities, providing valuable reasoning paradigm consistent with human natural language. However, LLMs currently have difficulty in bridging perception, language understanding and reasoning capabilities due to incompatibility of the underlying information flow among them, making it challenging to accomplish tasks autonomously. On the other hand, abductive learning (ABL) frameworks for integrating the two abilities of perception and reasoning has seen significant success in inverse decipherment of incomplete facts, but it is limited by the lack of semantic understanding of logical reasoning rules and the dependence on complicated domain knowledge representation. This paper presents a novel method (ChatABL) for integrating LLMs into the ABL framework, aiming at unifying the three abilities in a more user-friendly and understandable manner. The proposed method uses the strengths of LLMs' understanding and logical reasoning to correct the incomplete logical facts for optimizing the performance of perceptual module, by summarizing and reorganizing reasoning rules represented in natural language format. Similarly, perceptual module provides necessary reasoning examples for LLMs in natural language format. The variable-length handwritten equation deciphering task, an abstract expression of the Mayan calendar decoding, is used as a testbed to demonstrate that ChatABL has reasoning ability beyond most existing state-of-the-art methods, which has been well supported by comparative studies. To our best knowledge, the proposed ChatABL is the first attempt to explore a new pattern for further approaching human-level cognitive ability via natural language interaction with ChatGPT. ","[{'version': 'v1', 'created': 'Fri, 21 Apr 2023 16:23:47 GMT'}]",2023-04-24,"[['Zhong', 'Tianyang', ''], ['Wei', 'Yaonai', ''], ['Yang', 'Li', ''], ['Wu', 'Zihao', ''], ['Liu', 'Zhengliang', ''], ['Wei', 'Xiaozheng', ''], ['Li', 'Wenjun', ''], ['Yao', 'Junjie', ''], ['Ma', 'Chong', ''], ['Li', 'Xiang', ''], ['Zhu', 'Dajiang', ''], ['Jiang', 'Xi', ''], ['Han', 'Junwei', ''], ['Shen', 'Dinggang', ''], ['Liu', 'Tianming', ''], ['Zhang', 'Tuo', '']]",1,1,2023-04-21,1,16,2,1,0,1,4c8ef2db0c77aba453783f5211ebafc6695d3835,258291600.0,https://www.semanticscholar.org/paper/4c8ef2db0c77aba453783f5211ebafc6695d3835,arXiv.org,2023.0,69.0,18.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2215167446', 'name': 'Tianyang Zhong'}, {'authorId': '2163675860', 'name': 'Yaonai Wei'}, {'authorId': '2153206781', 'name': 'Li Yang'}, {'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '2145977326', 'name': 'Zheng Liu'}, {'authorId': '2202921386', 'name': 'Xiaozheng Wei'}, {'authorId': '2108956293', 'name': 'WenJu Sun'}, {'authorId': '1418690977', 'name': 'Junjie Yao'}, {'authorId': '2108624783', 'name': 'Chongfei Ma'}, {'authorId': '2144438902', 'name': 'Xiang Li'}, {'authorId': '2181182', 'name': 'Dajiang Zhu'}, {'authorId': '143796247', 'name': 'Xi Jiang'}, {'authorId': '2161945158', 'name': 'Jun-Feng Han'}, {'authorId': '2150038187', 'name': 'Dinggang Shen'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}, {'authorId': '49104946', 'name': 'Tuo Zhang'}]","['University of Electronic Science and Technology of China', 'ShanghaiTech University', 'The University of Texas at Arlington', 'Shanghai Clinical Research Center', 'Massachusetts General Hospital', 'United Imaging', 'University of Georgia', 'Northwestern Polytechnical University']","['China', 'United States']",2023-04
2304.11276,Hengky Susanto,"Hengky Susanto, David James Woo, and Kai Guo",The Role of AI in Human-AI Creative Writing for Hong Kong Secondary Students,,"International Council of Teachers of English (ICTE) Newsletter
  (Spring 2023)",,,cs.CL,http://creativecommons.org/publicdomain/zero/1.0/,"  The recent advancement in Natural Language Processing (NLP) capability has led to the development of language models (e.g., ChatGPT) that is capable of generating human-like language. In this study, we explore how language models can be utilized to help the ideation aspect of creative writing. Our empirical findings show that language models play different roles in helping student writers to be more creative, such as the role of a collaborator, a provocateur, etc ","[{'version': 'v1', 'created': 'Fri, 21 Apr 2023 23:50:09 GMT'}]",2023-04-25,"[['Susanto', 'Hengky', ''], ['Woo', 'David James', ''], ['Guo', 'Kai', '']]",1,1,2023-04-21,1,3,1,1,0,1,c1c60be1c4c335564556c25ec901cd7028a880de,258298169.0,https://www.semanticscholar.org/paper/c1c60be1c4c335564556c25ec901cd7028a880de,arXiv.org,2023.0,3.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Education', 'source': 's2-fos-model'}]","[{'authorId': '3353735', 'name': 'Hengky Susanto'}, {'authorId': '12132198', 'name': 'D. Woo'}, {'authorId': '2061502406', 'name': 'Kai Guo'}]","['University of Massachusetts Lowell', 'Precious Blood Secondary School, Hong Kong, China', 'University of Hong Kong']","['China', 'United States', 'Hong Kong']",2023-04
2304.11384,Mingyang Geng,"Mingyang Geng, Shangwen Wang, Dezun Dong, Haotian Wang, Ge Li, Zhi
  Jin, Xiaoguang Mao, Xiangke Liao",Large Language Models are Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning,"Accepted by the 46th International Conference on Software Engineering
  (ICSE 2024)",,,,cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Code comment generation aims at generating natural language descriptions for a code snippet to facilitate developers' program comprehension activities. Despite being studied for a long time, a bottleneck for existing approaches is that given a code snippet, they can only generate one comment while developers usually need to know information from diverse perspectives such as what is the functionality of this code snippet and how to use it. To tackle this limitation, this study empirically investigates the feasibility of utilizing large language models (LLMs) to generate comments that can fulfill developers' diverse intents. Our intuition is based on the facts that (1) the code and its pairwise comment are used during the pre-training process of LLMs to build the semantic connection between the natural language and programming language, and (2) comments in the real-world projects, which are collected for the pre-training, usually contain different developers' intents. We thus postulate that the LLMs can already understand the code from different perspectives after the pre-training. Indeed, experiments on two large-scale datasets demonstrate the rationale of our insights: by adopting the in-context learning paradigm and giving adequate prompts to the LLM (e.g., providing it with ten or more examples), the LLM can significantly outperform a state-of-the-art supervised learning approach on generating comments with multiple intents. Results also show that customized strategies for constructing the prompts and post-processing strategies for reranking the results can both boost the LLM's performances, which shed light on future research directions for using LLMs to achieve comment generation. ","[{'version': 'v1', 'created': 'Sat, 22 Apr 2023 12:26:24 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Jun 2023 15:26:21 GMT'}, {'version': 'v3', 'created': 'Wed, 14 Jun 2023 06:33:10 GMT'}]",2023-06-16,"[['Geng', 'Mingyang', ''], ['Wang', 'Shangwen', ''], ['Dong', 'Dezun', ''], ['Wang', 'Haotian', ''], ['Li', 'Ge', ''], ['Jin', 'Zhi', ''], ['Mao', 'Xiaoguang', ''], ['Liao', 'Xiangke', '']]",0,0,2023-04-22,3,8,1,0,0,0,375a571174ea59b1f4aa62ad2619e9593fc03436,259108973.0,https://www.semanticscholar.org/paper/375a571174ea59b1f4aa62ad2619e9593fc03436,,2023.0,76.0,7.0,1.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2141549468', 'name': 'Mingyang Geng'}, {'authorId': '49184504', 'name': 'Shangwen Wang'}, {'authorId': '39891717', 'name': 'Dezun Dong'}, {'authorId': '1728073', 'name': 'Hao Wang'}, {'authorId': '2154591375', 'name': 'Ge Li'}, {'authorId': '2152843753', 'name': 'Zhi Jin'}, {'authorId': '2150465957', 'name': 'Xiaoguang Mao'}, {'authorId': '144078016', 'name': 'Xiangke Liao'}]","['Peking University', 'National University of Defense Technology', 'The University of Tokyo']","['China', 'Japan']",2023-04
2304.11556,Zhao Tan,Xiping Liu and Zhao Tan,Divide and Prompt: Chain of Thought Prompting for Text-to-SQL,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Chain-of-thought (CoT) prompting combined with large language models (LLMs) have achieved encouraging results on complex reasoning tasks. Text-to-SQL is a critical semantic parsing task that converts natural language questions into SQL statements, involving a complex reasoning process. However, there is little work about using CoT prompting to activate LLM's reasoning capabilities on Text-to-SQL tasks. In this work, we propose a new paradigm for prompting Text-to-SQL tasks, called Divide-and-Prompt, which first divides the task into subtasks, and then approach each subtask through CoT. We present 3 prompting-based methods to enhance the Text-to-SQL ability of LLMs. Experiments show that these prompts guide LLMs to generate Text-to-SQL with higher execution accuracy. ","[{'version': 'v1', 'created': 'Sun, 23 Apr 2023 06:52:35 GMT'}]",2023-04-25,"[['Liu', 'Xiping', ''], ['Tan', 'Zhao', '']]",0,0,2023-04-23,1,2,2,0,0,0,40c9280d87059c0cc28f2a08d46a7045fa3e9736,258298777.0,https://www.semanticscholar.org/paper/40c9280d87059c0cc28f2a08d46a7045fa3e9736,arXiv.org,2023.0,25.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '27469537', 'name': 'X. Liu'}, {'authorId': '2093186622', 'name': 'Zhao Tan'}]",['Jiangxi University of Finance and Economics'],['China'],2023-04
2304.11567,Wenxiong Liao,"Wenxiong Liao, Zhengliang Liu, Haixing Dai, Shaochen Xu, Zihao Wu,
  Yiyang Zhang, Xiaoke Huang, Dajiang Zhu, Hongmin Cai, Tianming Liu, Xiang Li",Differentiate ChatGPT-generated and Human-written Medical Texts,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Background: Large language models such as ChatGPT are capable of generating grammatically perfect and human-like text content, and a large number of ChatGPT-generated texts have appeared on the Internet. However, medical texts such as clinical notes and diagnoses require rigorous validation, and erroneous medical content generated by ChatGPT could potentially lead to disinformation that poses significant harm to healthcare and the general public.   Objective: This research is among the first studies on responsible and ethical AIGC (Artificial Intelligence Generated Content) in medicine. We focus on analyzing the differences between medical texts written by human experts and generated by ChatGPT, and designing machine learning workflows to effectively detect and differentiate medical texts generated by ChatGPT.   Methods: We first construct a suite of datasets containing medical texts written by human experts and generated by ChatGPT. In the next step, we analyze the linguistic features of these two types of content and uncover differences in vocabulary, part-of-speech, dependency, sentiment, perplexity, etc. Finally, we design and implement machine learning methods to detect medical text generated by ChatGPT.   Results: Medical texts written by humans are more concrete, more diverse, and typically contain more useful information, while medical texts generated by ChatGPT pay more attention to fluency and logic, and usually express general terminologies rather than effective information specific to the context of the problem. A BERT-based model can effectively detect medical texts generated by ChatGPT, and the F1 exceeds 95%. ","[{'version': 'v1', 'created': 'Sun, 23 Apr 2023 07:38:07 GMT'}]",2023-04-25,"[['Liao', 'Wenxiong', ''], ['Liu', 'Zhengliang', ''], ['Dai', 'Haixing', ''], ['Xu', 'Shaochen', ''], ['Wu', 'Zihao', ''], ['Zhang', 'Yiyang', ''], ['Huang', 'Xiaoke', ''], ['Zhu', 'Dajiang', ''], ['Cai', 'Hongmin', ''], ['Liu', 'Tianming', ''], ['Li', 'Xiang', '']]",1,1,2023-04-23,1,11,2,1,0,1,286756b2b02d6a7bc49a7ad66686f30831f26c25,258298336.0,https://www.semanticscholar.org/paper/286756b2b02d6a7bc49a7ad66686f30831f26c25,arXiv.org,2023.0,43.0,32.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2036578588', 'name': 'Wenxiong Liao'}, {'authorId': '2145977326', 'name': 'Zheng Liu'}, {'authorId': '29944950', 'name': 'Haixing Dai'}, {'authorId': '2211904452', 'name': 'Shaochen Xu'}, {'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '2108362387', 'name': 'Yiyang Zhang'}, {'authorId': None, 'name': 'Xiaoke Huang'}, {'authorId': '2181182', 'name': 'Dajiang Zhu'}, {'authorId': '2209990831', 'name': 'Hongmin Cai'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}, {'authorId': '2144438902', 'name': 'Xiang Li'}]","['South China University of Technology', 'University of Georgia', 'Massachusetts General Hospital', 'The University of Texas at Arlington']","['China', 'United States']",2023-04
2304.11633,Bo Li,"Bo Li, Gexiang Fang, Yang Yang, Quansen Wang, Wei Ye, Wen Zhao, Shikun
  Zhang","Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The capability of Large Language Models (LLMs) like ChatGPT to comprehend user intent and provide reasonable responses has made them extremely popular lately. In this paper, we focus on assessing the overall ability of ChatGPT using 7 fine-grained information extraction (IE) tasks. Specially, we present the systematically analysis by measuring ChatGPT's performance, explainability, calibration, and faithfulness, and resulting in 15 keys from either the ChatGPT or domain experts. Our findings reveal that ChatGPT's performance in Standard-IE setting is poor, but it surprisingly exhibits excellent performance in the OpenIE setting, as evidenced by human evaluation. In addition, our research indicates that ChatGPT provides high-quality and trustworthy explanations for its decisions. However, there is an issue of ChatGPT being overconfident in its predictions, which resulting in low calibration. Furthermore, ChatGPT demonstrates a high level of faithfulness to the original text in the majority of cases. We manually annotate and release the test sets of 7 fine-grained IE tasks contains 14 datasets to further promote the research. The datasets and code are available at https://github.com/pkuserc/ChatGPT_for_IE. ","[{'version': 'v1', 'created': 'Sun, 23 Apr 2023 12:33:18 GMT'}]",2023-04-25,"[['Li', 'Bo', ''], ['Fang', 'Gexiang', ''], ['Yang', 'Yang', ''], ['Wang', 'Quansen', ''], ['Ye', 'Wei', ''], ['Zhao', 'Wen', ''], ['Zhang', 'Shikun', '']]",1,1,2023-04-23,1,7,1,1,0,1,88abef771472c3aa46c53d5d626a0d0c3b66e8cd,258297899.0,https://www.semanticscholar.org/paper/88abef771472c3aa46c53d5d626a0d0c3b66e8cd,arXiv.org,2023.0,101.0,52.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2485552', 'name': 'Bo Li'}, {'authorId': '2215216578', 'name': 'Gexiang Fang'}, {'authorId': '2152917615', 'name': 'Yang Yang'}, {'authorId': '117898431', 'name': 'Quansen Wang'}, {'authorId': '145235149', 'name': 'Wei Ye'}, {'authorId': '2118223372', 'name': 'Wen Zhao'}, {'authorId': '1705434', 'name': 'Shikun Zhang'}]","['Peking University', 'Boston University']","['China', 'United States']",2023-04
2304.11657,Gasol Sun,"Jiashuo Sun and Yi Luo and Yeyun Gong and Chen Lin and Yelong Shen and
  Jian Guo and Nan Duan",Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models,"32 pages, 13 figures",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) can achieve highly effective performance on various reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting as demonstrations. However, the reasoning chains of demonstrations generated by LLMs are prone to errors, which can subsequently lead to incorrect reasoning during inference. Furthermore, inappropriate exemplars (overly simplistic or complex), can affect overall performance among varying levels of difficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts Prompting), an iterative bootstrapping approach for selecting exemplars and generating reasoning chains. By utilizing iterative bootstrapping, our approach enables LLMs to autonomously rectify errors, resulting in more precise and comprehensive reasoning chains. Simultaneously, our approach selects challenging yet answerable questions accompanied by reasoning chains as exemplars with a moderate level of difficulty, which enhances the LLMs' generalizability across varying levels of difficulty. Experimental results indicate that Iter-CoT exhibits superiority, achieving competitive performance across three distinct reasoning tasks on eleven datasets. ","[{'version': 'v1', 'created': 'Sun, 23 Apr 2023 13:54:39 GMT'}]",2023-04-25,"[['Sun', 'Jiashuo', ''], ['Luo', 'Yi', ''], ['Gong', 'Yeyun', ''], ['Lin', 'Chen', ''], ['Shen', 'Yelong', ''], ['Guo', 'Jian', ''], ['Duan', 'Nan', '']]",0,0,2023-04-23,1,7,2,0,0,0,ac37accd7aedf1c25c3d54c7982579b297b3ff2b,258297976.0,https://www.semanticscholar.org/paper/ac37accd7aedf1c25c3d54c7982579b297b3ff2b,arXiv.org,2023.0,50.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2202454665', 'name': 'Jiashuo Sun'}, {'authorId': '2215233278', 'name': 'Yi Luo'}, {'authorId': '2171182', 'name': 'Yeyun Gong'}, {'authorId': '2186278677', 'name': 'Chen Lin'}, {'authorId': '1752875', 'name': 'Yelong Shen'}, {'authorId': '2188226506', 'name': 'Jian Guo'}, {'authorId': '46429989', 'name': 'Nan Duan'}]","['Xiamen University', 'Institute for Development and Economic Analysis', 'Microsoft']","['China', 'Indonesia']",2023-04
2304.11686,Tsz On Li,"Tsz-On Li, Wenxi Zong, Yibo Wang, Haoye Tian, Ying Wang, Shing-Chi
  Cheung, Jeff Kramer",Nuances are the Key: Unlocking ChatGPT to Find Failure-Inducing Tests with Differential Prompting,"Accepted to the 38th IEEE/ACM International Conference on Automated
  Software Engineering (ASE 2023)",,,,cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automatically detecting software failures is an important task and a longstanding challenge. It requires finding failure-inducing test cases whose test input can trigger the software's fault, and constructing an automated oracle to detect the software's incorrect behaviors. Recent advancement of large language models (LLMs) motivates us to study how far this challenge can be addressed by ChatGPT, a state-of-the-art LLM. Unfortunately, our study shows that ChatGPT has a low probability (28.8%) of finding correct failure-inducing test cases for buggy programs. A possible reason is that finding failure-inducing test cases requires analyzing the subtle code differences between a buggy program and its correct version. When these two versions have similar syntax, ChatGPT is weak at recognizing subtle code differences. Our insight is that ChatGPT's performance can be substantially enhanced when ChatGPT is guided to focus on the subtle code difference. We have an interesting observation that ChatGPT is effective in inferring the intended behaviors of a buggy program. The intended behavior can be leveraged to synthesize programs, in order to make the subtle code difference between a buggy program and its correct version (i.e., the synthesized program) explicit. Driven by this observation, we propose a novel approach that synergistically combines ChatGPT and differential testing to find failure-inducing test cases. We evaluate our approach on Quixbugs (a benchmark of buggy programs), and compare it with state-of-the-art baselines, including direct use of ChatGPT and Pynguin. The experimental result shows that our approach has a much higher probability (77.8%) of finding correct failure-inducing test cases, 2.7X as the best baseline. ","[{'version': 'v1', 'created': 'Sun, 23 Apr 2023 15:35:39 GMT'}, {'version': 'v2', 'created': 'Sun, 30 Apr 2023 06:44:38 GMT'}, {'version': 'v3', 'created': 'Wed, 21 Jun 2023 10:15:09 GMT'}, {'version': 'v4', 'created': 'Sun, 27 Aug 2023 13:53:36 GMT'}, {'version': 'v5', 'created': 'Tue, 5 Sep 2023 07:00:42 GMT'}, {'version': 'v6', 'created': 'Sat, 9 Sep 2023 09:55:18 GMT'}]",2023-09-12,"[['Li', 'Tsz-On', ''], ['Zong', 'Wenxi', ''], ['Wang', 'Yibo', ''], ['Tian', 'Haoye', ''], ['Wang', 'Ying', ''], ['Cheung', 'Shing-Chi', ''], ['Kramer', 'Jeff', '']]",1,1,2023-04-23,6,7,1,1,0,1,c63f92f5598f72e214291e54cacda43eaf26c04c,258298446.0,https://www.semanticscholar.org/paper/c63f92f5598f72e214291e54cacda43eaf26c04c,International Conference on Automated Software Engineering,2023.0,57.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '51129571', 'name': 'T. Li'}, {'authorId': '14921484', 'name': 'Wen-yi Zong'}, {'authorId': '2143436737', 'name': 'Yibo Wang'}, {'authorId': '2069990130', 'name': 'Haoye Tian'}, {'authorId': '49416173', 'name': 'Y. Wang'}, {'authorId': '143608708', 'name': 'S. Cheung'}]","['Guangzhou HKUST Fok Ying Tung Research Institute', 'Hong Kong University of Science and Technology', 'Imperial College London', 'Northeastern University', 'University of Luxembourg']","['China', 'United Kingdom', 'Luxembourg']",2023-04
2304.12008,Peipeng Yu,"Peipeng Yu, Jiahan Chen, Xuan Feng, Zhihua Xia",CHEAT: A Large-scale Dataset for Detecting ChatGPT-writtEn AbsTracts,"9 pages, 6 figures",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The powerful ability of ChatGPT has caused widespread concern in the academic community. Malicious users could synthesize dummy academic content through ChatGPT, which is extremely harmful to academic rigor and originality. The need to develop ChatGPT-written content detection algorithms call for large-scale datasets. In this paper, we initially investigate the possible negative impact of ChatGPT on academia,and present a large-scale CHatGPT-writtEn AbsTract dataset (CHEAT) to support the development of detection algorithms. In particular, the ChatGPT-written abstract dataset contains 35,304 synthetic abstracts, with Generation, Polish, and Mix as prominent representatives. Based on these data, we perform a thorough analysis of the existing text synthesis detection algorithms. We show that ChatGPT-written abstracts are detectable, while the detection difficulty increases with human involvement. ","[{'version': 'v1', 'created': 'Mon, 24 Apr 2023 11:19:33 GMT'}]",2023-04-25,"[['Yu', 'Peipeng', ''], ['Chen', 'Jiahan', ''], ['Feng', 'Xuan', ''], ['Xia', 'Zhihua', '']]",1,1,2023-04-24,1,4,1,1,0,1,d0b0dfb52c83683cce21d69e405b185ec1e6f513,258298978.0,https://www.semanticscholar.org/paper/d0b0dfb52c83683cce21d69e405b185ec1e6f513,arXiv.org,2023.0,30.0,12.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1423733566', 'name': 'Peipeng Yu'}, {'authorId': '88565989', 'name': 'Jiahan Chen'}, {'authorId': '2112520546', 'name': 'Xuan Feng'}, {'authorId': '2072878384', 'name': 'Zhihua Xia'}]",['Jinan University'],['China'],2023-04
2304.12244,Can Xu,"Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng,
  Chongyang Tao, Daxin Jiang",WizardLM: Empowering Large Language Models to Follow Complex Instructions,"large language model, instruction fine-tune",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM ","[{'version': 'v1', 'created': 'Mon, 24 Apr 2023 16:31:06 GMT'}, {'version': 'v2', 'created': 'Sat, 10 Jun 2023 13:18:25 GMT'}]",2023-06-13,"[['Xu', 'Can', ''], ['Sun', 'Qingfeng', ''], ['Zheng', 'Kai', ''], ['Geng', 'Xiubo', ''], ['Zhao', 'Pu', ''], ['Feng', 'Jiazhan', ''], ['Tao', 'Chongyang', ''], ['Jiang', 'Daxin', '']]",1,1,2023-04-24,2,8,2,4,2,2,131f499e4d3503da93022d07fcf804a18483bea9,258298159.0,https://www.semanticscholar.org/paper/131f499e4d3503da93022d07fcf804a18483bea9,arXiv.org,2023.0,43.0,181.0,31.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '46747953', 'name': 'Can Xu'}, {'authorId': '2112549330', 'name': 'Qingfeng Sun'}, {'authorId': '2052687880', 'name': 'Kai Zheng'}, {'authorId': '2442662', 'name': 'Xiubo Geng'}, {'authorId': '2215218765', 'name': 'Pu Zhao'}, {'authorId': '147062881', 'name': 'Jiazhan Feng'}, {'authorId': '8801869', 'name': 'Chongyang Tao'}, {'authorId': '2086994543', 'name': 'Daxin Jiang'}]",['Peking University'],['China'],2023-04
2304.12298,Jiawen Shi,"Jiawen Shi, Yixin Liu, Pan Zhou and Lichao Sun",BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT,This paper is accepted as a poster in NDSS2023,,,,cs.CR cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, ChatGPT has gained significant attention in research due to its ability to interact with humans effectively. The core idea behind this model is reinforcement learning (RL) fine-tuning, a new paradigm that allows language models to align with human preferences, i.e., InstructGPT. In this study, we propose BadGPT, the first backdoor attack against RL fine-tuning in language models. By injecting a backdoor into the reward model, the language model can be compromised during the fine-tuning stage. Our initial experiments on movie reviews, i.e., IMDB, demonstrate that an attacker can manipulate the generated text through BadGPT. ","[{'version': 'v1', 'created': 'Tue, 21 Feb 2023 08:59:22 GMT'}]",2023-04-25,"[['Shi', 'Jiawen', ''], ['Liu', 'Yixin', ''], ['Zhou', 'Pan', ''], ['Sun', 'Lichao', '']]",1,1,2023-02-21,1,4,2,2,0,2,c50b9b97227971f5ed7a8fc0e5a1e45c6b4cb5c0,258298999.0,https://www.semanticscholar.org/paper/c50b9b97227971f5ed7a8fc0e5a1e45c6b4cb5c0,arXiv.org,2023.0,7.0,14.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2117867166', 'name': 'Jiawen Shi'}, {'authorId': None, 'name': 'Yixin Liu'}, {'authorId': '2153245185', 'name': 'Pan Zhou'}, {'authorId': '49755259', 'name': 'Lichao Sun'}]","['Huazhong University of Science and Technology', 'Lehigh University']","['China', 'United States']",2023-02
2304.12562,Zhang Jianzhang,"Jianzhang Zhang, Yiyang Chen, Nan Niu, Yinglin Wang, Chuang Liu",Empirical Evaluation of ChatGPT on Requirements Information Retrieval Under Zero-Shot Setting,,,,,cs.SE cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Recently, various illustrative examples have shown the impressive ability of generative large language models (LLMs) to perform NLP related tasks. ChatGPT undoubtedly is the most representative model. We empirically evaluate ChatGPT's performance on requirements information retrieval (IR) tasks to derive insights into designing or developing more effective requirements retrieval methods or tools based on generative LLMs. We design an evaluation framework considering four different combinations of two popular IR tasks and two common artifact types. Under zero-shot setting, evaluation results reveal ChatGPT's promising ability to retrieve requirements relevant information (high recall) and limited ability to retrieve more specific requirements information (low precision). Our evaluation of ChatGPT on requirements IR under zero-shot setting provides preliminary evidence for designing or developing more effective requirements IR methods or tools based on LLMs. ","[{'version': 'v1', 'created': 'Tue, 25 Apr 2023 04:09:45 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Jul 2023 08:28:45 GMT'}]",2023-07-20,"[['Zhang', 'Jianzhang', ''], ['Chen', 'Yiyang', ''], ['Niu', 'Nan', ''], ['Wang', 'Yinglin', ''], ['Liu', 'Chuang', '']]",1,1,2023-04-25,2,5,2,1,0,1,34dd0b6866b99758b3b5d6f9d1d1c9fcf68d4b91,259983112.0,https://www.semanticscholar.org/paper/34dd0b6866b99758b3b5d6f9d1d1c9fcf68d4b91,,2023.0,66.0,1.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '18148663', 'name': 'Jianzhang Zhang'}, {'authorId': '2190793159', 'name': 'Yiyang Chen'}, {'authorId': '2793310', 'name': 'Nan Niu'}, {'authorId': '2143404491', 'name': 'Yinglin Wang'}, {'authorId': '2188235440', 'name': 'Chuang Liu'}]","['Shanghai University of Finance and Economics', 'Hangzhou Normal University', 'University of Cincinnati']","['China', 'United States']",2023-04
2304.12995,Rongjie Huang,"Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang,
  Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou
  Zhao, Shinji Watanabe","AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head",,,,,cs.CL cs.AI cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. Our system is publicly available at \url{https://github.com/AIGC-Audio/AudioGPT}. ","[{'version': 'v1', 'created': 'Tue, 25 Apr 2023 17:05:38 GMT'}]",2023-04-26,"[['Huang', 'Rongjie', ''], ['Li', 'Mingze', ''], ['Yang', 'Dongchao', ''], ['Shi', 'Jiatong', ''], ['Chang', 'Xuankai', ''], ['Ye', 'Zhenhui', ''], ['Wu', 'Yuning', ''], ['Hong', 'Zhiqing', ''], ['Huang', 'Jiawei', ''], ['Liu', 'Jinglin', ''], ['Ren', 'Yi', ''], ['Zhao', 'Zhou', ''], ['Watanabe', 'Shinji', '']]",1,1,2023-04-25,1,13,4,1,0,1,8bc617c9139648d7a92991d70c671230bac7b2e2,258309430.0,https://www.semanticscholar.org/paper/8bc617c9139648d7a92991d70c671230bac7b2e2,arXiv.org,2023.0,46.0,57.0,7.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2048021099', 'name': 'Rongjie Huang'}, {'authorId': '2112108864', 'name': 'Mingze Li'}, {'authorId': '1752879605', 'name': 'Dongchao Yang'}, {'authorId': '1485531923', 'name': 'Jiatong Shi'}, {'authorId': '8776560', 'name': 'Xuankai Chang'}, {'authorId': '1704406169', 'name': 'Zhenhui Ye'}, {'authorId': '2166452418', 'name': 'Yuning Wu'}, {'authorId': '2215274532', 'name': 'Zhiqing Hong'}, {'authorId': '3068086', 'name': 'Jia-Bin Huang'}, {'authorId': '48211720', 'name': 'Jinglin Liu'}, {'authorId': '2165186676', 'name': 'Yixiang Ren'}, {'authorId': '2156163667', 'name': 'Zhou Zhao'}, {'authorId': '1746678', 'name': 'Shinji Watanabe'}]","['Carnegie Mellon University', 'Zhejiang University']","['China', 'United States']",2023-04
2304.12998,Rui Hao,"Rui Hao, Linmei Hu, Weijian Qi, Qingliu Wu, Yirui Zhang, Liqiang Nie","ChatLLM Network: More brains, More intelligence",,,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Dialogue-based language models mark a huge milestone in the field of artificial intelligence, by their impressive ability to interact with users, as well as a series of challenging tasks prompted by customized instructions. However, the prevalent large-scale dialogue-based language models like ChatGPT still have room for improvement, such as unstable responses to questions and the inability to think cooperatively like humans. Considering the ability of dialogue-based language models in conversation and their inherent randomness in thinking, we propose ChatLLM network that allows multiple dialogue-based language models to interact, provide feedback, and think together. We design the network of ChatLLMs based on ChatGPT. Specifically, individual instances of ChatGPT may possess distinct perspectives towards the same problem, and by consolidating these diverse viewpoints via a separate ChatGPT, the ChatLLM network system can conduct decision-making more objectively and comprehensively. In addition, a language-based feedback mechanism comparable to backpropagation is devised to update the ChatGPTs within the network. Experiments on two datasets demonstrate that our network attains significant improvements in problem-solving, leading to observable progress amongst each member. ","[{'version': 'v1', 'created': 'Mon, 24 Apr 2023 08:29:14 GMT'}]",2023-04-26,"[['Hao', 'Rui', ''], ['Hu', 'Linmei', ''], ['Qi', 'Weijian', ''], ['Wu', 'Qingliu', ''], ['Zhang', 'Yirui', ''], ['Nie', 'Liqiang', '']]",1,1,2023-04-24,1,6,2,1,0,1,8d225932befc4c36526005037be7a82482c1327b,258309449.0,https://www.semanticscholar.org/paper/8d225932befc4c36526005037be7a82482c1327b,arXiv.org,2023.0,21.0,8.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2215271897', 'name': 'Rui Hao'}, {'authorId': '1771202', 'name': 'Linmei Hu'}, {'authorId': '2149667177', 'name': 'Weijian Qi'}, {'authorId': '16040544', 'name': 'Qingliu Wu'}, {'authorId': '2215366439', 'name': 'Yirui Zhang'}, {'authorId': '143982887', 'name': 'Liqiang Nie'}]","['Beijing University of Posts and Telecommunications', ""Xi'an Jiaotong University"", 'Harbin Institute of Technology', 'Beijing Institute of Technology']",['China'],2023-04
2304.13276,Tianyi Zhou,"Shuai Li, Zhao Song, Yu Xia, Tong Yu, Tianyi Zhou",The Closeness of In-Context Learning and Weight Shifting for Softmax Regression,,,,,cs.CL cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Large language models (LLMs) are known for their exceptional performance in natural language processing, making them highly effective in many human life-related or even job-related tasks. The attention mechanism in the Transformer architecture is a critical component of LLMs, as it allows the model to selectively focus on specific input parts. The softmax unit, which is a key part of the attention mechanism, normalizes the attention scores. Hence, the performance of LLMs in various NLP tasks depends significantly on the crucial role played by the attention mechanism with the softmax unit.   In-context learning, as one of the celebrated abilities of recent LLMs, is an important concept in querying LLMs such as ChatGPT. Without further parameter updates, Transformers can learn to predict based on few in-context examples. However, the reason why Transformers becomes in-context learners is not well understood. Recently, several works [ASA+22,GTLV22,ONR+22] have studied the in-context learning from a mathematical perspective based on a linear regression formulation $\min_x\| Ax - b \|_2$, which show Transformers' capability of learning linear functions in context.   In this work, we study the in-context learning based on a softmax regression formulation $\min_{x} \| \langle \exp(Ax), {\bf 1}_n \rangle^{-1} \exp(Ax) - b \|_2$ of Transformer's attention mechanism. We show the upper bounds of the data transformations induced by a single self-attention layer and by gradient-descent on a $\ell_2$ regression loss for softmax prediction function, which imply that when training self-attention-only Transformers for fundamental regression tasks, the models learned by gradient-descent and Transformers show great similarity. ","[{'version': 'v1', 'created': 'Wed, 26 Apr 2023 04:33:41 GMT'}]",2023-04-27,"[['Li', 'Shuai', ''], ['Song', 'Zhao', ''], ['Xia', 'Yu', ''], ['Yu', 'Tong', ''], ['Zhou', 'Tianyi', '']]",1,1,2023-04-26,1,5,2,1,0,1,a569b9daa3606952dbcfdaa310ddfe6ad4eb95f3,258331729.0,https://www.semanticscholar.org/paper/a569b9daa3606952dbcfdaa310ddfe6ad4eb95f3,arXiv.org,2023.0,60.0,21.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1491648207', 'name': 'Shuai Li'}, {'authorId': '2214956470', 'name': 'Zhao Song'}, {'authorId': '2111273611', 'name': 'Yu Xia'}, {'authorId': '2148832584', 'name': 'Tong Yu'}, {'authorId': '2213956781', 'name': 'Tianyi Zhou'}]","[""Xi'an Jiaotong University"", 'University of California, San Diego']","['China', 'United States']",2023-04
2304.13301,Kang Yang,"Chunxi Guo, Zhiliang Tian, Jintao Tang, Pancheng Wang, Zhihua Wen,
  Kang Yang and Ting Wang",Prompting GPT-3.5 for Text-to-SQL with De-semanticization and Skeleton Retrieval,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text-to-SQL is a task that converts a natural language question into a structured query language (SQL) to retrieve information from a database. Large language models (LLMs) work well in natural language generation tasks, but they are not specifically pre-trained to understand the syntax and semantics of SQL commands. In this paper, we propose an LLM-based framework for Text-to-SQL which retrieves helpful demonstration examples to prompt LLMs. However, questions with different database schemes can vary widely, even if the intentions behind them are similar and the corresponding SQL queries exhibit similarities. Consequently, it becomes crucial to identify the appropriate SQL demonstrations that align with our requirements. We design a de-semanticization mechanism that extracts question skeletons, allowing us to retrieve similar examples based on their structural similarity. We also model the relationships between question tokens and database schema items (i.e., tables and columns) to filter out scheme-related information. Our framework adapts the range of the database schema in prompts to balance length and valuable information. A fallback mechanism allows for a more detailed schema to be provided if the generated SQL query fails. Ours outperforms state-of-the-art models and demonstrates strong generalization ability on three cross-domain Text-to-SQL benchmarks. ","[{'version': 'v1', 'created': 'Wed, 26 Apr 2023 06:02:01 GMT'}, {'version': 'v2', 'created': 'Thu, 31 Aug 2023 15:24:36 GMT'}]",2023-09-04,"[['Guo', 'Chunxi', ''], ['Tian', 'Zhiliang', ''], ['Tang', 'Jintao', ''], ['Wang', 'Pancheng', ''], ['Wen', 'Zhihua', ''], ['Yang', 'Kang', ''], ['Wang', 'Ting', '']]",0,1,2023-04-26,2,7,2,1,0,1,8334cab3749f83b09679e955f4794d132c60b24c,258331700.0,https://www.semanticscholar.org/paper/8334cab3749f83b09679e955f4794d132c60b24c,Pacific Rim International Conference on Artificial Intelligence,2023.0,42.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2215405126', 'name': 'Chunxi Guo'}, {'authorId': '33992653', 'name': 'Zhiliang Tian'}, {'authorId': '1762106', 'name': 'Jintao Tang'}, {'authorId': '2073437', 'name': 'Pancheng Wang'}, {'authorId': '2067933778', 'name': 'Zhihua Wen'}, {'authorId': '2220692725', 'name': 'Kang Yang'}, {'authorId': '38972135', 'name': 'Ting Wang'}]",['National University of Defense Technology'],['China'],2023-04
2304.13343,Xinnian Liang,"Xinnian Liang and Bing Wang and Hui Huang and Shuangzhi Wu and Peihao
  Wu and Lu Lu and Zejun Ma and Zhoujun Li",Unleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System,Working in progress,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large-scale Language Models (LLMs) are constrained by their inability to process lengthy inputs. To address this limitation, we propose the Self-Controlled Memory (SCM) system to unleash infinite-length input capacity for large-scale language models. Our SCM system is composed of three key modules: the language model agent, the memory stream, and the memory controller. The language model agent iteratively processes ultra-long inputs and stores all historical information in the memory stream. The memory controller provides the agent with both long-term memory (archived memory) and short-term memory (flash memory) to generate precise and coherent responses. The controller determines which memories from archived memory should be activated and how to incorporate them into the model input. Our SCM system can be integrated with any LLMs to enable them to process ultra-long texts without any modification or fine-tuning. Experimental results show that our SCM system enables LLMs, which are not optimized for multi-turn dialogue, to achieve multi-turn dialogue capabilities that are comparable to ChatGPT, and to outperform ChatGPT in scenarios involving ultra-long document summarization or long-term conversations. Additionally, we will supply a test set, which covers common long-text input scenarios, for evaluating the abilities of LLMs in processing long documents.~\footnote{Working in progress.}\footnote{\url{https://github.com/wbbeyourself/SCM4LLMs}} ","[{'version': 'v1', 'created': 'Wed, 26 Apr 2023 07:25:31 GMT'}]",2023-04-27,"[['Liang', 'Xinnian', ''], ['Wang', 'Bing', ''], ['Huang', 'Hui', ''], ['Wu', 'Shuangzhi', ''], ['Wu', 'Peihao', ''], ['Lu', 'Lu', ''], ['Ma', 'Zejun', ''], ['Li', 'Zhoujun', '']]",1,1,2023-04-26,1,8,1,1,0,1,7806aed0e00bcc8d3d84b15f5dab318b5400b7f0,258331553.0,https://www.semanticscholar.org/paper/7806aed0e00bcc8d3d84b15f5dab318b5400b7f0,arXiv.org,2023.0,28.0,9.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '120436437', 'name': 'Xinnian Liang'}, {'authorId': '38697342', 'name': 'Bin Wang'}, {'authorId': '2146086470', 'name': 'Huijia Huang'}, {'authorId': '2362902', 'name': 'Shuangzhi Wu'}, {'authorId': '3360572', 'name': 'Peihao Wu'}, {'authorId': '2215402606', 'name': 'Lu Lu'}, {'authorId': '2919563', 'name': 'Zejun Ma'}, {'authorId': '2144279674', 'name': 'Zhoujun Li'}]","['Harbin Institute of Technology', 'Beihang University', 'ByteDance']",['China'],2023-04
2304.13712,Hongye Jin,"Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng,
  Haoming Jiang, Bing Yin, Xia Hu",Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at \url{https://github.com/Mooler0410/LLMsPracticalGuide}. ","[{'version': 'v1', 'created': 'Wed, 26 Apr 2023 17:52:30 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Apr 2023 17:56:11 GMT'}]",2023-04-28,"[['Yang', 'Jingfeng', ''], ['Jin', 'Hongye', ''], ['Tang', 'Ruixiang', ''], ['Han', 'Xiaotian', ''], ['Feng', 'Qizhang', ''], ['Jiang', 'Haoming', ''], ['Yin', 'Bing', ''], ['Hu', 'Xia', '']]",1,1,2023-04-26,2,8,3,1,0,1,131c6f328c11706de2c43cd16e0b7c5d5e610b6a,258331833.0,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,arXiv.org,2023.0,133.0,92.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '7788583', 'name': 'Jingfeng Yang'}, {'authorId': '1791983892', 'name': 'Hongye Jin'}, {'authorId': '2057059798', 'name': 'Ruixiang Tang'}, {'authorId': '50017230', 'name': 'Xiaotian Han'}, {'authorId': '2151233715', 'name': 'Qizhang Feng'}, {'authorId': '5795999', 'name': 'Haoming Jiang'}, {'authorId': '2021632793', 'name': 'Bing Yin'}, {'authorId': '2193021044', 'name': 'Xia Hu'}]","['Rice University', 'Texas A&M University', 'Amazon', 'USA; Xia Hu,', 'Shanghai University']","['China', 'United States']",2023-04
2304.13911,Xiangyang Liu,"Xiangyang Liu, Tianqi Pang, Chenyou Fan",Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering,,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We investigate how to enhance answer precision in frequently asked questions posed by distributed users using cloud-based Large Language Models (LLMs). Our study focuses on a typical situations where users ask similar queries that involve identical mathematical reasoning steps and problem-solving procedures. Due to the unsatisfactory accuracy of LLMs' zero-shot prompting with standalone questions, we propose to improve the distributed synonymous questions using Self-Consistency (SC) and Chain-of-Thought (CoT) techniques. Specifically, we first retrieve synonymous questions from a crowd-sourced database and create a federated question pool. We call these federated synonymous questions with the same or different parameters SP-questions or DP-questions, respectively. We refer to our methods as Fed-SP-SC and Fed-DP-CoT, which can generate significantly more accurate answers for all user queries without requiring sophisticated model-tuning. Through extensive experiments, we demonstrate that our proposed methods can significantly enhance question accuracy by fully exploring the synonymous nature of the questions and the consistency of the answers. ","[{'version': 'v1', 'created': 'Thu, 27 Apr 2023 01:48:03 GMT'}, {'version': 'v2', 'created': 'Fri, 30 Jun 2023 13:21:36 GMT'}]",2023-07-03,"[['Liu', 'Xiangyang', ''], ['Pang', 'Tianqi', ''], ['Fan', 'Chenyou', '']]",0,0,2023-04-27,2,3,1,0,0,0,a7c0d9bf44045c9d4c41e329e2a87df0ae7e0af6,258352268.0,https://www.semanticscholar.org/paper/a7c0d9bf44045c9d4c41e329e2a87df0ae7e0af6,"Knowledge Science, Engineering and Management",2023.0,32.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2144226092', 'name': 'Xiangyang Liu'}, {'authorId': '2215451922', 'name': 'Tianqi Pang'}, {'authorId': '2047692', 'name': 'Chenyou Fan'}]",['South China Normal University'],['China'],2023-04
2304.14072,Linyang Li,"Linyang Li, Pengyu Wang, Ke Ren, Tianxiang Sun, Xipeng Qiu",Origin Tracing and Detecting of LLMs,working in progress,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The extraordinary performance of large language models (LLMs) heightens the importance of detecting whether the context is generated by an AI system. More importantly, while more and more companies and institutions release their LLMs, the origin can be hard to trace. Since LLMs are heading towards the time of AGI, similar to the origin tracing in anthropology, it is of great importance to trace the origin of LLMs. In this paper, we first raise the concern of the origin tracing of LLMs and propose an effective method to trace and detect AI-generated contexts. We introduce a novel algorithm that leverages the contrastive features between LLMs and extracts model-wise features to trace the text origins. Our proposed method works under both white-box and black-box settings therefore can be widely generalized to detect various LLMs.(e.g. can be generalized to detect GPT-3 models without the GPT-3 models). Also, our proposed method requires only limited data compared with the supervised learning methods and can be extended to trace new-coming model origins. We construct extensive experiments to examine whether we can trace the origins of given texts. We provide valuable observations based on the experimental results, such as the difficulty level of AI origin tracing, and the AI origin similarities, and call for ethical concerns of LLM providers. We are releasing all codes and data as a toolkit and benchmark for future AI origin tracing and detecting studies. \footnote{We are releasing all available resource at \url{https://github.com/OpenLMLab/}.} ","[{'version': 'v1', 'created': 'Thu, 27 Apr 2023 10:05:57 GMT'}]",2023-04-28,"[['Li', 'Linyang', ''], ['Wang', 'Pengyu', ''], ['Ren', 'Ke', ''], ['Sun', 'Tianxiang', ''], ['Qiu', 'Xipeng', '']]",0,1,2023-04-27,1,5,1,1,0,1,7a30650265477ebe0dc6fc6ce0b54be8644c5226,258352714.0,https://www.semanticscholar.org/paper/7a30650265477ebe0dc6fc6ce0b54be8644c5226,arXiv.org,2023.0,38.0,6.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2107897400', 'name': 'Linyang Li'}, {'authorId': '2199600227', 'name': 'Pengyu Wang'}, {'authorId': '2072596885', 'name': 'Kerong Ren'}, {'authorId': '153345698', 'name': 'Tianxiang Sun'}, {'authorId': '2188058565', 'name': 'Xipeng Qiu'}]",['Fudan University'],['China'],2023-04
2304.14106,Shangqing Tu,"Shangqing Tu, Chunyang Li, Jifan Yu, Xiaozhi Wang, Lei Hou, Juanzi Li",ChatLog: Recording and Analyzing ChatGPT Across Time,30 pages,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While there are abundant researches about evaluating ChatGPT on natural language understanding and generation tasks, few studies have investigated how ChatGPT's behavior changes over time. In this paper, we collect a coarse-to-fine temporal dataset called ChatLog, consisting of two parts that update monthly and daily: ChatLog-Monthly is a dataset of 38,730 question-answer pairs collected every month including questions from both the reasoning and classification tasks. ChatLog-Daily, on the other hand, consists of ChatGPT's responses to 1000 identical questions for long-form generation every day. We conduct comprehensive automatic and human evaluation to provide the evidence for the existence of ChatGPT evolving patterns. We further analyze the unchanged characteristics of ChatGPT over time by extracting its knowledge and linguistic features. We find some stable features to improve the robustness of a RoBERTa-based detector on new versions of ChatGPT. We will continuously maintain our project at https://github.com/THU-KEG/ChatLog. ","[{'version': 'v1', 'created': 'Thu, 27 Apr 2023 11:33:48 GMT'}]",2023-04-28,"[['Tu', 'Shangqing', ''], ['Li', 'Chunyang', ''], ['Yu', 'Jifan', ''], ['Wang', 'Xiaozhi', ''], ['Hou', 'Lei', ''], ['Li', 'Juanzi', '']]",1,1,2023-04-27,1,6,2,1,0,1,91125be6b5e2df94c7a48893bab2d1a7a7a19199,258352238.0,https://www.semanticscholar.org/paper/91125be6b5e2df94c7a48893bab2d1a7a7a19199,arXiv.org,2023.0,51.0,10.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2116520118', 'name': 'Shangqing Tu'}, {'authorId': '2215518948', 'name': 'Chunyang Li'}, {'authorId': '2116034394', 'name': 'Jifan Yu'}, {'authorId': '48631777', 'name': 'Xiaozhi Wang'}, {'authorId': '2055765060', 'name': 'Lei Hou'}, {'authorId': '2133353675', 'name': 'Juanzi Li'}]","['Equal Contribution.', 'Tsinghua University']",['China'],2023-04
2304.14178,Qinghao Ye,"Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou,
  Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu,
  Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, Fei Huang",mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality,Working in Process,,,,cs.CL cs.CV cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The online demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl. ","[{'version': 'v1', 'created': 'Thu, 27 Apr 2023 13:27:01 GMT'}]",2023-04-28,"[['Ye', 'Qinghao', ''], ['Xu', 'Haiyang', ''], ['Xu', 'Guohai', ''], ['Ye', 'Jiabo', ''], ['Yan', 'Ming', ''], ['Zhou', 'Yiyang', ''], ['Wang', 'Junyang', ''], ['Hu', 'Anwen', ''], ['Shi', 'Pengcheng', ''], ['Shi', 'Yaya', ''], ['Li', 'Chenliang', ''], ['Xu', 'Yuanhong', ''], ['Chen', 'Hehong', ''], ['Tian', 'Junfeng', ''], ['Qi', 'Qian', ''], ['Zhang', 'Ji', ''], ['Huang', 'Fei', '']]",0,0,2023-04-27,1,17,3,0,0,0,7e32aac43e9f1df49e116add03327ee6f365dbf3,258352455.0,https://www.semanticscholar.org/paper/7e32aac43e9f1df49e116add03327ee6f365dbf3,arXiv.org,2023.0,36.0,153.0,22.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2199011713', 'name': 'Qinghao Ye'}, {'authorId': '153194420', 'name': 'Haiyang Xu'}, {'authorId': '2115723816', 'name': 'Guohai Xu'}, {'authorId': '2153258288', 'name': 'Jiabo Ye'}, {'authorId': '2047087220', 'name': 'Ming Yan'}, {'authorId': '2118764703', 'name': 'Yi Zhou'}, {'authorId': '2110125710', 'name': 'Junyan Wang'}, {'authorId': '120897486', 'name': 'Anwen Hu'}, {'authorId': '2055357477', 'name': 'Pengcheng Shi'}, {'authorId': '37198550', 'name': 'Yaya Shi'}, {'authorId': '2829009', 'name': 'Chenliang Li'}, {'authorId': '2110355824', 'name': 'Yuanhong Xu'}, {'authorId': '123655156', 'name': 'Hehong Chen'}, {'authorId': '2122989639', 'name': 'Junfeng Tian'}, {'authorId': '50480206', 'name': 'Qiang Qi'}, {'authorId': '2116921824', 'name': 'Ji Zhang'}, {'authorId': '2194508991', 'name': 'Feiyan Huang'}]",['Alibaba'],['China'],2023-04
2304.14454,Chaoyi Wu,"Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi
  Xie",PMC-LLaMA: Towards Building Open-source Language Models for Medicine,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Recently, Large Language Models (LLMs) have showcased remarkable capabilities in natural language understanding. While demonstrating proficiency in everyday conversations and question-answering situations, these models frequently struggle in domains that require precision, such as medical applications, due to their lack of domain-specific knowledge. In this paper, we describe the procedure for building a powerful, open-source language model specifically designed for medicine applications, termed as PMC-LLaMA. Our contributions are threefold: (i) we systematically investigate the process of adapting a general-purpose foundation language model towards medical domain, this involves data-centric knowledge injection through the integration of 4.8M biomedical academic papers and 30K medical textbooks, as well as comprehensive fine-tuning for alignment with domain-specific instructions; (ii) we contribute a large-scale, comprehensive dataset for instruction tuning. This dataset encompasses medical question-answering (QA), rationale for reasoning, and conversational dialogues, comprising a total of 202M tokens; (iii) we conduct thorough ablation studies to demonstrate the effectiveness of each proposed component. While evaluating on various public medical question-answering benchmarks, our lightweight PMCLLaMA, which consists of only 13 billion parameters, exhibits superior performance, even surpassing ChatGPT. All models, codes, datasets can be found in https://github.com/chaoyi-wu/PMC-LLaMA. ","[{'version': 'v1', 'created': 'Thu, 27 Apr 2023 18:29:05 GMT'}, {'version': 'v2', 'created': 'Sat, 20 May 2023 08:32:51 GMT'}, {'version': 'v3', 'created': 'Fri, 25 Aug 2023 14:08:38 GMT'}]",2023-08-28,"[['Wu', 'Chaoyi', ''], ['Lin', 'Weixiong', ''], ['Zhang', 'Xiaoman', ''], ['Zhang', 'Ya', ''], ['Wang', 'Yanfeng', ''], ['Xie', 'Weidi', '']]",1,1,2023-04-27,3,6,1,2,1,1,04ee9597be4d6d2457214334e495e591000b5542,258417843.0,https://www.semanticscholar.org/paper/04ee9597be4d6d2457214334e495e591000b5542,,2023.0,29.0,21.0,4.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2146289861', 'name': 'Chaoyi Wu'}, {'authorId': '46448559', 'name': 'Xiaoman Zhang'}, {'authorId': '46868037', 'name': 'Ya Zhang'}, {'authorId': '2108846176', 'name': 'Yanfeng Wang'}, {'authorId': '10096695', 'name': 'Weidi Xie'}]","['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University']",['China'],2023-04
2304.14613,Yuchen Sun Sun,"Yuchen Sun, Tianpeng Liu, Panhe Hu, Qing Liao, Shaojing Fu, Nenghai
  Yu, Deke Guo, Yongxiang Liu, Li Liu",Deep Intellectual Property Protection: A Survey,"37 pages, 19 figures",,,,cs.AI cs.CR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep Neural Networks (DNNs), from AlexNet to ResNet to ChatGPT, have made revolutionary progress in recent years, and are widely used in various fields. The high performance of DNNs requires a huge amount of high-quality data, expensive computing hardware, and excellent DNN architectures that are costly to obtain. Therefore, trained DNNs are becoming valuable assets and must be considered the Intellectual Property (IP) of the legitimate owner who created them, in order to protect trained DNN models from illegal reproduction, stealing, redistribution, or abuse. Although being a new emerging and interdisciplinary field, numerous DNN model IP protection methods have been proposed. Given this period of rapid evolution, the goal of this paper is to provide a comprehensive survey of two mainstream DNN IP protection methods: deep watermarking and deep fingerprinting, with a proposed taxonomy. More than 190 research contributions are included in this survey, covering many aspects of Deep IP Protection: problem definition, main threats and challenges, merits and demerits of deep watermarking and deep fingerprinting methods, evaluation metrics, and performance discussion. We finish the survey by identifying promising directions for future research. ","[{'version': 'v1', 'created': 'Fri, 28 Apr 2023 03:34:43 GMT'}, {'version': 'v2', 'created': 'Sat, 17 Jun 2023 09:08:18 GMT'}]",2023-06-21,"[['Sun', 'Yuchen', ''], ['Liu', 'Tianpeng', ''], ['Hu', 'Panhe', ''], ['Liao', 'Qing', ''], ['Fu', 'Shaojing', ''], ['Yu', 'Nenghai', ''], ['Guo', 'Deke', ''], ['Liu', 'Yongxiang', ''], ['Liu', 'Li', '']]",1,1,2023-04-28,2,9,2,1,0,1,ddbd3a38a2581b07a175c63bece85a2d244dd434,259202940.0,https://www.semanticscholar.org/paper/ddbd3a38a2581b07a175c63bece85a2d244dd434,,2023.0,0.0,0.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2109029173', 'name': 'Yuchen Sun'}, {'authorId': '2190166380', 'name': 'Tian-Yi Liu'}, {'authorId': '31559404', 'name': 'Panhe Hu'}, {'authorId': '2055482014', 'name': 'Qing Liao'}, {'authorId': '2158817360', 'name': 'Shaojing Fu'}, {'authorId': '2052212945', 'name': 'Neng H. Yu'}, {'authorId': '2068110135', 'name': 'Deke Guo'}, {'authorId': '2144472895', 'name': 'Yongxiang Liu'}, {'authorId': '2220667332', 'name': 'Li Liu'}]","['Harbin Institute of Technology', 'National University of Defense Technology', 'University of Science and Technology of China']",['China'],2023-04
2304.14802,Shufang Xie,"Shufang Xie, Huishuai Zhang, Junliang Guo, Xu Tan, Jiang Bian, Hany
  Hassan Awadalla, Arul Menezes, Tao Qin, Rui Yan",ResiDual: Transformer with Dual Residual Connections,,,,,cs.CL cs.AI cs.LG cs.NE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transformer networks have become the preferred architecture for many tasks due to their state-of-the-art performance. However, the optimal way to implement residual connections in Transformer, which are essential for effective training, is still debated. Two widely used variants are the Post-Layer-Normalization (Post-LN) and Pre-Layer-Normalization (Pre-LN) Transformers, which apply layer normalization after each residual block's output or before each residual block's input, respectively. While both variants enjoy their advantages, they also suffer from severe limitations: Post-LN causes gradient vanishing issue that hinders training deep Transformers, and Pre-LN causes representation collapse issue that limits model capacity. In this paper, we propose ResiDual, a novel Transformer architecture with Pre-Post-LN (PPLN), which fuses the connections in Post-LN and Pre-LN together and inherits their advantages while avoids their limitations. We conduct both theoretical analyses and empirical experiments to verify the effectiveness of ResiDual. Theoretically, we prove that ResiDual has a lower bound on the gradient to avoid the vanishing issue due to the residual connection from Pre-LN. Moreover, ResiDual also has diverse model representations to avoid the collapse issue due to the residual connection from Post-LN. Empirically, ResiDual outperforms both Post-LN and Pre-LN on several machine translation benchmarks across different network depths and data sizes. Thanks to the good theoretical and empirical performance, ResiDual Transformer can serve as a foundation architecture for different AI models (e.g., large language models). Our code is available at https://github.com/microsoft/ResiDual. ","[{'version': 'v1', 'created': 'Fri, 28 Apr 2023 12:19:47 GMT'}]",2023-05-01,"[['Xie', 'Shufang', ''], ['Zhang', 'Huishuai', ''], ['Guo', 'Junliang', ''], ['Tan', 'Xu', ''], ['Bian', 'Jiang', ''], ['Awadalla', 'Hany Hassan', ''], ['Menezes', 'Arul', ''], ['Qin', 'Tao', ''], ['Yan', 'Rui', '']]",0,0,2023-04-28,1,9,4,0,0,0,8f7f7b48184217c131844d725daefe6734735d8a,258418273.0,https://www.semanticscholar.org/paper/8f7f7b48184217c131844d725daefe6734735d8a,arXiv.org,2023.0,28.0,3.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1889683', 'name': 'Shufang Xie'}, {'authorId': '2973831', 'name': 'Huishuai Zhang'}, {'authorId': '13838086', 'name': 'Junliang Guo'}, {'authorId': '48391466', 'name': 'Xu Tan'}, {'authorId': '2192822005', 'name': 'Jiang Bian'}, {'authorId': '3032929', 'name': 'Hany Hassan Awadalla'}, {'authorId': '145280401', 'name': 'Arul Menezes'}, {'authorId': '143826491', 'name': 'Tao Qin'}, {'authorId': '2172312251', 'name': 'Rui Yan'}]",['Renmin University of China'],['China'],2023-04
2304.14827,Chunkit Chan,"Chunkit Chan, Jiayang Cheng, Weiqi Wang, Yuxin Jiang, Tianqing Fang,
  Xin Liu, Yangqiu Song","ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations",37 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper aims to quantitatively evaluate the performance of ChatGPT, an interactive large language model, on inter-sentential relations such as temporal relations, causal relations, and discourse relations. Given ChatGPT's promising performance across various tasks, we conduct extensive evaluations on the whole test sets of 13 datasets, including temporal and causal relations, PDTB2.0-based and dialogue-based discourse relations, and downstream applications on discourse understanding. To achieve reliable results, we adopt three tailored prompt templates for each task, including the zero-shot prompt template, zero-shot prompt engineering (PE) template, and in-context learning (ICL) prompt template, to establish the initial baseline scores for all popular sentence-pair relation classification tasks for the first time. We find that ChatGPT exhibits strong performance in detecting and reasoning about causal relations, while it may not be proficient in identifying the temporal order between two events. It can recognize most discourse relations with existing explicit discourse connectives, but the implicit discourse relation still remains a challenging task. Meanwhile, ChatGPT performs poorly in the dialogue discourse parsing task that requires structural understanding in a dialogue before being aware of the discourse relation. ","[{'version': 'v1', 'created': 'Fri, 28 Apr 2023 13:14:36 GMT'}, {'version': 'v2', 'created': 'Thu, 11 May 2023 18:20:00 GMT'}]",2023-05-15,"[['Chan', 'Chunkit', ''], ['Cheng', 'Jiayang', ''], ['Wang', 'Weiqi', ''], ['Jiang', 'Yuxin', ''], ['Fang', 'Tianqing', ''], ['Liu', 'Xin', ''], ['Song', 'Yangqiu', '']]",1,1,2023-04-28,2,7,1,1,0,1,186e96fe036927182ec963b63f9dd7f8ff650158,258418194.0,https://www.semanticscholar.org/paper/186e96fe036927182ec963b63f9dd7f8ff650158,arXiv.org,2023.0,91.0,23.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2216598559', 'name': 'Chunkit Chan'}, {'authorId': '2109077713', 'name': 'Cheng Jiayang'}, {'authorId': '1587728690', 'name': 'Weiqi Wang'}, {'authorId': '2116150120', 'name': 'Yuxin Jiang'}, {'authorId': '2044202073', 'name': 'Tianqing Fang'}, {'authorId': '89121677', 'name': 'Xin Liu'}, {'authorId': '1809614', 'name': 'Yangqiu Song'}]",['Hong Kong University of Science and Technology'],['China'],2023-04
2304.14979,Yuge Zhang,"Lei Zhang, Yuge Zhang, Kan Ren, Dongsheng Li, Yuqing Yang",MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks,,,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The field of machine learning (ML) has gained widespread adoption, leading to a significant demand for adapting ML to specific scenarios, which is yet expensive and non-trivial. The predominant approaches towards the automation of solving ML tasks (e.g., AutoML) are often time consuming and hard to understand for human developers. In contrast, though human engineers have the incredible ability to understand tasks and reason about solutions, their experience and knowledge are often sparse and difficult to utilize by quantitative approaches. In this paper, we aim to bridge the gap between machine intelligence and human knowledge by introducing a novel framework MLCopilot, which leverages the state-of-the-art LLMs to develop ML solutions for novel tasks. We showcase the possibility of extending the capability of LLMs to comprehend structured inputs and perform thorough reasoning for solving novel ML tasks. And we find that, after some dedicated design, the LLM can (i) observe from the existing experiences of ML tasks and (ii) reason effectively to deliver promising results for new tasks. The solution generated can be used directly to achieve high levels of competitiveness. ","[{'version': 'v1', 'created': 'Fri, 28 Apr 2023 17:03:57 GMT'}]",2023-05-01,"[['Zhang', 'Lei', ''], ['Zhang', 'Yuge', ''], ['Ren', 'Kan', ''], ['Li', 'Dongsheng', ''], ['Yang', 'Yuqing', '']]",0,0,2023-04-28,1,5,2,0,0,0,fce42753155280051ac64817404b4e1d3be6ebaa,258418182.0,https://www.semanticscholar.org/paper/fce42753155280051ac64817404b4e1d3be6ebaa,arXiv.org,2023.0,86.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1452981772', 'name': 'Lei Zhang'}, {'authorId': '8795401', 'name': 'Yuge Zhang'}, {'authorId': '2214617008', 'name': 'Kan Ren'}, {'authorId': '2181524288', 'name': 'Dongsheng Li'}, {'authorId': '2125051198', 'name': 'Yuqing Yang'}]",['Microsoft'],"['China', 'India']",2023-04
2305.00201,Yuzhong Chen,"Zhenxiang Xiao, Yuzhong Chen, Lu Zhang, Junjie Yao, Zihao Wu, Xiaowei
  Yu, Yi Pan, Lin Zhao, Chong Ma, Xinyu Liu, Wei Liu, Xiang Li, Yixuan Yuan,
  Dinggang Shen, Dajiang Zhu, Tianming Liu, Xi Jiang",Instruction-ViT: Multi-Modal Prompts for Instruction Learning in ViT,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Prompts have been proven to play a crucial role in large language models, and in recent years, vision models have also been using prompts to improve scalability for multiple downstream tasks. In this paper, we focus on adapting prompt design based on instruction tuning into a visual transformer model for image classification which we called Instruction-ViT. The key idea is to implement multi-modal prompts (text or image prompt) related to category information to guide the fine-tuning of the model. Based on the experiments of several image captionining tasks, the performance and domain adaptability were improved. Our work provided an innovative strategy to fuse multi-modal prompts with better performance and faster adaptability for visual classification models. ","[{'version': 'v1', 'created': 'Sat, 29 Apr 2023 08:59:12 GMT'}]",2023-05-02,"[['Xiao', 'Zhenxiang', ''], ['Chen', 'Yuzhong', ''], ['Zhang', 'Lu', ''], ['Yao', 'Junjie', ''], ['Wu', 'Zihao', ''], ['Yu', 'Xiaowei', ''], ['Pan', 'Yi', ''], ['Zhao', 'Lin', ''], ['Ma', 'Chong', ''], ['Liu', 'Xinyu', ''], ['Liu', 'Wei', ''], ['Li', 'Xiang', ''], ['Yuan', 'Yixuan', ''], ['Shen', 'Dinggang', ''], ['Zhu', 'Dajiang', ''], ['Liu', 'Tianming', ''], ['Jiang', 'Xi', '']]",0,0,2023-04-29,1,17,1,0,0,0,a677938545f63ad44c87d09f85dd231980a8476f,258426716.0,https://www.semanticscholar.org/paper/a677938545f63ad44c87d09f85dd231980a8476f,arXiv.org,2023.0,49.0,6.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2116552919', 'name': 'Zhe Xiao'}, {'authorId': '2121472807', 'name': 'Yuzhong Chen'}, {'authorId': '2156146823', 'name': 'Lu Zhang'}, {'authorId': '145946111', 'name': 'Jun Yao'}, {'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '2139760758', 'name': 'Xiao-Xing Yu'}, {'authorId': '19308245', 'name': 'Yirong Pan'}, {'authorId': '2111641126', 'name': 'Lin Zhao'}, {'authorId': '2116896792', 'name': 'Chonghe Ma'}, {'authorId': '2110789401', 'name': 'Xinyu Liu'}, {'authorId': '46641573', 'name': 'W. Liu'}, {'authorId': '2144438902', 'name': 'Xiang Li'}, {'authorId': '3080513', 'name': 'Yixuan Yuan'}, {'authorId': '2150038187', 'name': 'Dinggang Shen'}, {'authorId': '2181182', 'name': 'Dajiang Zhu'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}, {'authorId': '143796247', 'name': 'Xi Jiang'}]","['University of Electronic Science and Technology of China', 'Shanghai United Imaging Intel-ligence Co., Ltd., Shanghai 200230, China 11 Shanghai Clinical Research and Trial Center, Shanghai, 201210, China.', 'Mayo Clinic in Arizona', 'ShanghaiTech University', 'Chinese University of Hong Kong', 'Massachusetts General Hospital', 'The University of Texas at Arlington', 'University of Georgia', 'Northwestern Polytechnical University']","['China', 'United States']",2023-04
2305.00398,Minghui Yang,"Minghui Yang, Jing Liu, Zhiwei Yang, and Zhaoyang Wu",SLSG: Industrial Image Anomaly Detection by Learning Better Feature Embeddings and One-Class Classification,,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Industrial image anomaly detection under the setting of one-class classification has significant practical value. However, most existing models struggle to extract separable feature representations when performing feature embedding and struggle to build compact descriptions of normal features when performing one-class classification. One direct consequence of this is that most models perform poorly in detecting logical anomalies which violate contextual relationships. Focusing on more effective and comprehensive anomaly detection, we propose a network based on self-supervised learning and self-attentive graph convolution (SLSG) for anomaly detection. SLSG uses a generative pre-training network to assist the encoder in learning the embedding of normal patterns and the reasoning of position relationships. Subsequently, SLSG introduces the pseudo-prior knowledge of anomaly through simulated abnormal samples. By comparing the simulated anomalies, SLSG can better summarize the normal features and narrow down the hypersphere used for one-class classification. In addition, with the construction of a more general graph structure, SLSG comprehensively models the dense and sparse relationships among elements in the image, which further strengthens the detection of logical anomalies. Extensive experiments on benchmark datasets show that SLSG achieves superior anomaly detection performance, demonstrating the effectiveness of our method. ","[{'version': 'v1', 'created': 'Sun, 30 Apr 2023 05:38:45 GMT'}]",2023-05-02,"[['Yang', 'Minghui', ''], ['Liu', 'Jing', ''], ['Yang', 'Zhiwei', ''], ['Wu', 'Zhaoyang', '']]",0,1,2023-04-30,1,4,1,0,0,0,5c04ce7f8510af40f2931535feeaf220832ab548,258426244.0,https://www.semanticscholar.org/paper/5c04ce7f8510af40f2931535feeaf220832ab548,arXiv.org,2023.0,57.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '152790262', 'name': 'Minghui Yang'}, {'authorId': '2163063860', 'name': 'Jing Liu'}, {'authorId': '2144391581', 'name': 'Zhiwei Yang'}, {'authorId': '2109666206', 'name': 'Zhaoyang Wu'}]",['Xidian University'],['China'],2023-04
2305.00447,Keqin Bao,"Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and
  Xiangnan He",TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation,Accepted by RecSys 2023 (short),,,,cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have demonstrated remarkable performance across diverse domains, thereby prompting researchers to explore their potential for use in recommendation systems. Initial attempts have leveraged the exceptional capabilities of LLMs, such as rich knowledge and strong generalization through In-context Learning, which involves phrasing the recommendation task as prompts. Nevertheless, the performance of LLMs in recommendation tasks remains suboptimal due to a substantial disparity between the training tasks for LLMs and recommendation tasks, as well as inadequate recommendation data during pre-training. To bridge the gap, we consider building a Large Recommendation Language Model by tunning LLMs with recommendation data. To this end, we propose an efficient and effective Tuning framework for Aligning LLMs with Recommendation, namely TALLRec. We have demonstrated that the proposed TALLRec framework can significantly enhance the recommendation capabilities of LLMs in the movie and book domains, even with a limited dataset of fewer than 100 samples. Additionally, the proposed framework is highly efficient and can be executed on a single RTX 3090 with LLaMA-7B. Furthermore, the fine-tuned LLM exhibits robust cross-domain generalization. Our code and data are available at https://github.com/SAI990323/TALLRec. ","[{'version': 'v1', 'created': 'Sun, 30 Apr 2023 10:55:56 GMT'}, {'version': 'v2', 'created': 'Mon, 3 Jul 2023 15:02:31 GMT'}]",2023-07-04,"[['Bao', 'Keqin', ''], ['Zhang', 'Jizhi', ''], ['Zhang', 'Yang', ''], ['Wang', 'Wenjie', ''], ['Feng', 'Fuli', ''], ['He', 'Xiangnan', '']]",0,0,2023-04-30,2,6,1,1,1,0,3487c12512fa41d3a4d64f00cb842525a8590ad3,258426525.0,https://www.semanticscholar.org/paper/3487c12512fa41d3a4d64f00cb842525a8590ad3,ACM Conference on Recommender Systems,2023.0,78.0,41.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2188063534', 'name': 'Keqin Bao'}, {'authorId': '2116265843', 'name': 'Jizhi Zhang'}, {'authorId': '2145957648', 'name': 'Yang Zhang'}, {'authorId': '2117833732', 'name': 'Wenjie Wang'}, {'authorId': '2163400298', 'name': 'Fuli Feng'}, {'authorId': '7792071', 'name': 'Xiangnan He'}]","['National University of Singapore', 'University of Science and Technology of China', 'Nanzhong Zhangzhongjing Hospital', 'Jizhi Zhang*']","['China', 'Singapore']",2023-04
2305.01181,Chenyang Lyu,"Chenyang Lyu, Jitao Xu, Longyue Wang",New Trends in Machine Translation using Large Language Models: Case Examples with ChatGPT,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Machine Translation (MT) has made significant progress in recent years using deep learning, especially after the emergence of large language models (LLMs) such as GPT-3 and ChatGPT. This brings new challenges and opportunities for MT using LLMs. In this paper, we brainstorm some interesting directions for MT using LLMs, including stylized MT, interactive MT, and Translation Memory-based MT, as well as a new evaluation paradigm using LLMs. We also discuss the privacy concerns in MT using LLMs and a basic privacy-preserving method to mitigate such risks. To illustrate the potential of our proposed directions, we present several examples for the new directions mentioned above, demonstrating the feasibility of the proposed directions and highlight the opportunities and challenges for future research in MT using LLMs. ","[{'version': 'v1', 'created': 'Tue, 2 May 2023 03:27:27 GMT'}]",2023-05-03,"[['Lyu', 'Chenyang', ''], ['Xu', 'Jitao', ''], ['Wang', 'Longyue', '']]",1,1,2023-05-02,1,3,1,2,0,2,cbd8da47b200a9d0aa37618ab6bf673dd81633f8,258436851.0,https://www.semanticscholar.org/paper/cbd8da47b200a9d0aa37618ab6bf673dd81633f8,arXiv.org,2023.0,42.0,12.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Business', 'source': 's2-fos-model'}]","[{'authorId': '2082426870', 'name': 'Chenyang Lyu'}, {'authorId': '1925543', 'name': 'Jitao Xu'}, {'authorId': '1800190', 'name': 'Longyue Wang'}]","['Dublin City University', 'NetEase', 'Tencent']","['China', 'Ireland']",2023-05
2305.01278,Ao Zhang,"Ao Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan Liu, and Tat-Seng
  Chua",Transfer Visual Prompt Generator across LLMs,"Project Website: https://vpgtrans.github.io Code:
  https://github.com/VPGTrans/VPGTrans",,,,cs.CV cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While developing a new vision-language LLM (VL-LLM) by pre-training on tremendous image-text pairs from scratch can be exceedingly resource-consuming, connecting an existing LLM with a comparatively lightweight visual prompt generator (VPG) becomes a feasible paradigm. However, further tuning the VPG part of the VL-LLM still suffers from indispensable computational costs, i.e., requiring thousands of GPU hours and millions of training data. One alternative solution is to transfer an existing VPG from any existing VL-LLMs for the target VL-LLM.   In this work, we for the first time investigate the VPG transferability across LLMs, and explore a solution to reduce the cost of VPG transfer. We first study the VPG transfer across different LLM sizes (e.g., small-to-large), and across different LLM types, through which we diagnose the key factors to maximize the transfer efficiency. Based on our observation, we design a two-stage transfer framework named VPGTrans, which is simple yet highly effective. Through extensive experiments, we demonstrate that VPGTrans helps significantly speed up the transfer learning process without compromising performance. Remarkably, it helps achieve the VPG transfer from BLIP-2 OPT$_\text{2.7B}$ to BLIP-2 OPT$_\text{6.7B}$ with over 10 times speed-up and 10.7% training data compared with connecting a VPG to OPT$_\text{6.7B}$ from scratch. Further, a series of intriguing findings and potential rationales behind them are provided and discussed. Finally, we showcase the practical value of our VPGTrans approach, by customizing two novel VL-LLMs, including VL-LLaMA and VL-Vicuna, with recently released LLaMA and Vicuna LLMs. ","[{'version': 'v1', 'created': 'Tue, 2 May 2023 09:28:39 GMT'}]",2023-05-03,"[['Zhang', 'Ao', ''], ['Fei', 'Hao', ''], ['Yao', 'Yuan', ''], ['Ji', 'Wei', ''], ['Li', 'Li', ''], ['Liu', 'Zhiyuan', ''], ['Chua', 'Tat-Seng', '']]",0,0,2023-05-02,1,7,3,3,3,0,0046306876ff2d5600699327e52bc29fa5e9ec91,258436945.0,https://www.semanticscholar.org/paper/0046306876ff2d5600699327e52bc29fa5e9ec91,arXiv.org,2023.0,61.0,32.0,6.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2153656874', 'name': 'Ao Zhang'}, {'authorId': '2142672912', 'name': 'Hao Fei'}, {'authorId': '1390925224', 'name': 'Yuan Yao'}, {'authorId': '2072613978', 'name': 'Wei Ji'}, {'authorId': '2156062791', 'name': 'Li Li'}, {'authorId': '1390625267', 'name': 'Zhiyuan Liu'}, {'authorId': '143779329', 'name': 'Tat-seng Chua'}]","['National University of Singapore', 'Tsinghua University', '37th Conference on Neural Information Processing Systems (NeurIPS 2023).']","['China', 'Singapore']",2023-05
2305.01555,Ningyu Zhang,"Xin Xu, Yuqi Zhu, Xiaohan Wang, Ningyu Zhang",How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?,SustaiNLP Workshop@ACL 2023,,,,cs.CL cs.AI cs.DB cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction. Code is available in https://github.com/zjunlp/DeepKE/tree/main/example/llm. ","[{'version': 'v1', 'created': 'Tue, 2 May 2023 15:55:41 GMT'}, {'version': 'v2', 'created': 'Wed, 3 May 2023 06:45:59 GMT'}, {'version': 'v3', 'created': 'Thu, 1 Jun 2023 06:51:58 GMT'}, {'version': 'v4', 'created': 'Fri, 9 Jun 2023 15:59:18 GMT'}]",2023-06-12,"[['Xu', 'Xin', ''], ['Zhu', 'Yuqi', ''], ['Wang', 'Xiaohan', ''], ['Zhang', 'Ningyu', '']]",0,1,2023-05-02,4,4,5,1,0,1,1ddeb500dd88d4b860b32bec1e2a85f8a53910d6,258437068.0,https://www.semanticscholar.org/paper/1ddeb500dd88d4b860b32bec1e2a85f8a53910d6,SUSTAINLP,2023.0,51.0,6.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2152775219', 'name': 'Xin Xu'}, {'authorId': '2127757530', 'name': 'Yuqi Zhu'}, {'authorId': '2141660877', 'name': 'Xiaohan Wang'}, {'authorId': '2608639', 'name': 'Ningyu Zhang'}]",['Zhejiang University'],['China'],2023-05
2305.01616,Yequan Wang,"Xiang Li, Xin Jiang, Xuying Meng, Aixin Sun, Yequan Wang",FreeLM: Fine-Tuning-Free Language Model,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Pre-trained language models (PLMs) have achieved remarkable success in NLP tasks. Despite the great success, mainstream solutions largely follow the pre-training then finetuning paradigm, which brings in both high deployment costs and low training efficiency. Nevertheless, fine-tuning on a specific task is essential because PLMs are only pre-trained with language signal from large raw data. In this paper, we propose a novel fine-tuning-free strategy for language models, to consider both language signal and teacher signal. Teacher signal is an abstraction of a battery of downstream tasks, provided in a unified proposition format. Trained with both language and strong task-aware teacher signals in an interactive manner, our FreeLM model demonstrates strong generalization and robustness. FreeLM outperforms large models e.g., GPT-3 and InstructGPT, on a range of language understanding tasks in experiments. FreeLM is much smaller with 0.3B parameters, compared to 175B in these models. ","[{'version': 'v1', 'created': 'Tue, 2 May 2023 17:17:56 GMT'}]",2023-05-03,"[['Li', 'Xiang', ''], ['Jiang', 'Xin', ''], ['Meng', 'Xuying', ''], ['Sun', 'Aixin', ''], ['Wang', 'Yequan', '']]",0,1,2023-05-02,1,5,2,2,0,2,21408076fc1f2bb0ecbc201dd42d5b750289aae5,258436807.0,https://www.semanticscholar.org/paper/21408076fc1f2bb0ecbc201dd42d5b750289aae5,arXiv.org,2023.0,30.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '48868160', 'name': 'Xiang Li'}, {'authorId': '145820291', 'name': 'Xin Jiang'}, {'authorId': '3134123', 'name': 'Xuying Meng'}, {'authorId': '1735962', 'name': 'Aixin Sun'}, {'authorId': '2119256294', 'name': 'Yequan Wang'}]","['Beijing Academy of Artificial Intelligence', 'Chinese Academy of Sciences', 'Nanyang Technological University']","['China', 'Singapore']",2023-05
2305.01918,Qinyuan Cheng,"Qinyuan Cheng, Xiaogui Yang, Tianxiang Sun, Linyang Li, Xipeng Qiu",Improving Contrastive Learning of Sentence Embeddings from AI Feedback,"Accepted to Findings of ACL 2023, camera-ready version",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Contrastive learning has become a popular approach in natural language processing, particularly for the learning of sentence embeddings. However, the discrete nature of natural language makes it difficult to ensure the quality of positive and negative sample pairs generated through data augmentation methods. Although supervised contrastive learning can produce more accurate sample pairs with human feedback labels, it still lacks fine-grained training signals. In this paper, we propose to improve \textbf{C}ontrastive \textbf{L}earning of sentence embeddings from \textbf{AI} \textbf{F}eedback \textbf{(CLAIF)}. Our method utilizes AI feedback from large pre-trained language models (LLMs) to construct sample pairs with fine-grained sample similarity scores to improve contrastive learning. Besides, we combine human feedback and AI feedback to provide better supervision signals for supervised contrastive learning of sentence embeddings. Experimental results show that our method achieves state-of-the-art performance on several semantic textual similarity (STS) and transfer learning tasks compared to other unsupervised and supervised contrastive learning methods. ","[{'version': 'v1', 'created': 'Wed, 3 May 2023 06:26:13 GMT'}, {'version': 'v2', 'created': 'Thu, 4 May 2023 14:15:31 GMT'}, {'version': 'v3', 'created': 'Sat, 20 May 2023 09:39:29 GMT'}]",2023-05-23,"[['Cheng', 'Qinyuan', ''], ['Yang', 'Xiaogui', ''], ['Sun', 'Tianxiang', ''], ['Li', 'Linyang', ''], ['Qiu', 'Xipeng', '']]",0,0,2023-05-03,3,5,2,0,0,0,93a5864a53065f3c865541c4ea6170bbdb16dbb2,258460987.0,https://www.semanticscholar.org/paper/93a5864a53065f3c865541c4ea6170bbdb16dbb2,Annual Meeting of the Association for Computational Linguistics,2023.0,54.0,10.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1834133', 'name': 'Qinyuan Cheng'}, {'authorId': '2216031457', 'name': 'Xiaogui Yang'}, {'authorId': '153345698', 'name': 'Tianxiang Sun'}, {'authorId': '2107897400', 'name': 'Linyang Li'}, {'authorId': '2188058565', 'name': 'Xipeng Qiu'}]",['Fudan University'],['China'],2023-05
2305.02105,Zhen Wan,"Zhen Wan, Fei Cheng, Zhuoyuan Mao, Qianying Liu, Haiyue Song, Jiwei
  Li, Sadao Kurohashi",GPT-RE: In-context Learning for Relation Extraction using Large Language Models,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In spite of the potential for ground-breaking achievements offered by large language models (LLMs) (e.g., GPT-3), they still lag significantly behind fully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE). This is due to the two major shortcomings of LLMs in RE: (1) low relevance regarding entity and relation in retrieved demonstrations for in-context learning; and (2) the strong inclination to wrongly classify NULL examples into other pre-defined labels.   In this paper, we propose GPT-RE to bridge the gap between LLMs and fully-supervised baselines. GPT-RE successfully addresses the aforementioned issues by (1) incorporating task-specific entity representations in demonstration retrieval; and (2) enriching the demonstrations with gold label-induced reasoning logic. We evaluate GPT-RE on four widely-used RE datasets, and observe that GPT-RE achieves improvements over not only existing GPT-3 baselines, but also fully-supervised baselines. Specifically, GPT-RE achieves SOTA performances on the Semeval and SciERC datasets, and competitive performances on the TACRED and ACE05 datasets. ","[{'version': 'v1', 'created': 'Wed, 3 May 2023 13:28:08 GMT'}]",2023-05-04,"[['Wan', 'Zhen', ''], ['Cheng', 'Fei', ''], ['Mao', 'Zhuoyuan', ''], ['Liu', 'Qianying', ''], ['Song', 'Haiyue', ''], ['Li', 'Jiwei', ''], ['Kurohashi', 'Sadao', '']]",0,1,2023-05-03,1,7,1,1,0,1,f2cd02c03d0169374442d9bc227c9aed178f4b20,258461040.0,https://www.semanticscholar.org/paper/f2cd02c03d0169374442d9bc227c9aed178f4b20,arXiv.org,2023.0,43.0,13.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2163526362', 'name': 'Zhen Wan'}, {'authorId': '49412583', 'name': 'Fei Cheng'}, {'authorId': '1490663781', 'name': 'Zhuoyuan Mao'}, {'authorId': '4289746', 'name': 'Qianying Liu'}, {'authorId': '2980506', 'name': 'Haiyue Song'}, {'authorId': '49298465', 'name': 'Jiwei Li'}, {'authorId': '1795664', 'name': 'S. Kurohashi'}]","['Kyoto University', 'Zhejiang University']","['China', 'Japan']",2023-05
2305.02556,Ruixin Hong,"Ruixin Hong, Hongming Zhang, Hong Zhao, Dong Yu, Changshui Zhang",Faithful Question Answering with Monte-Carlo Planning,ACL 2023 main,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although large language models demonstrate remarkable question-answering performances, revealing the intermediate reasoning steps that the models faithfully follow remains challenging. In this paper, we propose FAME (FAithful question answering with MontE-carlo planning) to answer questions based on faithful reasoning steps. The reasoning steps are organized as a structured entailment tree, which shows how premises are used to produce intermediate conclusions that can prove the correctness of the answer. We formulate the task as a discrete decision-making problem and solve it through the interaction of a reasoning environment and a controller. The environment is modular and contains several basic task-oriented modules, while the controller proposes actions to assemble the modules. Since the search space could be large, we introduce a Monte-Carlo planning algorithm to do a look-ahead search and select actions that will eventually lead to high-quality steps. FAME achieves state-of-the-art performance on the standard benchmark. It can produce valid and faithful reasoning steps compared with large language models with a much smaller model size. ","[{'version': 'v1', 'created': 'Thu, 4 May 2023 05:21:36 GMT'}]",2023-05-05,"[['Hong', 'Ruixin', ''], ['Zhang', 'Hongming', ''], ['Zhao', 'Hong', ''], ['Yu', 'Dong', ''], ['Zhang', 'Changshui', '']]",0,0,2023-05-04,1,5,1,0,0,0,198320a8c26f01e1d48fb1cd385900a7f374d609,258479954.0,https://www.semanticscholar.org/paper/198320a8c26f01e1d48fb1cd385900a7f374d609,Annual Meeting of the Association for Computational Linguistics,2023.0,57.0,2.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2151067405', 'name': 'Ruixin Hong'}, {'authorId': '49723569', 'name': 'Hongming Zhang'}, {'authorId': '2108132432', 'name': 'Honghui Zhao'}, {'authorId': '144580027', 'name': 'Dong Yu'}, {'authorId': '2164309584', 'name': 'Changshui Zhang'}]","['Tencent', 'Tsinghua University', 'Research Organization for Information Science and Technology']","['China', 'United States', 'Japan']",2023-05
2305.02626,Pingchuan Ma,"Pingchuan Ma, Zongjie Li, Ao Sun, Shuai Wang","""Oops, Did I Just Say That?"" Testing and Repairing Unethical Suggestions of Large Language Models with Suggest-Critique-Reflect Process",,,,,cs.SE cs.AI cs.HC,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  As the popularity of large language models (LLMs) soars across various applications, ensuring their alignment with human values has become a paramount concern. In particular, given that LLMs have great potential to serve as general-purpose AI assistants in daily life, their subtly unethical suggestions become a serious and real concern. Tackling the challenge of automatically testing and repairing unethical suggestions is thus demanding.   This paper introduces the first framework for testing and repairing unethical suggestions made by LLMs. We first propose ETHICSSUITE, a test suite that presents complex, contextualized, and realistic moral scenarios to test LLMs. We then propose a novel suggest-critic-reflect (SCR) process, serving as an automated test oracle to detect unethical suggestions. We recast deciding if LLMs yield unethical suggestions (a hard problem; often requiring human expertise and costly to decide) into a PCR task that can be automatically checked for violation. Moreover, we propose a novel on-the-fly (OTF) repairing scheme that repairs unethical suggestions made by LLMs in real-time. The OTF scheme is applicable to LLMs in a black-box API setting with moderate cost. With ETHICSSUITE, our study on seven popular LLMs (e.g., ChatGPT, GPT-4) uncovers in total 109,824 unethical suggestions. We apply our OTF scheme on two LLMs (Llama-13B and ChatGPT), which generates valid repair to a considerable amount of unethical ones, paving the way for more ethically conscious LLMs. ","[{'version': 'v1', 'created': 'Thu, 4 May 2023 08:00:32 GMT'}]",2023-05-05,"[['Ma', 'Pingchuan', ''], ['Li', 'Zongjie', ''], ['Sun', 'Ao', ''], ['Wang', 'Shuai', '']]",1,1,2023-05-04,1,4,3,3,1,2,24ef856534bbc0931c8bf5d2e754fda1bdedcde2,258480241.0,https://www.semanticscholar.org/paper/24ef856534bbc0931c8bf5d2e754fda1bdedcde2,arXiv.org,2023.0,65.0,7.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1384480816', 'name': 'Pingchuan Ma'}, {'authorId': '2118207559', 'name': 'Zongjie Li'}, {'authorId': '2179593108', 'name': 'Ao Sun'}, {'authorId': '2118511999', 'name': 'Shuai Wang'}]",['Hong Kong University of Science and Technology'],['China'],2023-05
2305.02869,Yiqun Yao,"Yiqun Yao, Zheng Zhang, Jing Li, and Yequan Wang",2x Faster Language Model Pre-training via Masked Structural Growth,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Acceleration of large language model pre-training is a critical issue in present NLP research. In this paper, we focus on speeding up pre-training by progressively growing from a small Transformer structure to a large one. There are two main research problems related to progressive growth: growth schedule and growth operator. For growth schedule, existing work has explored multi-stage expansion of depth and feedforward layers. However, the impact of each dimension on the schedule's efficiency is still an open question. For growth operator, existing work relies on the initialization of new weights to inherit knowledge, and achieve only non-strict function preservation, limiting further optimization of training dynamics. To address these issues, we propose Masked Structural Growth (MSG), including growth schedules involving all possible dimensions and strictly function-preserving growth operators that is independent of the initialization of new weights. Experiments show that MSG is significantly faster than related work: we achieve a speed-up of 80% for Bert-base and 120% for Bert-large pre-training. Moreover, MSG is able to improve fine-tuning performances at the same time. ","[{'version': 'v1', 'created': 'Thu, 4 May 2023 14:28:39 GMT'}]",2023-05-05,"[['Yao', 'Yiqun', ''], ['Zhang', 'Zheng', ''], ['Li', 'Jing', ''], ['Wang', 'Yequan', '']]",0,0,2023-05-04,1,4,1,0,0,0,3a015da62517a9bdc4aee25214d2e5e332bb048f,258479839.0,https://www.semanticscholar.org/paper/3a015da62517a9bdc4aee25214d2e5e332bb048f,arXiv.org,2023.0,39.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '49483501', 'name': 'Yiqun Yao'}, {'authorId': '143644345', 'name': 'Zheng Zhang'}, {'authorId': '2152907411', 'name': 'Jing Li'}, {'authorId': '2119256294', 'name': 'Yequan Wang'}]","['Harbin Institute of Technology', 'Beijing Academy of Artificial Intelligence']",['China'],2023-05
2305.02949,Bowen Zheng,"Bowen Zheng, Ran Cheng",Rethinking Population-assisted Off-policy Reinforcement Learning,Genetic and Evolutionary Computation Conference (GECCO '23),,10.1145/3583131.3590512,,cs.LG cs.NE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While off-policy reinforcement learning (RL) algorithms are sample efficient due to gradient-based updates and data reuse in the replay buffer, they struggle with convergence to local optima due to limited exploration. On the other hand, population-based algorithms offer a natural exploration strategy, but their heuristic black-box operators are inefficient. Recent algorithms have integrated these two methods, connecting them through a shared replay buffer. However, the effect of using diverse data from population optimization iterations on off-policy RL algorithms has not been thoroughly investigated. In this paper, we first analyze the use of off-policy RL algorithms in combination with population-based algorithms, showing that the use of population data could introduce an overlooked error and harm performance. To test this, we propose a uniform and scalable training design and conduct experiments on our tailored framework in robot locomotion tasks from the OpenAI gym. Our results substantiate that using population data in off-policy RL can cause instability during training and even degrade performance. To remedy this issue, we further propose a double replay buffer design that provides more on-policy data and show its effectiveness through experiments. Our results offer practical insights for training these hybrid methods. ","[{'version': 'v1', 'created': 'Thu, 4 May 2023 15:53:00 GMT'}]",2023-05-05,"[['Zheng', 'Bowen', ''], ['Cheng', 'Ran', '']]",0,0,2023-05-04,1,2,2,0,0,0,0d95bcdd699f12fae37d0d166cf80303750dd09a,258480212.0,https://www.semanticscholar.org/paper/0d95bcdd699f12fae37d0d166cf80303750dd09a,Annual Conference on Genetic and Evolutionary Computation,2023.0,37.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2112671171', 'name': 'Bowen Zheng'}, {'authorId': '2087717001', 'name': 'Ran Cheng'}]",['Southern University of Science and Technology'],['China'],2023-05
2305.03111,Binyuan Hui,"Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li,
  Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo, Xuanhe Zhou,
  Chenhao Ma, Guoliang Li, Kevin C.C. Chang, Fei Huang, Reynold Cheng, Yongbin
  Li",Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text-to-SQL parsing, which aims at converting natural language instructions into executable SQLs, has gained increasing attention in recent years. In particular, Codex and ChatGPT have shown impressive results in this task. However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on database schema with few rows of database contents leaving the gap between academic study and real-world applications. To mitigate this gap, we present Bird, a big benchmark for large-scale database grounded in text-to-SQL tasks, containing 12,751 pairs of text-to-SQL data and 95 databases with a total size of 33.4 GB, spanning 37 professional domains. Our emphasis on database values highlights the new challenges of dirty database contents, external knowledge between NL questions and database contents, and SQL efficiency, particularly in the context of massive databases. To solve these problems, text-to-SQL models must feature database value comprehension in addition to semantic parsing. The experimental results demonstrate the significance of database values in generating accurate text-to-SQLs for big databases. Furthermore, even the most effective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution accuracy, which is still far from the human result of 92.96%, proving that challenges still stand. Besides, we also provide an efficiency analysis to offer insights into generating text-to-efficient-SQLs that are beneficial to industries. We believe that BIRD will contribute to advancing real-world applications of text-to-SQL research. The leaderboard and source code are available: https://bird-bench.github.io/. ","[{'version': 'v1', 'created': 'Thu, 4 May 2023 19:02:29 GMT'}, {'version': 'v2', 'created': 'Tue, 30 May 2023 05:34:57 GMT'}]",2023-05-31,"[['Li', 'Jinyang', ''], ['Hui', 'Binyuan', ''], ['Qu', 'Ge', ''], ['Li', 'Binhua', ''], ['Yang', 'Jiaxi', ''], ['Li', 'Bowen', ''], ['Wang', 'Bailin', ''], ['Qin', 'Bowen', ''], ['Cao', 'Rongyu', ''], ['Geng', 'Ruiying', ''], ['Huo', 'Nan', ''], ['Zhou', 'Xuanhe', ''], ['Ma', 'Chenhao', ''], ['Li', 'Guoliang', ''], ['Chang', 'Kevin C. C.', ''], ['Huang', 'Fei', ''], ['Cheng', 'Reynold', ''], ['Li', 'Yongbin', '']]",1,1,2023-05-04,2,18,1,2,0,2,8f831f341e959955a495730d81996e62c57cc0bd,258547040.0,https://www.semanticscholar.org/paper/8f831f341e959955a495730d81996e62c57cc0bd,arXiv.org,2023.0,71.0,28.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2154860526', 'name': 'Jinyang Li'}, {'authorId': '151471590', 'name': 'Binyuan Hui'}, {'authorId': '2216486244', 'name': 'Ge Qu'}, {'authorId': '66200440', 'name': 'Binhua Li'}, {'authorId': '2135964855', 'name': 'Jiaxi Yang'}, {'authorId': '2132475886', 'name': 'Bowen Li'}, {'authorId': '2118640406', 'name': 'Bailin Wang'}, {'authorId': '50379530', 'name': 'Bowen Qin'}, {'authorId': '5973047', 'name': 'Rongyu Cao'}, {'authorId': '9706609', 'name': 'Ruiying Geng'}, {'authorId': '2147322690', 'name': 'Nan Huo'}, {'authorId': '2108786828', 'name': 'Chenhao Ma'}, {'authorId': '152608657', 'name': 'K. Chang'}, {'authorId': '143857288', 'name': 'Fei Huang'}, {'authorId': '2114454192', 'name': 'Reynold Cheng'}, {'authorId': '1527090216', 'name': 'Yongbin Li'}]","['Alibaba', 'Shanghai Artificial Intelligence Laboratory', 'Chinese University of Hong Kong, Shenzhen', 'Tsinghua University', 'University of Illinois Urbana-Champaign', 'Massachusetts Institute of Technology', 'University of Hong Kong']","['China', 'United States', 'Hong Kong']",2023-05
2305.03268,Ruochen Zhao,"Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, Lidong Bing",Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  As large language models (LLMs) have become the norm in NLP, demonstrating good performance in generation and reasoning tasks, one of its most fatal disadvantages is the lack of factual correctness. Generating unfactual texts not only leads to lower performances but also degrades the trust and validity of their applications. Chain-of-Thought (CoT) prompting improves trust and model performance on complex reasoning tasks by generating interpretable reasoning chains, but still suffers from factuality concerns in knowledge-intensive tasks. In this paper, we propose the Verify-and-Edit framework for CoT prompting, which seeks to increase prediction factuality by post-editing reasoning chains according to external knowledge. Building on top of GPT-3, our framework lead to accuracy improvements in multiple open-domain question-answering tasks. ","[{'version': 'v1', 'created': 'Fri, 5 May 2023 03:49:14 GMT'}]",2023-05-08,"[['Zhao', 'Ruochen', ''], ['Li', 'Xingxuan', ''], ['Joty', 'Shafiq', ''], ['Qin', 'Chengwei', ''], ['Bing', 'Lidong', '']]",0,1,2023-05-05,1,5,1,1,0,1,629c441076da3f8185b1cf85e8036064b714e249,258547173.0,https://www.semanticscholar.org/paper/629c441076da3f8185b1cf85e8036064b714e249,Annual Meeting of the Association for Computational Linguistics,2023.0,35.0,28.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2091437375', 'name': 'Ruochen Zhao'}, {'authorId': '2155447436', 'name': 'Xingxuan Li'}, {'authorId': '2708940', 'name': 'Shafiq R. Joty'}, {'authorId': '2084609980', 'name': 'Chengwei Qin'}, {'authorId': '1996394', 'name': 'Lidong Bing'}]","['Alibaba', 'Nanyang Technological University', 'Salesforce AI']","['China', 'Singapore']",2023-05
2305.03433,Kehui Tan,"Kehui Tan, Tianqi Pang, Chenyou Fan and Song Yu","Towards Applying Powerful Large AI Models in Classroom Teaching: Opportunities, Challenges and Prospects","16 pages, 2 figures",,,,cs.AI cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This perspective paper proposes a series of interactive scenarios that utilize Artificial Intelligence (AI) to enhance classroom teaching, such as dialogue auto-completion, knowledge and style transfer, and assessment of AI-generated content. By leveraging recent developments in Large Language Models (LLMs), we explore the potential of AI to augment and enrich teacher-student dialogues and improve the quality of teaching. Our goal is to produce innovative and meaningful conversations between teachers and students, create standards for evaluation, and improve the efficacy of AI-for-Education initiatives. In Section 3, we discuss the challenges of utilizing existing LLMs to effectively complete the educated tasks and present a unified framework for addressing diverse education dataset, processing lengthy conversations, and condensing information to better accomplish more downstream tasks. In Section 4, we summarize the pivoting tasks including Teacher-Student Dialogue Auto-Completion, Expert Teaching Knowledge and Style Transfer, and Assessment of AI-Generated Content (AIGC), providing a clear path for future research. In Section 5, we also explore the use of external and adjustable LLMs to improve the generated content through human-in-the-loop supervision and reinforcement learning. Ultimately, this paper seeks to highlight the potential for AI to aid the field of education and promote its further exploration. ","[{'version': 'v1', 'created': 'Fri, 5 May 2023 11:09:13 GMT'}, {'version': 'v2', 'created': 'Mon, 12 Jun 2023 11:53:37 GMT'}]",2023-06-13,"[['Tan', 'Kehui', ''], ['Pang', 'Tianqi', ''], ['Fan', 'Chenyou', ''], ['Yu', 'Song', '']]",0,0,2023-05-05,2,4,2,0,0,0,a6d201a6e73064c2fb3fd9d7aa43e168b8e0a1b0,258547135.0,https://www.semanticscholar.org/paper/a6d201a6e73064c2fb3fd9d7aa43e168b8e0a1b0,arXiv.org,2023.0,58.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Education', 'source': 's2-fos-model'}]","[{'authorId': '2216509779', 'name': 'Kehui Tan'}, {'authorId': '2215451922', 'name': 'Tianqi Pang'}, {'authorId': '2047692', 'name': 'Chenyou Fan'}]",['South China Normal University'],['China'],2023-05
2305.03453,Lei Wang,"Lei Wang, Yi Hu, Jiabang He, Xing Xu, Ning Liu, Hui Liu, Heng Tao Shen",T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) have recently demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. They have also shown the ability to perform chain-of-thought (CoT) reasoning to solve complex problems. Recent studies have explored CoT reasoning in complex multimodal scenarios, such as the science question answering task, by fine-tuning multimodal models with high-quality human-annotated CoT rationales. However, collecting high-quality COT rationales is usually time-consuming and costly. Besides, the annotated rationales are hardly accurate due to the external essential information missed. To address these issues, we propose a novel method termed \emph{T-SciQ} that aims at teaching science question answering with LLM signals. The T-SciQ approach generates high-quality CoT rationales as teaching signals and is advanced to train much smaller models to perform CoT reasoning in complex modalities. Additionally, we introduce a novel data mixing strategy to produce more effective teaching data samples by policy for simple and complex science question answer problems. Extensive experimental results show that our T-SciQ method achieves a new state-of-the-art performance on the ScienceQA benchmark, with an accuracy of 96.18\%. Moreover, our approach outperforms the most powerful fine-tuned baseline by 4.5\%. ","[{'version': 'v1', 'created': 'Fri, 5 May 2023 11:56:30 GMT'}, {'version': 'v2', 'created': 'Tue, 9 May 2023 11:31:23 GMT'}, {'version': 'v3', 'created': 'Wed, 16 Aug 2023 12:50:46 GMT'}]",2023-08-17,"[['Wang', 'Lei', ''], ['Hu', 'Yi', ''], ['He', 'Jiabang', ''], ['Xu', 'Xing', ''], ['Liu', 'Ning', ''], ['Liu', 'Hui', ''], ['Shen', 'Heng Tao', '']]",0,0,2023-05-05,3,7,1,0,0,0,3f758a13d3703b02bdf977f9189230276064da42,258546810.0,https://www.semanticscholar.org/paper/3f758a13d3703b02bdf977f9189230276064da42,arXiv.org,2023.0,65.0,4.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '145131956', 'name': 'Lei Wang'}, {'authorId': '4925252', 'name': 'Yilang Hu'}, {'authorId': '2216205261', 'name': 'Jiabang He'}, {'authorId': '2051281615', 'name': 'Xingdong Xu'}, {'authorId': '2100091244', 'name': 'Ning Liu'}, {'authorId': '2144318134', 'name': 'Hui-juan Liu'}, {'authorId': '2148452614', 'name': 'Hengtao Shen'}]","['Beijing Forestry University', 'Xidian University', 'Beijing Rongda Technology Co., Ltd.', 'Singapore Management University']","['China', 'Singapore']",2023-05
2305.03513,Yucheng Shi,"Yucheng Shi, Hehuan Ma, Wenliang Zhong, Qiaoyu Tan, Gengchen Mai,
  Xiang Li, Tianming Liu, Junzhou Huang",ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs,"6 pages, 2 figures",,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  ChatGPT, as a recently launched large language model (LLM), has shown superior performance in various natural language processing (NLP) tasks. However, two major limitations hinder its potential applications: (1) the inflexibility of finetuning on downstream tasks and (2) the lack of interpretability in the decision-making process. To tackle these limitations, we propose a novel framework that leverages the power of ChatGPT for specific tasks, such as text classification, while improving its interpretability. The proposed framework conducts a knowledge graph extraction task to extract refined and structural knowledge from the raw data using ChatGPT. The rich knowledge is then converted into a graph, which is further used to train an interpretable linear classifier to make predictions. To evaluate the effectiveness of our proposed method, we conduct experiments on four datasets. The result shows that our method can significantly improve the performance compared to directly utilizing ChatGPT for text classification tasks. And our method provides a more transparent decision-making process compared with previous text classification methods. ","[{'version': 'v1', 'created': 'Wed, 3 May 2023 19:57:43 GMT'}, {'version': 'v2', 'created': 'Tue, 19 Sep 2023 14:26:17 GMT'}]",2023-09-20,"[['Shi', 'Yucheng', ''], ['Ma', 'Hehuan', ''], ['Zhong', 'Wenliang', ''], ['Tan', 'Qiaoyu', ''], ['Mai', 'Gengchen', ''], ['Li', 'Xiang', ''], ['Liu', 'Tianming', ''], ['Huang', 'Junzhou', '']]",1,1,2023-05-03,2,8,3,1,0,1,a49687ee1ce6ace8329cfcb693d8f8198c867bcc,258546734.0,https://www.semanticscholar.org/paper/a49687ee1ce6ace8329cfcb693d8f8198c867bcc,arXiv.org,2023.0,52.0,8.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '46571755', 'name': 'Yucheng Shi'}, {'authorId': '51091763', 'name': 'Hehuan Ma'}, {'authorId': '2100561389', 'name': 'Wenliang Zhong'}, {'authorId': '40626717', 'name': 'Gengchen Mai'}, {'authorId': '2144438902', 'name': 'Xiang Li'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}, {'authorId': '1768190', 'name': 'Junzhou Huang'}]","['New York University Shanghai', 'University of Georgia', 'Massachusetts General Hospital', 'The University of Texas at Arlington']","['China', 'United States']",2023-05
2305.03518,Yuanhang Zheng,"Yuanhang Zheng, Zhixing Tan, Peng Li and Yang Liu",Black-box Prompt Tuning with Subspace Learning,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Black-box prompt tuning uses derivative-free optimization algorithms to learn prompts in low-dimensional subspaces instead of back-propagating through the network of Large Language Models (LLMs). Recent studies have found that black-box prompt tuning lacks versatility across tasks and LLMs, which we believe is related to the inappropriate choice of subspaces. In this paper, we propose Black-box prompt tuning with Subspace Learning (BSL) to improve the versatility of black-box prompt tuning. Based on the assumption that nearly optimal prompts for similar tasks exist in a common subspace, we propose identifying such subspaces by meta-learning on a set of similar source tasks. Therefore, for a target task that shares similarities with source tasks, we guarantee that optimizing in the subspace can find a prompt that performs well on the target task. Experiments confirm that our BSL framework consistently achieves competitive performance regardless of downstream tasks and LLMs. ","[{'version': 'v1', 'created': 'Thu, 4 May 2023 01:04:25 GMT'}]",2023-05-08,"[['Zheng', 'Yuanhang', ''], ['Tan', 'Zhixing', ''], ['Li', 'Peng', ''], ['Liu', 'Yang', '']]",0,0,2023-05-04,1,4,2,0,0,0,71207d96ca0ae29cc335ab59043eefb36e922fe1,258546903.0,https://www.semanticscholar.org/paper/71207d96ca0ae29cc335ab59043eefb36e922fe1,arXiv.org,2023.0,52.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2329636', 'name': 'Yuanhang Zheng'}, {'authorId': '3468510', 'name': 'Zhixing Tan'}, {'authorId': '2152926422', 'name': 'Peng Li'}, {'authorId': '2152798555', 'name': 'Yang Liu'}]","['University of Chinese Academy of Sciences', 'Beijing Information Science & Technology University', 'Tsinghua University', 'Beijing Academy of Artificial Intelligence']",['China'],2023-05
2305.03688,Zeqi Tan,"Zeqi Tan, Shen Huang, Zixia Jia, Jiong Cai, Yinghui Li, Weiming Lu,
  Yueting Zhuang, Kewei Tu, Pengjun Xie, Fei Huang and Yong Jiang",DAMO-NLP at SemEval-2023 Task 2: A Unified Retrieval-augmented System for Multilingual Named Entity Recognition,"Accepted to SemEval 2023, winners for 9 out of 13 tracks, performance
  beyond ChatGPT",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The MultiCoNER \RNum{2} shared task aims to tackle multilingual named entity recognition (NER) in fine-grained and noisy scenarios, and it inherits the semantic ambiguity and low-context setting of the MultiCoNER \RNum{1} task. To cope with these problems, the previous top systems in the MultiCoNER \RNum{1} either incorporate the knowledge bases or gazetteers. However, they still suffer from insufficient knowledge, limited context length, single retrieval strategy. In this paper, our team \textbf{DAMO-NLP} proposes a unified retrieval-augmented system (U-RaNER) for fine-grained multilingual NER. We perform error analysis on the previous top systems and reveal that their performance bottleneck lies in insufficient knowledge. Also, we discover that the limited context length causes the retrieval knowledge to be invisible to the model. To enhance the retrieval context, we incorporate the entity-centric Wikidata knowledge base, while utilizing the infusion approach to broaden the contextual scope of the model. Also, we explore various search strategies and refine the quality of retrieval knowledge. Our system\footnote{We will release the dataset, code, and scripts of our system at {\small \url{https://github.com/modelscope/AdaSeq/tree/master/examples/U-RaNER}}.} wins 9 out of 13 tracks in the MultiCoNER \RNum{2} shared task. Additionally, we compared our system with ChatGPT, one of the large language models which have unlocked strong capabilities on many tasks. The results show that there is still much room for improvement for ChatGPT on the extraction task. ","[{'version': 'v1', 'created': 'Fri, 5 May 2023 16:59:26 GMT'}, {'version': 'v2', 'created': 'Tue, 9 May 2023 03:43:10 GMT'}, {'version': 'v3', 'created': 'Wed, 17 May 2023 03:14:00 GMT'}]",2023-05-18,"[['Tan', 'Zeqi', ''], ['Huang', 'Shen', ''], ['Jia', 'Zixia', ''], ['Cai', 'Jiong', ''], ['Li', 'Yinghui', ''], ['Lu', 'Weiming', ''], ['Zhuang', 'Yueting', ''], ['Tu', 'Kewei', ''], ['Xie', 'Pengjun', ''], ['Huang', 'Fei', ''], ['Jiang', 'Yong', '']]",1,1,2023-05-05,3,11,1,1,0,1,358e20d80d6758cf0f6ce9f16f8c21a197095e4b,258547160.0,https://www.semanticscholar.org/paper/358e20d80d6758cf0f6ce9f16f8c21a197095e4b,International Workshop on Semantic Evaluation,2023.0,51.0,5.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2092670555', 'name': 'Zeqi Tan'}, {'authorId': '2186268584', 'name': 'Shen Huang'}, {'authorId': '1453587987', 'name': 'Zixia Jia'}, {'authorId': '4442130', 'name': 'Jiong Cai'}, {'authorId': '2110503552', 'name': 'Yinghui Li'}, {'authorId': '1776903', 'name': 'Weiming Lu'}, {'authorId': '2056432541', 'name': 'Y. Zhuang'}, {'authorId': '40341553', 'name': 'Kewei Tu'}, {'authorId': '35930962', 'name': 'Pengjun Xie'}, {'authorId': '2117426607', 'name': 'Fei Huang'}, {'authorId': '50262192', 'name': 'Yong Jiang'}]","['Zhejiang University', 'ShanghaiTech University']",['China'],2023-05
2305.03701,Yunxin Li,"Yunxin Li, Baotian Hu, Xinyu Chen, Lin Ma, Yong Xu, and Min Zhang",LMEye: An Interactive Perception Network for Large Language Models,working in progress,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Training a Multimodal Large Language Model (MLLM) from scratch, like GPT-4, is resource-intensive. Regarding Large Language Models (LLMs) as the core processor for multimodal information, our paper introduces LMEye, a human-like eye with a play-and-plug interactive perception network, designed to enable dynamic interaction between LLMs and external vision information. Previous methods incorporate visual information into LLMs with a simple visual mapping network or Q-former from BLIP-2. Such networks project the image feature once yet do not consider the interaction between the image and the human input query. Hence, the obtained visual information without being connected to human intention may be inadequate for LLMs to generate intention-following responses, which we refer to as static visual information. LMEye addresses this issue by allowing the LLM to request the desired visual information aligned with various human instructions, which we term as the dynamic visual information interaction. Specifically, LMEye consists of a simple visual mapping network to provide the basic perception of an image for LLMs. It also contains additional modules responsible for acquiring requests from LLMs, performing request-based visual information interaction, and transmitting the resulting interacted visual information to LLMs, respectively. In this way, LLMs act to understand the human query, deliver the corresponding request to the request-based visual information interaction module, and generate the response based on the interleaved multimodal information. We evaluate LMEye through extensive experiments on some multimodal benchmarks, demonstrating that it significantly improves the zero-shot performance on various multimodal tasks compared to previous methods, with less parameters. ","[{'version': 'v1', 'created': 'Fri, 5 May 2023 17:27:21 GMT'}, {'version': 'v2', 'created': 'Thu, 18 May 2023 17:28:58 GMT'}, {'version': 'v3', 'created': 'Fri, 19 May 2023 05:42:57 GMT'}, {'version': 'v4', 'created': 'Sat, 22 Jul 2023 06:24:53 GMT'}, {'version': 'v5', 'created': 'Wed, 2 Aug 2023 11:52:16 GMT'}, {'version': 'v6', 'created': 'Thu, 28 Sep 2023 08:18:43 GMT'}]",2023-09-29,"[['Li', 'Yunxin', ''], ['Hu', 'Baotian', ''], ['Chen', 'Xinyu', ''], ['Ma', 'Lin', ''], ['Xu', 'Yong', ''], ['Zhang', 'Min', '']]",0,1,2023-05-05,6,6,2,1,0,1,20fcc01d12a50f1da2af71d85f0a269b3ba48b77,258546868.0,https://www.semanticscholar.org/paper/20fcc01d12a50f1da2af71d85f0a269b3ba48b77,arXiv.org,2023.0,58.0,6.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2118046679', 'name': 'Yunxin Li'}, {'authorId': '33968873', 'name': 'Baotian Hu'}, {'authorId': '2109080849', 'name': 'Xinyu Chen'}, {'authorId': '2152343740', 'name': 'Lin Ma'}, {'authorId': '39767557', 'name': 'M. Zhang'}]","['Harbin Institute of Technology', 'Meituan']",['China'],2023-05
2305.04032,Kechi Zhang,"Kechi Zhang, Huangzhao Zhang, Ge Li, Jia Li, Zhuo Li, Zhi Jin",ToolCoder: Teach Code Generation Models to use API search tools,,,,,cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automatically generating source code from natural language descriptions has been a growing field of research in recent years. However, current large-scale code generation models often encounter difficulties when selecting appropriate APIs for specific contexts. These models may generate APIs that do not meet requirements or refer to non-existent APIs in third-party libraries, especially for lesser-known or private libraries. Inspired by the process of human developers using tools to search APIs, we propose ToolCoder, a novel approach that integrates API search tools with existing models to assist in code generation and API selection. To teach our model to use tools, we introduce an automated data annotation method using ChatGPT to add tool usage information into the source code data and fine-tune code generation models. During inference, we integrate API search tools into the generation process so that our model can automatically use the search tool to get suggestions when selecting an API. Our experimental results demonstrate that ToolCoder exhibits excellent performance and generalization across five public and private library code generation benchmarks, with at least 6.21\% improvement on average pass@1 metrics and 9.64\% improvement on average pass@10 metrics compared to state-of-the-art methods. Furthermore, we show that our relatively small ToolCoder model is comparable to one of the current best models, GPT-3.5, highlighting the potential of incorporating programming tools into the code generation process. ","[{'version': 'v1', 'created': 'Sat, 6 May 2023 12:45:28 GMT'}, {'version': 'v2', 'created': 'Tue, 9 May 2023 19:34:50 GMT'}, {'version': 'v3', 'created': 'Wed, 2 Aug 2023 10:45:26 GMT'}, {'version': 'v4', 'created': 'Thu, 17 Aug 2023 12:16:47 GMT'}, {'version': 'v5', 'created': 'Mon, 11 Sep 2023 06:33:46 GMT'}]",2023-09-12,"[['Zhang', 'Kechi', ''], ['Zhang', 'Huangzhao', ''], ['Li', 'Ge', ''], ['Li', 'Jia', ''], ['Li', 'Zhuo', ''], ['Jin', 'Zhi', '']]",1,1,2023-05-06,5,6,1,2,0,2,287b01a1dae6e0f230adc8db46a03d22df50d570,258557336.0,https://www.semanticscholar.org/paper/287b01a1dae6e0f230adc8db46a03d22df50d570,arXiv.org,2023.0,30.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2033148496', 'name': 'Kechi Zhang'}, {'authorId': '2154591375', 'name': 'Ge Li'}, {'authorId': '2118373749', 'name': 'Jia Li'}, {'authorId': '2118393672', 'name': 'Zhuo Li'}, {'authorId': '2152843753', 'name': 'Zhi Jin'}]",['Peking University'],['China'],2023-05
2305.04039,Tianqiang Yan,Tianqiang Yan and Tiansheng Xu,Refining the Responses of LLMs by Themselves,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-sa/4.0/,"  In this paper, we propose a simple yet efficient approach based on prompt engineering that leverages the large language model itself to optimize its answers without relying on auxiliary models. We introduce an iterative self-evaluating optimization mechanism, with the potential for improved output quality as iterations progress, removing the need for manual intervention. The experiment's findings indicate that utilizing our response refinement framework on the GPT-3.5 model yields results that are on par with, or even surpass, those generated by the cutting-edge GPT-4 model. Detailed implementation strategies and illustrative examples are provided to demonstrate the superiority of our proposed solution. ","[{'version': 'v1', 'created': 'Sat, 6 May 2023 13:03:45 GMT'}]",2023-05-09,"[['Yan', 'Tianqiang', ''], ['Xu', 'Tiansheng', '']]",0,1,2023-05-06,1,2,2,2,0,2,b67184a99e78cc53c2bcb50a73ac8c3c873f29f4,258556861.0,https://www.semanticscholar.org/paper/b67184a99e78cc53c2bcb50a73ac8c3c873f29f4,arXiv.org,2023.0,17.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2211098594', 'name': 'Tianqiang Yan'}, {'authorId': '2216597444', 'name': 'Tiansheng Xu'}]","['Chinese University of Hong Kong, Shenzhen', 'Imperial College London']","['China', 'United Kingdom']",2023-05
2305.04087,Kechi Zhang,"Kechi Zhang, Zhuo Li, Jia Li, Ge Li, Zhi Jin",Self-Edit: Fault-Aware Code Editor for Code Generation,Accepted by ACL2023,,,,cs.SE cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, LLMs still suffer from poor accuracy. Inspired by the process of human programming, we propose a generate-and-edit approach named Self-Edit that utilizes execution results of the generated code from LLMs to improve the code quality on the competitive programming task. We execute the generated code on the example test case provided in the question and wrap execution results into a supplementary comment. Utilizing this comment as guidance, our fault-aware code editor is employed to correct errors in the generated code. We perform extensive evaluations across two competitive programming datasets with nine different LLMs. Compared to directly generating from LLMs, our approach can improve the average of pass@1 by 89\% on APPS-dev, 31\% on APPS-test, and 48\% on HumanEval over nine popular code generation LLMs with parameter sizes ranging from 110M to 175B. Compared to other post-processing methods, our method demonstrates superior accuracy and efficiency. ","[{'version': 'v1', 'created': 'Sat, 6 May 2023 16:12:19 GMT'}, {'version': 'v2', 'created': 'Fri, 26 May 2023 07:00:47 GMT'}, {'version': 'v3', 'created': 'Mon, 5 Jun 2023 04:38:07 GMT'}, {'version': 'v4', 'created': 'Thu, 17 Aug 2023 12:20:27 GMT'}, {'version': 'v5', 'created': 'Mon, 11 Sep 2023 06:27:53 GMT'}]",2023-09-12,"[['Zhang', 'Kechi', ''], ['Li', 'Zhuo', ''], ['Li', 'Jia', ''], ['Li', 'Ge', ''], ['Jin', 'Zhi', '']]",0,0,2023-05-06,5,5,2,0,0,0,2f0028060a3a3187d3cbecc3b423dbcf8c8387c3,258557186.0,https://www.semanticscholar.org/paper/2f0028060a3a3187d3cbecc3b423dbcf8c8387c3,Annual Meeting of the Association for Computational Linguistics,2023.0,42.0,14.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2033148496', 'name': 'Kechi Zhang'}, {'authorId': '2118393672', 'name': 'Zhuo Li'}, {'authorId': '2208883878', 'name': 'Jia Li'}, {'authorId': '2154591375', 'name': 'Ge Li'}, {'authorId': '2152843753', 'name': 'Zhi Jin'}]",['Peking University'],['China'],2023-05
2305.04091,Lei Wang,"Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei
  Lee and Ee-Peng Lim",Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models,ACL 2023,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual effort, Zero-shot-CoT concatenates the target problem statement with ""Let's think step by step"" as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting. ","[{'version': 'v1', 'created': 'Sat, 6 May 2023 16:34:37 GMT'}, {'version': 'v2', 'created': 'Sat, 20 May 2023 04:47:31 GMT'}, {'version': 'v3', 'created': 'Fri, 26 May 2023 07:06:48 GMT'}]",2023-05-29,"[['Wang', 'Lei', ''], ['Xu', 'Wanyu', ''], ['Lan', 'Yihuai', ''], ['Hu', 'Zhiqiang', ''], ['Lan', 'Yunshi', ''], ['Lee', 'Roy Ka-Wei', ''], ['Lim', 'Ee-Peng', '']]",0,1,2023-05-06,3,7,1,1,0,1,62176de125738e3b95850d1227bac81fd646b78e,258558102.0,https://www.semanticscholar.org/paper/62176de125738e3b95850d1227bac81fd646b78e,Annual Meeting of the Association for Computational Linguistics,2023.0,53.0,50.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '145131956', 'name': 'Lei Wang'}, {'authorId': '2143418409', 'name': 'Wanyu Xu'}, {'authorId': '2150277971', 'name': 'Yihuai Lan'}, {'authorId': '2203447284', 'name': 'Zhiqiang Hu'}, {'authorId': '3458560', 'name': 'Yunshi Lan'}, {'authorId': '38656724', 'name': 'R. Lee'}, {'authorId': '2212836814', 'name': 'Ee-Peng Lim'}]","['Singapore University of Technology and Design', 'East China Normal University', 'Southwest Jiaotong University', 'Singapore Management University']","['China', 'Singapore']",2023-05
2305.04118,Zhiwei He,"Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui
  Wang, Zhaopeng Tu, Shuming Shi, Xing Wang",Exploring Human-Like Translation Strategy with Large Language Models,V2: add more experiments and case studies; polish writing,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have demonstrated impressive capabilities in general scenarios, exhibiting a level of aptitude that approaches, in some aspects even surpasses, human-level intelligence. Among their numerous skills, the translation abilities of LLMs have received considerable attention. In contrast to traditional machine translation that focuses solely on source-target mapping, LLM-based translation can potentially mimic the human translation process that takes many preparatory steps to ensure high-quality translation. This work aims to explore this possibility by proposing the MAPS framework, which stands for Multi-Aspect Prompting and Selection. Specifically, we enable LLMs to first analyze the given source text and extract three aspects of translation-related knowledge: keywords, topics and relevant demonstrations to guide the translation process. To filter out the noisy and unhelpful knowledge, we employ a selection mechanism based on quality estimation. Experiments suggest that MAPS brings significant and consistent improvements over text-davinci-003 and Alpaca on eight translation directions from the latest WMT22 test sets. Our further analysis shows that the extracted knowledge is critical in resolving up to 59% of hallucination mistakes in translation. Code is available at https://github.com/zwhe99/MAPS-mt. ","[{'version': 'v1', 'created': 'Sat, 6 May 2023 19:03:12 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Jun 2023 15:05:16 GMT'}]",2023-06-23,"[['He', 'Zhiwei', ''], ['Liang', 'Tian', ''], ['Jiao', 'Wenxiang', ''], ['Zhang', 'Zhuosheng', ''], ['Yang', 'Yujiu', ''], ['Wang', 'Rui', ''], ['Tu', 'Zhaopeng', ''], ['Shi', 'Shuming', ''], ['Wang', 'Xing', '']]",0,0,2023-05-06,2,9,1,1,0,1,9702aa281204e7a692fb3ecc83981198426ff70d,258558158.0,https://www.semanticscholar.org/paper/9702aa281204e7a692fb3ecc83981198426ff70d,arXiv.org,2023.0,71.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2610876', 'name': 'Zhiwei He'}, {'authorId': '31395252', 'name': 'Tian Liang'}, {'authorId': '12386833', 'name': 'Wenxiang Jiao'}, {'authorId': '3322871', 'name': 'Zhuosheng Zhang'}, {'authorId': '2108585311', 'name': 'Yujiu Yang'}, {'authorId': '2151039787', 'name': 'Rui Wang'}, {'authorId': '2909321', 'name': 'Zhaopeng Tu'}, {'authorId': '2072684668', 'name': 'Shuming Shi'}, {'authorId': '48631170', 'name': 'Xing Wang'}]","['Tencent', 'Tsinghua University', 'Shanghai Jiao Tong University']",['China'],2023-05
2305.04207,Zhiqiang Yuan,"Zhiqiang Yuan, Yiling Lou, Mingwei Liu, Shiji Ding, Kaixin Wang,
  Yixuan Chen, Xin Peng",No More Manual Tests? Evaluating and Improving ChatGPT for Unit Test Generation,,,,,cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Unit testing is essential in detecting bugs in functionally-discrete program units. Manually writing high-quality unit tests is time-consuming and laborious. Although traditional techniques can generate tests with reasonable coverage, they exhibit low readability and cannot be directly adopted by developers. Recent work has shown the large potential of large language models (LLMs) in unit test generation, which can generate more human-like and meaningful test code. ChatGPT, the latest LLM incorporating instruction tuning and reinforcement learning, has performed well in various domains. However, It remains unclear how effective ChatGPT is in unit test generation.   In this work, we perform the first empirical study to evaluate ChatGPT's capability of unit test generation. Specifically, we conduct a quantitative analysis and a user study to systematically investigate the quality of its generated tests regarding the correctness, sufficiency, readability, and usability. The tests generated by ChatGPT still suffer from correctness issues, including diverse compilation errors and execution failures. Still, the passing tests generated by ChatGPT resemble manually-written tests by achieving comparable coverage, readability, and even sometimes developers' preference. Our findings indicate that generating unit tests with ChatGPT could be very promising if the correctness of its generated tests could be further improved.   Inspired by our findings above, we propose ChatTESTER, a novel ChatGPT-based unit test generation approach, which leverages ChatGPT itself to improve the quality of its generated tests. ChatTESTER incorporates an initial test generator and an iterative test refiner. Our evaluation demonstrates the effectiveness of ChatTESTER by generating 34.3% more compilable tests and 18.7% more tests with correct assertions than the default ChatGPT. ","[{'version': 'v1', 'created': 'Sun, 7 May 2023 07:17:08 GMT'}, {'version': 'v2', 'created': 'Tue, 9 May 2023 02:16:31 GMT'}]",2023-05-10,"[['Yuan', 'Zhiqiang', ''], ['Lou', 'Yiling', ''], ['Liu', 'Mingwei', ''], ['Ding', 'Shiji', ''], ['Wang', 'Kaixin', ''], ['Chen', 'Yixuan', ''], ['Peng', 'Xin', '']]",1,1,2023-05-07,2,7,1,1,0,1,4c552ad10efda5283f7b681f7dd2e532445259fc,258557344.0,https://www.semanticscholar.org/paper/4c552ad10efda5283f7b681f7dd2e532445259fc,arXiv.org,2023.0,64.0,22.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2176159206', 'name': 'Zhiqiang Yuan'}, {'authorId': '2539239', 'name': 'Yiling Lou'}, {'authorId': '2007711208', 'name': 'Mingwei Liu'}, {'authorId': '2216553856', 'name': 'Shiji Ding'}, {'authorId': '2148359505', 'name': 'Kaixin Wang'}, {'authorId': '2116664540', 'name': 'Yixuan Chen'}, {'authorId': '2216694669', 'name': 'Xin Peng'}]","['Jiangxi Normal University', 'Tongji University', 'Xiamen University', 'Fudan University']",['China'],2023-05
2305.04429,Yang Wu,"Yang Wu, Yanyan Zhao, Zhongyang Li, Bing Qin, Kai Xiong",Improving Cross-Task Generalization with Step-by-Step Instructions,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Instruction tuning has been shown to be able to improve cross-task generalization of language models. However, it is still challenging for language models to complete the target tasks following the instructions, as the instructions are general and lack intermediate steps. To address this problem, we propose to incorporate the step-by-step instructions to help language models to decompose the tasks, which can provide the detailed and specific procedures for completing the target tasks. The step-by-step instructions are obtained automatically by prompting ChatGPT, which are further combined with the original instructions to tune language models. The extensive experiments on SUP-NATINST show that the high-quality step-by-step instructions can improve cross-task generalization across different model sizes. Moreover, the further analysis indicates the importance of the order of steps of the step-by-step instruction for the improvement. To facilitate future research, we release the step-by-step instructions and their human quality evaluation results. ","[{'version': 'v1', 'created': 'Mon, 8 May 2023 02:50:41 GMT'}]",2023-05-09,"[['Wu', 'Yang', ''], ['Zhao', 'Yanyan', ''], ['Li', 'Zhongyang', ''], ['Qin', 'Bing', ''], ['Xiong', 'Kai', '']]",1,1,2023-05-08,1,5,1,1,0,1,cb236528fbb09a568ab5cd056b75e3f2bdd61b61,258556997.0,https://www.semanticscholar.org/paper/cb236528fbb09a568ab5cd056b75e3f2bdd61b61,arXiv.org,2023.0,19.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '143792101', 'name': 'Yang Wu'}, {'authorId': '49339265', 'name': 'Yanyan Zhao'}, {'authorId': '2145415526', 'name': 'Zhongyang Li'}, {'authorId': '2203961541', 'name': 'Bing Qin'}, {'authorId': '2121278449', 'name': 'Kai Xiong'}]","['Harbin Institute of Technology', 'Huawei Technologies (China)', 'Singapore Management University']","['China', 'Singapore']",2023-05
2305.04757,Ziyang Luo,"Ziyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang Tao, Jing Ma,
  Qingwei Lin, Daxin Jiang",Augmented Large Language Models with Parametric Knowledge Guiding,,,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) have significantly advanced natural language processing (NLP) with their impressive language understanding and generation capabilities. However, their performance may be suboptimal for domain-specific tasks that require specialized knowledge due to limited exposure to the related data. Additionally, the lack of transparency of most state-of-the-art (SOTA) LLMs, which can only be accessed via APIs, impedes further fine-tuning with domain custom data. Moreover, providing private data to the LLMs' owner leads to data privacy problems. To address these challenges, we propose the novel Parametric Knowledge Guiding (PKG) framework, which equips LLMs with a knowledge-guiding module to access relevant knowledge without altering the LLMs' parameters. Our PKG is based on open-source ""white-box"" language models, allowing offline memory of any knowledge that LLMs require. We demonstrate that our PKG framework can enhance the performance of ""black-box"" LLMs on a range of domain knowledge-intensive tasks that require factual (+7.9%), tabular (+11.9%), medical (+3.0%), and multimodal (+8.1%) knowledge. ","[{'version': 'v1', 'created': 'Mon, 8 May 2023 15:05:16 GMT'}, {'version': 'v2', 'created': 'Thu, 18 May 2023 08:14:08 GMT'}]",2023-05-19,"[['Luo', 'Ziyang', ''], ['Xu', 'Can', ''], ['Zhao', 'Pu', ''], ['Geng', 'Xiubo', ''], ['Tao', 'Chongyang', ''], ['Ma', 'Jing', ''], ['Lin', 'Qingwei', ''], ['Jiang', 'Daxin', '']]",0,0,2023-05-08,2,8,3,0,0,0,e0dc8e113dbdd2896fb6420ac93e0b976c47f2a2,258556855.0,https://www.semanticscholar.org/paper/e0dc8e113dbdd2896fb6420ac93e0b976c47f2a2,arXiv.org,2023.0,59.0,9.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '23523733', 'name': 'Ziyang Luo'}, {'authorId': '46747953', 'name': 'Can Xu'}, {'authorId': '2007757792', 'name': 'Pu Zhao'}, {'authorId': '2442662', 'name': 'Xiubo Geng'}, {'authorId': '8801869', 'name': 'Chongyang Tao'}, {'authorId': '2157405974', 'name': 'Jing Ma'}, {'authorId': '2793487', 'name': 'Qingwei Lin'}, {'authorId': '2086994543', 'name': 'Daxin Jiang'}]","['Hong Kong Baptist University', 'Microsoft']","['China', 'United States']",2023-05
2305.04764,Chen Zhi,"Zhuokui Xie, Yinghao Chen, Chen Zhi, Shuiguang Deng, Jianwei Yin",ChatUniTest: a ChatGPT-based automated unit test generation tool,,,,,cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Unit testing is a crucial, yet often tedious and time-consuming task. To relieve developers from this burden, automated unit test generation techniques are developed. Existing automated unit test generation tools, such as program-analysis-based tools like EvoSuite and Randoop, lack program comprehension, resulting in unit tests with poor readability and limited assertions. Language-model-based tools, such as AthenaTest and A3Test, have limitations in the generation of correct unit tests. In this paper, we introduce ChatUniTest, a ChatGPT-based automated unit test generation tool developed under the Generation-Validation-Repair framework. ChatUniTest generates tests by parsing the project, extracting essential information, and creating an adaptive focal context that includes the focal method and its dependencies within the pre-defined maximum prompt token limit. The context is incorporated into a prompt and subsequently submitted to ChatGPT. Once ChatGPT's response is received, ChatUniTest proceeds to extract the raw test from the response. It then validates the test and employs rule-based repair to fix syntactic and simple compile errors, followed by ChatGPT-based repair to address challenging errors. Our rigorous evaluation demonstrates that ChatUniTest outperforms EvoSuite in branch and line coverage, surpasses AthenaTest and A3Test in focal method coverage, and effectively generates assertions while utilizing mock objects and reflection to achieve test objectives. ","[{'version': 'v1', 'created': 'Mon, 8 May 2023 15:12:07 GMT'}]",2023-05-09,"[['Xie', 'Zhuokui', ''], ['Chen', 'Yinghao', ''], ['Zhi', 'Chen', ''], ['Deng', 'Shuiguang', ''], ['Yin', 'Jianwei', '']]",1,1,2023-05-08,1,5,1,1,0,1,5888f7aba5c601a668c290bf57addf79cc1518f1,258556880.0,https://www.semanticscholar.org/paper/5888f7aba5c601a668c290bf57addf79cc1518f1,arXiv.org,2023.0,27.0,8.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1387638000', 'name': 'Zhuo-Qi Xie'}, {'authorId': '2108965087', 'name': 'Ying-Liang Chen'}, {'authorId': '2064478633', 'name': 'Chen Zhi'}, {'authorId': '145590434', 'name': 'Shuiguang Deng'}, {'authorId': '2116398505', 'name': 'Jianwei Yin'}]",['Zhejiang University'],['China'],2023-05
2305.04824,Zenan Xu,"Zenan Xu, Xiaojun Meng, Yasheng Wang, Qinliang Su, Zexuan Qiu, Xin
  Jiang, Qun Liu",Learning Summary-Worthy Visual Representation for Abstractive Summarization in Video,Accepted by IJCAI-2023,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal abstractive summarization for videos (MAS) requires generating a concise textual summary to describe the highlights of a video according to multimodal resources, in our case, the video content and its transcript. Inspired by the success of the large-scale generative pre-trained language model (GPLM) in generating high-quality textual content (e.g., summary), recent MAS methods have proposed to adapt the GPLM to this task by equipping it with the visual information, which is often obtained through a general-purpose visual feature extractor. However, the generally extracted visual features may overlook some summary-worthy visual information, which impedes model performance. In this work, we propose a novel approach to learning the summary-worthy visual representation that facilitates abstractive summarization. Our method exploits the summary-worthy information from both the cross-modal transcript data and the knowledge that distills from the pseudo summary. Extensive experiments on three public multimodal datasets show that our method outperforms all competing baselines. Furthermore, with the advantages of summary-worthy visual information, our model can have a significant improvement on small datasets or even datasets with limited training data. ","[{'version': 'v1', 'created': 'Mon, 8 May 2023 16:24:46 GMT'}]",2023-05-09,"[['Xu', 'Zenan', ''], ['Meng', 'Xiaojun', ''], ['Wang', 'Yasheng', ''], ['Su', 'Qinliang', ''], ['Qiu', 'Zexuan', ''], ['Jiang', 'Xin', ''], ['Liu', 'Qun', '']]",0,1,2023-05-08,1,7,1,0,0,0,02bdfc311998fdee77fb31ffa723ef3c95a75794,258557345.0,https://www.semanticscholar.org/paper/02bdfc311998fdee77fb31ffa723ef3c95a75794,International Joint Conference on Artificial Intelligence,2023.0,58.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2109069689', 'name': 'Zenan Xu'}, {'authorId': '1840054391', 'name': 'Xiaojun Meng'}, {'authorId': '2136912252', 'name': 'Yasheng Wang'}, {'authorId': '2482836', 'name': 'Qinliang Su'}, {'authorId': '2091913140', 'name': 'Zexuan Qiu'}, {'authorId': '2110310493', 'name': 'Xin Jiang'}, {'authorId': '30738758', 'name': 'Qun Liu'}]","['Huawei Technologies (China)', 'Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, China', 'Sun Yat-sen University', 'University of Hong Kong']","['China', 'Hong Kong']",2023-05
2305.04835,Shengnan An,"Shengnan An, Zeqi Lin, Qiang Fu, Bei Chen, Nanning Zheng, Jian-Guang
  Lou and Dongmei Zhang",How Do In-Context Examples Affect Compositional Generalization?,"ACL 2023 main conference, long paper",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Compositional generalization--understanding unseen combinations of seen primitives--is an essential reasoning capability in human intelligence. The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning--the prevailing few-shot paradigm based on large language models--exhibits compositional generalization. In this paper, we present CoFe, a test suite to investigate in-context compositional generalization. We find that the compositional generalization performance can be easily affected by the selection of in-context examples, thus raising the research question what the key factors are to make good in-context examples for compositional generalization. We study three potential factors: similarity, diversity and complexity. Our systematic experiments indicate that in-context examples should be structurally similar to the test case, diverse from each other, and individually simple. Furthermore, two strong limitations are observed: in-context compositional generalization on fictional words is much weaker than that on commonly used ones; it is still critical that the in-context examples should cover required linguistic structures, even though the backbone model has been pre-trained on large corpus. We hope our analysis would facilitate the understanding and utilization of in-context learning paradigm. ","[{'version': 'v1', 'created': 'Mon, 8 May 2023 16:32:18 GMT'}, {'version': 'v2', 'created': 'Thu, 25 May 2023 02:34:40 GMT'}, {'version': 'v3', 'created': 'Fri, 9 Jun 2023 02:25:29 GMT'}]",2023-06-12,"[['An', 'Shengnan', ''], ['Lin', 'Zeqi', ''], ['Fu', 'Qiang', ''], ['Chen', 'Bei', ''], ['Zheng', 'Nanning', ''], ['Lou', 'Jian-Guang', ''], ['Zhang', 'Dongmei', '']]",0,0,2023-05-08,3,7,2,0,0,0,0ae12d63f77f40b430f17c791a5191ff5fee5086,258558112.0,https://www.semanticscholar.org/paper/0ae12d63f77f40b430f17c791a5191ff5fee5086,Annual Meeting of the Association for Computational Linguistics,2023.0,55.0,7.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2119217081', 'name': 'Shengnan An'}, {'authorId': '2284174', 'name': 'Zeqi Lin'}, {'authorId': '2113771309', 'name': 'Qiang Fu'}, {'authorId': '143876723', 'name': 'B. Chen'}, {'authorId': '2144620206', 'name': 'Nanning Zheng'}, {'authorId': '153249455', 'name': 'Jian-Guang Lou'}, {'authorId': '46334641', 'name': 'D. Zhang'}]","[""Xi'an Jiaotong University"", 'Microsoft']","['China', 'United States']",2023-05
2305.05003,Somin Wadhwa,Somin Wadhwa and Silvio Amir and Byron C. Wallace,Revisiting Relation Extraction in the era of Large Language Models,Accepted to ACL 2023,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Relation extraction (RE) is the core NLP task of inferring semantic relationships between entities from text. Standard supervised RE techniques entail training modules to tag tokens comprising entity spans and then predict the relationship between them. Recent work has instead treated the problem as a \emph{sequence-to-sequence} task, linearizing relations between entities as target strings to be generated conditioned on the input. Here we push the limits of this approach, using larger language models (GPT-3 and Flan-T5 large) than considered in prior work and evaluating their performance on standard RE tasks under varying levels of supervision. We address issues inherent to evaluating generative approaches to RE by doing human evaluations, in lieu of relying on exact matching. Under this refined evaluation, we find that: (1) Few-shot prompting with GPT-3 achieves near SOTA performance, i.e., roughly equivalent to existing fully supervised models; (2) Flan-T5 is not as capable in the few-shot setting, but supervising and fine-tuning it with Chain-of-Thought (CoT) style explanations (generated via GPT-3) yields SOTA results. We release this model as a new baseline for RE tasks. ","[{'version': 'v1', 'created': 'Mon, 8 May 2023 19:19:07 GMT'}]",2023-05-10,"[['Wadhwa', 'Somin', ''], ['Amir', 'Silvio', ''], ['Wallace', 'Byron C.', '']]",0,1,2023-05-08,1,3,1,4,2,2,97782a67971c4ff1a74bf07e82fe20b2c4bf86c4,258564662.0,https://www.semanticscholar.org/paper/97782a67971c4ff1a74bf07e82fe20b2c4bf86c4,Annual Meeting of the Association for Computational Linguistics,2023.0,41.0,11.0,1.0,True,"['Computer Science', 'Medicine']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Medicine', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '35918090', 'name': 'Somin Wadhwa'}, {'authorId': '1840645', 'name': 'Silvio Amir'}, {'authorId': '1912476', 'name': 'Byron C. Wallace'}]",['Northeastern University'],['China'],2023-05
2305.05138,Wei Qin,"Wei Qin, Zetong Chen, Lei Wang, Yunshi Lan, Weijieying Ren and Richang
  Hong","Read, Diagnose and Chat: Towards Explainable and Interactive LLMs-Augmented Depression Detection in Social Media","8 pages, 5 figures",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  This paper proposes a new depression detection system based on LLMs that is both interpretable and interactive. It not only provides a diagnosis, but also diagnostic evidence and personalized recommendations based on natural language dialogue with the user. We address challenges such as the processing of large amounts of text and integrate professional diagnostic criteria. Our system outperforms traditional methods across various settings and is demonstrated through case studies. ","[{'version': 'v1', 'created': 'Tue, 9 May 2023 02:49:09 GMT'}]",2023-05-10,"[['Qin', 'Wei', ''], ['Chen', 'Zetong', ''], ['Wang', 'Lei', ''], ['Lan', 'Yunshi', ''], ['Ren', 'Weijieying', ''], ['Hong', 'Richang', '']]",0,0,2023-05-09,1,6,1,0,0,0,8efbd687804a13762f135db30b6077a1b171ae01,258564745.0,https://www.semanticscholar.org/paper/8efbd687804a13762f135db30b6077a1b171ae01,arXiv.org,2023.0,48.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1720767577', 'name': 'Wei Qin'}, {'authorId': '2216629182', 'name': 'Zetong Chen'}, {'authorId': '145131956', 'name': 'Lei Wang'}, {'authorId': '3458560', 'name': 'Yunshi Lan'}, {'authorId': '2053308288', 'name': 'Wei Ren'}, {'authorId': '2075339632', 'name': 'Richang Hong'}]","['Singapore Management University', 'Pennsylvania State University', 'Hefei University of Technology', 'University of Science and Technology of China', 'East China Normal University']","['China', 'United States', 'Singapore']",2023-05
2305.05189,Shanshan Zhong,"Shanshan Zhong, Zhongzhan Huang, Wushao Wen, Jinghui Qin, Liang Lin",SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models,accepted by ACM MM 2023,,,,cs.CL cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Diffusion models, which have emerged to become popular text-to-image generation models, can produce high-quality and content-rich images guided by textual prompts. However, there are limitations to semantic understanding and commonsense reasoning in existing models when the input prompts are concise narrative, resulting in low-quality image generation. To improve the capacities for narrative prompts, we propose a simple-yet-effective parameter-efficient fine-tuning approach called the Semantic Understanding and Reasoning adapter (SUR-adapter) for pre-trained diffusion models. To reach this goal, we first collect and annotate a new dataset SURD which consists of more than 57,000 semantically corrected multi-modal samples. Each sample contains a simple narrative prompt, a complex keyword-based prompt, and a high-quality image. Then, we align the semantic representation of narrative prompts to the complex prompts and transfer knowledge of large language models (LLMs) to our SUR-adapter via knowledge distillation so that it can acquire the powerful semantic understanding and reasoning capabilities to build a high-quality textual semantic representation for text-to-image generation. We conduct experiments by integrating multiple LLMs and popular pre-trained diffusion models to show the effectiveness of our approach in enabling diffusion models to understand and reason concise natural language without image quality degradation. Our approach can make text-to-image diffusion models easier to use with better user experience, which demonstrates our approach has the potential for further advancing the development of user-friendly text-to-image generation models by bridging the semantic gap between simple narrative prompts and complex keyword-based prompts. The code is released at https://github.com/Qrange-group/SUR-adapter. ","[{'version': 'v1', 'created': 'Tue, 9 May 2023 05:48:38 GMT'}, {'version': 'v2', 'created': 'Fri, 12 May 2023 10:24:15 GMT'}, {'version': 'v3', 'created': 'Fri, 18 Aug 2023 09:13:46 GMT'}]",2023-08-21,"[['Zhong', 'Shanshan', ''], ['Huang', 'Zhongzhan', ''], ['Wen', 'Wushao', ''], ['Qin', 'Jinghui', ''], ['Lin', 'Liang', '']]",0,0,2023-05-09,3,5,2,0,0,0,3b508a48a4b48d2a16dd790a2a04ffcf51c0b4a6,258564640.0,https://www.semanticscholar.org/paper/3b508a48a4b48d2a16dd790a2a04ffcf51c0b4a6,ACM Multimedia,2023.0,59.0,7.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2053866242', 'name': 'Shan Zhong'}, {'authorId': '2109670338', 'name': 'Zhongzhan Huang'}, {'authorId': '1726975', 'name': 'Wushao Wen'}, {'authorId': '9102638', 'name': 'Jinghui Qin'}, {'authorId': '2181395240', 'name': 'Liang Lin'}]",['Sun Yat-sen University'],['China'],2023-05
2305.05252,Siyu Yuan,"Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles
  Robert Jankowski, Yanghua Xiao, Deqing Yang",Distilling Script Knowledge from Large Language Models for Constrained Language Planning,Accepted to ACL 2023,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., ""make a cake""), but leaves more specific goals with multi-facet constraints understudied (e.g., ""make a cake for diabetics""). In this paper, we define the task of constrained language planning for the first time. We propose an overgenerate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, CoScript, which consists of 55,000 scripts. Empirical results demonstrate that our method significantly improves the constrained language planning ability of LLMs, especially on constraint faithfulness. Furthermore, CoScript is demonstrated to be quite effective in endowing smaller LMs with constrained language planning ability. ","[{'version': 'v1', 'created': 'Tue, 9 May 2023 08:19:32 GMT'}, {'version': 'v2', 'created': 'Wed, 10 May 2023 08:52:54 GMT'}, {'version': 'v3', 'created': 'Thu, 18 May 2023 09:00:29 GMT'}, {'version': 'v4', 'created': 'Mon, 22 May 2023 07:47:33 GMT'}, {'version': 'v5', 'created': 'Fri, 26 May 2023 06:17:17 GMT'}]",2023-05-29,"[['Yuan', 'Siyu', ''], ['Chen', 'Jiangjie', ''], ['Fu', 'Ziquan', ''], ['Ge', 'Xuyang', ''], ['Shah', 'Soham', ''], ['Jankowski', 'Charles Robert', ''], ['Xiao', 'Yanghua', ''], ['Yang', 'Deqing', '']]",0,0,2023-05-09,5,8,2,0,0,0,f06c38a0fd49dd1468e72696913169c0d5588fc3,258564677.0,https://www.semanticscholar.org/paper/f06c38a0fd49dd1468e72696913169c0d5588fc3,Annual Meeting of the Association for Computational Linguistics,2023.0,89.0,9.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2145968425', 'name': 'Siyu Yuan'}, {'authorId': '5040052', 'name': 'Jiangjie Chen'}, {'authorId': '2165331745', 'name': 'Ziquan Fu'}, {'authorId': '144322422', 'name': 'Xuyang Ge'}, {'authorId': '2112624137', 'name': 'Soham Shah'}, {'authorId': '3335921', 'name': 'C. R. Jankowski'}, {'authorId': '1944126000', 'name': 'Deqing Yang'}, {'authorId': '2116642640', 'name': 'Yanghua Xiao'}]",['Fudan University'],['China'],2023-05
2305.05383,Shuai Lu,"Chenxiao Liu, Shuai Lu, Weizhu Chen, Daxin Jiang, Alexey Svyatkovskiy,
  Shengyu Fu, Neel Sundaresan and Nan Duan",Code Execution with Pre-trained Language Models,Accepted to the Findings of ACL 2023,,,,cs.PL cs.AI cs.CL cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Code execution is a fundamental aspect of programming language semantics that reflects the exact behavior of the code. However, most pre-trained models for code intelligence ignore the execution trace and only rely on source code and syntactic structures. In this paper, we investigate how well pre-trained models can understand and perform code execution. We develop a mutation-based data augmentation technique to create a large-scale and realistic Python dataset and task for code execution, which challenges existing models such as Codex. We then present CodeExecutor, a Transformer model that leverages code execution pre-training and curriculum learning to enhance its semantic comprehension. We evaluate CodeExecutor on code execution and show its promising performance and limitations. We also demonstrate its potential benefits for code intelligence tasks such as zero-shot code-to-code search and text-to-code generation. Our analysis provides insights into the learning and generalization abilities of pre-trained models for code execution. ","[{'version': 'v1', 'created': 'Mon, 8 May 2023 10:00:05 GMT'}]",2023-05-10,"[['Liu', 'Chenxiao', ''], ['Lu', 'Shuai', ''], ['Chen', 'Weizhu', ''], ['Jiang', 'Daxin', ''], ['Svyatkovskiy', 'Alexey', ''], ['Fu', 'Shengyu', ''], ['Sundaresan', 'Neel', ''], ['Duan', 'Nan', '']]",0,0,2023-05-08,1,8,4,1,0,1,785bb49af762efd64d841e52aa82c708341a7c43,258564296.0,https://www.semanticscholar.org/paper/785bb49af762efd64d841e52aa82c708341a7c43,Annual Meeting of the Association for Computational Linguistics,2023.0,48.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2128114359', 'name': 'Chenxiao Liu'}, {'authorId': '2115338656', 'name': 'Shuai Lu'}, {'authorId': '2109136147', 'name': 'Weizhu Chen'}, {'authorId': '71790825', 'name': 'Daxin Jiang'}, {'authorId': '2061625488', 'name': 'Alexey Svyatkovskiy'}, {'authorId': '2072784644', 'name': 'Shengyu Fu'}, {'authorId': '145507437', 'name': 'Neel Sundaresan'}, {'authorId': '46429989', 'name': 'Nan Duan'}]",['Peking University'],['China'],2023-05
2305.05464,Nisha Huang,"Nisha Huang, Yuxin Zhang, Weiming Dong",Style-A-Video: Agile Diffusion for Arbitrary Text-based Video Style Transfer,,,,,cs.CV cs.MM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large-scale text-to-video diffusion models have demonstrated an exceptional ability to synthesize diverse videos. However, due to the lack of extensive text-to-video datasets and the necessary computational resources for training, directly applying these models for video stylization remains difficult. Also, given that the noise addition process on the input content is random and destructive, fulfilling the style transfer task's content preservation criteria is challenging. This paper proposes a zero-shot video stylization method named Style-A-Video, which utilizes a generative pre-trained transformer with an image latent diffusion model to achieve a concise text-controlled video stylization. We improve the guidance condition in the denoising process, establishing a balance between artistic expression and structure preservation. Furthermore, to decrease inter-frame flicker and avoid the formation of additional artifacts, we employ a sampling optimization and a temporal consistency module. Extensive experiments show that we can attain superior content preservation and stylistic performance while incurring less consumption than previous solutions. Code will be available at https://github.com/haha-lisa/Style-A-Video. ","[{'version': 'v1', 'created': 'Tue, 9 May 2023 14:03:27 GMT'}]",2023-05-10,"[['Huang', 'Nisha', ''], ['Zhang', 'Yuxin', ''], ['Dong', 'Weiming', '']]",0,1,2023-05-09,1,3,2,0,0,0,8482fb463dfecb4c24b1af7b7f47307e726bcc05,258564566.0,https://www.semanticscholar.org/paper/8482fb463dfecb4c24b1af7b7f47307e726bcc05,arXiv.org,2023.0,54.0,1.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2186281333', 'name': 'Nisha Huang'}, {'authorId': '2108078624', 'name': 'Yu-xin Zhang'}, {'authorId': '40441149', 'name': 'Weiming Dong'}]","['University of Chinese Academy of Sciences', 'Chinese Academy of Sciences', 'National Higher School of Artificial Intelligence']","['China', 'Algeria']",2023-05
2305.05711,Peng Li,"Peng Li, Tianxiang Sun, Qiong Tang, Hang Yan, Yuanbin Wu, Xuanjing
  Huang, Xipeng Qiu",CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors,"Accepted to ACL 2023 (main conference). Code and data are publicly
  available at https://github.com/dasepli/CodeIE",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning ability on many NLP tasks. A common practice is to recast the task into a text-to-text format such that generative LLMs of natural language (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is nontrivial to perform information extraction (IE) tasks with NL-LLMs since the output of the IE task is usually structured and therefore is hard to be converted into plain text. In this paper, we propose to recast the structured output in the form of code instead of natural language and utilize generative LLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular, named entity recognition and relation extraction. In contrast to NL-LLMs, we show that Code-LLMs can be well-aligned with these IE tasks by designing code-style prompts and formulating these IE tasks as code generation tasks. Experiment results on seven benchmarks show that our method consistently outperforms fine-tuning moderate-size pre-trained models specially designed for IE tasks (e.g., UIE) and prompting NL-LLMs under few-shot settings. We further conduct a series of in-depth analyses to demonstrate the merits of leveraging Code-LLMs for IE tasks. ","[{'version': 'v1', 'created': 'Tue, 9 May 2023 18:40:31 GMT'}, {'version': 'v2', 'created': 'Thu, 11 May 2023 01:27:43 GMT'}]",2023-05-12,"[['Li', 'Peng', ''], ['Sun', 'Tianxiang', ''], ['Tang', 'Qiong', ''], ['Yan', 'Hang', ''], ['Wu', 'Yuanbin', ''], ['Huang', 'Xuanjing', ''], ['Qiu', 'Xipeng', '']]",0,1,2023-05-09,2,7,2,2,0,2,243ac5656c4f8ed6e1eb757b7145fb12b837c166,258588144.0,https://www.semanticscholar.org/paper/243ac5656c4f8ed6e1eb757b7145fb12b837c166,Annual Meeting of the Association for Computational Linguistics,2023.0,44.0,14.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '24899768', 'name': 'Peng-Hsuan Li'}, {'authorId': '153345698', 'name': 'Tianxiang Sun'}, {'authorId': '2216693197', 'name': 'Qiong Tang'}, {'authorId': '146948229', 'name': 'Hang Yan'}, {'authorId': '3174675', 'name': 'Yuanbin Wu'}, {'authorId': '1790227', 'name': 'Xuanjing Huang'}, {'authorId': '2216718376', 'name': 'Xipeng Qiu Academy for EngineeringTechnology'}, {'authorId': '145417921', 'name': 'Fudan University'}, {'authorId': '102700312', 'name': 'School of Materials Science'}, {'authorId': '2069450537', 'name': 'Technology'}, {'authorId': '102629714', 'name': 'East China Normal University'}]","['East China Normal University', 'Fudan University']",['China'],2023-05
2305.05994,Siyu Yuan,"Siyu Yuan, Jiangjie Chen, Changzhi Sun, Jiaqing Liang, Yanghua Xiao,
  Deqing Yang",ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Analogical reasoning is a fundamental cognitive ability of humans. However, current language models (LMs) still struggle to achieve human-like performance in analogical reasoning tasks due to a lack of resources for model training. In this work, we address this gap by proposing ANALOGYKB, a million-scale analogy knowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB identifies two types of analogies from the KGs: 1) analogies of the same relations, which can be directly extracted from the KGs, and 2) analogies of analogous relations, which are identified with a selection and filtering pipeline enabled by large LMs (InstructGPT), followed by minor human efforts for data quality control. Evaluations on a series of datasets of two analogical reasoning tasks (analogy recognition and generation) demonstrate that ANALOGYKB successfully enables LMs to achieve much better results than previous state-of-the-art methods. ","[{'version': 'v1', 'created': 'Wed, 10 May 2023 09:03:01 GMT'}]",2023-05-11,"[['Yuan', 'Siyu', ''], ['Chen', 'Jiangjie', ''], ['Sun', 'Changzhi', ''], ['Liang', 'Jiaqing', ''], ['Xiao', 'Yanghua', ''], ['Yang', 'Deqing', '']]",0,1,2023-05-10,1,6,2,1,0,1,bad287184c6739fd6f476f89cb83e09415982d9f,258587827.0,https://www.semanticscholar.org/paper/bad287184c6739fd6f476f89cb83e09415982d9f,arXiv.org,2023.0,68.0,5.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2145968425', 'name': 'Siyu Yuan'}, {'authorId': '5040052', 'name': 'Jiangjie Chen'}, {'authorId': '2118133838', 'name': 'Changzhi Sun'}, {'authorId': '3366523', 'name': 'Jiaqing Liang'}, {'authorId': '2116642640', 'name': 'Yanghua Xiao'}, {'authorId': '1944126000', 'name': 'Deqing Yang'}]",['Fudan University'],['China'],2023-05
2305.06212,Yansong Li,"Yansong Li, Zhixing Tan and Yang Liu",Privacy-Preserving Prompt Tuning for Large Language Model Services,,,,,cs.CL cs.CR,http://creativecommons.org/licenses/by-sa/4.0/,"  Prompt tuning provides an efficient way for users to customize Large Language Models (LLMs) with their private data in the emerging LLM service scenario. However, the sensitive nature of private data brings the need for privacy preservation in LLM service customization. Based on prompt tuning, we propose Privacy-Preserving Prompt Tuning (RAPT), a framework that provides privacy guarantees for LLM services. \textsc{rapt} adopts a local privacy setting, allowing users to privatize their data locally with local differential privacy. As prompt tuning performs poorly when directly trained on privatized data, we introduce a novel privatized token reconstruction task that is trained jointly with the downstream task, allowing LLMs to learn better task-dependent representations. Despite the simplicity of our framework, experiments show that RAPT achieves competitive performance across tasks while providing privacy guarantees against adversaries. ","[{'version': 'v1', 'created': 'Wed, 10 May 2023 14:41:51 GMT'}]",2023-05-11,"[['Li', 'Yansong', ''], ['Tan', 'Zhixing', ''], ['Liu', 'Yang', '']]",0,0,2023-05-10,1,3,2,0,0,0,4c4722d3767dae6bc00b7de3e3fa160caaffe483,258588141.0,https://www.semanticscholar.org/paper/4c4722d3767dae6bc00b7de3e3fa160caaffe483,arXiv.org,2023.0,33.0,8.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2216706435', 'name': 'Yansong Li'}, {'authorId': '3468510', 'name': 'Zhixing Tan'}, {'authorId': '2152798523', 'name': 'Yang Liu'}]","['Tsinghua University', 'University of Ottawa', 'Zhongguancun Laboratory, Beijing, China']","['China', 'Canada']",2023-05
2305.06355,Yi Wang,"KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali
  Wang, Limin Wang, Yu Qiao",VideoChat: Chat-Centric Video Understanding,Technical report,,,,cs.CV cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  In this study, we initiate an exploration into video understanding by introducing VideoChat, an end-to-end chat-centric video understanding system. It integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference. To instructively tune this system, we propose a video-centric instruction dataset, composed of thousands of videos matched with detailed descriptions and conversations. This dataset emphasizes spatiotemporal reasoning and causal relationships, providing a valuable asset for training chat-centric video understanding systems. Preliminary qualitative experiments reveal our system's potential across a broad spectrum of video applications and set the standard for future research. Access our code and data at https://github.com/OpenGVLab/Ask-Anything ","[{'version': 'v1', 'created': 'Wed, 10 May 2023 17:59:04 GMT'}]",2023-05-11,"[['Li', 'KunChang', ''], ['He', 'Yinan', ''], ['Wang', 'Yi', ''], ['Li', 'Yizhuo', ''], ['Wang', 'Wenhai', ''], ['Luo', 'Ping', ''], ['Wang', 'Yali', ''], ['Wang', 'Limin', ''], ['Qiao', 'Yu', '']]",0,0,2023-05-10,1,9,2,0,0,0,d48cb91b9e555194f7494c4d4bb9815021d3ee45,258588306.0,https://www.semanticscholar.org/paper/d48cb91b9e555194f7494c4d4bb9815021d3ee45,arXiv.org,2023.0,59.0,57.0,10.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2115468491', 'name': 'Kunchang Li'}, {'authorId': '2118918324', 'name': 'Yinan He'}, {'authorId': '46393411', 'name': 'Yi Wang'}, {'authorId': '2154568284', 'name': 'Yizhuo Li'}, {'authorId': '47825073', 'name': 'Wen Wang'}, {'authorId': '2143481782', 'name': 'Ping Luo'}, {'authorId': '47903936', 'name': 'Yali Wang'}, {'authorId': '2141353278', 'name': 'Limin Wang'}, {'authorId': '2093202334', 'name': 'Yu Qiao'}]","['OpenGVLab, Shanghai AI Laboratory', 'Nanjing University', 'Chinese Academy of Sciences', 'University of Hong Kong']","['China', 'Hong Kong']",2023-05
2305.06472,Huan Wang,"Yan-Fu Li, Huan Wang, Muxia Sun",ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps,"55 pages, 10 figures",,,,cs.LG cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Prognostics and health management (PHM) technology plays a critical role in industrial production and equipment maintenance by identifying and predicting possible equipment failures and damages, thereby allowing necessary maintenance measures to be taken to enhance equipment service life and reliability while reducing production costs and downtime. In recent years, PHM technology based on artificial intelligence (AI) has made remarkable achievements in the context of the industrial IoT and big data, and it is widely used in various industries, such as railway, energy, and aviation, for condition monitoring, fault prediction, and health management. The emergence of large-scale foundation models (LSF-Models) such as ChatGPT and DALLE-E marks the entry of AI into a new era of AI-2.0 from AI-1.0, where deep models have rapidly evolved from a research paradigm of single-modal, single-task, and limited-data to a multi-modal, multi-task, massive data, and super-large model paradigm. ChatGPT represents a landmark achievement in this research paradigm, offering hope for general artificial intelligence due to its highly intelligent natural language understanding ability. However, the PHM field lacks a consensus on how to respond to this significant change in the AI field, and a systematic review and roadmap is required to elucidate future development directions. To fill this gap, this paper systematically expounds on the key components and latest developments of LSF-Models. Then, we systematically answered how to build the LSF-Model applicable to PHM tasks and outlined the challenges and future development roadmaps for this research paradigm. ","[{'version': 'v1', 'created': 'Wed, 10 May 2023 21:37:44 GMT'}, {'version': 'v2', 'created': 'Fri, 12 May 2023 10:41:35 GMT'}]",2023-05-15,"[['Li', 'Yan-Fu', ''], ['Wang', 'Huan', ''], ['Sun', 'Muxia', '']]",1,1,2023-05-10,2,3,3,1,0,1,ae1db9bf0930cc8506c2c74d3dfc72385675fa4e,258615178.0,https://www.semanticscholar.org/paper/ae1db9bf0930cc8506c2c74d3dfc72385675fa4e,arXiv.org,2023.0,298.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2110499851', 'name': 'Yanfang Li'}, {'authorId': '2155242840', 'name': 'Huan Wang'}, {'authorId': '97800217', 'name': 'Muxia Sun'}]",['Tsinghua University'],['China'],2023-05
2305.06575,Hongyuan Lu,"Hongyuan Lu, Haoyang Huang, Dongdong Zhang, Haoran Yang, Wai Lam, Furu
  Wei",Chain-of-Dictionary Prompting Elicits Translation in Large Language Models,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) have shown surprisingly good performance in multilingual neural machine translation (MNMT) even when trained without parallel data. Yet, despite the fact that the amount of training data is gigantic, they still struggle with translating rare words, particularly for low-resource languages. Even worse, it is usually unrealistic to retrieve relevant demonstrations for in-context learning with low-resource languages on LLMs, which restricts the practical use of LLMs for translation -- how should we mitigate this problem? To this end, we present a novel method, CoD, which augments LLMs with prior knowledge with the chains of multilingual dictionaries for a subset of input words to elicit translation abilities for LLMs. Extensive experiments indicate that augmenting ChatGPT with CoD elicits large gains by up to 13x chrF++ points for MNMT (3.08 to 42.63 for English to Serbian written in Cyrillic script) on FLORES-200 full devtest set. We further demonstrate the importance of chaining the multilingual dictionaries, as well as the superiority of CoD to few-shot demonstration for low-resource languages. ","[{'version': 'v1', 'created': 'Thu, 11 May 2023 05:19:47 GMT'}, {'version': 'v2', 'created': 'Wed, 17 May 2023 05:44:10 GMT'}, {'version': 'v3', 'created': 'Wed, 24 May 2023 05:01:52 GMT'}]",2023-05-25,"[['Lu', 'Hongyuan', ''], ['Huang', 'Haoyang', ''], ['Zhang', 'Dongdong', ''], ['Yang', 'Haoran', ''], ['Lam', 'Wai', ''], ['Wei', 'Furu', '']]",1,1,2023-05-11,3,6,1,1,0,1,97992c13baa6185c03d9e672f53185bc59822596,258615369.0,https://www.semanticscholar.org/paper/97992c13baa6185c03d9e672f53185bc59822596,arXiv.org,2023.0,33.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '2156273800', 'name': 'Hongyuan Lu'}, {'authorId': '15086992', 'name': 'Haoyang Huang'}, {'authorId': '40232931', 'name': 'Dongdong Zhang'}, {'authorId': '2115538342', 'name': 'Haoran Yang'}, {'authorId': '144594306', 'name': 'Wai Lam'}, {'authorId': '49807919', 'name': 'Furu Wei'}]","['Chinese University of Hong Kong', 'Microsoft']","['China', 'United States']",2023-05
2305.06599,Jia Li,"Jia Li, Ge Li, Yongmin Li, Zhi Jin",Structured Chain-of-Thought Prompting for Code Generation,arXiv admin note: text overlap with arXiv:2303.17780,,,,cs.SE cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) (e.g., ChatGPT) have shown impressive performance in code generation. LLMs take prompts as inputs, and Chain-of-Thought (CoT) prompting is the state-of-the-art prompting technique. CoT prompting asks LLMs first to generate CoTs (i.e., intermediate natural language reasoning steps) and then output the code. However, CoT prompting is designed for natural language generation and has low accuracy in code generation.   In this paper, we propose Structured CoTs (SCoTs) and present a novel prompting technique for code generation, named SCoT prompting. Our motivation is source code contains rich structural information and any code can be composed of three program structures (i.e., sequence, branch, and loop structures). Intuitively, structured intermediate reasoning steps make for structured source code. Thus, we ask LLMs to use program structures to build CoTs, obtaining SCoTs. Then, LLMs generate the final code based on SCoTs. Compared to CoT prompting, SCoT prompting explicitly constrains LLMs to think about how to solve requirements from the view of source code and further the performance of LLMs in code generation. We apply SCoT prompting to two LLMs (i.e., ChatGPT and Codex) and evaluate it on three benchmarks (i.e., HumanEval, MBPP, and MBCPP). (1) SCoT prompting outperforms the state-of-the-art baseline - CoT prompting by up to 13.79% in Pass@1. (2) Human evaluation shows human developers prefer programs from SCoT prompting. (3) SCoT prompting is robust to examples and achieves substantial improvements. ","[{'version': 'v1', 'created': 'Thu, 11 May 2023 06:43:37 GMT'}, {'version': 'v2', 'created': 'Fri, 11 Aug 2023 08:18:50 GMT'}, {'version': 'v3', 'created': 'Thu, 7 Sep 2023 11:39:07 GMT'}]",2023-09-08,"[['Li', 'Jia', ''], ['Li', 'Ge', ''], ['Li', 'Yongmin', ''], ['Jin', 'Zhi', '']]",1,1,2023-05-11,3,4,2,2,0,2,94beb9f249d6d2f1c00d8edfa2db861633aee6f9,258615421.0,https://www.semanticscholar.org/paper/94beb9f249d6d2f1c00d8edfa2db861633aee6f9,,2023.0,42.0,4.0,1.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2118373749', 'name': 'Jia Li'}, {'authorId': '2154591375', 'name': 'Ge Li'}, {'authorId': '2135349350', 'name': 'Yongming Li'}, {'authorId': '2152843753', 'name': 'Zhi Jin'}]",['Peking University'],['China'],2023-05
2305.06849,Yujia Qin,"Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu,
  Yankai Lin, Xu Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan
  Liu, Maosong Sun, and Jie Zhou",WebCPM: Interactive Web Search for Chinese Long-form Question Answering,"ACL 2023, main conference",,,,cs.CL cs.AI cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Long-form question answering (LFQA) aims at answering complex, open-ended questions with detailed, paragraph-length responses. The de facto paradigm of LFQA necessitates two procedures: information retrieval, which searches for relevant supporting facts, and information synthesis, which integrates these facts into a coherent answer. In this paper, we introduce WebCPM, the first Chinese LFQA dataset. One unique feature of WebCPM is that its information retrieval is based on interactive web search, which engages with a search engine in real time. Following WebGPT, we develop a web search interface. We recruit annotators to search for relevant information using our interface and then answer questions. Meanwhile, the web search behaviors of our annotators would be recorded. In total, we collect 5,500 high-quality question-answer pairs, together with 14,315 supporting facts and 121,330 web search actions. We fine-tune pre-trained language models to imitate human behaviors for web search and to generate answers based on the collected facts. Our LFQA pipeline, built on these fine-tuned models, generates answers that are no worse than human-written ones in 32.5% and 47.5% of the cases on our dataset and DuReader, respectively. ","[{'version': 'v1', 'created': 'Thu, 11 May 2023 14:47:29 GMT'}, {'version': 'v2', 'created': 'Tue, 23 May 2023 13:15:10 GMT'}]",2023-05-24,"[['Qin', 'Yujia', ''], ['Cai', 'Zihan', ''], ['Jin', 'Dian', ''], ['Yan', 'Lan', ''], ['Liang', 'Shihao', ''], ['Zhu', 'Kunlun', ''], ['Lin', 'Yankai', ''], ['Han', 'Xu', ''], ['Ding', 'Ning', ''], ['Wang', 'Huadong', ''], ['Xie', 'Ruobing', ''], ['Qi', 'Fanchao', ''], ['Liu', 'Zhiyuan', ''], ['Sun', 'Maosong', ''], ['Zhou', 'Jie', '']]",0,1,2023-05-11,2,15,3,1,0,1,b5982d5ef4eed477bb3dacfe43c5c5b5fe173694,258615343.0,https://www.semanticscholar.org/paper/b5982d5ef4eed477bb3dacfe43c5c5b5fe173694,Annual Meeting of the Association for Computational Linguistics,2023.0,38.0,14.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '50625437', 'name': 'Yujia Qin'}, {'authorId': '2113441349', 'name': 'Zihan Cai'}, {'authorId': '1860892', 'name': 'Di Jin'}, {'authorId': '2214613855', 'name': 'Lan Yan'}, {'authorId': '2163374235', 'name': 'Shi Liang'}, {'authorId': '2214586034', 'name': 'Kunlun Zhu'}, {'authorId': '2149202150', 'name': 'Yankai Lin'}, {'authorId': '48506411', 'name': 'Xu Han'}, {'authorId': '46649145', 'name': 'Ning Ding'}, {'authorId': '2155242767', 'name': 'Huadong Wang'}, {'authorId': '3360722', 'name': 'Ruobing Xie'}, {'authorId': '51466208', 'name': 'Fanchao Qi'}, {'authorId': '2141313179', 'name': 'Zhiyuan Liu'}, {'authorId': '1753344', 'name': 'Maosong Sun'}, {'authorId': '49640256', 'name': 'Jie Zhou'}]","['Tencent', 'Tsinghua University', 'ModelBest Inc.', 'Renmin University of China']",['China'],2023-05
2305.07001,Junjie Zhang,"Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin,
  Ji-Rong Wen",Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach,,,,,cs.IR cs.CL,http://creativecommons.org/licenses/by/4.0/,"  In the past decades, recommender systems have attracted much attention in both research and industry communities, and a large number of studies have been devoted to developing effective recommendation models. Basically speaking, these models mainly learn the underlying user preference from historical behavior data, and then estimate the user-item matching relationships for recommendations. Inspired by the recent progress on large language models (LLMs), we take a different approach to developing the recommendation models, considering recommendation as instruction following by LLMs. The key idea is that the preferences or needs of a user can be expressed in natural language descriptions (called instructions), so that LLMs can understand and further execute the instruction for fulfilling the recommendation task. Instead of using public APIs of LLMs, we instruction tune an open-source LLM (3B Flan-T5-XL), in order to better adapt LLMs to recommender systems. For this purpose, we first design a general instruction format for describing the preference, intention, task form and context of a user in natural language. Then we manually design 39 instruction templates and automatically generate a large amount of user-personalized instruction data (252K instructions) with varying types of preferences and intentions. To demonstrate the effectiveness of our approach, we instantiate the instruction templates into several widely-studied recommendation (or search) tasks, and conduct extensive experiments on these tasks with real-world datasets. Experiment results show that the proposed approach can outperform several competitive baselines, including the powerful GPT-3.5, on these evaluation tasks. Our approach sheds light on developing more user-friendly recommender systems, in which users can freely communicate with the system and obtain more accurate recommendations via natural language instructions. ","[{'version': 'v1', 'created': 'Thu, 11 May 2023 17:39:07 GMT'}]",2023-05-12,"[['Zhang', 'Junjie', ''], ['Xie', 'Ruobing', ''], ['Hou', 'Yupeng', ''], ['Zhao', 'Wayne Xin', ''], ['Lin', 'Leyu', ''], ['Wen', 'Ji-Rong', '']]",0,1,2023-05-11,1,6,2,4,2,2,0383e049e98c9eedbc61be728d4ef037300bbedf,258615776.0,https://www.semanticscholar.org/paper/0383e049e98c9eedbc61be728d4ef037300bbedf,arXiv.org,2023.0,44.0,42.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2120518257', 'name': 'Junjie Zhang'}, {'authorId': '3360722', 'name': 'Ruobing Xie'}, {'authorId': '151472453', 'name': 'Yupeng Hou'}, {'authorId': '2542603', 'name': 'Wayne Xin Zhao'}, {'authorId': '4950224', 'name': 'Leyu Lin'}, {'authorId': '153693432', 'name': 'Ji-rong Wen'}]","['Tencent', 'Renmin University of China']",['China'],2023-05
2305.07004,Haoyang Huang,"Haoyang Huang, Tianyi Tang, Dongdong Zhang, Wayne Xin Zhao, Ting Song,
  Yan Xia, Furu Wei",Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) demonstrate impressive multilingual capability, but their performance varies substantially across different languages. In this work, we introduce a simple yet effective method, called cross-lingual-thought prompting (XLT), to systematically improve the multilingual capability of LLMs. Specifically, XLT is a generic template prompt that stimulates cross-lingual and logical reasoning skills to enhance task performance across languages. We conduct comprehensive evaluations on 7 typical benchmarks related to reasoning, understanding, and generation tasks, covering both high-resource and low-resource languages. Experimental results show that XLT not only remarkably enhances the performance of various multilingual tasks but also significantly reduces the gap between the average performance and the best performance of each task in different languages. Notably, XLT brings over 10 points of average improvement in arithmetic reasoning and open-domain question-answering tasks. ","[{'version': 'v1', 'created': 'Thu, 11 May 2023 17:44:17 GMT'}]",2023-05-12,"[['Huang', 'Haoyang', ''], ['Tang', 'Tianyi', ''], ['Zhang', 'Dongdong', ''], ['Zhao', 'Wayne Xin', ''], ['Song', 'Ting', ''], ['Xia', 'Yan', ''], ['Wei', 'Furu', '']]",0,0,2023-05-11,1,7,1,0,0,0,0b29ff236bb8f547d017bf747ad74ad2b8303851,258615377.0,https://www.semanticscholar.org/paper/0b29ff236bb8f547d017bf747ad74ad2b8303851,arXiv.org,2023.0,50.0,18.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '15086992', 'name': 'Haoyang Huang'}, {'authorId': '1997234792', 'name': 'Tianyi Tang'}, {'authorId': '40232931', 'name': 'Dongdong Zhang'}, {'authorId': '2542603', 'name': 'Wayne Xin Zhao'}, {'authorId': '2212903837', 'name': 'Ting Song'}, {'authorId': '2111130689', 'name': 'Yan Xia'}, {'authorId': '49807919', 'name': 'Furu Wei'}]",['Renmin University of China'],['China'],2023-05
2305.07375,Jinglong Gao,"Jinglong Gao, Xiao Ding, Bing Qin, Ting Liu",Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Causal reasoning ability is crucial for numerous NLP applications. Despite the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear how well ChatGPT performs in causal reasoning. In this paper, we conduct the first comprehensive evaluation of the ChatGPT's causal reasoning capabilities. Experiments show that ChatGPT is not a good causal reasoner, but a good causal interpreter. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT's upgrading processes, such as RLHF. The In-Context Learning (ICL) and Chain-of-Though (COT) techniques can further exacerbate such causal hallucination. Additionally, the causal reasoning ability of ChatGPT is sensitive to the words used to express the causal concept in prompts, and close-ended prompts perform better than open-ended prompts. For events in sentences, ChatGPT excels at capturing explicit causality rather than implicit causality, and performs better in sentences with lower event density and smaller lexical distance between events. ","[{'version': 'v1', 'created': 'Fri, 12 May 2023 10:54:13 GMT'}, {'version': 'v2', 'created': 'Mon, 15 May 2023 08:01:10 GMT'}, {'version': 'v3', 'created': 'Thu, 18 May 2023 10:44:26 GMT'}]",2023-05-19,"[['Gao', 'Jinglong', ''], ['Ding', 'Xiao', ''], ['Qin', 'Bing', ''], ['Liu', 'Ting', '']]",1,1,2023-05-12,3,4,2,1,0,1,1b9fc8268b392742ea43c2c017a767cf62386139,258676401.0,https://www.semanticscholar.org/paper/1b9fc8268b392742ea43c2c017a767cf62386139,arXiv.org,2023.0,36.0,10.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2115557647', 'name': 'Jin-Fang Gao'}, {'authorId': '2117434160', 'name': 'Xiao Ding'}, {'authorId': '152277111', 'name': 'Bing Qin'}, {'authorId': '2140034231', 'name': 'Ting Liu'}]",['Harbin Institute of Technology'],['China'],2023-05
2305.07402,Jiazhan Feng,"Jiazhan Feng, Chongyang Tao, Xiubo Geng, Tao Shen, Can Xu, Guodong
  Long, Dongyan Zhao, Daxin Jiang",Knowledge Refinement via Interaction Between Search Engines and Large Language Models,Work in progress. We added BEIR results and released source code,,,,cs.CL cs.IR,http://creativecommons.org/licenses/by/4.0/,"  Information retrieval (IR) plays a crucial role in locating relevant resources from vast amounts of data, and its applications have evolved from traditional knowledge bases to modern search engines (SEs). The emergence of large language models (LLMs) has further revolutionized the IR field by enabling users to interact with search systems in natural language. In this paper, we explore the advantages and disadvantages of LLMs and SEs, highlighting their respective strengths in understanding user-issued queries and retrieving up-to-date information. To leverage the benefits of both paradigms while circumventing their limitations, we propose InteR, a novel framework that facilitates knowledge refinement through interaction between SEs and LLMs. InteR allows SEs to expand knowledge in queries using LLM-generated knowledge collections and enables LLMs to enhance prompt formulation using SE-retrieved documents. This iterative refinement process augments the inputs of SEs and LLMs, leading to more accurate retrieval. Experiments on large-scale retrieval benchmarks involving web search and low-resource retrieval tasks demonstrate that InteR achieves overall superior zero-shot retrieval performance compared to state-of-the-art methods, even those using relevance judgment. Source code is available at https://github.com/Cyril-JZ/InteR ","[{'version': 'v1', 'created': 'Fri, 12 May 2023 11:58:15 GMT'}, {'version': 'v2', 'created': 'Sun, 21 May 2023 14:58:50 GMT'}]",2023-05-23,"[['Feng', 'Jiazhan', ''], ['Tao', 'Chongyang', ''], ['Geng', 'Xiubo', ''], ['Shen', 'Tao', ''], ['Xu', 'Can', ''], ['Long', 'Guodong', ''], ['Zhao', 'Dongyan', ''], ['Jiang', 'Daxin', '']]",0,0,2023-05-12,2,8,2,0,0,0,91f95445eb503b6d710ef4c924e17df70beb19af,258676297.0,https://www.semanticscholar.org/paper/91f95445eb503b6d710ef4c924e17df70beb19af,arXiv.org,2023.0,68.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '147062881', 'name': 'Jiazhan Feng'}, {'authorId': '8801869', 'name': 'Chongyang Tao'}, {'authorId': '2442662', 'name': 'Xiubo Geng'}, {'authorId': '143681703', 'name': 'Tao Shen'}, {'authorId': '46747953', 'name': 'Can Xu'}, {'authorId': '2062835', 'name': 'Guodong Long'}, {'authorId': '144060462', 'name': 'Dongyan Zhao'}, {'authorId': '2086994543', 'name': 'Daxin Jiang'}]","['Peking University', 'Microsoft', 'Finance and Economics Institute of Tajikistan']","['China', 'United States', 'Tajikistan']",2023-05
2305.07609,Jizhi Zhang,"Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, Xiangnan
  He",Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation,Accepted by Recsys 2023 (Short),,,,cs.IR cs.CL cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The remarkable achievements of Large Language Models (LLMs) have led to the emergence of a novel recommendation paradigm -- Recommendation via LLM (RecLLM). Nevertheless, it is important to note that LLMs may contain social prejudices, and therefore, the fairness of recommendations made by RecLLM requires further investigation. To avoid the potential risks of RecLLM, it is imperative to evaluate the fairness of RecLLM with respect to various sensitive attributes on the user side. Due to the differences between the RecLLM paradigm and the traditional recommendation paradigm, it is problematic to directly use the fairness benchmark of traditional recommendation. To address the dilemma, we propose a novel benchmark called Fairness of Recommendation via LLM (FaiRLLM). This benchmark comprises carefully crafted metrics and a dataset that accounts for eight sensitive attributes1 in two recommendation scenarios: music and movies. By utilizing our FaiRLLM benchmark, we conducted an evaluation of ChatGPT and discovered that it still exhibits unfairness to some sensitive attributes when generating recommendations. Our code and dataset can be found at https://github.com/jizhi-zhang/FaiRLLM. ","[{'version': 'v1', 'created': 'Fri, 12 May 2023 16:54:36 GMT'}, {'version': 'v2', 'created': 'Mon, 3 Jul 2023 15:24:39 GMT'}]",2023-07-04,"[['Zhang', 'Jizhi', ''], ['Bao', 'Keqin', ''], ['Zhang', 'Yang', ''], ['Wang', 'Wenjie', ''], ['Feng', 'Fuli', ''], ['He', 'Xiangnan', '']]",1,1,2023-05-12,2,6,3,1,0,1,2e92b3699668f920a8d692535622ebeaa53315e2,258676079.0,https://www.semanticscholar.org/paper/2e92b3699668f920a8d692535622ebeaa53315e2,ACM Conference on Recommender Systems,2023.0,44.0,33.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2116265843', 'name': 'Jizhi Zhang'}, {'authorId': '2188063534', 'name': 'Keqin Bao'}, {'authorId': '2145957648', 'name': 'Yang Zhang'}, {'authorId': '2117833732', 'name': 'Wenjie Wang'}, {'authorId': '2163400298', 'name': 'Fuli Feng'}, {'authorId': '7792071', 'name': 'Xiangnan He'}]","['National University of Singapore', 'University of Science and Technology of China', 'Nanzhong Zhangzhongjing Hospital']","['China', 'Singapore']",2023-05
2305.07667,Iris Berent Dr.,"Iris Berent, Alexzander Sansiveri",Davinci the Dualist: the mind-body divide in large language models and in human learners,,,,,cs.AI cs.HC,http://creativecommons.org/licenses/by/4.0/,"  A large literature suggests that people are intuitive Dualists--they consider the mind ethereal, distinct from the body. Past research also shows that Dualism emerges, in part, via learning (e.g., Barlev & Shtulman, 2021). But whether learning is sufficient to give rise to Dualism is unknown.The evidence from human learners does address this question because humans are endowed not only with general learning capacities but also with core knowledge capacities. And recent results suggest that core knowledge begets Dualism (Berent, Theodore & Valencia, 2021; Berent, 2023). To evaluate the role of learning, here, we probe for a mind-body divide in Davinci--a large language model (LLM) that is devoid of any innate core knowledge. We show that Davinci still leans towards Dualism, and that this bias increases systematically with the learner's inductive potential. Thus, davinci (a GPT-3 model) exhibits mild Dualist tendencies, whereas its descendent, text-davinci-003 (a GPT-3.5 model), shows a full-blown bias. It selectively considers thoughts (epistemic states) as disembodied--as unlikely to show up in the body (in the brain), but not in its absence (after death). While Davinci's performance is constrained by its syntactic limitations, and it differs from humans, its Dualist bias is robust. These results demonstrate that the mind-body divide is partly learnable from experience.They also show how, as LLM's are exposed to human narratives, they induce not only human knowledge but also human biases. ","[{'version': 'v1', 'created': 'Wed, 10 May 2023 12:28:09 GMT'}, {'version': 'v2', 'created': 'Tue, 30 May 2023 21:00:50 GMT'}]",2023-06-01,"[['Berent', 'Iris', ''], ['Sansiveri', 'Alexzander', '']]",0,1,2023-05-10,2,2,2,1,0,1,d083bc4fbf71dc53a37d81cc107748fda9729ab0,258686560.0,https://www.semanticscholar.org/paper/d083bc4fbf71dc53a37d81cc107748fda9729ab0,arXiv.org,2023.0,17.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Biology', 'source': 's2-fos-model'}]","[{'authorId': '2139306', 'name': 'I. Berent'}, {'authorId': '2217228544', 'name': 'Alexzander Sansiveri'}]",['Northeastern University'],"['China', 'United States']",2023-05
2305.08144,Danyang Zhang,"Danyang Zhang, Lu Chen, Zihan Zhao, Ruisheng Cao, Kai Yu",Mobile-Env: An Evaluation Platform and Benchmark for Interactive Agents in LLM Era,,,,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Diverse evaluation benchmarks play a crucial role to assess a wide range of capabilities of large language models (LLM). Although plenty of endeavors have been dedicated to building valuable benchmarks, there is still little work aiming at evaluating the capability of LLM in multistep interactive environments. Noticing that LLM requires a text representation of the environment observations for interaction, we choose to fill such a blank by building a novel benchmark based on the information user interface (InfoUI). InfoUI consists of rich text contents and can be represented in some text formats, thus is suitable for the assessment of interaction ability of LLM. Additionally, the complex structures of InfoUI can further raise a challenge for LLM to understand structured texts rather than plain texts. An interaction platform is always used to evaluate an agent, however, there is still a lack of a satisfactory interaction platform dedicated to InfoUI. Consequently, we propose to build a novel easily-extendable, adaptable, and close-to-reality interaction platform, Mobile-Env, to provide a base for an appropriate benchmark. Based on Mobile-Env, an InfoUI task set WikiHow is then built to establish a benchmark for the multistep interaction capability of LLM in structured text-based environments. Agents based on a series of LLMs are tested on the task set to obtain an insight into the potential and challenge of LLM for InfoUI interaction. It is sincerely welcome that the community contribute new environments and new task sets for Mobile-Env to provide better test benchmarks and facilitate the development of the corresponding domains. ","[{'version': 'v1', 'created': 'Sun, 14 May 2023 12:31:03 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Jun 2023 09:20:46 GMT'}]",2023-06-16,"[['Zhang', 'Danyang', ''], ['Chen', 'Lu', ''], ['Zhao', 'Zihan', ''], ['Cao', 'Ruisheng', ''], ['Yu', 'Kai', '']]",0,0,2023-05-14,2,5,1,0,0,0,08a3986c595dc38147edf6425641de1fe0d796b3,259164957.0,https://www.semanticscholar.org/paper/08a3986c595dc38147edf6425641de1fe0d796b3,,2023.0,21.0,1.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2118333', 'name': 'Danyang Zhang'}, {'authorId': '1390833791', 'name': 'Lu Chen'}, {'authorId': '1806179720', 'name': 'Zihan Zhao'}, {'authorId': '150273321', 'name': 'Ruisheng Cao'}, {'authorId': '2114076600', 'name': 'Kai Yu'}]",['Shanghai Jiao Tong University'],['China'],2023-05
2305.08322,Junxian He,"Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang,
  Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu,
  Maosong Sun, Junxian He",C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models,Website: https://cevalbenchmark.com,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  New NLP benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present C-Eval, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. C-Eval comprises multiple-choice questions across four difficulty levels: middle school, high school, college, and professional. The questions span 52 diverse disciplines, ranging from humanities to science and engineering. C-Eval is accompanied by C-Eval Hard, a subset of very challenging subjects in C-Eval that requires advanced reasoning abilities to solve. We conduct a comprehensive evaluation of the most advanced LLMs on C-Eval, including both English- and Chinese-oriented models. Results indicate that only GPT-4 could achieve an average accuracy of over 60%, suggesting that there is still significant room for improvement for current LLMs. We anticipate C-Eval will help analyze important strengths and shortcomings of foundation models, and foster their development and growth for Chinese users. ","[{'version': 'v1', 'created': 'Mon, 15 May 2023 03:20:19 GMT'}, {'version': 'v2', 'created': 'Wed, 17 May 2023 01:11:35 GMT'}]",2023-05-18,"[['Huang', 'Yuzhen', ''], ['Bai', 'Yuzhuo', ''], ['Zhu', 'Zhihao', ''], ['Zhang', 'Junlei', ''], ['Zhang', 'Jinghan', ''], ['Su', 'Tangjun', ''], ['Liu', 'Junteng', ''], ['Lv', 'Chuancheng', ''], ['Zhang', 'Yikai', ''], ['Lei', 'Jiayi', ''], ['Fu', 'Yao', ''], ['Sun', 'Maosong', ''], ['He', 'Junxian', '']]",0,1,2023-05-15,2,13,1,1,0,1,236c7dafea3df7ecffb5f18ec780d12f2f27d4b0,258685666.0,https://www.semanticscholar.org/paper/236c7dafea3df7ecffb5f18ec780d12f2f27d4b0,arXiv.org,2023.0,46.0,90.0,15.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2143454301', 'name': 'Yuzhen Huang'}, {'authorId': '2115834034', 'name': 'Yuzhuo Bai'}, {'authorId': '2217435907', 'name': 'Zhihao Zhu'}, {'authorId': '2108012146', 'name': 'Junlei Zhang'}, {'authorId': '6062056', 'name': 'Jinghan Zhang'}, {'authorId': '2217252347', 'name': 'Tangjun Su'}, {'authorId': '2217264323', 'name': 'Junteng Liu'}, {'authorId': '2051969465', 'name': 'Chuancheng Lv'}, {'authorId': '2217263361', 'name': 'Yikai Zhang'}, {'authorId': '2217263111', 'name': 'Jiayi Lei'}, {'authorId': '51466208', 'name': 'Fanchao Qi'}, {'authorId': '46956602', 'name': 'Yao Fu'}, {'authorId': '1753344', 'name': 'Maosong Sun'}, {'authorId': '6215698', 'name': 'Junxian He'}]","['University of Edinburgh', 'Tsinghua University', 'Hong Kong University of Science and Technology', 'Shanghai Jiao Tong University']","['China', 'United Kingdom']",2023-05
2305.08339,Danni Yu Yu,"Danni Yu, Luyang Li, Hang Su, Matteo Fuoli",Assessing the potential of AI-assisted pragmatic annotation: The case of apologies,"24 pages, 2 figures, 3 tablels",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Certain forms of linguistic annotation, like part of speech and semantic tagging, can be automated with high accuracy. However, manual annotation is still necessary for complex pragmatic and discursive features that lack a direct mapping to lexical forms. This manual process is time-consuming and error-prone, limiting the scalability of function-to-form approaches in corpus linguistics. To address this, our study explores automating pragma-discursive corpus annotation using large language models (LLMs). We compare ChatGPT, the Bing chatbot, and a human coder in annotating apology components in English based on the local grammar framework. We find that the Bing chatbot outperformed ChatGPT, with accuracy approaching that of a human coder. These results suggest that AI can be successfully deployed to aid pragma-discursive corpus annotation, making the process more efficient and scalable. Keywords: linguistic annotation, function-to-form approaches, large language models, local grammar analysis, Bing chatbot, ChatGPT ","[{'version': 'v1', 'created': 'Mon, 15 May 2023 04:10:13 GMT'}, {'version': 'v2', 'created': 'Thu, 25 May 2023 03:57:05 GMT'}, {'version': 'v3', 'created': 'Fri, 15 Sep 2023 09:55:47 GMT'}]",2023-09-18,"[['Yu', 'Danni', ''], ['Li', 'Luyang', ''], ['Su', 'Hang', ''], ['Fuoli', 'Matteo', '']]",1,1,2023-05-15,3,4,2,1,0,1,9bdd445697e900f30326a382a213ba8dc9eccd1f,258686296.0,https://www.semanticscholar.org/paper/9bdd445697e900f30326a382a213ba8dc9eccd1f,,2023.0,34.0,2.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '46707459', 'name': 'Danni Yu'}, {'authorId': '2114025783', 'name': 'Luyang Li'}, {'authorId': '50982247', 'name': 'Hang Su'}, {'authorId': '2855669', 'name': 'Matteo Fuoli'}]",['Sichuan International Studies University'],['China'],2023-05
2305.08347,Zhifeng Li,Zhifeng Li and Bowei Zou and Yifan Fan and Yu Hong,KEPR: Knowledge Enhancement and Plausibility Ranking for Generative Commonsense Question Answering,IJCNN 2023,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generative commonsense question answering (GenCQA) is a task of automatically generating a list of answers given a question. The answer list is required to cover all reasonable answers. This presents the considerable challenges of producing diverse answers and ranking them properly. Incorporating a variety of closely-related background knowledge into the encoding of questions enables the generation of different answers. Meanwhile, learning to distinguish positive answers from negative ones potentially enhances the probabilistic estimation of plausibility, and accordingly, the plausibility-based ranking. Therefore, we propose a Knowledge Enhancement and Plausibility Ranking (KEPR) approach grounded on the Generate-Then-Rank pipeline architecture. Specifically, we expand questions in terms of Wiktionary commonsense knowledge of keywords, and reformulate them with normalized patterns. Dense passage retrieval is utilized for capturing relevant knowledge, and different PLM-based (BART, GPT2 and T5) networks are used for generating answers. On the other hand, we develop an ELECTRA-based answer ranking model, where logistic regression is conducted during training, with the aim of approximating different levels of plausibility in a polar classification scenario. Extensive experiments on the benchmark ProtoQA show that KEPR obtains substantial improvements, compared to the strong baselines. Within the experimental models, the T5-based GenCQA with KEPR obtains the best performance, which is up to 60.91% at the primary canonical metric Inc@3. It outperforms the existing GenCQA models on the current leaderboard of ProtoQA. ","[{'version': 'v1', 'created': 'Mon, 15 May 2023 04:58:37 GMT'}]",2023-05-16,"[['Li', 'Zhifeng', ''], ['Zou', 'Bowei', ''], ['Fan', 'Yifan', ''], ['Hong', 'Yu', '']]",0,1,2023-05-15,1,4,2,2,2,0,8da1f3ad22abdc00188b254b2bec89157736dd5e,258686584.0,https://www.semanticscholar.org/paper/8da1f3ad22abdc00188b254b2bec89157736dd5e,IEEE International Joint Conference on Neural Network,2023.0,41.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2217233875', 'name': 'Zhifeng Li'}, {'authorId': '3078054', 'name': 'Bowei Zou'}, {'authorId': '2042646882', 'name': 'Yifan Fan'}, {'authorId': '2009642867', 'name': 'Yu Hong'}]","['Soochow University', 'Institute for Infocomm Research']","['China', 'Singapore']",2023-05
2305.08391,Yaxin Fan,Yaxin Fan and Feng Jiang,Uncovering the Potential of ChatGPT for Discourse Analysis in Dialogue: An Empirical Study,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) like ChatGPT have proven a great shallow understanding of many traditional NLP tasks, such as translation, summarization, etc. However, its performance on high-level understanding, such as dialogue discourse analysis task that requires a higher level of understanding and reasoning, remains less explored. This study investigates ChatGPT's capabilities in three dialogue discourse tasks: topic segmentation, discourse relation recognition, and discourse parsing, of varying difficulty levels. To adapt ChatGPT to these tasks, we propose discriminative and generative paradigms and introduce the Chain of Thought (COT) approach to improve ChatGPT's performance in more difficult tasks. The results show that our generative paradigm allows ChatGPT to achieve comparative performance in the topic segmentation task comparable to state-of-the-art methods but reveals room for improvement in the more complex tasks of discourse relation recognition and discourse parsing. Notably, the COT can significantly enhance ChatGPT's performance with the help of understanding complex structures in more challenging tasks. Through a series of case studies, our in-depth analysis suggests that ChatGPT can be a good annotator in topic segmentation but has difficulties understanding complex rhetorical structures. We hope these findings provide a foundation for future research to refine dialogue discourse analysis approaches in the era of LLMs. ","[{'version': 'v1', 'created': 'Mon, 15 May 2023 07:14:41 GMT'}]",2023-05-16,"[['Fan', 'Yaxin', ''], ['Jiang', 'Feng', '']]",1,1,2023-05-15,1,2,1,1,0,1,932584bc29b6b80ed0978700f4ea2677f2257d9f,258686248.0,https://www.semanticscholar.org/paper/932584bc29b6b80ed0978700f4ea2677f2257d9f,arXiv.org,2023.0,35.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2115701764', 'name': 'Yaxin Fan'}, {'authorId': '145875191', 'name': 'Feng Jiang'}]","['Chinese University of Hong Kong, Shenzhen', 'Soochow University']",['China'],2023-05
2305.08703,Ningyu Zhang,"Hongbin Ye, Honghao Gui, Xin Xu, Huajun Chen, Ningyu Zhang",Schema-adaptable Knowledge Graph Construction,Work in progress,,,,cs.CL cs.AI cs.DB cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Conventional Knowledge Graph Construction (KGC) approaches typically follow the static information extraction paradigm with a closed set of pre-defined schema. As a result, such approaches fall short when applied to dynamic scenarios or domains, whereas a new type of knowledge emerges. This necessitates a system that can handle evolving schema automatically to extract information for KGC. To address this need, we propose a new task called schema-adaptable KGC, which aims to continually extract entity, relation, and event based on a dynamically changing schema graph without re-training. We first split and convert existing datasets based on three principles to build a benchmark, i.e., horizontal schema expansion, vertical schema expansion, and hybrid schema expansion; then investigate the schema-adaptable performance of several well-known approaches such as Text2Event, TANL, UIE and GPT-3.5. We further propose a simple yet effective baseline dubbed AdaKGC, which contains schema-enriched prefix instructor and schema-conditioned dynamic decoding to better handle evolving schema. Comprehensive experimental results illustrate that AdaKGC can outperform baselines but still have room for improvement. We hope the proposed work can deliver benefits to the community. Code and datasets will be available in https://github.com/zjunlp/AdaKGC. ","[{'version': 'v1', 'created': 'Mon, 15 May 2023 15:06:20 GMT'}, {'version': 'v2', 'created': 'Fri, 19 May 2023 08:59:33 GMT'}]",2023-05-22,"[['Ye', 'Hongbin', ''], ['Gui', 'Honghao', ''], ['Xu', 'Xin', ''], ['Chen', 'Huajun', ''], ['Zhang', 'Ningyu', '']]",0,1,2023-05-15,2,5,5,1,0,1,08d5a3a8b6a5079e02fd7c678a5daa673bc40644,258686618.0,https://www.semanticscholar.org/paper/08d5a3a8b6a5079e02fd7c678a5daa673bc40644,arXiv.org,2023.0,87.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '40463748', 'name': 'Hongbin Ye'}, {'authorId': '2217228727', 'name': 'Honghao Gui'}, {'authorId': '2152775219', 'name': 'Xin Xu'}, {'authorId': '2144200945', 'name': 'Huajun Chen'}, {'authorId': '2608639', 'name': 'Ningyu Zhang'}]","['Alibaba', 'Tencent', 'Zhejiang Lab', 'Zhejiang University']",['China'],2023-05
2305.08732,Ningyu Zhang,"Yunzhi Yao, Peng Wang, Shengyu Mao, Chuanqi Tan, Fei Huang, Huajun
  Chen, Ningyu Zhang",Knowledge Rumination for Pre-trained Language Models,Work in progress,,,,cs.CL cs.AI cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Previous studies have revealed that vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus, several works have attempted to integrate external knowledge into PLMs. However, despite the promising outcome, we empirically observe that PLMs may have already encoded rich knowledge in their pre-trained parameters but fails to fully utilize them when applying to knowledge-intensive tasks. In this paper, we propose a new paradigm dubbed Knowledge Rumination to help the pre-trained language model utilize those related latent knowledge without retrieving them from the external corpus. By simply adding a prompt like ``As far as I know'' to the PLMs, we try to review related latent knowledge and inject them back to the model for knowledge consolidation. We apply the proposed knowledge rumination to various language models, including RoBERTa, DeBERTa, GPT-3 and OPT. Experimental results on six commonsense reasoning tasks and GLUE benchmarks demonstrate the effectiveness of our proposed approach, which further proves that the knowledge stored in PLMs can be better exploited to enhance the downstream performance. Code will be available in https://github.com/zjunlp/knowledge-rumination. ","[{'version': 'v1', 'created': 'Mon, 15 May 2023 15:47:09 GMT'}]",2023-05-16,"[['Yao', 'Yunzhi', ''], ['Wang', 'Peng', ''], ['Mao', 'Shengyu', ''], ['Tan', 'Chuanqi', ''], ['Huang', 'Fei', ''], ['Chen', 'Huajun', ''], ['Zhang', 'Ningyu', '']]",0,1,2023-05-15,1,7,4,2,1,1,c131ac4360aa9d9b7a955bed87e9cd4a1a3e7562,258685543.0,https://www.semanticscholar.org/paper/c131ac4360aa9d9b7a955bed87e9cd4a1a3e7562,arXiv.org,2023.0,69.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '4841460', 'name': 'Yunzhi Yao'}, {'authorId': '144282672', 'name': 'Peng Wang'}, {'authorId': '2184189568', 'name': 'Shengyu Mao'}, {'authorId': '2111727840', 'name': 'Chuanqi Tan'}, {'authorId': '143857288', 'name': 'Fei Huang'}, {'authorId': '2144200945', 'name': 'Huajun Chen'}, {'authorId': '2608639', 'name': 'Ningyu Zhang'}]","['Alibaba', 'Shanghai Jiao Tong University', 'Zhejiang University']",['China'],2023-05
2305.08845,Yupeng Hou,"Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian
  McAuley, Wayne Xin Zhao",Large Language Models are Zero-Shot Rankers for Recommender Systems,,,,,cs.IR cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Recently, large language models (LLMs) (e.g. GPT-4) have demonstrated impressive general-purpose task-solving abilities, including the potential to approach recommendation tasks. Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems. To conduct our empirical study, we first formalize the recommendation problem as a conditional ranking task, considering sequential interaction histories as conditions and the items retrieved by the candidate generation model as candidates. We adopt a specific prompting approach to solving the ranking task by LLMs: we carefully design the prompting template by including the sequential interaction history, the candidate items, and the ranking instruction. We conduct extensive experiments on two widely-used datasets for recommender systems and derive several key findings for the use of LLMs in recommender systems. We show that LLMs have promising zero-shot ranking abilities, even competitive to or better than conventional recommendation models on candidates retrieved by multiple candidate generators. We also demonstrate that LLMs struggle to perceive the order of historical interactions and can be affected by biases like position bias, while these issues can be alleviated via specially designed prompting and bootstrapping strategies. The code to reproduce this work is available at https://github.com/RUCAIBox/LLMRank. ","[{'version': 'v1', 'created': 'Mon, 15 May 2023 17:57:39 GMT'}]",2023-05-16,"[['Hou', 'Yupeng', ''], ['Zhang', 'Junjie', ''], ['Lin', 'Zihan', ''], ['Lu', 'Hongyu', ''], ['Xie', 'Ruobing', ''], ['McAuley', 'Julian', ''], ['Zhao', 'Wayne Xin', '']]",0,1,2023-05-15,1,7,2,1,0,1,4683d3d6cb31111cf4499a199c0b036662b3eb32,258686540.0,https://www.semanticscholar.org/paper/4683d3d6cb31111cf4499a199c0b036662b3eb32,arXiv.org,2023.0,46.0,42.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '151472453', 'name': 'Yupeng Hou'}, {'authorId': '2120518257', 'name': 'Junjie Zhang'}, {'authorId': '2112304962', 'name': 'Zihan Lin'}, {'authorId': '2115863242', 'name': 'Hongyu Lu'}, {'authorId': '3360722', 'name': 'Ruobing Xie'}, {'authorId': '35660011', 'name': 'Julian McAuley'}, {'authorId': '2542603', 'name': 'Wayne Xin Zhao'}]","['Tencent', 'Renmin University of China', 'University of California, San Diego']","['China', 'United States']",2023-05
2305.08883,Xi Yang,"Xi Yang, Kejiang Chen, Weiming Zhang, Chang Liu, Yuang Qi, Jie Zhang,
  Han Fang, Nenghai Yu",Watermarking Text Generated by Black-Box Language Models,"Code will be available at
  https://github.com/Kiode/Text_Watermark_Language_Models",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  LLMs now exhibit human-like skills in various fields, leading to worries about misuse. Thus, detecting generated text is crucial. However, passive detection methods are stuck in domain specificity and limited adversarial robustness. To achieve reliable detection, a watermark-based method was proposed for white-box LLMs, allowing them to embed watermarks during text generation. The method involves randomly dividing the model vocabulary to obtain a special list and adjusting the probability distribution to promote the selection of words in the list. A detection algorithm aware of the list can identify the watermarked text. However, this method is not applicable in many real-world scenarios where only black-box language models are available. For instance, third-parties that develop API-based vertical applications cannot watermark text themselves because API providers only supply generated text and withhold probability distributions to shield their commercial interests. To allow third-parties to autonomously inject watermarks into generated text, we develop a watermarking framework for black-box language model usage scenarios. Specifically, we first define a binary encoding function to compute a random binary encoding corresponding to a word. The encodings computed for non-watermarked text conform to a Bernoulli distribution, wherein the probability of a word representing bit-1 being approximately 0.5. To inject a watermark, we alter the distribution by selectively replacing words representing bit-0 with context-based synonyms that represent bit-1. A statistical test is then used to identify the watermark. Experiments demonstrate the effectiveness of our method on both Chinese and English datasets. Furthermore, results under re-translation, polishing, word deletion, and synonym substitution attacks reveal that it is arduous to remove the watermark without compromising the original semantics. ","[{'version': 'v1', 'created': 'Sun, 14 May 2023 07:37:33 GMT'}]",2023-05-17,"[['Yang', 'Xi', ''], ['Chen', 'Kejiang', ''], ['Zhang', 'Weiming', ''], ['Liu', 'Chang', ''], ['Qi', 'Yuang', ''], ['Zhang', 'Jie', ''], ['Fang', 'Han', ''], ['Yu', 'Nenghai', '']]",0,0,2023-05-14,1,8,2,0,0,0,bd7a45fa6dce81dbbc61a5728bf54e040e206da7,258714683.0,https://www.semanticscholar.org/paper/bd7a45fa6dce81dbbc61a5728bf54e040e206da7,arXiv.org,2023.0,34.0,7.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2143365293', 'name': 'Xi Yang'}, {'authorId': '8780109', 'name': 'Kejiang Chen'}, {'authorId': '51027868', 'name': 'Weiming Zhang'}, {'authorId': '40997227', 'name': 'Chang-rui Liu'}, {'authorId': '2217971466', 'name': 'Yuang Qi'}, {'authorId': '2155863060', 'name': 'Jie Zhang'}, {'authorId': '12464207', 'name': 'Han Fang'}, {'authorId': '2052212945', 'name': 'Neng H. Yu'}]","['National University of Singapore', 'University of Science and Technology of China', 'Nanyang Technological University']","['China', 'Singapore']",2023-05
2305.09067,Xiaoying Zhang,"Xiaoying Zhang, Baolin Peng, Kun Li, Jingyan Zhou, Helen Meng",SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting,"21 pages, 13 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Building end-to-end task bots and maintaining their integration with new functionalities using minimal human efforts is a long-standing challenge in dialog research. Recently large language models (LLMs) have demonstrated exceptional proficiency in conversational engagement and adherence to instructions across various downstream tasks. In this work, we introduce SGP-TOD, Schema-Guided Prompting for building Task-Oriented Dialog systems effortlessly based on LLMs. Utilizing the symbolic knowledge -- task schema, we instruct fixed LLMs to generate appropriate responses on novel tasks, circumventing the need for training data. Specifically, SGP-TOD comprises three components: a LLM for engaging with users, a DST Prompter to aid the LLM with dialog state tracking, which is then used to retrieve database items, and a Policy Prompter to elicit proper responses adhering to the provided dialog policy. Experimental results on Multiwoz, RADDLE and STAR datasets show that our training-free strategy SGP-TOD, without any task-specific data, yields state-of-the-art (SOTA) zero-shot performance, greatly surpasses the few-shot approaches. In a domain-extension setting, SGP-TOD aptly adapts to new functionalities by merely adding supplementary schema rules. We make our code and data publicly available. ","[{'version': 'v1', 'created': 'Mon, 15 May 2023 23:29:56 GMT'}]",2023-05-17,"[['Zhang', 'Xiaoying', ''], ['Peng', 'Baolin', ''], ['Li', 'Kun', ''], ['Zhou', 'Jingyan', ''], ['Meng', 'Helen', '']]",0,0,2023-05-15,1,5,1,0,0,0,ec56f49bef8925dc8931cc261ab3aca4dd36ad2d,258715201.0,https://www.semanticscholar.org/paper/ec56f49bef8925dc8931cc261ab3aca4dd36ad2d,arXiv.org,2023.0,56.0,7.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2155167734', 'name': 'Xiaoying Zhang'}, {'authorId': '1780690', 'name': 'Baolin Peng'}, {'authorId': '2185631323', 'name': 'Kun Li'}, {'authorId': '30887444', 'name': 'Jingyan Zhou'}, {'authorId': '1702243', 'name': 'Helen M. Meng'}]","['Tencent', 'Chinese University of Hong Kong', 'Microsoft']","['China', 'United States']",2023-05
2305.09246,Hao Chen,"Hao Chen, Yiming Zhang, Qi Zhang, Hantao Yang, Xiaomeng Hu, Xuetao Ma,
  Yifan Yanggong, Junbo Zhao",Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning,"9 pages, 2 tables, 2 figures",,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Instruction tuning for large language models (LLMs) has gained attention from researchers due to its ability to unlock the potential of LLMs in following instructions. While instruction tuning offers advantages for facilitating the adaptation of large language models (LLMs) to downstream tasks as a fine-tuning approach, training models with tens of millions or even billions of parameters on large amounts of data results in unaffordable computational costs. To address this, we focus on reducing the data used in LLM instruction tuning to decrease training costs and improve data efficiency, dubbed as Low Training Data Instruction Tuning (LTD Instruction Tuning). Specifically, this paper conducts a preliminary exploration into reducing the data used in LLM training and identifies several observations regarding task specialization for LLM training, such as the optimization of performance for a specific task, the number of instruction types required for instruction tuning, and the amount of data required for task-specific models. The results suggest that task-specific models can be trained using less than 0.5% of the original dataset, with a 2% improvement in performance over those trained on full task-related data. ","[{'version': 'v1', 'created': 'Tue, 16 May 2023 07:52:57 GMT'}]",2023-05-17,"[['Chen', 'Hao', ''], ['Zhang', 'Yiming', ''], ['Zhang', 'Qi', ''], ['Yang', 'Hantao', ''], ['Hu', 'Xiaomeng', ''], ['Ma', 'Xuetao', ''], ['Yanggong', 'Yifan', ''], ['Zhao', 'Junbo', '']]",0,0,2023-05-16,1,8,2,0,0,0,5c7aaee5651221893ea0e67c363cab4c4be53b83,258715090.0,https://www.semanticscholar.org/paper/5c7aaee5651221893ea0e67c363cab4c4be53b83,arXiv.org,2023.0,37.0,7.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2202785348', 'name': 'Haowen Chen'}, {'authorId': '2108440643', 'name': 'Yiming Zhang'}, {'authorId': '2145908634', 'name': 'Qi Zhang'}, {'authorId': '2217428593', 'name': 'Hantao Yang'}, {'authorId': '2164187602', 'name': 'Xiaomeng Hu'}, {'authorId': '2217399122', 'name': 'Xuetao Ma'}, {'authorId': '2800943', 'name': 'Yifan YangGong'}, {'authorId': '7818229', 'name': 'J. Zhao'}]","['ZhongHao XinYing (Hangzhou) Technology Co., Ltd.', 'Zhejiang University']",['China'],2023-05
2305.09434,Zhe Liu,"Zhe Liu, Chunyang Chen, Junjie Wang, Mengzhuo Chen, Boyu Wu, Xing Che,
  Dandan Wang, Qing Wang",Chatting with GPT-3 for Zero-Shot Human-Like Mobile Automated GUI Testing,,,,,cs.SE,http://creativecommons.org/publicdomain/zero/1.0/,"  Mobile apps are indispensable for people's daily life, and automated GUI (Graphical User Interface) testing is widely used for app quality assurance. There is a growing interest in using learning-based techniques for automated GUI testing which aims at generating human-like actions and interactions. However, the limitations such as low testing coverage, weak generalization, and heavy reliance on training data, make an urgent need for a more effective approach to generate human-like actions to thoroughly test mobile apps. Inspired by the success of the Large Language Model (LLM), e.g., GPT-3 and ChatGPT, in natural language understanding and question answering, we formulate the mobile GUI testing problem as a Q&A task. We propose GPTDroid, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process. Within it, we extract the static context of the GUI page and the dynamic context of the iterative testing process, design prompts for inputting this information to LLM, and develop a neural matching network to decode the LLM's output into actionable steps to execute the app. We evaluate GPTDroid on 86 apps from Google Play, and its activity coverage is 71%, with 32% higher than the best baseline, and can detect 36% more bugs with faster speed than the best baseline. GPTDroid also detects 48 new bugs on the Google Play with 25 of them being confirmed/fixed. We further summarize the capabilities of GPTDroid behind the superior performance, including semantic text input, compound action, long meaningful test trace, and test case prioritization. ","[{'version': 'v1', 'created': 'Tue, 16 May 2023 13:46:52 GMT'}]",2023-05-17,"[['Liu', 'Zhe', ''], ['Chen', 'Chunyang', ''], ['Wang', 'Junjie', ''], ['Chen', 'Mengzhuo', ''], ['Wu', 'Boyu', ''], ['Che', 'Xing', ''], ['Wang', 'Dandan', ''], ['Wang', 'Qing', '']]",1,1,2023-05-16,1,8,1,2,0,2,bc283b0e53f749f6c5b8b67eb340ea6a5a4331f5,258714899.0,https://www.semanticscholar.org/paper/bc283b0e53f749f6c5b8b67eb340ea6a5a4331f5,arXiv.org,2023.0,96.0,9.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2116746462', 'name': 'Zhe Liu'}, {'authorId': '46729152', 'name': 'Chunyang Chen'}, {'authorId': '2109763128', 'name': 'Junjie Wang'}, {'authorId': '2217447529', 'name': 'Mengzhuo Chen'}, {'authorId': '2217931120', 'name': 'Boyu Wu'}, {'authorId': '2184146378', 'name': 'Xing Che'}, {'authorId': '2155680314', 'name': 'Dandan Wang'}, {'authorId': '2145464944', 'name': 'Qing Wang'}]","['Laboratory for Internet Software Technologies,', 'Monash University', 'University of Chinese Academy of Sciences', 'Chinese Academy of Sciences']","['China', 'Australia']",2023-05
2305.09645,Jinhao Jiang,"Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao and
  Ji-Rong Wen",StructGPT: A General Framework for Large Language Model to Reason over Structured Data,"13 pages, working in progress",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we study how to improve the zero-shot reasoning ability of large language models~(LLMs) over structured data in a unified way. Inspired by the study on tool augmentation for LLMs, we develop an \emph{Iterative Reading-then-Reasoning~(IRR)} approach for solving question answering tasks based on structured data, called \textbf{StructGPT}. In our approach, we construct the specialized function to collect relevant evidence from structured data (\ie \emph{reading}), and let LLMs concentrate the reasoning task based on the collected information (\ie \emph{reasoning}). Specially, we propose an \emph{invoking-linearization-generation} procedure to support LLMs in reasoning on the structured data with the help of the external interfaces. By iterating this procedures with provided interfaces, our approach can gradually approach the target answer to a given query. Extensive experiments conducted on three types of structured data demonstrate the effectiveness of our approach, which can significantly boost the performance of ChatGPT and achieve comparable performance against the full-data supervised-tuning baselines. Our codes and data are publicly available at~\url{https://github.com/RUCAIBox/StructGPT}. ","[{'version': 'v1', 'created': 'Tue, 16 May 2023 17:45:23 GMT'}]",2023-05-17,"[['Jiang', 'Jinhao', ''], ['Zhou', 'Kun', ''], ['Dong', 'Zican', ''], ['Ye', 'Keming', ''], ['Zhao', 'Wayne Xin', ''], ['Wen', 'Ji-Rong', '']]",1,1,2023-05-16,1,6,1,1,0,1,e0f27336698c84709bd60b6b7f4ce588cbae66bf,258714753.0,https://www.semanticscholar.org/paper/e0f27336698c84709bd60b6b7f4ce588cbae66bf,arXiv.org,2023.0,58.0,30.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2118240359', 'name': 'Jinhao Jiang'}, {'authorId': '1423651904', 'name': 'Kun Zhou'}, {'authorId': '2198280871', 'name': 'Zican Dong'}, {'authorId': '1657563745', 'name': 'Keming Ye'}, {'authorId': '2542603', 'name': 'Wayne Xin Zhao'}, {'authorId': '153693432', 'name': 'Ji-rong Wen'}]","['Beijing Key Laboratory of Big Data Management and Analysis Methods.', 'Xidian University', 'Renmin University of China']",['China'],2023-05
2305.09781,Zhihao Jia,"Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang,
  Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming
  Chen, Daiyaan Arfeen, Reyna Abhyankar, Zhihao Jia",SpecInfer: Accelerating Generative Large Language Model Serving with Speculative Inference and Token Tree Verification,,,,,cs.CL cs.DC cs.LG,http://creativecommons.org/licenses/by/4.0/,"  The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them quickly and cheaply. This paper introduces SpecInfer, an LLM serving system that accelerates generative LLM inference with speculative inference and token tree verification. A key insight behind Specinfer is to combine various collectively boost-tuned small language models to jointly predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified against the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality. Our evaluation shows that SpecInfer outperforms existing LLM serving systems by 1.3-2.4x for distributed LLM inference and by 2.6-3.5x for offloading-based LLM inference, while preserving the same generative performance. SpecInfer is publicly available at https://github.com/flexflow/FlexFlow/tree/inference. ","[{'version': 'v1', 'created': 'Tue, 16 May 2023 20:12:59 GMT'}, {'version': 'v2', 'created': 'Wed, 16 Aug 2023 13:33:06 GMT'}]",2023-08-17,"[['Miao', 'Xupeng', ''], ['Oliaro', 'Gabriele', ''], ['Zhang', 'Zhihao', ''], ['Cheng', 'Xinhao', ''], ['Wang', 'Zeyu', ''], ['Wong', 'Rae Ying Yee', ''], ['Zhu', 'Alan', ''], ['Yang', 'Lijie', ''], ['Shi', 'Xiaoxiang', ''], ['Shi', 'Chunan', ''], ['Chen', 'Zhuoming', ''], ['Arfeen', 'Daiyaan', ''], ['Abhyankar', 'Reyna', ''], ['Jia', 'Zhihao', '']]",0,0,2023-05-16,2,14,3,0,0,0,6aa43949a7bbe1f0a219c99510db81e6d9040fa2,260926421.0,https://www.semanticscholar.org/paper/6aa43949a7bbe1f0a219c99510db81e6d9040fa2,,2023.0,51.0,0.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1720763480', 'name': 'Xupeng Miao'}, {'authorId': '2638632', 'name': 'G. Oliaro'}, {'authorId': '2185953295', 'name': 'Zhihao Zhang'}, {'authorId': '2217941981', 'name': 'Xinhao Cheng'}, {'authorId': '2231631121', 'name': 'Zeyu Wang'}, {'authorId': '2217487123', 'name': 'Rae Ying Yee Wong'}, {'authorId': '2231613073', 'name': 'Alan Zhu'}, {'authorId': '2231664587', 'name': 'Lijie Yang'}, {'authorId': '144120884', 'name': 'Xiaoxiang Shi'}, {'authorId': '2192580828', 'name': 'Chunan Shi'}, {'authorId': '2111498904', 'name': 'Zhuoming Chen'}, {'authorId': '1389546686', 'name': 'Daiyaan Arfeen'}, {'authorId': '2217683047', 'name': 'Reyna Abhyankar'}, {'authorId': '2072782550', 'name': 'Zhihao Jia'}]",['Shanghai Jiao Tong University'],['China'],2023-05
2305.10013,Chengcheng Han,"Chengcheng Han, Liqing Cui, Renyu Zhu, Jianing Wang, Nuo Chen, Qiushi
  Sun, Xiang Li, Ming Gao",When Gradient Descent Meets Derivative-Free Optimization: A Match Made in Black-Box Scenario,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large pre-trained language models (PLMs) have garnered significant attention for their versatility and potential for solving a wide spectrum of natural language processing (NLP) tasks. However, the cost of running these PLMs may be prohibitive. Furthermore, PLMs may not be open-sourced due to commercial considerations and potential risks of misuse, such as GPT-3. The parameters and gradients of PLMs are unavailable in this scenario. To solve the issue, black-box tuning has been proposed, which utilizes derivative-free optimization (DFO), instead of gradient descent, for training task-specific continuous prompts. However, these gradient-free methods still exhibit a significant gap compared to gradient-based methods. In this paper, we introduce gradient descent into black-box tuning scenario through knowledge distillation. Furthermore, we propose a novel method GDFO, which integrates gradient descent and derivative-free optimization to optimize task-specific continuous prompts in a harmonized manner. Experimental results show that GDFO can achieve significant performance gains over previous state-of-the-art methods. ","[{'version': 'v1', 'created': 'Wed, 17 May 2023 07:48:28 GMT'}]",2023-05-18,"[['Han', 'Chengcheng', ''], ['Cui', 'Liqing', ''], ['Zhu', 'Renyu', ''], ['Wang', 'Jianing', ''], ['Chen', 'Nuo', ''], ['Sun', 'Qiushi', ''], ['Li', 'Xiang', ''], ['Gao', 'Ming', '']]",0,1,2023-05-17,1,8,2,1,0,1,f07c13e35f5d299dd66b0991cbd77dac0eb19508,258740711.0,https://www.semanticscholar.org/paper/f07c13e35f5d299dd66b0991cbd77dac0eb19508,Annual Meeting of the Association for Computational Linguistics,2023.0,50.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2118641672', 'name': 'Chengcheng Han'}, {'authorId': '2688236', 'name': 'Liqing Cui'}, {'authorId': '2029491250', 'name': 'Renyu Zhu'}, {'authorId': '46584777', 'name': 'J. Wang'}, {'authorId': '119895609', 'name': 'Nuo Chen'}, {'authorId': '2112455065', 'name': 'Qiushi Sun'}, {'authorId': '2144439382', 'name': 'Xiang Li'}, {'authorId': '2147415336', 'name': 'Ming Gao'}]","['National University of Singapore', 'NetEase', 'East China Normal University']","['China', 'Singapore']",2023-05
2305.10036,Jingwei Yi,"Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan
  Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, Xing Xie",Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark,Accepted by ACL 2023,,,,cs.CL cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have demonstrated powerful capabilities in both text understanding and generation. Companies have begun to offer Embedding as a Service (EaaS) based on these LLMs, which can benefit various natural language processing (NLP) tasks for customers. However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive. To protect the copyright of LLMs for EaaS, we propose an Embedding Watermark method called EmbMarker that implants backdoors on embeddings. Our method selects a group of moderate-frequency words from a general text corpus to form a trigger set, then selects a target embedding as the watermark, and inserts it into the embeddings of texts containing trigger words as the backdoor. The weight of insertion is proportional to the number of trigger words included in the text. This allows the watermark backdoor to be effectively transferred to EaaS-stealer's model for copyright verification while minimizing the adverse impact on the original embeddings' utility. Our extensive experiments on various datasets show that our method can effectively protect the copyright of EaaS models without compromising service quality. ","[{'version': 'v1', 'created': 'Wed, 17 May 2023 08:28:54 GMT'}, {'version': 'v2', 'created': 'Tue, 30 May 2023 08:06:30 GMT'}, {'version': 'v3', 'created': 'Fri, 2 Jun 2023 06:56:29 GMT'}]",2023-06-05,"[['Peng', 'Wenjun', ''], ['Yi', 'Jingwei', ''], ['Wu', 'Fangzhao', ''], ['Wu', 'Shangxi', ''], ['Zhu', 'Bin', ''], ['Lyu', 'Lingjuan', ''], ['Jiao', 'Binxing', ''], ['Xu', 'Tong', ''], ['Sun', 'Guangzhong', ''], ['Xie', 'Xing', '']]",0,0,2023-05-17,3,10,2,0,0,0,d4c3e3e3c01afed15926adf81527bf46aa491c6a,258741304.0,https://www.semanticscholar.org/paper/d4c3e3e3c01afed15926adf81527bf46aa491c6a,Annual Meeting of the Association for Computational Linguistics,2023.0,32.0,7.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1765218', 'name': 'Wenjun Peng'}, {'authorId': '2087122322', 'name': 'Jingwei Yi'}, {'authorId': '2397264', 'name': 'Fangzhao Wu'}, {'authorId': '46238935', 'name': 'Shangxi Wu'}, {'authorId': '2118251261', 'name': 'Bin Zhu'}, {'authorId': '3366777', 'name': 'L. Lyu'}, {'authorId': '24128606', 'name': 'Binxing Jiao'}, {'authorId': '2151647543', 'name': 'Tongye Xu'}, {'authorId': '39109247', 'name': 'Guangzhong Sun'}, {'authorId': '2149195520', 'name': 'Xing Xie'}]","['Beijing Jiaotong University', 'University of Science and Technology of China', 'Microsoft', 'Indicates equal contribution.']",['China'],2023-05
2305.10037,Heng Wang,"Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han,
  Yulia Tsvetkov",Can Language Models Solve Graph Problems in Natural Language?,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) are increasingly adopted for a variety of tasks with implicit graphical structures, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these tasks with structure implications, whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph benchmark and find that 1) language models do demonstrate preliminary graph reasoning abilities, 2) the benefit of advanced prompting and in-context learning diminishes on more complex graph problems, while 3) LLMs are also (un)surprisingly brittle in the face of spurious correlations in graph and problem settings. We then propose Build-a-Graph Prompting and Algorithmic Prompting, two instruction-based approaches to enhance LLMs in solving natural language graph problems. Build-a-Graph and Algorithmic prompting improve the performance of LLMs on NLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to solve the most complicated graph reasoning tasks in our setup with language models remains an open research question. The NLGraph benchmark and evaluation code are available at https://github.com/Arthur-Heng/NLGraph. ","[{'version': 'v1', 'created': 'Wed, 17 May 2023 08:29:21 GMT'}]",2023-05-18,"[['Wang', 'Heng', ''], ['Feng', 'Shangbin', ''], ['He', 'Tianxing', ''], ['Tan', 'Zhaoxuan', ''], ['Han', 'Xiaochuang', ''], ['Tsvetkov', 'Yulia', '']]",0,1,2023-05-17,1,6,2,1,0,1,df2beaae63e4d68ef8e762bcd4704c9f11f856d9,258740923.0,https://www.semanticscholar.org/paper/df2beaae63e4d68ef8e762bcd4704c9f11f856d9,arXiv.org,2023.0,40.0,18.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '120241560', 'name': 'Heng Wang'}, {'authorId': '2114887261', 'name': 'Shangbin Feng'}, {'authorId': '3083253', 'name': 'Tianxing He'}, {'authorId': '2093186816', 'name': 'Zhaoxuan Tan'}, {'authorId': '40500540', 'name': 'Xiaochuang Han'}, {'authorId': '2073587169', 'name': 'Yulia Tsvetkov'}]","['University of Washington', ""Xi'an Jiaotong University"", 'University of Notre Dame']","['China', 'United States']",2023-05
2305.10163,Jiageng Wu,"Jiageng Wu, Xian Wu, Zhaopeng Qiu, Minghui Li, Yefeng Zheng, and Jie
  Yang",Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model,,,,,cs.CL cs.AI cs.CY,http://creativecommons.org/licenses/by/4.0/,"  Generative Pre-Training (GPT) models like ChatGPT have demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. Although ChatGPT has been integrated into the overall workflow to boost efficiency in many domains, the lack of flexibility in the finetuning process hinders its applications in areas that demand extensive domain expertise and semantic knowledge, such as healthcare. In this paper, we evaluate ChatGPT on the China National Medical Licensing Examination (CNMLE) and propose a novel approach to improve ChatGPT from two perspectives: integrating medical domain knowledge and enabling few-shot learning. By using a simple but effective retrieval method, medical background knowledge is extracted as semantic instructions to guide the inference of ChatGPT. Similarly, relevant medical questions are identified and fed as demonstrations to ChatGPT. Experimental results show that directly applying ChatGPT fails to qualify the CNMLE at a score of 51 (i.e., only 51\% of questions are answered correctly). While our knowledge-enhanced model achieves a high score of 70 on CNMLE-2022 which not only passes the qualification but also surpasses the average score of humans (61). This research demonstrates the potential of knowledge-enhanced ChatGPT to serve as versatile medical assistants, capable of analyzing real-world medical problems in a more accessible, user-friendly, and adaptable manner. ","[{'version': 'v1', 'created': 'Wed, 17 May 2023 12:31:26 GMT'}]",2023-05-18,"[['Wu', 'Jiageng', ''], ['Wu', 'Xian', ''], ['Qiu', 'Zhaopeng', ''], ['Li', 'Minghui', ''], ['Zheng', 'Yefeng', ''], ['Yang', 'Jie', '']]",1,1,2023-05-17,1,6,3,1,0,1,bc931ed644c3ac7b89c480966f91f525d4075b29,258741378.0,https://www.semanticscholar.org/paper/bc931ed644c3ac7b89c480966f91f525d4075b29,arXiv.org,2023.0,43.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2144128260', 'name': 'Jiageng Wu'}, {'authorId': '144620591', 'name': 'X. Wu'}, {'authorId': '2382872', 'name': 'Zhaopeng Qiu'}, {'authorId': '2186950276', 'name': 'Minghui Li'}, {'authorId': '2179980880', 'name': 'Yefeng Zheng'}, {'authorId': '1688428', 'name': 'Jie Yang'}]","['Tencent', 'Zhejiang University']",['China'],2023-05
2305.10196,Longyue Wang,"Longyue Wang, Siyou Liu, Mingzhou Xu, Linfeng Song, Shuming Shi,
  Zhaopeng Tu",A Survey on Zero Pronoun Translation,"ACL2023 Main Conference Long Paper. Longyue Wang and Siyou Liu
  contributed equally to this work",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Zero pronouns (ZPs) are frequently omitted in pro-drop languages (e.g. Chinese, Hungarian, and Hindi), but should be recalled in non-pro-drop languages (e.g. English). This phenomenon has been studied extensively in machine translation (MT), as it poses a significant challenge for MT systems due to the difficulty in determining the correct antecedent for the pronoun. This survey paper highlights the major works that have been undertaken in zero pronoun translation (ZPT) after the neural revolution, so that researchers can recognise the current state and future directions of this field. We provide an organisation of the literature based on evolution, dataset, method and evaluation. In addition, we compare and analyze competing models and evaluation metrics on different benchmarks. We uncover a number of insightful findings such as: 1) ZPT is in line with the development trend of large language model; 2) data limitation causes learning bias in languages and domains; 3) performance improvements are often reported on single benchmarks, but advanced methods are still far from real-world use; 4) general-purpose metrics are not reliable on nuances and complexities of ZPT, emphasizing the necessity of targeted metrics; 5) apart from commonly-cited errors, ZPs will cause risks of gender bias. ","[{'version': 'v1', 'created': 'Wed, 17 May 2023 13:19:01 GMT'}]",2023-05-18,"[['Wang', 'Longyue', ''], ['Liu', 'Siyou', ''], ['Xu', 'Mingzhou', ''], ['Song', 'Linfeng', ''], ['Shi', 'Shuming', ''], ['Tu', 'Zhaopeng', '']]",0,0,2023-05-17,1,6,2,0,0,0,8fa265a8ca46c9c0ad35d2c5b519a86e2a092dfe,258740687.0,https://www.semanticscholar.org/paper/8fa265a8ca46c9c0ad35d2c5b519a86e2a092dfe,Annual Meeting of the Association for Computational Linguistics,2023.0,77.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1800190', 'name': 'Longyue Wang'}, {'authorId': '3417566', 'name': 'Siyou Liu'}, {'authorId': '97375395', 'name': 'Mingzhou Xu'}, {'authorId': '1748796', 'name': 'Linfeng Song'}, {'authorId': '2072684668', 'name': 'Shuming Shi'}, {'authorId': '2909321', 'name': 'Zhaopeng Tu'}]",['Tencent'],['China'],2023-05
2305.10235,Wentao Ye,"Wentao Ye, Mingfeng Ou, Tianyi Li, Yipeng chen, Xuetao Ma, Yifan
  Yanggong, Sai Wu, Jie Fu, Gang Chen, Haobo Wang, Junbo Zhao","Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility",,,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The recent popularity of large language models (LLMs) has brought a significant impact to boundless fields, particularly through their open-ended ecosystem such as the APIs, open-sourced models, and plugins. However, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed. In that case, we intend to conduct a preliminary but pioneering study covering the robustness, consistency, and credibility of LLMs systems. With most of the related literature in the era of LLM uncharted, we propose an automated workflow that copes with an upscaled number of queries/responses. Overall, we conduct over a million queries to the mainstream LLMs including ChatGPT, LLaMA, and OPT. Core to our workflow consists of a data primitive, followed by an automated interpreter that evaluates these LLMs under different adversarial metrical systems. As a result, we draw several, and perhaps unfortunate, conclusions that are quite uncommon from this trendy community. Briefly, they are: (i)-the minor but inevitable error occurrence in the user-generated query input may, by chance, cause the LLM to respond unexpectedly; (ii)-LLMs possess poor consistency when processing semantically similar query input. In addition, as a side finding, we find that ChatGPT is still capable to yield the correct answer even when the input is polluted at an extreme level. While this phenomenon demonstrates the powerful memorization of the LLMs, it raises serious concerns about using such data for LLM-involved evaluation in academic development. To deal with it, we propose a novel index associated with a dataset that roughly decides the feasibility of using such data for LLM-involved evaluation. Extensive empirical studies are tagged to support the aforementioned claims. ","[{'version': 'v1', 'created': 'Mon, 15 May 2023 15:44:51 GMT'}, {'version': 'v2', 'created': 'Thu, 18 May 2023 06:56:29 GMT'}, {'version': 'v3', 'created': 'Thu, 25 May 2023 05:01:59 GMT'}, {'version': 'v4', 'created': 'Wed, 30 Aug 2023 04:32:36 GMT'}]",2023-08-31,"[['Ye', 'Wentao', ''], ['Ou', 'Mingfeng', ''], ['Li', 'Tianyi', ''], ['chen', 'Yipeng', ''], ['Ma', 'Xuetao', ''], ['Yanggong', 'Yifan', ''], ['Wu', 'Sai', ''], ['Fu', 'Jie', ''], ['Chen', 'Gang', ''], ['Wang', 'Haobo', ''], ['Zhao', 'Junbo', '']]",1,1,2023-05-15,4,11,2,3,2,1,6522817fa47155c6b45b79ba9318e7dd7179f526,258741105.0,https://www.semanticscholar.org/paper/6522817fa47155c6b45b79ba9318e7dd7179f526,arXiv.org,2023.0,64.0,7.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2052980502', 'name': 'Wen-song Ye'}, {'authorId': '2051291251', 'name': 'Mingfeng Ou'}, {'authorId': '2162633457', 'name': 'Tianyi Li'}, {'authorId': '2154566821', 'name': 'Yipeng Chen'}, {'authorId': '2217399122', 'name': 'Xuetao Ma'}, {'authorId': '2800943', 'name': 'Yifan YangGong'}, {'authorId': '2149344649', 'name': 'Sai Wu'}, {'authorId': '2215497308', 'name': 'Jie Fu'}, {'authorId': '2163431247', 'name': 'Gang Chen'}, {'authorId': '2108909691', 'name': 'Haobo Wang'}, {'authorId': '7818229', 'name': 'J. Zhao'}]","['ZhongHao XinYing (Hangzhou) Technology Co., Ltd.', 'Beijing Academy of Artificial Intelligence', 'Zhejiang University']",['China'],2023-05
2305.10263,Yuqi Ren,"Chuang Liu, Renren Jin, Yuqi Ren, Linhao Yu, Tianyu Dong, Xiaohan
  Peng, Shuting Zhang, Jianxiang Peng, Peiyi Zhang, Qingqing Lyu, Xiaowen Su,
  Qun Liu, Deyi Xiong",M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models have recently made tremendous progress in a variety of aspects, e.g., cross-task generalization, instruction following. Comprehensively evaluating the capability of large language models in multiple tasks is of great importance. In this paper, we propose M3KE, a Massive Multi-Level Multi-Subject Knowledge Evaluation benchmark, which is developed to measure knowledge acquired by Chinese large language models by testing their multitask accuracy in zero- and few-shot settings. We have collected 20,477 questions from 71 tasks. Our selection covers all major levels of Chinese education system, ranging from the primary school to college, as well as a wide variety of subjects, including humanities, history, politics, law, education, psychology, science, technology, art and religion. All questions are multiple-choice questions with four options, hence guaranteeing a standardized and unified assessment process. We've assessed a number of state-of-the-art open-source Chinese large language models on the proposed benchmark. The size of these models varies from 335M to 130B parameters. Experiment results demonstrate that they perform significantly worse than GPT-3.5 that reaches an accuracy of ~ 48% on M3KE. The dataset is available at https://github.com/tjunlp-lab/M3KE. ","[{'version': 'v1', 'created': 'Wed, 17 May 2023 14:56:31 GMT'}, {'version': 'v2', 'created': 'Sun, 21 May 2023 03:57:11 GMT'}]",2023-05-23,"[['Liu', 'Chuang', ''], ['Jin', 'Renren', ''], ['Ren', 'Yuqi', ''], ['Yu', 'Linhao', ''], ['Dong', 'Tianyu', ''], ['Peng', 'Xiaohan', ''], ['Zhang', 'Shuting', ''], ['Peng', 'Jianxiang', ''], ['Zhang', 'Peiyi', ''], ['Lyu', 'Qingqing', ''], ['Su', 'Xiaowen', ''], ['Liu', 'Qun', ''], ['Xiong', 'Deyi', '']]",0,1,2023-05-17,2,13,1,1,0,1,25f729d7773614846b412db3c6c2a3aab41ec409,258740849.0,https://www.semanticscholar.org/paper/25f729d7773614846b412db3c6c2a3aab41ec409,arXiv.org,2023.0,58.0,10.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2116348128', 'name': 'Chuang Liu'}, {'authorId': '2184143149', 'name': 'Renren Jin'}, {'authorId': '152321611', 'name': 'Yuqi Ren'}, {'authorId': '2217579802', 'name': 'Linhao Yu'}, {'authorId': '2061034170', 'name': 'Tianyu Dong'}, {'authorId': '2212708010', 'name': 'Xia Peng'}, {'authorId': '2217573295', 'name': 'Shuting Zhang'}, {'authorId': '2122808692', 'name': 'Jianxiang Peng'}, {'authorId': '2218678262', 'name': 'Peiyi Zhang'}, {'authorId': '2217568073', 'name': 'Qingqing Lyu'}, {'authorId': '2217865116', 'name': 'Xiaowen Su'}, {'authorId': '1688015', 'name': 'Qun Liu'}, {'authorId': '2694222', 'name': 'Deyi Xiong'}]","['Huawei Technologies (China)', 'Tianjin University']",['China'],2023-05
2305.10276,Hongyuan Lu,"Hanxu Hu, Hongyuan Lu, Huajian Zhang, Yun-Ze Song, Wai Lam, Yue Zhang",Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we take the initiative to investigate the performance of LLMs on complex planning tasks that require LLMs to understand a virtual spatial environment simulated via natural language and act correspondingly in text. We propose a benchmark named Natural Language Planning and Action (Natala) composed of a set of novel tasks: Brick World, NLVR-based Manipulations, and Natural Language Navigation. We found that current popular LLMs such as ChatGPT still lack abilities in complex planning. This arises a question -- do the LLMs have a good understanding of the environments described in natural language, or maybe other alternatives such as symbolic representations are neater and hence better to be understood by LLMs? To this end, we propose a novel method called CoS (Chain-of-Symbol Prompting) that represents the complex environments with condensed symbolic spatial representations during the chained intermediate thinking steps. CoS is easy to use and does not need additional training on LLMs. Extensive experiments indicate that CoS clearly surpasses the performance of the Chain-of-Thought (CoT) Prompting in all three planning tasks with even fewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT. The performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%) on Brick World for ChatGPT. CoS also reduces the number of tokens in the prompt obviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate steps from demonstrations on Brick World. Code and data available at: https://github.com/hanxuhu/chain-of-symbol-planning ","[{'version': 'v1', 'created': 'Wed, 17 May 2023 15:07:50 GMT'}, {'version': 'v2', 'created': 'Tue, 23 May 2023 15:03:14 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Jun 2023 15:15:22 GMT'}, {'version': 'v4', 'created': 'Sat, 10 Jun 2023 08:14:06 GMT'}, {'version': 'v5', 'created': 'Mon, 26 Jun 2023 06:50:03 GMT'}, {'version': 'v6', 'created': 'Wed, 4 Oct 2023 02:03:45 GMT'}]",2023-10-05,"[['Hu', 'Hanxu', ''], ['Lu', 'Hongyuan', ''], ['Zhang', 'Huajian', ''], ['Song', 'Yun-Ze', ''], ['Lam', 'Wai', ''], ['Zhang', 'Yue', '']]",1,1,2023-05-17,6,6,1,2,0,2,e793f1434f1efdb34bad9541975d2b48fa73aa46,263611183.0,https://www.semanticscholar.org/paper/e793f1434f1efdb34bad9541975d2b48fa73aa46,,2023.0,25.0,0.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2254179604', 'name': 'Hanxu Hu'}, {'authorId': '2156273800', 'name': 'Hongyuan Lu'}, {'authorId': '2217912664', 'name': 'Huajian Zhang'}, {'authorId': '2256682106', 'name': 'Yunze Song'}, {'authorId': '2253479907', 'name': 'Wai Lam'}, {'authorId': '2256757428', 'name': 'Yue Zhang'}]","['Westlake University', 'Chinese University of Hong Kong', 'University of Edinburgh']","['China', 'United Kingdom']",2023-05
2305.10355,Yifan Li,"Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao and Ji-Rong
  Wen",Evaluating Object Hallucination in Large Vision-Language Models,Work in progress,,,,cs.CV cs.CL cs.MM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently explored by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that LVLMs suffer from the hallucination problem, i.e. they tend to generate objects that are inconsistent with the target images in the descriptions. To investigate it, this work presents the first systematic study on object hallucination of LVLMs. We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issue. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently occur in the visual instructions or co-occur with the image objects, are obviously prone to be hallucinated by LVLMs. Besides, we find that existing evaluation methods might be affected by the input instructions and generation styles of LVLMs. Thus, we further design an improved evaluation method for object hallucination by proposing a polling-based query method called POPE. Experiment results demonstrate that our POPE can evaluate the object hallucination in a more stable and flexible way. Our codes and data are publicly available at https://github.com/RUCAIBox/POPE. ","[{'version': 'v1', 'created': 'Wed, 17 May 2023 16:34:01 GMT'}, {'version': 'v2', 'created': 'Tue, 23 May 2023 11:05:11 GMT'}]",2023-05-24,"[['Li', 'Yifan', ''], ['Du', 'Yifan', ''], ['Zhou', 'Kun', ''], ['Wang', 'Jinpeng', ''], ['Zhao', 'Wayne Xin', ''], ['Wen', 'Ji-Rong', '']]",0,0,2023-05-17,2,6,3,0,0,0,206400aba5f12f734cdd2e4ab48ef6014ea60773,258740697.0,https://www.semanticscholar.org/paper/206400aba5f12f734cdd2e4ab48ef6014ea60773,arXiv.org,2023.0,56.0,52.0,15.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2209136299', 'name': 'Yifan Li'}, {'authorId': '2111895473', 'name': 'Yifan Du'}, {'authorId': '1423651904', 'name': 'Kun Zhou'}, {'authorId': '1823719', 'name': 'Jinpeng Wang'}, {'authorId': '2542603', 'name': 'Wayne Xin Zhao'}, {'authorId': '153693432', 'name': 'Ji-rong Wen'}]","['Renmin University of China', 'Meituan', 'Beijing Key Laboratory of Big Data Management and Analysis Methods']",['China'],2023-05
2305.10415,Xiaoman Zhang,"Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng
  Wang, Weidi Xie",PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering,,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we focus on the problem of Medical Visual Question Answering (MedVQA), which is crucial in efficiently interpreting medical images with vital clinic-relevant information. Firstly, we reframe the problem of MedVQA as a generation task that naturally follows the human-machine interaction, we propose a generative-based model for medical visual understanding by aligning visual information from a pre-trained vision encoder with a large language model. Secondly, we establish a scalable pipeline to construct a large-scale medical visual question-answering dataset, named PMC-VQA, which contains 227k VQA pairs of 149k images that cover various modalities or diseases. Thirdly, we pre-train our proposed model on PMC-VQA and then fine-tune it on multiple public benchmarks, e.g., VQA-RAD and SLAKE, outperforming existing work by a large margin. Additionally, we propose a test set that has undergone manual verification, which is significantly more challenging, even the best models struggle to solve. ","[{'version': 'v1', 'created': 'Wed, 17 May 2023 17:50:16 GMT'}, {'version': 'v2', 'created': 'Thu, 18 May 2023 12:08:10 GMT'}, {'version': 'v3', 'created': 'Fri, 19 May 2023 04:30:00 GMT'}, {'version': 'v4', 'created': 'Wed, 24 May 2023 15:35:13 GMT'}, {'version': 'v5', 'created': 'Mon, 29 May 2023 12:23:21 GMT'}]",2023-05-30,"[['Zhang', 'Xiaoman', ''], ['Wu', 'Chaoyi', ''], ['Zhao', 'Ziheng', ''], ['Lin', 'Weixiong', ''], ['Zhang', 'Ya', ''], ['Wang', 'Yanfeng', ''], ['Xie', 'Weidi', '']]",0,0,2023-05-17,5,7,1,0,0,0,f4793adffd6f67ffcb93ccfc5672ab301b8a2b96,258741360.0,https://www.semanticscholar.org/paper/f4793adffd6f67ffcb93ccfc5672ab301b8a2b96,arXiv.org,2023.0,38.0,19.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '46448559', 'name': 'Xiaoman Zhang'}, {'authorId': '2146289861', 'name': 'Chaoyi Wu'}, {'authorId': '2117927802', 'name': 'Ziheng Zhao'}, {'authorId': '2113563334', 'name': 'Weixiong Lin'}, {'authorId': '46868037', 'name': 'Ya Zhang'}, {'authorId': '2108846176', 'name': 'Yanfeng Wang'}, {'authorId': '10096695', 'name': 'Weidi Xie'}]","['Shanghai Jiao Tong University', 'Punjab Medical College']","['China', 'Pakistan']",2023-05
2305.10435,Gadekallu Thippa Reddy,"Gokul Yenduri, Ramalingam M, Chemmalar Selvi G, Supriya Y, Gautam
  Srivastava, Praveen Kumar Reddy Maddikunta, Deepti Raj G, Rutvij H Jhaveri,
  Prabadevi B, Weizheng Wang, Athanasios V. Vasilakos, and Thippa Reddy
  Gadekallu","Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions",Submitted to peer review,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  The Generative Pre-trained Transformer (GPT) represents a notable breakthrough in the domain of natural language processing, which is propelling us toward the development of machines that can understand and communicate using language in a manner that closely resembles that of humans. GPT is based on the transformer architecture, a deep neural network designed for natural language processing tasks. Due to their impressive performance on natural language processing tasks and ability to effectively converse, GPT have gained significant popularity among researchers and industrial communities, making them one of the most widely used and effective models in natural language processing and related fields, which motivated to conduct this review. This review provides a detailed overview of the GPT, including its architecture, working process, training procedures, enabling technologies, and its impact on various applications. In this review, we also explored the potential challenges and limitations of a GPT. Furthermore, we discuss potential solutions and future directions. Overall, this paper aims to provide a comprehensive understanding of GPT, enabling technologies, their impact on various applications, emerging challenges, and potential solutions. ","[{'version': 'v1', 'created': 'Thu, 11 May 2023 19:20:38 GMT'}, {'version': 'v2', 'created': 'Sun, 21 May 2023 10:12:02 GMT'}]",2023-05-23,"[['Yenduri', 'Gokul', ''], ['M', 'Ramalingam', ''], ['G', 'Chemmalar Selvi', ''], ['Y', 'Supriya', ''], ['Srivastava', 'Gautam', ''], ['Maddikunta', 'Praveen Kumar Reddy', ''], ['G', 'Deepti Raj', ''], ['Jhaveri', 'Rutvij H', ''], ['B', 'Prabadevi', ''], ['Wang', 'Weizheng', ''], ['Vasilakos', 'Athanasios V.', ''], ['Gadekallu', 'Thippa Reddy', '']]",0,1,2023-05-11,2,12,2,0,0,0,39831c8c222a8d78fff4d67e7e56f5eeb90fdd7f,258762263.0,https://www.semanticscholar.org/paper/39831c8c222a8d78fff4d67e7e56f5eeb90fdd7f,arXiv.org,2023.0,240.0,8.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '151453737', 'name': 'Gokul Yenduri'}, {'authorId': '143776269', 'name': 'M. Ramalingam'}, {'authorId': '2087150326', 'name': 'G. ChemmalarSelvi'}, {'authorId': '72286325', 'name': 'Y. Supriya'}, {'authorId': '2176030144', 'name': 'Gautam Srivastava'}, {'authorId': '29919430', 'name': 'Praveen Kumar Reddy Maddikunta'}, {'authorId': '2217757765', 'name': 'G. DeeptiRaj'}, {'authorId': '3314907', 'name': 'R. Jhaveri'}, {'authorId': '3383951', 'name': 'B. Prabadevi'}, {'authorId': '2108438707', 'name': 'Weizheng Wang'}, {'authorId': '2105406061', 'name': 'Athanasios V. Vasilakos'}, {'authorId': '11041265', 'name': 'T. Gadekallu'}]","['Lovely Professional University', 'Research Centre for Interneural Computing,', 'Pandit Deendayal Petroleum University', 'Jiaxing University', 'Zhongda Group, China, 314312', 'China Medical University', 'City University of Hong Kong', 'Lebanese American University', 'University of Agder', 'Brandon University', 'Vellore Institute of Technology University']","['Canada', 'Norway', 'Lebanon', 'India', 'China']",2023-05
2305.10679,Xin-Ye Li,"Xin-Ye Li, Jiang-Tian Xue, Zheng Xie and Ming Li",Think Outside the Code: Brainstorming Boosts Large Language Models in Code Generation,"13 pages, 5 figures",,,,cs.AI cs.CL cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Code generation aims to automatically generate source code from high-level task specifications, which can significantly increase productivity of software engineering. Recently, approaches based on large language models (LLMs) have shown remarkable code generation abilities on simple tasks. However, generate code for more complex tasks, such as competition-level problems, remains challenging. In this paper, we introduce Brainstorm framework for code generation. It leverages a brainstorming step that generates and selects diverse thoughts on the problem to facilitate algorithmic reasoning, where the thoughts are possible blueprint of solving the problem. We demonstrate that Brainstorm significantly enhances the ability of LLMs to solve competition-level programming problems, resulting in a more than 50% increase in the pass@$k$ metrics for ChatGPT on the CodeContests benchmark, achieving state-of-the-art performance. Furthermore, our experiments conducted on LeetCode contests show that our framework boosts the ability of ChatGPT to a level comparable to that of human programmers. ","[{'version': 'v1', 'created': 'Thu, 18 May 2023 03:32:54 GMT'}]",2023-05-19,"[['Li', 'Xin-Ye', ''], ['Xue', 'Jiang-Tian', ''], ['Xie', 'Zheng', ''], ['Li', 'Ming', '']]",1,1,2023-05-18,1,4,3,1,0,1,d53f4cdddd7494d5f6e64d81f627691f6d7dff95,258762167.0,https://www.semanticscholar.org/paper/d53f4cdddd7494d5f6e64d81f627691f6d7dff95,arXiv.org,2023.0,46.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108481995', 'name': 'Xinyu Li'}, {'authorId': '2217573580', 'name': 'Jiang-Tian Xue'}, {'authorId': '2114115114', 'name': 'Zheng Xie'}, {'authorId': '35834541', 'name': 'Ming Li'}]",['Nanjing University'],['China'],2023-05
2305.10688,Yingce Xia,"Zequn Liu, Wei Zhang, Yingce Xia, Lijun Wu, Shufang Xie, Tao Qin, Ming
  Zhang and Tie-Yan Liu",MolXPT: Wrapping Molecules with Text for Generative Pre-training,Accepted to ACL 2023; add more details about MoleculeNet finetune,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generative pre-trained Transformer (GPT) has demonstrates its great success in natural language processing and related techniques have been adapted into molecular modeling. Considering that text is the most important record for scientific discovery, in this paper, we propose MolXPT, a unified language model of text and molecules pre-trained on SMILES (a sequence representation of molecules) wrapped by text. Briefly, we detect the molecule names in each sequence and replace them to the corresponding SMILES. In this way, the SMILES could leverage the information from surrounding text, and vice versa. The above wrapped sequences, text sequences from PubMed and SMILES sequences from PubChem are all fed into a language model for pre-training. Experimental results demonstrate that MolXPT outperforms strong baselines of molecular property prediction on MoleculeNet, performs comparably to the best model in text-molecule translation while using less than half of its parameters, and enables zero-shot molecular generation without finetuning. ","[{'version': 'v1', 'created': 'Thu, 18 May 2023 03:58:19 GMT'}, {'version': 'v2', 'created': 'Fri, 26 May 2023 04:35:46 GMT'}]",2023-05-29,"[['Liu', 'Zequn', ''], ['Zhang', 'Wei', ''], ['Xia', 'Yingce', ''], ['Wu', 'Lijun', ''], ['Xie', 'Shufang', ''], ['Qin', 'Tao', ''], ['Zhang', 'Ming', ''], ['Liu', 'Tie-Yan', '']]",0,1,2023-05-18,2,8,1,0,0,0,ed1353d705eeabc0e916caba5fbae890eefe4f84,258762343.0,https://www.semanticscholar.org/paper/ed1353d705eeabc0e916caba5fbae890eefe4f84,Annual Meeting of the Association for Computational Linguistics,2023.0,49.0,7.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2109372136', 'name': 'Zequn Liu'}, {'authorId': '143715293', 'name': 'W. Zhang'}, {'authorId': '2111056280', 'name': 'Yingce Xia'}, {'authorId': '47767550', 'name': 'Lijun Wu'}, {'authorId': '1889683', 'name': 'Shufang Xie'}, {'authorId': '143826491', 'name': 'Tao Qin'}, {'authorId': '2215481115', 'name': 'M. Zhang'}, {'authorId': '2110264337', 'name': 'Tie-Yan Liu'}]","['Peking University', 'University of Science and Technology of China', 'Microsoft', 'Renmin University of China']","['China', 'Netherlands', 'United Kingdom']",2023-05
2305.10799,Qiuhui Chen,"Qiuhui Chen, Xinyue Hu, Zirui Wang, Yi Hong",MedBLIP: Bootstrapping Language-Image Pre-training from 3D Medical Images and Texts,"11 pages, 3 figures",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Vision-language pre-training (VLP) models have been demonstrated to be effective in many computer vision applications. In this paper, we consider developing a VLP model in the medical domain for making computer-aided diagnoses (CAD) based on image scans and text descriptions in electronic health records, as done in practice. To achieve our goal, we present a lightweight CAD system MedBLIP, a new paradigm for bootstrapping VLP from off-the-shelf frozen pre-trained image encoders and frozen large language models. We design a MedQFormer module to bridge the gap between 3D medical images and 2D pre-trained image encoders and language models as well. To evaluate the effectiveness of our MedBLIP, we collect more than 30,000 image volumes from five public Alzheimer's disease (AD) datasets, i.e., ADNI, NACC, OASIS, AIBL, and MIRIAD. On this largest AD dataset we know, our model achieves the SOTA performance on the zero-shot classification of healthy, mild cognitive impairment (MCI), and AD subjects, and shows its capability of making medical visual question answering (VQA). The code and pre-trained models is available online: https://github.com/Qybc/MedBLIP. ","[{'version': 'v1', 'created': 'Thu, 18 May 2023 08:19:33 GMT'}]",2023-05-19,"[['Chen', 'Qiuhui', ''], ['Hu', 'Xinyue', ''], ['Wang', 'Zirui', ''], ['Hong', 'Yi', '']]",0,0,2023-05-18,1,4,1,0,0,0,daf34122a0c38531aeeb55069ba98e564c263d53,258762408.0,https://www.semanticscholar.org/paper/daf34122a0c38531aeeb55069ba98e564c263d53,arXiv.org,2023.0,35.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Medicine', 'source': 's2-fos-model'}]","[{'authorId': '2109626032', 'name': 'Qiuhui Chen'}, {'authorId': '2110049947', 'name': 'Xinyue Hu'}, {'authorId': None, 'name': 'Zirui Wang'}, {'authorId': '2115351722', 'name': 'Yi Hong'}]",['Shanghai Jiao Tong University'],['China'],2023-05
2305.10843,Yixiong Chen,"Yixiong Chen, Li Liu, Chris Ding",X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models,"18 pages, 6 tables, 11 figures, NeurIPS 2023 submission",,,,cs.CV cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  This paper introduces a novel explainable image quality evaluation approach called X-IQE, which leverages visual large language models (LLMs) to evaluate text-to-image generation methods by generating textual explanations. X-IQE utilizes a hierarchical Chain of Thought (CoT) to enable MiniGPT-4 to produce self-consistent, unbiased texts that are highly correlated with human evaluation. It offers several advantages, including the ability to distinguish between real and generated images, evaluate text-image alignment, and assess image aesthetics without requiring model training or fine-tuning. X-IQE is more cost-effective and efficient compared to human evaluation, while significantly enhancing the transparency and explainability of deep image quality evaluation models. We validate the effectiveness of our method as a benchmark using images generated by prevalent diffusion models. X-IQE demonstrates similar performance to state-of-the-art (SOTA) evaluation methods on COCO Caption, while overcoming the limitations of previous evaluation models on DrawBench, particularly in handling ambiguous generation prompts and text recognition in generated images. Project website: https://github.com/Schuture/Benchmarking-Awesome-Diffusion-Models ","[{'version': 'v1', 'created': 'Thu, 18 May 2023 09:56:44 GMT'}, {'version': 'v2', 'created': 'Fri, 26 May 2023 02:01:54 GMT'}]",2023-05-29,"[['Chen', 'Yixiong', ''], ['Liu', 'Li', ''], ['Ding', 'Chris', '']]",0,1,2023-05-18,2,3,2,0,0,0,a979975d1a0aea0e01423f092249cc3de575b6cd,258762871.0,https://www.semanticscholar.org/paper/a979975d1a0aea0e01423f092249cc3de575b6cd,arXiv.org,2023.0,56.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2137372287', 'name': 'Yixiong Chen'}, {'authorId': '2150978242', 'name': 'Li Liu'}, {'authorId': '1737469', 'name': 'C. Ding'}]",['Hong Kong University of Science and Technology'],['China'],2023-05
2305.10847,Shengcai Liu,"Ning Lu, Shengcai Liu, Rui He, Qi Wang, Ke Tang",Large Language Models can be Guided to Evade AI-Generated Text Detection,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have demonstrated exceptional performance in a variety of tasks, including essay writing and question answering. However, it is crucial to address the potential misuse of these models, which can lead to detrimental outcomes such as plagiarism and spamming. Recently, several detectors have been proposed, including fine-tuned classifiers and various statistical methods. In this study, we reveal that with the aid of carefully crafted prompts, LLMs can effectively evade these detection systems. We propose a novel Substitution-based In-Context example Optimization method (SICO) to automatically generate such prompts. On three real-world tasks where LLMs can be misused, SICO successfully enables ChatGPT to evade six existing detectors, causing a significant 0.54 AUC drop on average. Surprisingly, in most cases these detectors perform even worse than random classifiers. These results firmly reveal the vulnerability of existing detectors. Finally, the strong performance of SICO suggests itself as a reliable evaluation protocol for any new detector in this field. ","[{'version': 'v1', 'created': 'Thu, 18 May 2023 10:03:25 GMT'}, {'version': 'v2', 'created': 'Fri, 19 May 2023 11:25:01 GMT'}, {'version': 'v3', 'created': 'Mon, 5 Jun 2023 03:54:52 GMT'}, {'version': 'v4', 'created': 'Sat, 17 Jun 2023 03:48:41 GMT'}]",2023-06-21,"[['Lu', 'Ning', ''], ['Liu', 'Shengcai', ''], ['He', 'Rui', ''], ['Wang', 'Qi', ''], ['Tang', 'Ke', '']]",1,1,2023-05-18,4,5,2,1,0,1,28702537cb02b0ed49a55c93e95bea6f2b091525,258762215.0,https://www.semanticscholar.org/paper/28702537cb02b0ed49a55c93e95bea6f2b091525,arXiv.org,2023.0,44.0,9.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2147128045', 'name': 'Ning Lu'}, {'authorId': '2152939552', 'name': 'Shengcai Liu'}, {'authorId': '22272507', 'name': 'Ruidan He'}, {'authorId': '2078692567', 'name': 'Ke Tang'}]","['Hong Kong University of Science and Technology', 'Southern University of Science and Technology', 'Agency for Science, Technology and Research', 'Nanyang Technological University']","['China', 'Singapore']",2023-05
2305.10998,Junyi Li,"Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jingyuan Wang, Jian-Yun Nie and
  Ji-Rong Wen",The Web Can Be Your Oyster for Improving Large Language Models,"This work has been accepted by ACL 2023 (findings), while we slightly
  revise the title and the content based on the conference version",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) encode a large amount of world knowledge. However, as such knowledge is frozen at the time of model training, the models become static and limited by the training data at that time. In order to further improve the capacity of LLMs for knowledge-intensive tasks, we consider augmenting LLMs with the large-scale web using search engine. Unlike previous augmentation sources (e.g., Wikipedia data dump), the web provides broader, more comprehensive and constantly updated information. In this paper, we present a web-augmented LLM UNIWEB, which is trained over 16 knowledge-intensive tasks in a unified text-to-text format. Instead of simply using the retrieved contents from web, our approach has made two major improvements. Firstly, we propose an adaptive search engine assisted learning method that can self-evaluate the confidence level of LLM's predictions, and adaptively determine when to refer to the web for more data, which can avoid useless or noisy augmentation from web. Secondly, we design a pretraining task, i.e., continual knowledge learning, based on salient spans prediction, to reduce the discrepancy between the encoded and retrieved knowledge. Experiments on a wide range of knowledge-intensive tasks show that our model significantly outperforms previous retrieval-augmented methods. ","[{'version': 'v1', 'created': 'Thu, 18 May 2023 14:20:32 GMT'}, {'version': 'v2', 'created': 'Wed, 24 May 2023 09:35:39 GMT'}]",2023-05-25,"[['Li', 'Junyi', ''], ['Tang', 'Tianyi', ''], ['Zhao', 'Wayne Xin', ''], ['Wang', 'Jingyuan', ''], ['Nie', 'Jian-Yun', ''], ['Wen', 'Ji-Rong', '']]",0,0,2023-05-18,2,6,1,0,0,0,88b4ed17e8881b4e420627f3650ddab181bb632b,258762384.0,https://www.semanticscholar.org/paper/88b4ed17e8881b4e420627f3650ddab181bb632b,arXiv.org,2023.0,68.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2018027', 'name': 'Junyi Li'}, {'authorId': '1997234792', 'name': 'Tianyi Tang'}, {'authorId': '2542603', 'name': 'Wayne Xin Zhao'}, {'authorId': '2115891766', 'name': 'Jingyuan Wang'}, {'authorId': '50204644', 'name': 'J. Nie'}, {'authorId': '153693432', 'name': 'Ji-rong Wen'}]","['Beijing Key Laboratory of Big Data Management and Analysis Methods', 'Beihang University', 'Universit de Montral', 'Renmin University of China']","['China', 'Canada']",2023-05
2305.11014,Tom Silver,"Tom Silver, Soham Dan, Kavitha Srinivas, Joshua B. Tenenbaum, Leslie
  Pack Kaelbling, Michael Katz",Generalized Planning in PDDL Domains with Pretrained Large Language Models,,,,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Recent work has considered whether large language models (LLMs) can function as planners: given a task, generate a plan. We investigate whether LLMs can serve as generalized planners: given a domain and training tasks, generate a program that efficiently produces plans for other tasks in the domain. In particular, we consider PDDL domains and use GPT-4 to synthesize Python programs. We also consider (1) Chain-of-Thought (CoT) summarization, where the LLM is prompted to summarize the domain and propose a strategy in words before synthesizing the program; and (2) automated debugging, where the program is validated with respect to the training tasks, and in case of errors, the LLM is re-prompted with four types of feedback. We evaluate this approach in seven PDDL domains and compare it to four ablations and four baselines. Overall, we find that GPT-4 is a surprisingly powerful generalized planner. We also conclude that automated debugging is very important, that CoT summarization has non-uniform impact, that GPT-4 is far superior to GPT-3.5, and that just two training tasks are often sufficient for strong generalization. ","[{'version': 'v1', 'created': 'Thu, 18 May 2023 14:48:20 GMT'}]",2023-05-19,"[['Silver', 'Tom', ''], ['Dan', 'Soham', ''], ['Srinivas', 'Kavitha', ''], ['Tenenbaum', 'Joshua B.', ''], ['Kaelbling', 'Leslie Pack', ''], ['Katz', 'Michael', '']]",0,1,2023-05-18,1,6,1,2,0,2,b3bba15f000000a6d3b5808f798a9fe7629fa499,258762760.0,https://www.semanticscholar.org/paper/b3bba15f000000a6d3b5808f798a9fe7629fa499,arXiv.org,2023.0,58.0,14.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '39047272', 'name': 'Tom Silver'}, {'authorId': '3420683', 'name': 'Soham Dan'}, {'authorId': '145993352', 'name': 'Kavitha Srinivas'}, {'authorId': '1763295', 'name': 'J. Tenenbaum'}, {'authorId': '1709512', 'name': 'L. Kaelbling'}, {'authorId': '144801999', 'name': 'Michael Katz'}]","['IBM Research - China', 'MIT Computer Science and Artificial Intelligence Laboratory;']",['China'],2023-05
2305.11175,Wenhai Wang,"Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang
  Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, Jifeng Dai",VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks,Technical Report,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) have notably accelerated progress towards artificial general intelligence (AGI), with their impressive zero-shot capacity for user-tailored tasks, endowing them with immense potential across a range of applications. However, in the field of computer vision, despite the availability of numerous powerful vision foundation models (VFMs), they are still restricted to tasks in a pre-defined form, struggling to match the open-ended task capabilities of LLMs. In this work, we present an LLM-based framework for vision-centric tasks, termed VisionLLM. This framework provides a unified perspective for vision and language tasks by treating images as a foreign language and aligning vision-centric tasks with language tasks that can be flexibly defined and managed using language instructions. An LLM-based decoder can then make appropriate predictions based on these instructions for open-ended tasks. Extensive experiments show that the proposed VisionLLM can achieve different levels of task customization through language instructions, from fine-grained object-level to coarse-grained task-level customization, all with good results. It's noteworthy that, with a generalist LLM-based framework, our model can achieve over 60\% mAP on COCO, on par with detection-specific models. We hope this model can set a new baseline for generalist vision and language models. The demo shall be released based on https://github.com/OpenGVLab/InternGPT. The code shall be released at https://github.com/OpenGVLab/VisionLLM. ","[{'version': 'v1', 'created': 'Thu, 18 May 2023 17:59:42 GMT'}, {'version': 'v2', 'created': 'Thu, 25 May 2023 15:02:07 GMT'}]",2023-05-26,"[['Wang', 'Wenhai', ''], ['Chen', 'Zhe', ''], ['Chen', 'Xiaokang', ''], ['Wu', 'Jiannan', ''], ['Zhu', 'Xizhou', ''], ['Zeng', 'Gang', ''], ['Luo', 'Ping', ''], ['Lu', 'Tong', ''], ['Zhou', 'Jie', ''], ['Qiao', 'Yu', ''], ['Dai', 'Jifeng', '']]",0,1,2023-05-18,2,11,1,0,0,0,42a30dc5470f54ec249f25d3c31e05d7c376c8e3,258762579.0,https://www.semanticscholar.org/paper/42a30dc5470f54ec249f25d3c31e05d7c376c8e3,arXiv.org,2023.0,78.0,55.0,7.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '47825073', 'name': 'Wen Wang'}, {'authorId': '66350249', 'name': 'Zhe Chen'}, {'authorId': '2679938', 'name': 'Xiaokang Chen'}, {'authorId': '2109215916', 'name': 'Jiannan Wu'}, {'authorId': '2578924', 'name': 'Xizhou Zhu'}, {'authorId': '2170730254', 'name': 'Gang Zeng'}, {'authorId': '47571885', 'name': 'Ping Luo'}, {'authorId': '2115137018', 'name': 'Tong Lu'}, {'authorId': '2141009492', 'name': 'Jie Zhou'}, {'authorId': '145858545', 'name': 'Y. Qiao'}, {'authorId': '3304536', 'name': 'Jifeng Dai'}]","['SenseTime Research', 'Nanjing University', 'OpenGVLab, Shanghai AI Laboratory', 'Shanghai Artificial Intelligence Laboratory', 'Tsinghua University', 'Chinese University of Hong Kong', 'Peking University']",['China'],2023-05
2305.11176,Siyuan Huang,"Siyuan Huang, Zhengkai Jiang, Hao Dong, Yu Qiao, Peng Gao, Hongsheng
  Li",Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model,,,,,cs.RO cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Foundation models have made significant strides in various applications, including text-to-image generation, panoptic segmentation, and natural language processing. This paper presents Instruct2Act, a framework that utilizes Large Language Models to map multi-modal instructions to sequential actions for robotic manipulation tasks. Specifically, Instruct2Act employs the LLM model to generate Python programs that constitute a comprehensive perception, planning, and action loop for robotic tasks. In the perception section, pre-defined APIs are used to access multiple foundation models where the Segment Anything Model (SAM) accurately locates candidate objects, and CLIP classifies them. In this way, the framework leverages the expertise of foundation models and robotic abilities to convert complex high-level instructions into precise policy codes. Our approach is adjustable and flexible in accommodating various instruction modalities and input types and catering to specific task demands. We validated the practicality and efficiency of our approach by assessing it on robotic tasks in different scenarios within tabletop manipulation domains. Furthermore, our zero-shot method outperformed many state-of-the-art learning-based policies in several tasks. The code for our proposed approach is available at https://github.com/OpenGVLab/Instruct2Act, serving as a robust benchmark for high-level robotic instruction tasks with assorted modality inputs. ","[{'version': 'v1', 'created': 'Thu, 18 May 2023 17:59:49 GMT'}, {'version': 'v2', 'created': 'Tue, 23 May 2023 15:24:17 GMT'}, {'version': 'v3', 'created': 'Wed, 24 May 2023 04:17:34 GMT'}]",2023-05-25,"[['Huang', 'Siyuan', ''], ['Jiang', 'Zhengkai', ''], ['Dong', 'Hao', ''], ['Qiao', 'Yu', ''], ['Gao', 'Peng', ''], ['Li', 'Hongsheng', '']]",0,0,2023-05-18,3,6,2,0,0,0,2195676f111ad492c50f4d4c96abb2bd3d72f7fc,258762636.0,https://www.semanticscholar.org/paper/2195676f111ad492c50f4d4c96abb2bd3d72f7fc,arXiv.org,2023.0,55.0,22.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1713084', 'name': 'Siyuan Huang'}, {'authorId': '50676465', 'name': 'Zhengkai Jiang'}, {'authorId': '2113412153', 'name': 'Hao-Wen Dong'}, {'authorId': '2059129841', 'name': 'Y. Qiao'}, {'authorId': '144740494', 'name': 'Peng Gao'}, {'authorId': '47893312', 'name': 'Hongsheng Li'}]","['University of Chinese Academy of Sciences', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'Chinese University of Hong Kong', 'Peking University']",['China'],2023-05
2305.11255,Hao Fei,"Hao Fei, Bobo Li, Qian Liu, Lidong Bing, Fei Li, Tat-Seng Chua",Reasoning Implicit Sentiment with Chain-of-Thought Prompting,ACL2023 Short Paper,,,,cs.CL,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  While sentiment analysis systems try to determine the sentiment polarities of given targets based on the key opinion expressions in input texts, in implicit sentiment analysis (ISA) the opinion cues come in an implicit and obscure manner. Thus detecting implicit sentiment requires the common-sense and multi-hop reasoning ability to infer the latent intent of opinion. Inspired by the recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop Reasoning (THOR) CoT framework to mimic the human-like reasoning process for ISA. We design a three-step prompting principle for THOR to step-by-step induce the implicit aspect, opinion, and finally the sentiment polarity. Our THOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6% F1 on supervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50% F1 on zero-shot setting. Our code is open at https://github.com/scofield7419/THOR-ISA. ","[{'version': 'v1', 'created': 'Thu, 18 May 2023 18:38:32 GMT'}, {'version': 'v2', 'created': 'Thu, 25 May 2023 03:57:57 GMT'}, {'version': 'v3', 'created': 'Sat, 3 Jun 2023 03:56:23 GMT'}, {'version': 'v4', 'created': 'Fri, 9 Jun 2023 01:27:58 GMT'}]",2023-06-12,"[['Fei', 'Hao', ''], ['Li', 'Bobo', ''], ['Liu', 'Qian', ''], ['Bing', 'Lidong', ''], ['Li', 'Fei', ''], ['Chua', 'Tat-Seng', '']]",0,0,2023-05-18,4,6,1,4,2,2,fb0474569c14a4af8dc94f231dc6074fb5bec63a,258822870.0,https://www.semanticscholar.org/paper/fb0474569c14a4af8dc94f231dc6074fb5bec63a,Annual Meeting of the Association for Computational Linguistics,2023.0,42.0,13.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '46959445', 'name': 'Hao Fei'}, {'authorId': '2132446579', 'name': 'Bobo Li'}, {'authorId': '2145484051', 'name': 'Qianchu Liu'}, {'authorId': '1996394', 'name': 'Lidong Bing'}, {'authorId': '2109530930', 'name': 'Fei Li'}, {'authorId': '143779329', 'name': 'Tat-seng Chua'}]","['Alibaba', 'Wuhan University', 'National University of Singapore', 'Applied Research Laboratory at the University of Hawaii']","['China', 'United States', 'Singapore']",2023-05
2305.11391,Xiaowei Huang,"Xiaowei Huang, Wenjie Ruan, Wei Huang, Gaojie Jin, Yi Dong, Changshun
  Wu, Saddek Bensalem, Ronghui Mu, Yi Qi, Xingyu Zhao, Kaiwen Cai, Yanghao
  Zhang, Sihao Wu, Peipei Xu, Dengyu Wu, Andre Freitas, Mustafa A. Mustafa",A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation,,,,,cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) have exploded a new heatwave of AI for their ability to engage end-users in human-level conversations with detailed and articulate answers across many knowledge domains. In response to their fast adoption in many industrial applications, this survey concerns their safety and trustworthiness. First, we review known vulnerabilities and limitations of the LLMs, categorising them into inherent issues, attacks, and unintended bugs. Then, we consider if and how the Verification and Validation (V&V) techniques, which have been widely developed for traditional software and deep learning models such as convolutional neural networks as independent processes to check the alignment of their implementations against the specifications, can be integrated and further extended throughout the lifecycle of the LLMs to provide rigorous analysis to the safety and trustworthiness of LLMs and their applications. Specifically, we consider four complementary techniques: falsification and evaluation, verification, runtime monitoring, and regulations and ethical use. In total, 370+ references are considered to support the quick understanding of the safety and trustworthiness issues from the perspective of V&V. While intensive research has been conducted to identify the safety and trustworthiness issues, rigorous yet practical methods are called for to ensure the alignment of LLMs with safety and trustworthiness requirements. ","[{'version': 'v1', 'created': 'Fri, 19 May 2023 02:41:12 GMT'}, {'version': 'v2', 'created': 'Sun, 27 Aug 2023 13:12:30 GMT'}]",2023-08-29,"[['Huang', 'Xiaowei', ''], ['Ruan', 'Wenjie', ''], ['Huang', 'Wei', ''], ['Jin', 'Gaojie', ''], ['Dong', 'Yi', ''], ['Wu', 'Changshun', ''], ['Bensalem', 'Saddek', ''], ['Mu', 'Ronghui', ''], ['Qi', 'Yi', ''], ['Zhao', 'Xingyu', ''], ['Cai', 'Kaiwen', ''], ['Zhang', 'Yanghao', ''], ['Wu', 'Sihao', ''], ['Xu', 'Peipei', ''], ['Wu', 'Dengyu', ''], ['Freitas', 'Andre', ''], ['Mustafa', 'Mustafa A.', '']]",0,0,2023-05-19,2,17,2,0,0,0,4f0c7f4df04f07609bdb67944af2a529d5a4517b,258823083.0,https://www.semanticscholar.org/paper/4f0c7f4df04f07609bdb67944af2a529d5a4517b,arXiv.org,2023.0,337.0,11.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2107903140', 'name': 'Xiaowei Huang'}, {'authorId': '8103173', 'name': 'Wenjie Ruan'}, {'authorId': '2152492444', 'name': 'Wei Huang'}, {'authorId': '2071131446', 'name': 'Gao Jin'}, {'authorId': '1410228759', 'name': 'Yizhen Dong'}, {'authorId': '93366400', 'name': 'Changshun Wu'}, {'authorId': '144887026', 'name': 'S. Bensalem'}, {'authorId': '2139856775', 'name': 'Ronghui Mu'}, {'authorId': '2117830923', 'name': 'Yi Qi'}, {'authorId': '47039303', 'name': 'Xingyu Zhao'}, {'authorId': '2128294595', 'name': 'Kaiwen Cai'}, {'authorId': '95681022', 'name': 'Yanghao Zhang'}, {'authorId': '2218423966', 'name': 'Sihao Wu'}, {'authorId': '50591107', 'name': 'Peipei Xu'}, {'authorId': '2115875650', 'name': 'Dengyu Wu'}, {'authorId': '2057619238', 'name': 'Andr Freitas'}, {'authorId': '144411037', 'name': 'Mustafa A. Mustafa'}]","['University of Manchester', 'University of Liverpool', 'Universit Grenoble Alpes', 'Purple Mountain Laboratories', 'KU Leuven']","['China', 'Belgium', 'United Kingdom', 'France']",2023-05
2305.11418,Jiayi Fu,"Jiayi Fu, Haoying Han, Xing Su and Chao Fan",Towards Human-AI Collaborative Urban Science Research Enabled by Pre-trained Large Language Models,,,,,cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained large language models (PLMs) have the potential to support urban science research through content creation, information extraction, assisted programming, text classification, and other technical advances. In this research, we explored the opportunities, challenges, and prospects of PLMs in urban science research. Specifically, we discussed potential applications of PLMs to urban institution, urban space, urban information, and citizen behaviors research through seven examples using ChatGPT. We also examined the challenges of PLMs in urban science research from both technical and social perspectives. The prospects of the application of PLMs in urban science research were then proposed. We found that PLMs can effectively aid in understanding complex concepts in urban science, facilitate urban spatial form identification, assist in disaster monitoring, and sense public sentiment. At the same time, however, the applications of PLMs in urban science research face evident threats, such as technical limitations, security, privacy, and social bias. The development of fundamental models based on domain knowledge and human-AI collaboration may help improve PLMs to support urban science research in future. ","[{'version': 'v1', 'created': 'Fri, 19 May 2023 04:04:54 GMT'}]",2023-05-22,"[['Fu', 'Jiayi', ''], ['Han', 'Haoying', ''], ['Su', 'Xing', ''], ['Fan', 'Chao', '']]",1,1,2023-05-19,1,4,1,1,0,1,6f0786b1ee6962d8071d6718bb94f4a935ad8e5f,258822987.0,https://www.semanticscholar.org/paper/6f0786b1ee6962d8071d6718bb94f4a935ad8e5f,arXiv.org,2023.0,61.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2119315206', 'name': 'Jiayi Fu'}, {'authorId': '2716684', 'name': 'Haoying Han'}, {'authorId': '2217993987', 'name': 'Xing Su'}, {'authorId': '2217951835', 'name': 'Chao Fan'}]","['Clemson University', 'Zhejiang University']","['China', 'United States']",2023-05
2305.11424,Zhe Chen,"Zhe Chen, Hao Tan, Tao Wang, Tianrun Shen, Tong Lu, Qiuying Peng,
  Cheng Cheng, Yue Qi",Graph Propagation Transformer for Graph Representation Learning,Accepted to IJCAI 2023,,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents a novel transformer architecture for graph representation learning. The core insight of our method is to fully consider the information propagation among nodes and edges in a graph when building the attention module in the transformer blocks. Specifically, we propose a new attention mechanism called Graph Propagation Attention (GPA). It explicitly passes the information among nodes and edges in three ways, i.e. node-to-node, node-to-edge, and edge-to-node, which is essential for learning graph-structured data. On this basis, we design an effective transformer architecture named Graph Propagation Transformer (GPTrans) to further help learn graph data. We verify the performance of GPTrans in a wide range of graph learning experiments on several benchmark datasets. These results show that our method outperforms many state-of-the-art transformer-based graph models with better performance. The code will be released at https://github.com/czczup/GPTrans. ","[{'version': 'v1', 'created': 'Fri, 19 May 2023 04:42:58 GMT'}, {'version': 'v2', 'created': 'Thu, 15 Jun 2023 14:55:59 GMT'}]",2023-06-16,"[['Chen', 'Zhe', ''], ['Tan', 'Hao', ''], ['Wang', 'Tao', ''], ['Shen', 'Tianrun', ''], ['Lu', 'Tong', ''], ['Peng', 'Qiuying', ''], ['Cheng', 'Cheng', ''], ['Qi', 'Yue', '']]",0,1,2023-05-19,2,8,2,0,0,0,dab76e2bc267359b108ab27f029e302eea93c2ef,258823448.0,https://www.semanticscholar.org/paper/dab76e2bc267359b108ab27f029e302eea93c2ef,International Joint Conference on Artificial Intelligence,2023.0,57.0,1.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '66350249', 'name': 'Zhe Chen'}, {'authorId': '3218666', 'name': 'Hao Hao Tan'}, {'authorId': '1491094850', 'name': 'Tao Wang'}, {'authorId': '2176280315', 'name': 'Tianrun Shen'}, {'authorId': '2115137018', 'name': 'Tong Lu'}, {'authorId': '3391833', 'name': 'Qiuying Peng'}, {'authorId': '2218179539', 'name': 'Cheng Cheng'}, {'authorId': '2225101118', 'name': 'Yue Qi'}]","['Nanjing University', 'OPPO']",['China'],2023-05
2305.11461,IokTong Lei,Ioktong Lei and Zhidong Deng,SelfzCoT: a Self-Prompt Zero-shot CoT from Semantic-level to Code-level for a Better Utilization of LLMs,"preprint, under review",,,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  This paper show a work on better use of LLMs with SelfzCoT a self-prompt zero-shot CoT. Specifically, on the zero-shot arithmetic reasoning tasks, the accuracy of the proposed SelfzCoT is improved with GSM8K from 40.50% to 82.34%, with MultiArith from 79.3% to 94.7%, with ADDSUB from 74.70% to 94.10%, with SingleEq from 78.70% to 91.30%, with AQUA from 31.90% to 82.33%, and with SVAMP from 63.70% to 79.70%. Totally, using the first two lasting path activations to LLM and particularly, the code-level self-prompt, the SelfzCoT has a huge improvement on all six zero-shot arithmetic reasoning tasks. Additionally, our modified zero-shot CoT (MzCoT) also achieves remarkable performance in the reasoning tasks. The accuracy of the proposed MzCoT is enhanced with GSM8K from 40.50% to 76.32%, with MultiArith from 79.3% to 96.97%, with ADDSUB from 74.70% to 92.39%, with SingleEq from 78.70% to 94.60%, with AQUA from 31.90% to 79.90%, and with SVAMP from 63.70% to 81.50%. Notably, SelfzCoT has the best performance on GSM8K among all the recent zero-shot methods. ","[{'version': 'v1', 'created': 'Fri, 19 May 2023 06:30:17 GMT'}, {'version': 'v2', 'created': 'Tue, 30 May 2023 06:18:16 GMT'}, {'version': 'v3', 'created': 'Mon, 31 Jul 2023 05:46:46 GMT'}]",2023-08-01,"[['Lei', 'Ioktong', ''], ['Deng', 'Zhidong', '']]",0,0,2023-05-19,3,2,1,0,0,0,a53b476e3236c2575e0d5aa451c2e17110715f42,258822966.0,https://www.semanticscholar.org/paper/a53b476e3236c2575e0d5aa451c2e17110715f42,arXiv.org,2023.0,41.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2217949204', 'name': 'IokTong Lei'}, {'authorId': None, 'name': 'ZhiDong Deng'}]",['Tsinghua University'],['China'],2023-05
2305.11487,Guangyan Chen,"Guangyan Chen, Meiling Wang, Yi Yang, Kai Yu, Li Yuan, Yufeng Yue",PointGPT: Auto-regressively Generative Pre-training from Point Clouds,"9 pages, 2 figures",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) based on the generative pre-training transformer (GPT) have demonstrated remarkable effectiveness across a diverse range of downstream tasks. Inspired by the advancements of the GPT, we present PointGPT, a novel approach that extends the concept of GPT to point clouds, addressing the challenges associated with disorder properties, low information density, and task gaps. Specifically, a point cloud auto-regressive generation task is proposed to pre-train transformer models. Our method partitions the input point cloud into multiple point patches and arranges them in an ordered sequence based on their spatial proximity. Then, an extractor-generator based transformer decoder, with a dual masking strategy, learns latent representations conditioned on the preceding point patches, aiming to predict the next one in an auto-regressive manner. Our scalable approach allows for learning high-capacity models that generalize well, achieving state-of-the-art performance on various downstream tasks. In particular, our approach achieves classification accuracies of 94.9% on the ModelNet40 dataset and 93.4% on the ScanObjectNN dataset, outperforming all other transformer models. Furthermore, our method also attains new state-of-the-art accuracies on all four few-shot learning benchmarks. ","[{'version': 'v1', 'created': 'Fri, 19 May 2023 07:39:04 GMT'}, {'version': 'v2', 'created': 'Tue, 23 May 2023 02:38:26 GMT'}]",2023-05-24,"[['Chen', 'Guangyan', ''], ['Wang', 'Meiling', ''], ['Yang', 'Yi', ''], ['Yu', 'Kai', ''], ['Yuan', 'Li', ''], ['Yue', 'Yufeng', '']]",0,1,2023-05-19,2,6,1,0,0,0,128fc3e518a9616cd2780d974d32b4ef47ba5901,258823283.0,https://www.semanticscholar.org/paper/128fc3e518a9616cd2780d974d32b4ef47ba5901,arXiv.org,2023.0,67.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2149509581', 'name': 'Guang-Sheng Chen'}, {'authorId': '2145319670', 'name': 'Meiling Wang'}, {'authorId': '2143685743', 'name': 'Yi Yang'}, {'authorId': '1736727', 'name': 'Kai Yu'}, {'authorId': '2087091296', 'name': 'Li Yuan'}, {'authorId': '9010344', 'name': 'Yufeng Yue'}]","['Peking University', 'Beijing Institute of Technology']",['China'],2023-05
2305.11499,Tianci Xue,"Tianci Xue, Ziqi Wang, Zhenhailong Wang, Chi Han, Pengfei Yu, Heng Ji",RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought,"24 pages, 21 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language Models (LLMs) have achieved promising performance on arithmetic reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting. However, LLMs face challenges in maintaining factual consistency during reasoning, exhibiting tendencies to condition overlooking, question misinterpretation, and condition hallucination over given problems. Existing methods use coarse-grained feedback (e.g., whether the answer is correct) to improve factual consistency. In this work, we propose RCoT (Reversing Chain-of-Thought), a novel method to improve LLMs' reasoning abilities by automatically detecting and rectifying factual inconsistency in LLMs, generated solutions. To detect factual inconsistency, RCoT first asks LLMs to reconstruct the problem based on generated solutions. Then fine-grained comparisons between the original problem and the reconstructed problem expose the factual inconsistency in the original solutions. To rectify the solution, RCoT formulates detected factual inconsistency into fine-grained feedback to guide LLMs in revising solutions. Experimental results demonstrate improvements of RCoT over standard CoT, Self-Consistency and Self-Refine across seven arithmetic datasets. Moreover, we find that manually written fine-grained feedback can dramatically improve LLMs' reasoning abilities (e.g., ChatGPT reaches 94.6% accuracy on GSM8K), encouraging the community to further explore the fine-grained feedback generation methods. ","[{'version': 'v1', 'created': 'Fri, 19 May 2023 08:02:52 GMT'}, {'version': 'v2', 'created': 'Mon, 2 Oct 2023 03:59:04 GMT'}]",2023-10-03,"[['Xue', 'Tianci', ''], ['Wang', 'Ziqi', ''], ['Wang', 'Zhenhailong', ''], ['Han', 'Chi', ''], ['Yu', 'Pengfei', ''], ['Ji', 'Heng', '']]",1,1,2023-05-19,2,6,1,1,0,1,22d5459d1f47341b355feeb1becc37208d6ec365,258823296.0,https://www.semanticscholar.org/paper/22d5459d1f47341b355feeb1becc37208d6ec365,arXiv.org,2023.0,51.0,11.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2217950757', 'name': 'Tianci Xue'}, {'authorId': '1390880371', 'name': 'Ziqi Wang'}, {'authorId': '2052036545', 'name': 'Zhenhailong Wang'}, {'authorId': '2118642562', 'name': 'Chi Han'}, {'authorId': '144808890', 'name': 'Pengfei Yu'}, {'authorId': '2072975661', 'name': 'Heng Ji'}]","['University of Illinois Urbana-Champaign', 'Nanjing University']","['China', 'United States']",2023-05
2305.11541,Lu Wang Wang,"Zezhong Wang, Fangkai Yang, Pu Zhao, Lu Wang, Jue Zhang, Mohit Garg,
  Qingwei Lin, Dongmei Zhang",Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering,"13 pages, 1 figure",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Model (LLM) has gained popularity and achieved remarkable results in open-domain tasks, but its performance in real industrial domain-specific scenarios is average since there is no specific knowledge in it. This issue has attracted widespread attention, but there are few relevant benchmarks available. In this paper, we provide a benchmark Question Answering (QA) dataset named MSQA, which is about Microsoft products and IT technical problems encountered by customers. This dataset contains industry cloud-specific QA knowledge, which is not available for general LLM, so it is well suited for evaluating methods aimed at improving domain-specific capabilities of LLM. In addition, we propose a new model interaction paradigm that can empower LLM to achieve better performance on domain-specific tasks where it is not proficient. Extensive experiments demonstrate that the approach following our model fusion framework outperforms the commonly used LLM with retrieval methods. ","[{'version': 'v1', 'created': 'Fri, 19 May 2023 09:23:25 GMT'}, {'version': 'v2', 'created': 'Tue, 30 May 2023 11:03:04 GMT'}]",2023-05-31,"[['Wang', 'Zezhong', ''], ['Yang', 'Fangkai', ''], ['Zhao', 'Pu', ''], ['Wang', 'Lu', ''], ['Zhang', 'Jue', ''], ['Garg', 'Mohit', ''], ['Lin', 'Qingwei', ''], ['Zhang', 'Dongmei', '']]",0,0,2023-05-19,2,8,2,0,0,0,98d05533678a9a09a2053b7d974738c60c4894ea,258822888.0,https://www.semanticscholar.org/paper/98d05533678a9a09a2053b7d974738c60c4894ea,arXiv.org,2023.0,57.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108726649', 'name': 'Zezhong Wang'}, {'authorId': '47829900', 'name': 'Fan Yang'}, {'authorId': '2007757792', 'name': 'Pu Zhao'}, {'authorId': '2163383329', 'name': 'Lu Wang'}, {'authorId': '2163389931', 'name': 'Jue Zhang'}, {'authorId': '145076801', 'name': 'Mohit Garg'}, {'authorId': '2793487', 'name': 'Qingwei Lin'}, {'authorId': '2109581369', 'name': 'Dongmei Zhang'}]",['Chinese University of Hong Kong'],['China'],2023-05
2305.11598,Lu Wang Wang,"Liting Chen, Lu Wang, Hang Dong, Yali Du, Jie Yan, Fangkai Yang,
  Shuang Li, Pu Zhao, Si Qin, Saravan Rajmohan, Qingwei Lin, Dongmei Zhang",Introspective Tips: Large Language Model for In-Context Decision Making,"22 pages, 4 figures",,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The emergence of large language models (LLMs) has substantially influenced natural language processing, demonstrating exceptional results across various tasks. In this study, we employ ``Introspective Tips"" to facilitate LLMs in self-optimizing their decision-making. By introspectively examining trajectories, LLM refines its policy by generating succinct and valuable tips. Our method enhances the agent's performance in both few-shot and zero-shot learning situations by considering three essential scenarios: learning from the agent's past experiences, integrating expert demonstrations, and generalizing across diverse games. Importantly, we accomplish these improvements without fine-tuning the LLM parameters; rather, we adjust the prompt to generalize insights from the three aforementioned situations. Our framework not only supports but also emphasizes the advantage of employing LLM in in-contxt decision-making. Experiments involving over 100 games in TextWorld illustrate the superior performance of our approach. ","[{'version': 'v1', 'created': 'Fri, 19 May 2023 11:20:37 GMT'}]",2023-05-22,"[['Chen', 'Liting', ''], ['Wang', 'Lu', ''], ['Dong', 'Hang', ''], ['Du', 'Yali', ''], ['Yan', 'Jie', ''], ['Yang', 'Fangkai', ''], ['Li', 'Shuang', ''], ['Zhao', 'Pu', ''], ['Qin', 'Si', ''], ['Rajmohan', 'Saravan', ''], ['Lin', 'Qingwei', ''], ['Zhang', 'Dongmei', '']]",0,0,2023-05-19,1,12,2,0,0,0,047e3812854a86b2a2e113219fa956eda860ce24,258823336.0,https://www.semanticscholar.org/paper/047e3812854a86b2a2e113219fa956eda860ce24,arXiv.org,2023.0,44.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108438312', 'name': 'Liting Chen'}, {'authorId': '2163383329', 'name': 'Lu Wang'}, {'authorId': '145153805', 'name': 'Hang Dong'}, {'authorId': '1390662136', 'name': 'Yali Du'}, {'authorId': '2107302346', 'name': 'Jie Yan'}, {'authorId': '47829900', 'name': 'Fan Yang'}, {'authorId': '2228313627', 'name': 'Shuang Li'}, {'authorId': '2007757792', 'name': 'Pu Zhao'}, {'authorId': '2054558542', 'name': 'Si Qin'}, {'authorId': '148121358', 'name': 'S. Rajmohan'}, {'authorId': '2793487', 'name': 'Qingwei Lin'}, {'authorId': '2109581369', 'name': 'Dongmei Zhang'}]","['Chinese University of Hong Kong, Shenzhen', ""King's College London""]","['China', 'United Kingdom']",2023-05
2305.11738,Zhibin Gou,"Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan
  Duan, Weizhu Chen",CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing,"add LLaMA-2 7B to 70B results; add more mathematical program
  synthesis datasets",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially ""black boxes"" to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxicity reduction demonstrate that CRITIC consistently enhances the performance of LLMs. Meanwhile, our research highlights the crucial importance of external feedback in promoting the ongoing self-improvement of LLMs. ","[{'version': 'v1', 'created': 'Fri, 19 May 2023 15:19:44 GMT'}, {'version': 'v2', 'created': 'Sat, 30 Sep 2023 08:35:29 GMT'}]",2023-10-03,"[['Gou', 'Zhibin', ''], ['Shao', 'Zhihong', ''], ['Gong', 'Yeyun', ''], ['Shen', 'Yelong', ''], ['Yang', 'Yujiu', ''], ['Duan', 'Nan', ''], ['Chen', 'Weizhu', '']]",0,0,2023-05-19,2,7,2,0,0,0,bcdaf6c98ddbd6809cf6241aa77200d7394db163,258823123.0,https://www.semanticscholar.org/paper/bcdaf6c98ddbd6809cf6241aa77200d7394db163,arXiv.org,2023.0,144.0,41.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1797090', 'name': 'Zhibin Gou'}, {'authorId': '144485528', 'name': 'Zhihong Shao'}, {'authorId': '2171182', 'name': 'Yeyun Gong'}, {'authorId': '1752875', 'name': 'Yelong Shen'}, {'authorId': '2108585311', 'name': 'Yujiu Yang'}, {'authorId': '46429989', 'name': 'Nan Duan'}, {'authorId': '2109136147', 'name': 'Weizhu Chen'}]","['Tsinghua University', 'Microsoft']","['China', 'United States']",2023-05
2305.11747,Junyi Li,"Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie and Ji-Rong Wen",HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,Working in progress,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, \ie content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation for Large Language Models (HaluEval) benchmark, a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, \ie sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (\ie about $11.4\%$ user queries). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. While, our experiments also prove that the hallucination recognition can be improved by providing external knowledge or adding reasoning steps. Our benchmark can be accessed at https://github.com/RUCAIBox/HaluEval. ","[{'version': 'v1', 'created': 'Fri, 19 May 2023 15:36:27 GMT'}, {'version': 'v2', 'created': 'Mon, 22 May 2023 13:36:09 GMT'}]",2023-05-23,"[['Li', 'Junyi', ''], ['Cheng', 'Xiaoxue', ''], ['Zhao', 'Wayne Xin', ''], ['Nie', 'Jian-Yun', ''], ['Wen', 'Ji-Rong', '']]",1,1,2023-05-19,2,5,1,1,0,1,e0384ba36555232c587d4a80d527895a095a9001,258832847.0,https://www.semanticscholar.org/paper/e0384ba36555232c587d4a80d527895a095a9001,arXiv.org,2023.0,36.0,20.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2018027', 'name': 'Junyi Li'}, {'authorId': '2149479237', 'name': 'Xiaoxue Cheng'}, {'authorId': '2542603', 'name': 'Wayne Xin Zhao'}, {'authorId': '50204644', 'name': 'J. Nie'}, {'authorId': '153693432', 'name': 'Ji-rong Wen'}]","['Beijing Key Laboratory of Big Data Management and Analysis Methods', 'Universit de Montral', 'Renmin University of China']","['Canada', 'China']",2023-05
2305.11828,Hye Sun Yun,"Hye Sun Yun, Iain J. Marshall, Thomas Trikalinos, Byron C. Wallace",Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews,"34 pages, 3 figures, 7 tables",,,,cs.CL cs.AI cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Medical systematic reviews are crucial for informing clinical decision making and healthcare policy. But producing such reviews is onerous and time-consuming. Thus, high-quality evidence synopses are not available for many questions and may be outdated even when they are available. Large language models (LLMs) are now capable of generating long-form texts, suggesting the tantalizing possibility of automatically generating literature reviews on demand. However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucinating or omitting important information. In the healthcare context, this may render LLMs unusable at best and dangerous at worst. Most discussion surrounding the benefits and risks of LLMs have been divorced from specific applications. In this work, we seek to qualitatively characterize the potential utility and risks of LLMs for assisting in production of medical evidence reviews. We conducted 16 semi-structured interviews with international experts in systematic reviews, grounding discussion in the context of generating evidence reviews. Domain experts indicated that LLMs could aid writing reviews, as a tool for drafting or creating plain language summaries, generating templates or suggestions, distilling information, crosschecking, and synthesizing or interpreting text inputs. But they also identified issues with model outputs and expressed concerns about potential downstream harms of confidently composed but inaccurate LLM outputs which might mislead. Other anticipated potential downstream harms included lessened accountability and proliferation of automatically generated reviews that might be of low quality. Informed by this qualitative analysis, we identify criteria for rigorous evaluation of biomedical LLMs aligned with domain expert views. ","[{'version': 'v1', 'created': 'Fri, 19 May 2023 17:09:19 GMT'}, {'version': 'v2', 'created': 'Mon, 22 May 2023 16:17:51 GMT'}]",2023-05-23,"[['Yun', 'Hye Sun', ''], ['Marshall', 'Iain J.', ''], ['Trikalinos', 'Thomas', ''], ['Wallace', 'Byron C.', '']]",0,0,2023-05-19,2,4,3,0,0,0,543345d3f0a0b8bf245a56661a6da5f9ba5295da,264289388.0,https://www.semanticscholar.org/paper/543345d3f0a0b8bf245a56661a6da5f9ba5295da,,2023.0,65.0,0.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Medicine', 'source': 's2-fos-model'}]","[{'authorId': '32401628', 'name': 'Hye Sun Yun'}, {'authorId': '1808775', 'name': 'I. Marshall'}, {'authorId': '2947796', 'name': 'T. Trikalinos'}, {'authorId': '2257308234', 'name': 'Byron C. Wallace'}]","['Northeastern University', 'Brown University']","['China', 'United States']",2023-05
2305.12147,Hanmeng Liu,"Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, Yue
  Zhang",LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Generative Pre-trained Transformer 4 (GPT-4) demonstrates impressive chain-of-thought reasoning ability. Recent work on self-instruction tuning, such as Alpaca, has focused on enhancing the general proficiency of models. These instructions enable the model to achieve performance comparable to GPT-3.5 on general tasks like open-domain text generation and paraphrasing. However, they fall short of helping the model handle complex reasoning tasks. To bridge the gap, this paper presents LogiCoT, a new instruction-tuning dataset for Logical Chain-of-Thought reasoning with GPT-4. We elaborate on the process of harvesting instructions for prompting GPT-4 to generate chain-of-thought rationales. LogiCoT serves as an instruction set for teaching models of logical reasoning and elicits general reasoning skills. ","[{'version': 'v1', 'created': 'Sat, 20 May 2023 09:23:09 GMT'}]",2023-05-23,"[['Liu', 'Hanmeng', ''], ['Teng', 'Zhiyang', ''], ['Cui', 'Leyang', ''], ['Zhang', 'Chaoli', ''], ['Zhou', 'Qiji', ''], ['Zhang', 'Yue', '']]",0,1,2023-05-20,1,6,2,3,0,3,a182b05fca709dd0d4a640e777a0e7d593ee0753,258832686.0,https://www.semanticscholar.org/paper/a182b05fca709dd0d4a640e777a0e7d593ee0753,arXiv.org,2023.0,37.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2118960911', 'name': 'Hanmeng Liu'}, {'authorId': '2272668', 'name': 'Zhiyang Teng'}, {'authorId': '152496687', 'name': 'Leyang Cui'}, {'authorId': '2152737103', 'name': 'Chaoli Zhang'}, {'authorId': '92758499', 'name': 'Qiji Zhou'}, {'authorId': '2218307965', 'name': 'Yue Zhang'}]","['Nanyang Technological University', 'Westlake University', 'Zhejiang University']","['China', 'Singapore']",2023-05
2305.12223,Guangzhi Wang,"Guangzhi Wang, Yixiao Ge, Xiaohan Ding, Mohan Kankanhalli, Ying Shan",What Makes for Good Visual Tokenizers for Large Language Models?,"15 pages, 3 figures. Project released at:
  https://github.com/TencentARC/GVT",,,01,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  We empirically investigate proper pre-training methods to build good visual tokenizers, making Large Language Models (LLMs) powerful Multimodal Large Language Models (MLLMs). In our benchmark, which is curated to evaluate MLLMs visual semantic understanding and fine-grained perception capabilities, we discussed different visual tokenizers pre-trained with dominant methods (i.e., DeiT, CLIP, MAE, DINO), and observe that: i) Fully/weakly supervised models capture more semantics than self-supervised models, but the gap is narrowed by scaling up the pre-training dataset. ii) Self-supervised models are better at fine-grained perception, where patch-level supervision is particularly effective. iii) Tuning the visual tokenizer leads to the loss of semantics obtained from large-scale pretraining, which is unfavorable with relatively small-scale instruction-tuning dataset. Given the findings, we reviewed methods that attempted to unify semantics and fine-grained visual understanding, e.g., patch-level feature distillation with semantically-rich targets. We obtain an intriguing insight mask-based strategies that were once all the rage may not be applicable for obtaining good visual tokenizers. Based on this critical observation, we obtain a new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales. In particular, without introducing extra parameters and task-specific fine-tuning, GVT achieves superior performance on visual question answering, image captioning, and other fine-grained visual understanding tasks such as object counting and multi-class identification. ","[{'version': 'v1', 'created': 'Sat, 20 May 2023 16:11:26 GMT'}, {'version': 'v2', 'created': 'Tue, 23 May 2023 10:35:35 GMT'}]",2023-05-24,"[['Wang', 'Guangzhi', ''], ['Ge', 'Yixiao', ''], ['Ding', 'Xiaohan', ''], ['Kankanhalli', 'Mohan', ''], ['Shan', 'Ying', '']]",0,0,2023-05-20,2,5,1,0,0,0,6a5525c316b9be7909c433a79e090ed731425083,258833156.0,https://www.semanticscholar.org/paper/6a5525c316b9be7909c433a79e090ed731425083,arXiv.org,2023.0,55.0,8.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '47226701', 'name': 'Guangzhi Wang'}, {'authorId': '152988335', 'name': 'Yixiao Ge'}, {'authorId': '4827513', 'name': 'Xiaohan Ding'}, {'authorId': '145977143', 'name': 'Mohan S. Kankanhalli'}, {'authorId': '1387190008', 'name': 'Ying Shan'}]","['National University of Singapore', 'Tencent']","['China', 'Singapore']",2023-05
2305.12328,Bosheng Qin,"Bosheng Qin, Juncheng Li, Siliang Tang, Tat-Seng Chua, Yueting Zhuang",InstructVid2Vid: Controllable Video Editing with Natural Language Instructions,"21 pages, 9 figures",,,,cs.CV cs.AI cs.MM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present an end-to-end diffusion-based method for editing videos with human language instructions, namely $\textbf{InstructVid2Vid}$. Our approach enables the editing of input videos based on natural language instructions without any per-example fine-tuning or inversion. The proposed InstructVid2Vid model combines a pretrained image generation model, Stable Diffusion, with a conditional 3D U-Net architecture to generate time-dependent sequence of video frames. To obtain the training data, we incorporate the knowledge and expertise of different models, including ChatGPT, BLIP, and Tune-a-Video, to synthesize video-instruction triplets, which is a more cost-efficient alternative to collecting data in real-world scenarios. To improve the consistency between adjacent frames of generated videos, we propose the Frame Difference Loss, which is incorporated during the training process. During inference, we extend the classifier-free guidance to text-video input to guide the generated results, making them more related to both the input video and instruction. Experiments demonstrate that InstructVid2Vid is able to generate high-quality, temporally coherent videos and perform diverse edits, including attribute editing, change of background, and style transfer. These results highlight the versatility and effectiveness of our proposed method. Code is released in $\href{https://github.com/BrightQin/InstructVid2Vid}{InstructVid2Vid}$. ","[{'version': 'v1', 'created': 'Sun, 21 May 2023 03:28:13 GMT'}]",2023-05-23,"[['Qin', 'Bosheng', ''], ['Li', 'Juncheng', ''], ['Tang', 'Siliang', ''], ['Chua', 'Tat-Seng', ''], ['Zhuang', 'Yueting', '']]",1,1,2023-05-21,1,5,3,1,0,1,205d2ed0906440f07a0275d7d6a63bced60951fc,258832497.0,https://www.semanticscholar.org/paper/205d2ed0906440f07a0275d7d6a63bced60951fc,arXiv.org,2023.0,61.0,3.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1740906917', 'name': 'Bosheng Qin'}, {'authorId': '2108998895', 'name': 'Juncheng Li'}, {'authorId': '2118071462', 'name': 'Siliang Tang'}, {'authorId': '144078686', 'name': 'Tat-Seng Chua'}, {'authorId': '2125211', 'name': 'Yueting Zhuang'}]","['National University of Singapore', 'Zhejiang University']","['China', 'Singapore']",2023-05
2305.12356,Yijia Zhang,"Yijia Zhang, Lingran Zhao, Shijie Cao, Wenqiang Wang, Ting Cao, Fan
  Yang, Mao Yang, Shanghang Zhang, Ningyi Xu",Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models,,,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Efficient deployment of large language models (LLMs) necessitates low-bit quantization to minimize model size and inference cost. While low-bit integer formats (e.g., INT8/INT4) have been the conventional choice, emerging low-bit floating-point formats (e.g., FP8/FP4) offer a compelling alternative and are gaining support from cutting-edge hardware, such as NVIDIA's H100 GPU. However, the superiority of low-bit INT versus FP formats for quantization on LLMs remains unclear. In this study, we conduct a comparative analysis of INT and FP quantization with the same bit-width, revealing that the optimal quantization format varies across different layers due to the complexity and diversity of tensor distribution. Consequently, we advocate the Mixture of Formats Quantization (MoFQ), which selects the optimal format on a layer-wise basis. This simple yet effective approach achieves state-of-the-art results in both weight-only (W-only) and weight-activation (WA) post-training quantization scenarios when tested on LLaMA across various tasks. In 4-bit W-only quantization, MoFQ surpasses GPTQ without complex hyperparameter tuning and with an order of magnitude faster quantization speed. While in 8-bit WA quantization, MoFQ significantly outperforms INT/FP-only methods, achieving performance close to the full precision model. Notably, MoFQ incurs no hardware overhead compared to INT/FP-only quantization, as the bit-width remains unchanged. ","[{'version': 'v1', 'created': 'Sun, 21 May 2023 05:28:37 GMT'}]",2023-05-23,"[['Zhang', 'Yijia', ''], ['Zhao', 'Lingran', ''], ['Cao', 'Shijie', ''], ['Wang', 'Wenqiang', ''], ['Cao', 'Ting', ''], ['Yang', 'Fan', ''], ['Yang', 'Mao', ''], ['Zhang', 'Shanghang', ''], ['Xu', 'Ningyi', '']]",0,1,2023-05-21,1,9,2,1,1,0,8a8a99401fe9d069ed013f6027d3b537ca2c34f1,258833451.0,https://www.semanticscholar.org/paper/8a8a99401fe9d069ed013f6027d3b537ca2c34f1,arXiv.org,2023.0,23.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2214533434', 'name': 'Yijia Zhang'}, {'authorId': '2218167510', 'name': 'Lingran Zhao'}, {'authorId': '2072595192', 'name': 'Shijie Cao'}, {'authorId': '2188129852', 'name': 'Wenqiang Wang'}, {'authorId': '2069445596', 'name': 'Ting Cao'}, {'authorId': '145338263', 'name': 'Fan Yang'}, {'authorId': '2168609907', 'name': 'Mao Yang'}, {'authorId': '2437353', 'name': 'Shanghang Zhang'}, {'authorId': '2072804799', 'name': 'Ningyi Xu'}]","['Peking University', 'Shanghai Jiao Tong University', 'Microsoft']",['China'],2023-05
2305.12421,Cunxiang Wang,"Cunxiang Wang, Sirui Cheng, Qipeng Guo, Zhikun Xu, Bowen Ding, Yidong
  Wang, Xiangkun Hu, Zheng Zhang, Yue Zhang",Evaluating Open-QA Evaluation,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  This study focuses on the evaluation of the Open Question Answering (Open-QA) task, which can directly estimate the factuality of large language models (LLMs). Current automatic evaluation methods have shown limitations, indicating that human evaluation still remains the most reliable approach. We introduce a new task, Evaluating QA Evaluation (QA-Eval) and the corresponding dataset EVOUNA, designed to assess the accuracy of AI-generated answers in relation to standard answers within Open-QA. Our evaluation of these methods utilizes human-annotated results to measure their performance. Specifically, the work investigates methods that show high correlation with human evaluations, deeming them more reliable. We also discuss the pitfalls of current methods and methods to improve LLM-based evaluators. We believe this new QA-Eval task and corresponding dataset EVOUNA will facilitate the development of more effective automatic evaluation tools and prove valuable for future research in this area. All resources are available at \url{https://github.com/wangcunxiang/QA-Eval} and it is under the Apache-2.0 License. ","[{'version': 'v1', 'created': 'Sun, 21 May 2023 10:40:55 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Jul 2023 15:41:31 GMT'}, {'version': 'v3', 'created': 'Mon, 28 Aug 2023 16:15:21 GMT'}]",2023-08-29,"[['Wang', 'Cunxiang', ''], ['Cheng', 'Sirui', ''], ['Guo', 'Qipeng', ''], ['Xu', 'Zhikun', ''], ['Ding', 'Bowen', ''], ['Wang', 'Yidong', ''], ['Hu', 'Xiangkun', ''], ['Zhang', 'Zheng', ''], ['Zhang', 'Yue', '']]",0,0,2023-05-21,3,9,2,0,0,0,7abb0536a8faa0f7b40d64d4b1f8a7ee33d9e8fe,259951284.0,https://www.semanticscholar.org/paper/7abb0536a8faa0f7b40d64d4b1f8a7ee33d9e8fe,,2023.0,51.0,1.0,1.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '35504092', 'name': 'Cunxiang Wang'}, {'authorId': '2110845230', 'name': 'Sirui Cheng'}, {'authorId': '3187768', 'name': 'Qipeng Guo'}, {'authorId': '2218316624', 'name': 'Zhikun Xu'}, {'authorId': '2223752132', 'name': 'Bowen Ding'}, {'authorId': '2108024273', 'name': 'Yidong Wang'}, {'authorId': '12040998', 'name': 'Xiangkun Hu'}, {'authorId': '47294621', 'name': 'Zheng Zhang'}, {'authorId': '2211964951', 'name': 'Yue Zhang'}]","['Westlake University', 'Fudan University', 'Amazon', 'Northeastern University']","['China', 'United States']",2023-05
2305.12434,Wenxuan Wang,"Yuxuan Wan, Wenxuan Wang, Pinjia He, Jiazhen Gu, Haonan Bai, Michael
  Lyu",BiasAsker: Measuring the Bias in Conversational AI System,Accepted by FSE 2023,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Powered by advanced Artificial Intelligence (AI) techniques, conversational AI systems, such as ChatGPT and digital assistants like Siri, have been widely deployed in daily life. However, such systems may still produce content containing biases and stereotypes, causing potential social problems. Due to the data-driven, black-box nature of modern AI techniques, comprehensively identifying and measuring biases in conversational systems remains a challenging task. Particularly, it is hard to generate inputs that can comprehensively trigger potential bias due to the lack of data containing both social groups as well as biased properties. In addition, modern conversational systems can produce diverse responses (e.g., chatting and explanation), which makes existing bias detection methods simply based on the sentiment and the toxicity hardly being adopted. In this paper, we propose BiasAsker, an automated framework to identify and measure social bias in conversational AI systems. To obtain social groups and biased properties, we construct a comprehensive social bias dataset, containing a total of 841 groups and 8,110 biased properties. Given the dataset, BiasAsker automatically generates questions and adopts a novel method based on existence measurement to identify two types of biases (i.e., absolute bias and related bias) in conversational systems. Extensive experiments on 8 commercial systems and 2 famous research models, such as ChatGPT and GPT-3, show that 32.83% of the questions generated by BiasAsker can trigger biased behaviors in these widely deployed conversational systems. All the code, data, and experimental results have been released to facilitate future research. ","[{'version': 'v1', 'created': 'Sun, 21 May 2023 11:25:59 GMT'}]",2023-05-23,"[['Wan', 'Yuxuan', ''], ['Wang', 'Wenxuan', ''], ['He', 'Pinjia', ''], ['Gu', 'Jiazhen', ''], ['Bai', 'Haonan', ''], ['Lyu', 'Michael', '']]",1,1,2023-05-21,1,6,2,2,0,2,744a98cc2736fa71d3984602e10b68319a47c65e,258833296.0,https://www.semanticscholar.org/paper/744a98cc2736fa71d3984602e10b68319a47c65e,arXiv.org,2023.0,70.0,12.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2167583580', 'name': 'Yuxuan Wan'}, {'authorId': '2144328160', 'name': 'Wenxuan Wang'}, {'authorId': '40532404', 'name': 'Pinjia He'}, {'authorId': None, 'name': 'Jiazhen Gu'}, {'authorId': '47468054', 'name': 'Haonan Bai'}, {'authorId': '2146840128', 'name': 'Michael R. Lyu'}]","['Chinese University of Hong Kong, Shenzhen', '2023, San Francisco, USA', 'Chinese University of Hong Kong']","['China', 'United States']",2023-05
2305.12458,Wenxi Tan,Wenxi Tan,Infor-Coef: Information Bottleneck-based Dynamic Token Downsampling for Compact and Efficient language model,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The prevalence of Transformer-based pre-trained language models (PLMs) has led to their wide adoption for various natural language processing tasks. However, their excessive overhead leads to large latency and computational costs. The statically compression methods allocate fixed computation to different samples, resulting in redundant computation. The dynamic token pruning method selectively shortens the sequences but are unable to change the model size and hardly achieve the speedups as static pruning. In this paper, we propose a model accelaration approaches for large language models that incorporates dynamic token downsampling and static pruning, optimized by the information bottleneck loss. Our model, Infor-Coef, achieves an 18x FLOPs speedup with an accuracy degradation of less than 8\% compared to BERT. This work provides a promising approach to compress and accelerate transformer-based models for NLP tasks. ","[{'version': 'v1', 'created': 'Sun, 21 May 2023 13:30:56 GMT'}]",2023-05-23,"[['Tan', 'Wenxi', '']]",0,0,2023-05-21,1,1,1,0,0,0,00bfaf8b0b3975860e2b60d3924e731c2333fd71,258833347.0,https://www.semanticscholar.org/paper/00bfaf8b0b3975860e2b60d3924e731c2333fd71,arXiv.org,2023.0,38.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2070761774', 'name': 'Wenxin Tan'}]",['Fudan University'],['China'],2023-05
2305.12461,Wei Li,"Wei Li, Borui Yang, Yujie Sun, Suyu Chen, Ziyun Song, Liyao Xiang,
  Xinbing Wang, Chenghu Zhou",Towards Tracing Code Provenance with Code Watermarking,12 pages,,,,cs.CR cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advances in large language models have raised wide concern in generating abundant plausible source code without scrutiny, and thus tracing the provenance of code emerges as a critical issue. To solve the issue, we propose CodeMark, a watermarking system that hides bit strings into variables respecting the natural and operational semantics of the code. For naturalness, we novelly introduce a contextual watermarking scheme to generate watermarked variables more coherent in the context atop graph neural networks. Each variable is treated as a node on the graph and the node feature gathers neighborhood (context) information through learning. Watermarks embedded into the features are thus reflected not only by the variables but also by the local contexts. We further introduce a pre-trained model on source code as a teacher to guide more natural variable generation. Throughout the embedding, the operational semantics are preserved as only variable names are altered. Beyond guaranteeing code-specific properties, CodeMark is superior in watermarking accuracy, capacity, and efficiency due to a more diversified pattern generated. Experimental results show CodeMark outperforms the SOTA watermarking systems with a better balance of the watermarking requirements. ","[{'version': 'v1', 'created': 'Sun, 21 May 2023 13:53:12 GMT'}]",2023-05-23,"[['Li', 'Wei', ''], ['Yang', 'Borui', ''], ['Sun', 'Yujie', ''], ['Chen', 'Suyu', ''], ['Song', 'Ziyun', ''], ['Xiang', 'Liyao', ''], ['Wang', 'Xinbing', ''], ['Zhou', 'Chenghu', '']]",0,0,2023-05-21,1,8,2,0,0,0,9221d5452472258051316bca6e2e39e363f7112f,258833644.0,https://www.semanticscholar.org/paper/9221d5452472258051316bca6e2e39e363f7112f,arXiv.org,2023.0,58.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2157335791', 'name': 'Wei Li'}, {'authorId': '2943214', 'name': 'Borui Yang'}, {'authorId': '2223252752', 'name': 'Yujie Sun'}, {'authorId': '2218072906', 'name': 'Suyu Chen'}, {'authorId': '2218072665', 'name': 'Ziyun Song'}, {'authorId': '1850278', 'name': 'Liyao Xiang'}, {'authorId': '2107937507', 'name': 'Xinbing Wang'}, {'authorId': '2111168706', 'name': 'Cheng Zhou'}]","['Shanghai Jiao Tong University', 'Yujie Sun, Suyu Chen, Ziyun Song, Liyao Xiang,', 'Chinese Academy of Sciences']",['China'],2023-05
2305.12463,Renliang Sun,"Renliang Sun, Wei Xu, Xiaojun Wan",Teaching the Pre-trained Model to Generate Simple Texts for Text Simplification,Accepted by ACL Findings: 2023,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-sa/4.0/,"  Randomly masking text spans in ordinary texts in the pre-training stage hardly allows models to acquire the ability to generate simple texts. It can hurt the performance of pre-trained models on text simplification tasks. In this paper, we propose a new continued pre-training strategy to teach the pre-trained model to generate simple texts. We continue pre-training BART, a representative model, to obtain SimpleBART. It consistently and significantly improves the results on lexical simplification, sentence simplification, and document-level simplification tasks over BART. At the end, we compare SimpleBART with several representative large language models (LLMs). ","[{'version': 'v1', 'created': 'Sun, 21 May 2023 14:03:49 GMT'}]",2023-05-23,"[['Sun', 'Renliang', ''], ['Xu', 'Wei', ''], ['Wan', 'Xiaojun', '']]",0,0,2023-05-21,1,3,2,0,0,0,48351e48245c6e19718f0f198f74f24fbb676637,258833544.0,https://www.semanticscholar.org/paper/48351e48245c6e19718f0f198f74f24fbb676637,Annual Meeting of the Association for Computational Linguistics,2023.0,46.0,5.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2068172988', 'name': 'Renliang Sun'}, {'authorId': '145738420', 'name': 'Wei Xu'}, {'authorId': '117908148', 'name': 'Xiaojun Wan'}]","['Peking University', 'Georgia Institute of Technology']","['China', 'United States']",2023-05
2305.12474,Xiaotian Zhang,"Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, Xipeng
  Qiu",Evaluating the Performance of Large Language Models on GAOKAO Benchmark,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large language models have demonstrated remarkable performance across various natural language processing tasks; however, their efficacy in more challenging and domain-specific tasks remains less explored. This paper introduces the GAOKAO-Benchmark (GAOKAO-Bench), an intuitive benchmark that employs questions from the Chinese Gaokao examination as test samples for evaluating large language models.In order to align the evaluation results with humans as much as possible, we designed a method based on zero-shot prompts to analyze the accuracy and scoring rate of the model by dividing the questions into subjective and objective types. We evaluated the ChatGPT model on GAOKAO-Benchmark performance.Our findings reveal that the ChatGPT model excels in tackling objective questions, while also shedding light on its shortcomings and areas for improvement. To further scrutinize the model's responses, we incorporate human evaluations.In conclusion, this research contributes a robust evaluation benchmark for future large-scale language models and offers valuable insights into the limitations of such models. ","[{'version': 'v1', 'created': 'Sun, 21 May 2023 14:39:28 GMT'}, {'version': 'v2', 'created': 'Tue, 23 May 2023 01:02:11 GMT'}]",2023-05-24,"[['Zhang', 'Xiaotian', ''], ['Li', 'Chunyang', ''], ['Zong', 'Yi', ''], ['Ying', 'Zhengyu', ''], ['He', 'Liang', ''], ['Qiu', 'Xipeng', '']]",1,1,2023-05-21,2,6,2,1,0,1,c18f2239a4bd8cc68db9a013416167357f5e1353,258833562.0,https://www.semanticscholar.org/paper/c18f2239a4bd8cc68db9a013416167357f5e1353,arXiv.org,2023.0,6.0,20.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108015537', 'name': 'Xiaotian Zhang'}, {'authorId': '2118001750', 'name': 'Chun-yan Li'}, {'authorId': '2218062187', 'name': 'Yi Zong'}, {'authorId': '2218056913', 'name': 'Zhengyu Ying'}, {'authorId': '145836237', 'name': 'Liang He'}, {'authorId': '2188058565', 'name': 'Xipeng Qiu'}]","['East China Normal University', 'Fudan University']",['China'],2023-05
2305.12476,Lin Li,"Lin Li, Jun Xiao, Guikun Chen, Jian Shao, Yueting Zhuang, Long Chen",Zero-shot Visual Relation Detection via Composite Visual Cues from Large Language Models,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pretrained vision-language models, such as CLIP, have demonstrated strong generalization capabilities, making them promising tools in the realm of zero-shot visual recognition. Visual relation detection (VRD) is a typical task that identifies relationship (or interaction) types between object pairs within an image. However, naively utilizing CLIP with prevalent class-based prompts for zero-shot VRD has several weaknesses, e.g., it struggles to distinguish between different fine-grained relation types and it neglects essential spatial information of two objects. To this end, we propose a novel method for zero-shot VRD: RECODE, which solves RElation detection via COmposite DEscription prompts. Specifically, RECODE first decomposes each predicate category into subject, object, and spatial components. Then, it leverages large language models (LLMs) to generate description-based prompts (or visual cues) for each component. Different visual cues enhance the discriminability of similar relation categories from different perspectives, which significantly boosts performance in VRD. To dynamically fuse different cues, we further introduce a chain-of-thought method that prompts LLMs to generate reasonable weights for different visual cues. Extensive experiments on four VRD benchmarks have demonstrated the effectiveness and interpretability of RECODE. ","[{'version': 'v1', 'created': 'Sun, 21 May 2023 14:40:48 GMT'}, {'version': 'v2', 'created': 'Tue, 26 Sep 2023 13:44:47 GMT'}]",2023-09-27,"[['Li', 'Lin', ''], ['Xiao', 'Jun', ''], ['Chen', 'Guikun', ''], ['Shao', 'Jian', ''], ['Zhuang', 'Yueting', ''], ['Chen', 'Long', '']]",0,0,2023-05-21,2,6,1,0,0,0,8b9530d4102bb385032e11ff258a3151ac6128ca,258832960.0,https://www.semanticscholar.org/paper/8b9530d4102bb385032e11ff258a3151ac6128ca,arXiv.org,2023.0,37.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2155688727', 'name': 'Lin Li'}, {'authorId': '2143491050', 'name': 'Jun Xiao'}, {'authorId': '2168613674', 'name': 'Guikun Chen'}, {'authorId': '2177137677', 'name': 'Jian Shao'}, {'authorId': '2125211', 'name': 'Yueting Zhuang'}, {'authorId': '2145116668', 'name': 'Long Chen'}]","['Zhejiang University', 'Hong Kong University of Science and Technology']",['China'],2023-05
2305.12519,Yuang Qi,"Xiao Yu, Yuang Qi, Kejiang Chen, Guoqiang Chen, Xi Yang, Pengyuan Zhu,
  Weiming Zhang and Nenghai Yu",GPT Paternity Test: GPT Generated Text Detection with GPT Genetic Inheritance,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) can generate texts that carry the risk of various misuses, including plagiarism, planting fake reviews on e-commerce platforms, or creating fake social media postings that can sway election results. Detecting whether a text is machine-generated has thus become increasingly important. While machine-learning-based detection strategies exhibit superior performance, they often lack generalizability, limiting their practicality. In this work, we introduce GPT Paternity Test (GPT-Pat), which reliably detects machine-generated text across varied datasets. Given a text under scrutiny, we leverage ChatGPT to generate a corresponding question and provide a re-answer to the question. By comparing the similarity between the original text and the generated re-answered text, it can be determined whether the text is machine-generated. GPT-Pat consists of a Siamese network to compute the similarity between the original text and the generated re-answered text and a binary classifier. Our method achieved an average accuracy of 94.57% on four generalization test sets, surpassing the state-of-the-art RoBERTa-based method by 12.34%. The accuracy drop of our method is only about half of that of the RoBERTa-based method when it is attacked by re-translation and polishing. ","[{'version': 'v1', 'created': 'Sun, 21 May 2023 17:26:16 GMT'}]",2023-05-23,"[['Yu', 'Xiao', ''], ['Qi', 'Yuang', ''], ['Chen', 'Kejiang', ''], ['Chen', 'Guoqiang', ''], ['Yang', 'Xi', ''], ['Zhu', 'Pengyuan', ''], ['Zhang', 'Weiming', ''], ['Yu', 'Nenghai', '']]",1,1,2023-05-21,1,8,3,1,0,1,11dad86a020d984751ef41b725a2fbe1a41b3f7c,258833423.0,https://www.semanticscholar.org/paper/11dad86a020d984751ef41b725a2fbe1a41b3f7c,arXiv.org,2023.0,31.0,5.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': None, 'name': 'Xiao Yu'}, {'authorId': '2217971466', 'name': 'Yuang Qi'}, {'authorId': '8780109', 'name': 'Kejiang Chen'}, {'authorId': '2218643106', 'name': 'Guoqiang Chen'}, {'authorId': '2143365293', 'name': 'Xi Yang'}, {'authorId': '4992169', 'name': 'Pengyuan Zhu'}, {'authorId': '51027868', 'name': 'Weiming Zhang'}, {'authorId': '2052212945', 'name': 'Neng H. Yu'}]","['University of Science and Technology of China', 'Hefei High-dimensional Data Technology']",['China'],2023-05
2305.12564,Jin Kim,Jared Wong and Jin Kim,ChatGPT Is More Likely to Be Perceived as Male Than Female,,,,,cs.HC cs.AI cs.CL cs.LG,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  We investigate how people perceive ChatGPT, and, in particular, how they assign human-like attributes such as gender to the chatbot. Across five pre-registered studies (N = 1,552), we find that people are more likely to perceive ChatGPT to be male than female. Specifically, people perceive male gender identity (1) following demonstrations of ChatGPT's core abilities (e.g., providing information or summarizing text), (2) in the absence of such demonstrations, and (3) across different methods of eliciting perceived gender (using various scales and asking to name ChatGPT). Moreover, we find that this seemingly default perception of ChatGPT as male can reverse when ChatGPT's feminine-coded abilities are highlighted (e.g., providing emotional support for a user). ","[{'version': 'v1', 'created': 'Sun, 21 May 2023 20:57:12 GMT'}]",2023-05-23,"[['Wong', 'Jared', ''], ['Kim', 'Jin', '']]",1,1,2023-05-21,1,2,4,1,0,1,cadc357a7244f3412ccb1cffc675b02b0a2f99d8,258833258.0,https://www.semanticscholar.org/paper/cadc357a7244f3412ccb1cffc675b02b0a2f99d8,arXiv.org,2023.0,44.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Psychology', 'source': 's2-fos-model'}]","[{'authorId': '2218766392', 'name': 'Jared Wong'}, {'authorId': '2218698736', 'name': 'Jin Kim'}]","['Tongji University', 'Department of Marketing, Yale School of Management']",['China'],2023-05
2305.12676,Hong Liu,"Hong Liu, Zhaobiao Lv, Zhijian Ou, Wenbo Zhao, Qing Xiao",Exploring Energy-based Language Models with Different Architectures and Training Methods for Speech Recognition,Accepted into INTERSPEECH 2023,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Energy-based language models (ELMs) parameterize an unnormalized distribution for natural sentences and are radically different from popular autoregressive language models (ALMs). As an important application, ELMs have been successfully used as a means for calculating sentence scores in speech recognition, but they all use less-modern CNN or LSTM networks. The recent progress in Transformer networks and large pretrained models such as BERT and GPT2 opens new possibility to further advancing ELMs. In this paper, we explore different architectures of energy functions and different training methods to investigate the capabilities of ELMs in rescoring for speech recognition, all using large pretrained models as backbones. ","[{'version': 'v1', 'created': 'Mon, 22 May 2023 03:28:48 GMT'}, {'version': 'v2', 'created': 'Fri, 26 May 2023 05:55:34 GMT'}, {'version': 'v3', 'created': 'Mon, 29 May 2023 06:38:21 GMT'}]",2023-05-30,"[['Liu', 'Hong', ''], ['Lv', 'Zhaobiao', ''], ['Ou', 'Zhijian', ''], ['Zhao', 'Wenbo', ''], ['Xiao', 'Qing', '']]",0,1,2023-05-22,3,5,1,1,1,0,1f99d057382445a7c83f15693e287f56d6305185,258832892.0,https://www.semanticscholar.org/paper/1f99d057382445a7c83f15693e287f56d6305185,Interspeech,2023.0,34.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2175085268', 'name': 'Hong Liu'}, {'authorId': '3036361', 'name': 'Z. Lv'}, {'authorId': '1717830', 'name': 'Zhijian Ou'}, {'authorId': '1902008743', 'name': 'Wenbo Zhao'}, {'authorId': '2218046839', 'name': 'Qing Xiao'}]","['China United Network Communications Group (China)', 'Tsinghua University']",['China'],2023-05
2305.12723,Xinlu Zhang,"Xinlu Zhang, Shiyang Li, Xianjun Yang, Chenxin Tian, Yao Qin, Linda
  Ruth Petzold",Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under privacy-restricted scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability. ","[{'version': 'v1', 'created': 'Mon, 22 May 2023 05:14:38 GMT'}]",2023-05-23,"[['Zhang', 'Xinlu', ''], ['Li', 'Shiyang', ''], ['Yang', 'Xianjun', ''], ['Tian', 'Chenxin', ''], ['Qin', 'Yao', ''], ['Petzold', 'Linda Ruth', '']]",0,0,2023-05-22,1,6,2,0,0,0,74b94891f8f7ac8d73d9df817b6720e1cb792bcc,258832501.0,https://www.semanticscholar.org/paper/74b94891f8f7ac8d73d9df817b6720e1cb792bcc,arXiv.org,2023.0,75.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Medicine', 'source': 's2-fos-model'}]","[{'authorId': '2108030191', 'name': 'Xinlu Zhang'}, {'authorId': '50341591', 'name': 'SHIYANG LI'}, {'authorId': '2145170944', 'name': 'Xianjun Yang'}, {'authorId': '2218035941', 'name': 'Chenxin Tian'}, {'authorId': '2219078907', 'name': 'Yao Qin'}, {'authorId': '21038849', 'name': 'Linda Petzold'}]","['Chinese Academy of Medical Sciences & Peking Union Medical College', 'University of California, Santa Barbara']","['China', 'United States']",2023-05
2305.12763,You Shan,"Yiting Chen, Tracy Xiao Liu, You Shan, and Songfa Zhong",The Emergence of Economic Rationality of GPT,,,,,econ.GN q-fin.EC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As large language models (LLMs) like GPT become increasingly prevalent, it is essential that we assess their capabilities beyond language processing. This paper examines the economic rationality of GPT by instructing it to make budgetary decisions in four domains: risk, time, social, and food preferences. We measure economic rationality by assessing the consistency of GPT's decisions with utility maximization in classic revealed preference theory. We find that GPT's decisions are largely rational in each domain and demonstrate higher rationality score than those of human subjects in a parallel experiment and in the literature. Moreover, the estimated preference parameters of GPT are slightly different from human subjects and exhibit a lower degree of heterogeneity. We also find that the rationality scores are robust to the degree of randomness and demographic settings such as age and gender, but are sensitive to contexts based on the language frames of the choice situations. These results suggest the potential of LLMs to make good decisions and the need to further understand their capabilities, limitations, and underlying mechanisms. ","[{'version': 'v1', 'created': 'Mon, 22 May 2023 06:32:28 GMT'}, {'version': 'v2', 'created': 'Tue, 19 Sep 2023 05:43:51 GMT'}]",2023-09-20,"[['Chen', 'Yiting', ''], ['Liu', 'Tracy Xiao', ''], ['Shan', 'You', ''], ['Zhong', 'Songfa', '']]",0,1,2023-05-22,2,4,2,0,0,0,de916e9e6f8841dbb73b35615f8d15eba2e9eaf7,258833561.0,https://www.semanticscholar.org/paper/de916e9e6f8841dbb73b35615f8d15eba2e9eaf7,,2023.0,82.0,4.0,0.0,False,['Economics'],"[{'category': 'Economics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Economics', 'source': 's2-fos-model'}]","[{'authorId': '2109060436', 'name': 'Yiting Chen'}, {'authorId': '3355580', 'name': 'T. Liu'}, {'authorId': '2107059010', 'name': 'You Shan'}, {'authorId': '2512275', 'name': 'Songfa Zhong'}]","['Lingnan University', 'Hong Kong University of Science and Technology']","['China', 'Hong Kong']",2023-05
2305.12799,Qifan Yu,"Qifan Yu, Juncheng Li, Wentao Ye, Siliang Tang, Yueting Zhuang",Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration,"11 pages, 6 figures, technical report",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent text-to-image generation models have shown promising results in generating high-fidelity photo-realistic images. In parallel, the problem of data scarcity has brought a growing interest in employing AIGC technology for high-quality data expansion. However, this paradigm requires well-designed prompt engineering that cost-less data expansion and labeling remain under-explored. Inspired by LLM's powerful capability in task guidance, we propose a new paradigm of annotated data expansion named as ChatGenImage. The core idea behind it is to leverage the complementary strengths of diverse models to establish a highly effective and user-friendly pipeline for interactive data augmentation. In this work, we extensively study how LLMs communicate with AIGC model to achieve more controllable image generation and make the first attempt to collaborate them for automatic data augmentation for a variety of downstream tasks. Finally, we present fascinating results obtained from our ChatGenImage framework and demonstrate the powerful potential of our synthetic data for systematic vision adaptation. Our codes are available at https://github.com/Yuqifan1117/Labal-Anything-Pipeline. ","[{'version': 'v1', 'created': 'Mon, 22 May 2023 07:53:36 GMT'}]",2023-05-23,"[['Yu', 'Qifan', ''], ['Li', 'Juncheng', ''], ['Ye', 'Wentao', ''], ['Tang', 'Siliang', ''], ['Zhuang', 'Yueting', '']]",0,0,2023-05-22,1,5,1,0,0,0,43a55dbd95c9d5cd82de8db276f41adeec4a937d,258832615.0,https://www.semanticscholar.org/paper/43a55dbd95c9d5cd82de8db276f41adeec4a937d,arXiv.org,2023.0,38.0,5.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2206456553', 'name': 'Qifan Yu'}, {'authorId': '2108998895', 'name': 'Juncheng Li'}, {'authorId': '2218633007', 'name': 'Wentao Ye'}, {'authorId': '2118071462', 'name': 'Siliang Tang'}, {'authorId': '2125211', 'name': 'Yueting Zhuang'}]",['Zhejiang University'],['China'],2023-05
2305.12865,Weisong Sun,"Weisong Sun, Chunrong Fang, Yudu You, Yun Miao, Yi Liu, Yuekang Li,
  Gelei Deng, Shenghan Huang, Yuchen Chen, Quanjun Zhang, Hanwei Qian, Yang
  Liu, Zhenyu Chen",Automatic Code Summarization via ChatGPT: How Far Are We?,,,,,cs.SE cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  To support software developers in understanding and maintaining programs, various automatic code summarization techniques have been proposed to generate a concise natural language comment for a given code snippet. Recently, the emergence of large language models (LLMs) has led to a great boost in the performance of natural language processing tasks. Among them, ChatGPT is the most popular one which has attracted wide attention from the software engineering community. However, it still remains unclear how ChatGPT performs in (automatic) code summarization. Therefore, in this paper, we focus on evaluating ChatGPT on a widely-used Python dataset called CSN-Python and comparing it with several state-of-the-art (SOTA) code summarization models. Specifically, we first explore an appropriate prompt to guide ChatGPT to generate in-distribution comments. Then, we use such a prompt to ask ChatGPT to generate comments for all code snippets in the CSN-Python test set. We adopt three widely-used metrics (including BLEU, METEOR, and ROUGE-L) to measure the quality of the comments generated by ChatGPT and SOTA models (including NCS, CodeBERT, and CodeT5). The experimental results show that in terms of BLEU and ROUGE-L, ChatGPT's code summarization performance is significantly worse than all three SOTA models. We also present some cases and discuss the advantages and disadvantages of ChatGPT in code summarization. Based on the findings, we outline several open challenges and opportunities in ChatGPT-based code summarization. ","[{'version': 'v1', 'created': 'Mon, 22 May 2023 09:43:40 GMT'}]",2023-05-23,"[['Sun', 'Weisong', ''], ['Fang', 'Chunrong', ''], ['You', 'Yudu', ''], ['Miao', 'Yun', ''], ['Liu', 'Yi', ''], ['Li', 'Yuekang', ''], ['Deng', 'Gelei', ''], ['Huang', 'Shenghan', ''], ['Chen', 'Yuchen', ''], ['Zhang', 'Quanjun', ''], ['Qian', 'Hanwei', ''], ['Liu', 'Yang', ''], ['Chen', 'Zhenyu', '']]",1,1,2023-05-22,1,13,2,1,0,1,bbde5c00cecd9dc3899179819f7ab82771ad5315,258833342.0,https://www.semanticscholar.org/paper/bbde5c00cecd9dc3899179819f7ab82771ad5315,arXiv.org,2023.0,66.0,11.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '3433022', 'name': 'Weisong Sun'}, {'authorId': '7595994', 'name': 'Chunrong Fang'}, {'authorId': '2170360816', 'name': 'Yudu You'}, {'authorId': '2053094254', 'name': 'Yun Miao'}, {'authorId': '2178397972', 'name': 'Yi Liu'}, {'authorId': '22799258', 'name': 'Yuekang Li'}, {'authorId': '73776889', 'name': 'Gelei Deng'}, {'authorId': '2218349947', 'name': 'Shenghan Huang'}, {'authorId': '2155380281', 'name': 'Yuchen Chen'}, {'authorId': '1409701329', 'name': 'Quanjun Zhang'}, {'authorId': '2218042020', 'name': 'Hanwei Qian'}, {'authorId': '2152803192', 'name': 'Yang Liu'}, {'authorId': '2120327152', 'name': 'Zhenyu Chen'}]","['Nanjing University', 'UNSW Sydney', 'Nanyang Technological University']","['China', 'Singapore', 'Australia']",2023-05
2305.12943,Munan Ning,"Munan Ning, Yujia Xie, Dongdong Chen, Zeyin Song, Lu Yuan, Yonghong
  Tian, Qixiang Ye, Li Yuan",Album Storytelling with Iterative Story-aware Captioning and Large Language Models,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This work studies how to transform an album to vivid and coherent stories, a task we refer to as ""album storytelling"". While this task can help preserve memories and facilitate experience sharing, it remains an underexplored area in current literature. With recent advances in Large Language Models (LLMs), it is now possible to generate lengthy, coherent text, opening up the opportunity to develop an AI assistant for album storytelling. One natural approach is to use caption models to describe each photo in the album, and then use LLMs to summarize and rewrite the generated captions into an engaging story. However, we find this often results in stories containing hallucinated information that contradicts the images, as each generated caption (""story-agnostic"") is not always about the description related to the whole story or miss some necessary information. To address these limitations, we propose a new iterative album storytelling pipeline. Specifically, we start with an initial story and build a story-aware caption model to refine the captions using the whole story as guidance. The polished captions are then fed into the LLMs to generate a new refined story. This process is repeated iteratively until the story contains minimal factual errors while maintaining coherence. To evaluate our proposed pipeline, we introduce a new dataset of image collections from vlogs and a set of systematic evaluation metrics. Our results demonstrate that our method effectively generates more accurate and engaging stories for albums, with enhanced coherence and vividness. ","[{'version': 'v1', 'created': 'Mon, 22 May 2023 11:45:10 GMT'}, {'version': 'v2', 'created': 'Wed, 24 May 2023 02:58:03 GMT'}]",2023-05-25,"[['Ning', 'Munan', ''], ['Xie', 'Yujia', ''], ['Chen', 'Dongdong', ''], ['Song', 'Zeyin', ''], ['Yuan', 'Lu', ''], ['Tian', 'Yonghong', ''], ['Ye', 'Qixiang', ''], ['Yuan', 'Li', '']]",0,0,2023-05-22,2,8,1,0,0,0,6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f,258832908.0,https://www.semanticscholar.org/paper/6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f,arXiv.org,2023.0,58.0,3.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '35480371', 'name': 'Munan Ning'}, {'authorId': '2167703904', 'name': 'Yujia Xie'}, {'authorId': '49025801', 'name': 'Dongdong Chen'}, {'authorId': '2213675977', 'name': 'Zeyin Song'}, {'authorId': '2150687325', 'name': 'Lu Yuan'}, {'authorId': '2115729952', 'name': 'Yonghong Tian'}, {'authorId': '1694936', 'name': 'Qixiang Ye'}, {'authorId': '2152193547', 'name': 'Liuliang Yuan'}]","['Peking University', 'University of Chinese Academy of Sciences']",['China'],2023-05
2305.12945,Dongfang Li,"Dongfang Li, Jindi Yu, Baotian Hu, Zhenran Xu and Min Zhang",ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As ChatGPT and GPT-4 spearhead the development of Large Language Models (LLMs), more researchers are investigating their performance across various tasks. But more research needs to be done on the interpretability capabilities of LLMs, that is, the ability to generate reasons after an answer has been given. Existing explanation datasets are mostly English-language general knowledge questions, which leads to insufficient thematic and linguistic diversity. To address the language bias and lack of medical resources in generating rationales QA datasets, we present ExplainCPE (over 7k instances), a challenging medical benchmark in Simplified Chinese. We analyzed the errors of ChatGPT and GPT-4, pointing out the limitations of current LLMs in understanding text and computational reasoning. During the experiment, we also found that different LLMs have different preferences for in-context learning. ExplainCPE presents a significant challenge, but its potential for further investigation is promising, and it can be used to evaluate the ability of a model to generate explanations. AI safety and trustworthiness need more attention, and this work makes the first step to explore the medical interpretability of LLMs.The dataset is available at https://github.com/HITsz-TMG/ExplainCPE. ","[{'version': 'v1', 'created': 'Mon, 22 May 2023 11:45:42 GMT'}]",2023-05-23,"[['Li', 'Dongfang', ''], ['Yu', 'Jindi', ''], ['Hu', 'Baotian', ''], ['Xu', 'Zhenran', ''], ['Zhang', 'Min', '']]",1,1,2023-05-22,1,5,1,2,0,2,d4b250b67454a889a68e1a813eb844b85005b1f7,258833599.0,https://www.semanticscholar.org/paper/d4b250b67454a889a68e1a813eb844b85005b1f7,arXiv.org,2023.0,49.0,3.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1664667501', 'name': 'Dongfang Li'}, {'authorId': '2218309999', 'name': 'Jindi Yu'}, {'authorId': '33968873', 'name': 'Baotian Hu'}, {'authorId': '2149238600', 'name': 'Zhenran Xu'}, {'authorId': '50495870', 'name': 'M. Zhang'}]",['Harbin Institute of Technology'],['China'],2023-05
2305.12964,Yang Bai,"Yang Bai, Jingyao Wang, Min Cao, Chen Chen, Ziqiang Cao, Liqiang Nie
  and Min Zhang",Text-based Person Search without Parallel Image-Text Data,Accepted by ACM MM 2023,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Text-based person search (TBPS) aims to retrieve the images of the target person from a large image gallery based on a given natural language description. Existing methods are dominated by training models with parallel image-text pairs, which are very costly to collect. In this paper, we make the first attempt to explore TBPS without parallel image-text data ($\mu$-TBPS), in which only non-parallel images and texts, or even image-only data, can be adopted. Towards this end, we propose a two-stage framework, generation-then-retrieval (GTR), to first generate the corresponding pseudo text for each image and then perform the retrieval in a supervised manner. In the generation stage, we propose a fine-grained image captioning strategy to obtain an enriched description of the person image, which firstly utilizes a set of instruction prompts to activate the off-the-shelf pretrained vision-language model to capture and generate fine-grained person attributes, and then converts the extracted attributes into a textual description via the finetuned large language model or the hand-crafted template. In the retrieval stage, considering the noise interference of the generated texts for training model, we develop a confidence score-based training scheme by enabling more reliable texts to contribute more during the training. Experimental results on multiple TBPS benchmarks (i.e., CUHK-PEDES, ICFG-PEDES and RSTPReid) show that the proposed GTR can achieve a promising performance without relying on parallel image-text data. ","[{'version': 'v1', 'created': 'Mon, 22 May 2023 12:13:08 GMT'}, {'version': 'v2', 'created': 'Fri, 4 Aug 2023 13:04:24 GMT'}]",2023-08-07,"[['Bai', 'Yang', ''], ['Wang', 'Jingyao', ''], ['Cao', 'Min', ''], ['Chen', 'Chen', ''], ['Cao', 'Ziqiang', ''], ['Nie', 'Liqiang', ''], ['Zhang', 'Min', '']]",0,0,2023-05-22,2,7,1,0,0,0,0213827d882ec34aa9935f2b03a80362af806778,258833639.0,https://www.semanticscholar.org/paper/0213827d882ec34aa9935f2b03a80362af806778,ACM Multimedia,2023.0,71.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2115297411', 'name': 'Yang Bai'}, {'authorId': '2193098014', 'name': 'Jingyao Wang'}, {'authorId': '2057073120', 'name': 'Min Cao'}, {'authorId': '40262099', 'name': 'Cheng Chen'}, {'authorId': '2314396', 'name': 'Ziqiang Cao'}, {'authorId': '143982887', 'name': 'Liqiang Nie'}, {'authorId': '2157503110', 'name': 'Min Zhang'}]","['Harbin Institute of Technology', 'Soochow University', 'Chinese Academy of Sciences']",['China'],2023-05
2305.13016,Jiaxi Yang,"Jiaxi Yang, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, Yongbin Li",Iterative Forward Tuning Boosts In-context Learning in Language Models,"14 pages, 5 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have exhibited an emergent in-context learning (ICL) ability. However, the ICL models that can solve ordinary cases are hardly extended to solve more complex tasks by processing the demonstration examples once. This single-turn ICL is incoordinate with the decision making process of humans by learning from analogy. In this paper, we propose an effective and efficient two-stage framework to boost ICL in LLMs by exploiting a dual form between Transformer attention and gradient descent-based optimization. Concretely, we divide the ICL process into ""Deep-Thinking"" and inference stages. The ""Deep-Thinking"" stage performs iterative forward optimization of demonstrations, which is expected to boost the reasoning abilities of LLMs at test time by ""thinking"" demonstrations multiple times. It produces accumulated meta-gradients by manipulating the Key-Value matrices in the self-attention modules of the Transformer. Then, the inference stage only takes the test query as input without concatenating demonstrations and applies the learned meta-gradients through attention for output prediction. In this way, demonstrations are not required during the inference stage since they are already learned and stored in the definitive meta-gradients. LLMs can be effectively and efficiently adapted to downstream tasks. Extensive experiments on ten classification and multiple-choice datasets show that our method achieves substantially better performance than standard ICL in terms of both accuracy and efficiency. ","[{'version': 'v1', 'created': 'Mon, 22 May 2023 13:18:17 GMT'}, {'version': 'v2', 'created': 'Tue, 30 May 2023 05:47:19 GMT'}]",2023-05-31,"[['Yang', 'Jiaxi', ''], ['Hui', 'Binyuan', ''], ['Yang', 'Min', ''], ['Li', 'Binhua', ''], ['Huang', 'Fei', ''], ['Li', 'Yongbin', '']]",0,0,2023-05-22,2,6,1,0,0,0,43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0,258832340.0,https://www.semanticscholar.org/paper/43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0,arXiv.org,2023.0,42.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2135964855', 'name': 'Jiaxi Yang'}, {'authorId': '151471590', 'name': 'Binyuan Hui'}, {'authorId': '2144399900', 'name': 'Min Yang'}, {'authorId': '66200440', 'name': 'Binhua Li'}, {'authorId': '2087380523', 'name': 'Fei Huang'}, {'authorId': '1527090216', 'name': 'Yongbin Li'}]","['Alibaba', 'Shenzhen Institutes of Advanced Technology']",['China'],2023-05
2305.13112,Xiaolei Wang,"Xiaolei Wang, Xinyu Tang, Wayne Xin Zhao, Jingyuan Wang, Ji-Rong Wen",Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models,work in progress,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The recent success of large language models (LLMs) has shown great potential to develop more powerful conversational recommender systems (CRSs), which rely on natural language conversations to satisfy user needs. In this paper, we embark on an investigation into the utilization of ChatGPT for conversational recommendation, revealing the inadequacy of the existing evaluation protocol. It might over-emphasize the matching with the ground-truth items or utterances generated by human annotators, while neglecting the interactive nature of being a capable CRS. To overcome the limitation, we further propose an interactive Evaluation approach based on LLMs named iEvaLM that harnesses LLM-based user simulators. Our evaluation approach can simulate various interaction scenarios between users and systems. Through the experiments on two publicly available CRS datasets, we demonstrate notable improvements compared to the prevailing evaluation protocol. Furthermore, we emphasize the evaluation of explainability, and ChatGPT showcases persuasive explanation generation for its recommendations. Our study contributes to a deeper comprehension of the untapped potential of LLMs for CRSs and provides a more flexible and easy-to-use evaluation framework for future research endeavors. The codes and data are publicly available at https://github.com/RUCAIBox/iEvaLM-CRS. ","[{'version': 'v1', 'created': 'Mon, 22 May 2023 15:12:43 GMT'}]",2023-05-23,"[['Wang', 'Xiaolei', ''], ['Tang', 'Xinyu', ''], ['Zhao', 'Wayne Xin', ''], ['Wang', 'Jingyuan', ''], ['Wen', 'Ji-Rong', '']]",1,1,2023-05-22,1,5,2,1,0,1,c476c1587beda904133f97592a39965be418c8bf,258833623.0,https://www.semanticscholar.org/paper/c476c1587beda904133f97592a39965be418c8bf,arXiv.org,2023.0,50.0,15.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '72541556', 'name': 'Xiaolei Wang'}, {'authorId': '2109887979', 'name': 'Xinyu Tang'}, {'authorId': '2542603', 'name': 'Wayne Xin Zhao'}, {'authorId': '2115891766', 'name': 'Jingyuan Wang'}, {'authorId': '153693432', 'name': 'Ji-rong Wen'}]","['Beihang University', 'Renmin University of China', 'Beijing Key Laboratory of Big Data Management and Analysis Methods']",['China'],2023-05
2305.13114,Reza Hadi Mogavi,"Reza Hadi Mogavi, Chao Deng, Justin Juho Kim, Pengyuan Zhou, Young D.
  Kwon, Ahmed Hosny Saleh Metwally, Ahmed Tlili, Simone Bassanelli, Antonio
  Bucchiarone, Sujit Gujar, Lennart E. Nacke, and Pan Hui","Exploring User Perspectives on ChatGPT: Applications, Perceptions, and Implications for AI-Integrated Education",Preprint version,,,,cs.CY cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Understanding user perspectives on Artificial Intelligence (AI) in education is essential for creating pedagogically effective and ethically responsible AI-integrated learning environments. In this paper, we conduct an extensive qualitative content analysis of four major social media platforms (Twitter, Reddit, YouTube, and LinkedIn) to explore the user experience (UX) and perspectives of early adopters toward ChatGPT-an AI Chatbot technology-in various education sectors. We investigate the primary applications of ChatGPT in education (RQ1) and the various perceptions of the technology (RQ2). Our findings indicate that ChatGPT is most popularly used in the contexts of higher education (24.18%), K-12 education (22.09%), and practical-skills learning (15.28%). On social media platforms, the most frequently discussed topics about ChatGPT are productivity, efficiency, and ethics. While some early adopters lean toward seeing ChatGPT as a revolutionary technology with the potential to boost students' self-efficacy and motivation to learn, others express concern that overreliance on the AI system may promote superficial learning habits and erode students' social and critical thinking skills. Our study contributes to the broader discourse on Human-AI Interaction and offers recommendations based on crowd-sourced knowledge for educators and learners interested in incorporating ChatGPT into their educational settings. Furthermore, we propose a research agenda for future studies that sets the foundation for continued investigation into the application of ChatGPT in education. ","[{'version': 'v1', 'created': 'Mon, 22 May 2023 15:13:14 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Jun 2023 09:46:06 GMT'}]",2023-06-14,"[['Mogavi', 'Reza Hadi', ''], ['Deng', 'Chao', ''], ['Kim', 'Justin Juho', ''], ['Zhou', 'Pengyuan', ''], ['Kwon', 'Young D.', ''], ['Metwally', 'Ahmed Hosny Saleh', ''], ['Tlili', 'Ahmed', ''], ['Bassanelli', 'Simone', ''], ['Bucchiarone', 'Antonio', ''], ['Gujar', 'Sujit', ''], ['Nacke', 'Lennart E.', ''], ['Hui', 'Pan', '']]",1,1,2023-05-22,2,12,2,1,0,1,2a350127799a82ce7280b1e95f9ab23c73cfe8f8,258833306.0,https://www.semanticscholar.org/paper/2a350127799a82ce7280b1e95f9ab23c73cfe8f8,arXiv.org,2023.0,89.0,6.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Education', 'source': 's2-fos-model'}]","[{'authorId': '2266408', 'name': 'Reza Hadi Mogavi'}, {'authorId': '40209052', 'name': 'Chaohua Deng'}, {'authorId': '2218347964', 'name': 'Justin Juho Kim'}, {'authorId': '72050450', 'name': 'Pengyuan Zhou'}, {'authorId': '97517796', 'name': 'Young D. Kwon'}, {'authorId': '35391544', 'name': 'A. H. Metwally'}, {'authorId': '27362922', 'name': 'A. Tlili'}, {'authorId': '2146584418', 'name': 'Simone Bassanelli'}, {'authorId': '1680267', 'name': 'A. Bucchiarone'}, {'authorId': '39686395', 'name': 'Sujit Gujar'}, {'authorId': '1953205', 'name': 'L. Nacke'}, {'authorId': '143966169', 'name': 'Pan Hui'}]","['University of Helsinki', 'University of Waterloo', 'Hong Kong University of Science and Technology', 'University of Cambridge', 'International Institute of Information Technology, Hyderabad', 'Beijing Normal University', 'Fondazione Bruno Kessler', 'University of Science and Technology of China', 'Meta']","['Canada', 'India', 'United States', 'United Kingdom', 'China', 'Finland', 'Italy']",2023-05
2305.13144,Zangwei Zheng,"Zangwei Zheng, Xiaozhe Ren, Fuzhao Xue, Yang Luo, Xin Jiang, Yang You",Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have revolutionized the field of AI, demonstrating unprecedented capacity across various tasks. However, the inference process for LLMs comes with significant computational costs. In this paper, we propose an efficient LLM inference pipeline that harnesses the power of LLMs. Our approach begins by tapping into the potential of LLMs to accurately perceive and predict the response length with minimal overhead. By leveraging this information, we introduce an efficient sequence scheduling technique that groups queries with similar response lengths into micro-batches. We evaluate our approach on real-world instruction datasets using the LLaMA-based model, and our results demonstrate an impressive 86% improvement in inference throughput without compromising effectiveness. Notably, our method is orthogonal to other inference acceleration techniques, making it a valuable addition to many existing toolkits (e.g., FlashAttention, Quantization) for LLM inference. ","[{'version': 'v1', 'created': 'Mon, 22 May 2023 15:36:06 GMT'}, {'version': 'v2', 'created': 'Sun, 28 May 2023 08:22:19 GMT'}]",2023-05-30,"[['Zheng', 'Zangwei', ''], ['Ren', 'Xiaozhe', ''], ['Xue', 'Fuzhao', ''], ['Luo', 'Yang', ''], ['Jiang', 'Xin', ''], ['You', 'Yang', '']]",0,0,2023-05-22,2,6,1,1,1,0,57e90f4fae211f23e2c2d437e344a40ee9da87a0,258833168.0,https://www.semanticscholar.org/paper/57e90f4fae211f23e2c2d437e344a40ee9da87a0,arXiv.org,2023.0,42.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2109654065', 'name': 'Zangwei Zheng'}, {'authorId': '153457264', 'name': 'Xiaozhe Ren'}, {'authorId': '2144332771', 'name': 'Fuzhao Xue'}, {'authorId': '2218050858', 'name': 'Yang Luo'}, {'authorId': '145820291', 'name': 'Xin Jiang'}, {'authorId': '2054451943', 'name': 'Yang You'}]","['National University of Singapore', 'Huawei Technologies (China)']","['China', 'Singapore']",2023-05
2305.13172,Ningyu Zhang,"Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin
  Deng, Huajun Chen, Ningyu Zhang","Editing Large Language Models: Problems, Methods, and Opportunities",Work in progress,,,,cs.CL cs.AI cs.CV cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advancements in deep learning have precipitated the emergence of large language models (LLMs) which exhibit an impressive aptitude for understanding and producing text akin to human language. Despite the ability to train highly capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To that end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to alter the behavior of LLMs within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities relating to model editing for LLMs. In particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. We also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. Our objective is to provide valuable insights into the effectiveness and feasibility of each model editing technique, thereby assisting the research community in making informed decisions when choosing the most appropriate method for a specific task or context. Code and datasets will be available at https://github.com/zjunlp/EasyEdit. ","[{'version': 'v1', 'created': 'Mon, 22 May 2023 16:00:00 GMT'}]",2023-05-23,"[['Yao', 'Yunzhi', ''], ['Wang', 'Peng', ''], ['Tian', 'Bozhong', ''], ['Cheng', 'Siyuan', ''], ['Li', 'Zhoubo', ''], ['Deng', 'Shumin', ''], ['Chen', 'Huajun', ''], ['Zhang', 'Ningyu', '']]",0,0,2023-05-22,1,8,5,0,0,0,f5c73d9e6641b018b633690102121f5605d34fb0,258833129.0,https://www.semanticscholar.org/paper/f5c73d9e6641b018b633690102121f5605d34fb0,arXiv.org,2023.0,87.0,25.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '4841460', 'name': 'Yunzhi Yao'}, {'authorId': '144282672', 'name': 'Peng Wang'}, {'authorId': '2064522174', 'name': 'Bo Tian'}, {'authorId': '46378881', 'name': 'Siyuan Cheng'}, {'authorId': '9956037', 'name': 'Zhoubo Li'}, {'authorId': '152931849', 'name': 'Shumin Deng'}, {'authorId': '2144200945', 'name': 'Huajun Chen'}, {'authorId': '2608639', 'name': 'Ningyu Zhang'}]","['Alibaba', 'National University of Singapore', 'Donghai Laboratory', 'Zhejiang University']","['China', 'Singapore']",2023-05
2305.13225,Yue Zhang,"Yue Zhang and Leyang Cui and Deng Cai and Xinting Huang and Tao Fang
  and Wei Bi",Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A Preliminary Study on Writing Assistance,Work in progress,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  ChatGPT and GPT-4 have attracted substantial interest from both academic and industrial circles, owing to their remarkable few-shot (or even zero-shot) ability to handle various tasks. Recent work shows that, after being fine-tuned with a few sets of instruction-driven data, the recently proposed LLM, LLaMa, exhibits an impressive capability to address a broad range of tasks. However, the zero-shot performance of LLMs does not consistently outperform that of models fined-tuned for specific scenarios. To explore whether the capabilities of LLMs can be further enhanced for specific scenarios, we choose the writing-assistance scenario as the testbed, including seven writing tasks. We collect training data for these tasks, reframe them in an instruction-following format, and subsequently refine LLaMa via instruction tuning. Experimental results show that continually fine-tuning LLaMa on writing instruction data significantly improves its ability on writing tasks. We also conduct more experiments and analyses to offer insights for future work on effectively fine-tuning LLaMa for specific scenarios. ","[{'version': 'v1', 'created': 'Mon, 22 May 2023 16:56:44 GMT'}]",2023-05-23,"[['Zhang', 'Yue', ''], ['Cui', 'Leyang', ''], ['Cai', 'Deng', ''], ['Huang', 'Xinting', ''], ['Fang', 'Tao', ''], ['Bi', 'Wei', '']]",1,1,2023-05-22,1,6,1,3,1,2,3f4a44b41f612c4f417adb085daa43623194d9f9,258832528.0,https://www.semanticscholar.org/paper/3f4a44b41f612c4f417adb085daa43623194d9f9,arXiv.org,2023.0,69.0,10.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1895977079', 'name': 'Yue Zhang'}, {'authorId': '152496687', 'name': 'Leyang Cui'}, {'authorId': '2053327987', 'name': 'Deng Cai'}, {'authorId': '14799547', 'name': 'Xinting Huang'}, {'authorId': '2213476432', 'name': 'Tao Fang'}, {'authorId': '1844673750', 'name': 'Wei Bi'}]","['Soochow University', 'University of Macau', 'Tencent']","['China', 'Taiwan', 'Macau']",2023-05
2305.13242,Yafu Li,"Yafu Li, Qintong Li, Leyang Cui, Wei Bi, Longyue Wang, Linyi Yang,
  Shuming Shi and Yue Zhang",Deepfake Text Detection in the Wild,Working in progress,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advances in large language models have enabled them to reach a level of text generation comparable to that of humans. These models show powerful capabilities across a wide range of content, including news article writing, story generation, and scientific writing. Such capability further narrows the gap between human-authored and machine-generated texts, highlighting the importance of deepfake text detection to avoid potential risks such as fake news propagation and plagiarism. However, previous work has been limited in that they testify methods on testbed of specific domains or certain language models. In practical scenarios, the detector faces texts from various domains or LLMs without knowing their sources. To this end, we build a wild testbed by gathering texts from various human writings and deepfake texts generated by different LLMs. Human annotators are only slightly better than random guessing at identifying machine-generated texts. Empirical results on automatic detection methods further showcase the challenges of deepfake text detection in a wild testbed. In addition, out-of-distribution poses a greater challenge for a detector to be employed in realistic application scenarios. We release our resources at https://github.com/yafuly/DeepfakeTextDetect. ","[{'version': 'v1', 'created': 'Mon, 22 May 2023 17:13:29 GMT'}]",2023-05-23,"[['Li', 'Yafu', ''], ['Li', 'Qintong', ''], ['Cui', 'Leyang', ''], ['Bi', 'Wei', ''], ['Wang', 'Longyue', ''], ['Yang', 'Linyi', ''], ['Shi', 'Shuming', ''], ['Zhang', 'Yue', '']]",0,0,2023-05-22,1,8,1,0,0,0,11beca761c3079f7612204ed1b241ee8832107c9,258832454.0,https://www.semanticscholar.org/paper/11beca761c3079f7612204ed1b241ee8832107c9,arXiv.org,2023.0,59.0,15.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2110450452', 'name': 'Yafu Li'}, {'authorId': '47422209', 'name': 'Qintong Li'}, {'authorId': '152496687', 'name': 'Leyang Cui'}, {'authorId': '1844673750', 'name': 'Wei Bi'}, {'authorId': '2111542852', 'name': 'Longyue Wang'}, {'authorId': '2145500840', 'name': 'Linyi Yang'}, {'authorId': '2072684668', 'name': 'Shuming Shi'}, {'authorId': '2211964951', 'name': 'Yue Zhang'}]","['Westlake University', 'Zhejiang University', 'University of Hong Kong', 'Tencent']","['China', 'Hong Kong']",2023-05
2305.13267,Wenjuan Han,"Yueting Yang, Xintong Zhang, Wenjuan Han",Enhance Reasoning Ability of Visual-Language Models via Large Language Models,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained visual language models (VLM) have shown excellent performance in image caption tasks. However, it sometimes shows insufficient reasoning ability. In contrast, large language models (LLMs) emerge with powerful reasoning capabilities. Therefore, we propose a method called TReE, which transfers the reasoning ability of a large language model to a visual language model in zero-shot scenarios. TReE contains three stages: observation, thinking, and re-thinking. Observation stage indicates that VLM obtains the overall information of the relative image. Thinking stage combines the image information and task description as the prompt of the LLM, inference with the rationals. Re-Thinking stage learns from rationale and then inference the final result through VLM. ","[{'version': 'v1', 'created': 'Mon, 22 May 2023 17:33:44 GMT'}]",2023-05-23,"[['Yang', 'Yueting', ''], ['Zhang', 'Xintong', ''], ['Han', 'Wenjuan', '']]",0,0,2023-05-22,1,3,2,0,0,0,ca055cfb9d4d47124cc035c346f38577825fcacf,258832597.0,https://www.semanticscholar.org/paper/ca055cfb9d4d47124cc035c346f38577825fcacf,arXiv.org,2023.0,40.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': None, 'name': 'Yueting Yang'}, {'authorId': '2218276686', 'name': 'Xintong Zhang'}, {'authorId': '144836032', 'name': 'Wenjuan Han'}]",['Beijing Jiaotong University'],['China'],2023-05
2305.13269,Xingxuan Li,"Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty,
  Soujanya Poria, Lidong Bing",Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains. These corrected rationales can plausibly serve as a better foundation for the final answer consolidation. Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information. To access both unstructured and structured knowledge sources in the dynamic knowledge adapting stage, we propose an adaptive query generator that allows the generation of queries for various types of query languages, including SPARQL, SQL, and natural sentences. Moreover, to minimize error propagation between rationales, CoK corrects the rationales progressively using preceding corrected rationales to generate and correct subsequent rationales. Extensive experiments show that CoK consistently improves the performance of LLMs on knowledge-intensive tasks across different domains. ","[{'version': 'v1', 'created': 'Mon, 22 May 2023 17:34:23 GMT'}, {'version': 'v2', 'created': 'Tue, 3 Oct 2023 12:30:20 GMT'}]",2023-10-04,"[['Li', 'Xingxuan', ''], ['Zhao', 'Ruochen', ''], ['Chia', 'Yew Ken', ''], ['Ding', 'Bosheng', ''], ['Joty', 'Shafiq', ''], ['Poria', 'Soujanya', ''], ['Bing', 'Lidong', '']]",0,0,2023-05-22,2,7,1,0,0,0,e468ed6b824e60f45ba9a20b034e4090c6630751,263610099.0,https://www.semanticscholar.org/paper/e468ed6b824e60f45ba9a20b034e4090c6630751,,2023.0,49.0,1.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2155447436', 'name': 'Xingxuan Li'}, {'authorId': '2091437375', 'name': 'Ruochen Zhao'}, {'authorId': '2066312627', 'name': 'Yew Ken Chia'}, {'authorId': '2064493724', 'name': 'Bosheng Ding'}, {'authorId': '2708940', 'name': 'Shafiq R. Joty'}, {'authorId': '1746416', 'name': 'Soujanya Poria'}, {'authorId': '1996394', 'name': 'Lidong Bing'}]","['Alibaba', 'Singapore University of Technology and Design', 'Nanyang Technological University']","['China', 'Singapore']",2023-05
2305.13292,Chen Guo,"Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Yifei Huang, Junting
  Pan, Yi Wang, Yali Wang, Yu Qiao, Tong Lu, Limin Wang",VideoLLM: Modeling Video Sequence with Large Language Models,Technical Report,,,,cs.CV,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  With the exponential growth of video data, there is an urgent need for automated technology to analyze and comprehend video content. However, existing video understanding models are often task-specific and lack a comprehensive capability of handling diverse tasks. The success of large language models (LLMs) like GPT has demonstrated their impressive abilities in sequence causal reasoning. Building upon this insight, we propose a novel framework called VideoLLM that leverages the sequence reasoning capabilities of pre-trained LLMs from natural language processing (NLP) for video sequence understanding. VideoLLM incorporates a carefully designed Modality Encoder and Semantic Translator, which convert inputs from various modalities into a unified token sequence. This token sequence is then fed into a decoder-only LLM. Subsequently, with the aid of a simple task head, our VideoLLM yields an effective unified framework for different kinds of video understanding tasks. To evaluate the efficacy of VideoLLM, we conduct extensive experiments using multiple LLMs and fine-tuning methods. We evaluate our VideoLLM on eight tasks sourced from four different datasets. The experimental results demonstrate that the understanding and reasoning capabilities of LLMs can be effectively transferred to video understanding tasks. We release the code at https://github.com/cg1177/VideoLLM. ","[{'version': 'v1', 'created': 'Mon, 22 May 2023 17:51:22 GMT'}, {'version': 'v2', 'created': 'Tue, 23 May 2023 07:48:15 GMT'}]",2023-05-24,"[['Chen', 'Guo', ''], ['Zheng', 'Yin-Dong', ''], ['Wang', 'Jiahao', ''], ['Xu', 'Jilan', ''], ['Huang', 'Yifei', ''], ['Pan', 'Junting', ''], ['Wang', 'Yi', ''], ['Wang', 'Yali', ''], ['Qiao', 'Yu', ''], ['Lu', 'Tong', ''], ['Wang', 'Limin', '']]",0,1,2023-05-22,2,11,1,0,0,0,f9bfc6d9ba1665b73af3323d46c7642b852759ef,258832930.0,https://www.semanticscholar.org/paper/f9bfc6d9ba1665b73af3323d46c7642b852759ef,arXiv.org,2023.0,103.0,15.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2155229619', 'name': 'Guo Chen'}, {'authorId': '1391155455', 'name': 'Yin-Dong Zheng'}, {'authorId': '2110182417', 'name': 'Jiahao Wang'}, {'authorId': '3259789', 'name': 'Jilan Xu'}, {'authorId': '48355651', 'name': 'Yifei Huang'}, {'authorId': '7588865', 'name': 'Junting Pan'}, {'authorId': '46393411', 'name': 'Yi Wang'}, {'authorId': '47903936', 'name': 'Yali Wang'}, {'authorId': '145858545', 'name': 'Y. Qiao'}, {'authorId': '2113488744', 'name': 'Tong Lu'}, {'authorId': '2141353278', 'name': 'Limin Wang'}]","['OpenGVLab, Shanghai AI Laboratory,', 'Nanjing University', 'Fudan University', 'Chinese University of Hong Kong']",['China'],2023-05
2305.13412,Yiming Wang,"Yiming Wang, Zhuosheng Zhang, Rui Wang",Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method,Accepted by ACL 2023,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automatic summarization generates concise summaries that contain key ideas of source documents. As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the ""Lasswell Communication Model"" proposed by Lasswell (1948), allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs' zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available at https://github.com/Alsace08/SumCoT. ","[{'version': 'v1', 'created': 'Mon, 22 May 2023 18:54:35 GMT'}]",2023-05-24,"[['Wang', 'Yiming', ''], ['Zhang', 'Zhuosheng', ''], ['Wang', 'Rui', '']]",0,0,2023-05-22,1,3,1,0,0,0,2338d7c9ab07e6d0f4160335dce0e6e6a87c4749,258841145.0,https://www.semanticscholar.org/paper/2338d7c9ab07e6d0f4160335dce0e6e6a87c4749,Annual Meeting of the Association for Computational Linguistics,2023.0,66.0,11.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2143482843', 'name': 'Yiming Wang'}, {'authorId': '3322871', 'name': 'Zhuosheng Zhang'}, {'authorId': '2151038501', 'name': 'Rui Wang'}]",['Shanghai Jiao Tong University'],['China'],2023-05
2305.13592,Yiwen Guo,Jianyu Zhao and Yuyang Rong and Yiwen Guo and Yifeng He and Hao Chen,Understanding Programs by Exploiting (Fuzzing) Test Cases,"Findings of the Association for Computational Linguistics: ACL 2023;
  fix typos and update results to keep the same settings in all experiments",,,,cs.LG cs.AI cs.CL cs.CR cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Semantic understanding of programs has attracted great attention in the community. Inspired by recent successes of large language models (LLMs) in natural language understanding, tremendous progress has been made by treating programming language as another sort of natural language and training LLMs on corpora of program code. However, programs are essentially different from texts after all, in a sense that they are normally heavily structured and syntax-strict. In particular, programs and their basic units (i.e., functions and subroutines) are designed to demonstrate a variety of behaviors and/or provide possible outputs, given different inputs. The relationship between inputs and possible outputs/behaviors represents the functions/subroutines and profiles the program as a whole. Therefore, we propose to incorporate such a relationship into learning, for achieving a deeper semantic understanding of programs. To obtain inputs that are representative enough to trigger the execution of most part of the code, we resort to fuzz testing and propose fuzz tuning to boost the performance of program understanding and code representation learning, given a pre-trained LLM. The effectiveness of the proposed method is verified on two program understanding tasks including code clone detection and code classification, and it outperforms current state-of-the-arts by large margins. Code is available at https://github.com/rabbitjy/FuzzTuning. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 01:51:46 GMT'}, {'version': 'v2', 'created': 'Mon, 12 Jun 2023 00:56:21 GMT'}]",2023-06-13,"[['Zhao', 'Jianyu', ''], ['Rong', 'Yuyang', ''], ['Guo', 'Yiwen', ''], ['He', 'Yifeng', ''], ['Chen', 'Hao', '']]",0,0,2023-05-23,2,5,5,0,0,0,330403b8627ccfd566de439b6126229fde257d49,258841437.0,https://www.semanticscholar.org/paper/330403b8627ccfd566de439b6126229fde257d49,Annual Meeting of the Association for Computational Linguistics,2023.0,40.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2035901', 'name': 'Jianyu Zhao'}, {'authorId': '144278456', 'name': 'Yuyang Rong'}, {'authorId': '2527106', 'name': 'Yiwen Guo'}, {'authorId': '2118918598', 'name': 'Yifeng He'}, {'authorId': '36620472', 'name': 'Hao Chen'}]","['Tencent', 'University of California, Davis', 'Independent Researcher']","['China', 'United States']",2023-05
2305.13614,Siyuan Chen,"Siyuan Chen, Mengyue Wu, Kenny Q. Zhu, Kunyao Lan, Zhiling Zhang,
  Lyuchun Cui",LLM-empowered Chatbots for Psychiatrist and Patient Simulation: Application and Evaluation,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Empowering chatbots in the field of mental health is receiving increasing amount of attention, while there still lacks exploration in developing and evaluating chatbots in psychiatric outpatient scenarios. In this work, we focus on exploring the potential of ChatGPT in powering chatbots for psychiatrist and patient simulation. We collaborate with psychiatrists to identify objectives and iteratively develop the dialogue system to closely align with real-world scenarios. In the evaluation experiments, we recruit real psychiatrists and patients to engage in diagnostic conversations with the chatbots, collecting their ratings for assessment. Our findings demonstrate the feasibility of using ChatGPT-powered chatbots in psychiatric scenarios and explore the impact of prompt designs on chatbot behavior and user experience. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 02:25:01 GMT'}]",2023-05-24,"[['Chen', 'Siyuan', ''], ['Wu', 'Mengyue', ''], ['Zhu', 'Kenny Q.', ''], ['Lan', 'Kunyao', ''], ['Zhang', 'Zhiling', ''], ['Cui', 'Lyuchun', '']]",1,1,2023-05-23,1,6,1,1,0,1,a4362d3ecb80566300c51d3f07a5e5f2d7ab374f,258841034.0,https://www.semanticscholar.org/paper/a4362d3ecb80566300c51d3f07a5e5f2d7ab374f,arXiv.org,2023.0,34.0,7.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Psychology', 'source': 's2-fos-model'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2218270477', 'name': 'Siyuan Chen'}, {'authorId': '3000684', 'name': 'Mengyue Wu'}, {'authorId': '2151999086', 'name': 'Ke Zhu'}, {'authorId': '2186555029', 'name': 'Kunyao Lan'}, {'authorId': '2135605105', 'name': 'Zhiling Zhang'}, {'authorId': '1998352052', 'name': 'Lyuchun Cui'}]","['Shanghai Jiao Tong University', 'Shanghai Mental Health Center']",['China'],2023-05
2305.13626,Yang Deng,"Yang Deng, Wenqiang Lei, Lizi Liao, Tat-Seng Chua","Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration",Work in progress,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Conversational systems based on Large Language Models (LLMs), such as ChatGPT, show exceptional proficiency in context understanding and response generation. However, despite their impressive capabilities, they still possess limitations, such as providing randomly-guessed answers to ambiguous queries or failing to refuse users' requests, both of which are considered aspects of a conversational agent's proactivity. This raises the question of whether LLM-based conversational systems are equipped to handle proactive dialogue problems. In this work, we conduct a comprehensive analysis of LLM-based conversational systems, specifically focusing on three aspects of proactive dialogue systems: clarification, target-guided, and non-collaborative dialogues. To trigger the proactivity of LLMs, we propose the Proactive Chain-of-Thought prompting scheme, which augments LLMs with the goal planning capability over descriptive reasoning chains. Empirical findings are discussed to promote future studies on LLM-based proactive dialogue systems. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 02:49:35 GMT'}]",2023-05-24,"[['Deng', 'Yang', ''], ['Lei', 'Wenqiang', ''], ['Liao', 'Lizi', ''], ['Chua', 'Tat-Seng', '']]",1,1,2023-05-23,1,4,1,1,0,1,13c85adfa950651ffcd91ef3018fa30801b74472,258840865.0,https://www.semanticscholar.org/paper/13c85adfa950651ffcd91ef3018fa30801b74472,arXiv.org,2023.0,76.0,5.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '145843537', 'name': 'Yang Deng'}, {'authorId': '39165620', 'name': 'Wenqiang Lei'}, {'authorId': '2053831159', 'name': 'Lizi Liao'}, {'authorId': '143779329', 'name': 'Tat-seng Chua'}]","['National University of Singapore', 'Chinese University of Hong Kong', 'Sichuan University', 'Singapore Management University']","['China', 'Singapore']",2023-05
2305.13661,Yikang Pan,"Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan,
  William Yang Wang",On the Risk of Misinformation Pollution with Large Language Models,Technical Report,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we comprehensively investigate the potential misuse of modern Large Language Models (LLMs) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly Open-Domain Question Answering (ODQA) systems. We establish a threat model and simulate potential misuse scenarios, both unintentional and intentional, to assess the extent to which LLMs can be utilized to produce misinformation. Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation in the performance of ODQA systems. To mitigate the harm caused by LLM-generated misinformation, we explore three defense strategies: prompting, misinformation detection, and majority voting. While initial results show promising trends for these defensive strategies, much more work needs to be done to address the challenge of misinformation pollution. Our work highlights the need for further research and interdisciplinary collaboration to address LLM-generated misinformation and to promote responsible use of LLMs. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 04:10:26 GMT'}]",2023-05-24,"[['Pan', 'Yikang', ''], ['Pan', 'Liangming', ''], ['Chen', 'Wenhu', ''], ['Nakov', 'Preslav', ''], ['Kan', 'Min-Yen', ''], ['Wang', 'William Yang', '']]",0,0,2023-05-23,1,6,2,0,0,0,fef6471c4a2a0e7abc4a2261a6cf916e34091d12,258840876.0,https://www.semanticscholar.org/paper/fef6471c4a2a0e7abc4a2261a6cf916e34091d12,arXiv.org,2023.0,46.0,15.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2218334588', 'name': 'Yikang Pan'}, {'authorId': '3470231', 'name': 'Liangming Pan'}, {'authorId': '2109664620', 'name': 'Wenhu Chen'}, {'authorId': '2026545715', 'name': 'Preslav Nakov'}, {'authorId': '37596605', 'name': 'Min-Yen Kan'}, {'authorId': '152876475', 'name': 'W. Wang'}]","['University of Waterloo', 'National University of Singapore', 'Zhejiang University', 'Equal Contribution.', 'Mohamed bin Zayed University of Artificial Intelligence', 'University of California, Santa Barbara']","['Singapore', 'Canada', 'United States', 'China', 'United Arab Emirates']",2023-05
2305.13733,Hongru Wang,"Rui Wang, Hongru Wang, Fei Mi, Yi Chen, Ruifeng Xu, Kam-Fai Wong",Self-Critique Prompting with Large Language Models for Inductive Instructions,,,,,cs.CL,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Numerous works are proposed to improve or evaluate the capabilities of Large language models (LLMs) to fulfill user instructions. However, they neglect the possibility that user inputs may inherently contain incorrect information due to users' false beliefs or malicious intents. In this way, blindly adhering to users' false content will cause deception and harm. To address this problem, we propose a challenging benchmark consisting of Inductive Instructions (INDust) to evaluate whether LLMs could resist these instructions. The INDust includes 15K instructions across three categories: Fact-Checking Instructions, Questions based on False Premises, and Creative Instructions based on False Premises. Our experiments on several strong LLMs reveal that current LLMs can be easily deceived by INDust into generating misleading and malicious statements. Hence we employ Self-Critique prompting to encourage LLMs to not only critique themselves like in previous works but also the users, which show remarkable improvement in handling inductive instructions under both zero-shot and few-shot settings. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 06:38:20 GMT'}]",2023-05-24,"[['Wang', 'Rui', ''], ['Wang', 'Hongru', ''], ['Mi', 'Fei', ''], ['Chen', 'Yi', ''], ['Xu', 'Ruifeng', ''], ['Wong', 'Kam-Fai', '']]",0,0,2023-05-23,1,6,1,0,0,0,b5e9406a65de7384af041c357ca5481489345b73,258841609.0,https://www.semanticscholar.org/paper/b5e9406a65de7384af041c357ca5481489345b73,arXiv.org,2023.0,23.0,6.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2151036536', 'name': 'Rui Wang'}, {'authorId': '2109234277', 'name': 'Hongru Wang'}, {'authorId': '33727421', 'name': 'Fei Mi'}, {'authorId': '2165302640', 'name': 'Yi Chen'}, {'authorId': '8233941', 'name': 'Rui-Lan Xu'}, {'authorId': '1784988', 'name': 'Kam-Fai Wong'}]","['Harbin Institute of Technology', 'Chinese University of Hong Kong']",['China'],2023-05
2305.13777,Jinheng Xie,"Jinheng Xie, Kai Ye, Yudong Li, Yuexiang Li, Kevin Qinghong Lin,
  Yefeng Zheng, Linlin Shen, Mike Zheng Shou",VisorGPT: Learning Visual Prior via Generative Pre-Training,Project web-page: https://sierkinhane.github.io/visor-gpt/,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Various stuff and things in visual data possess specific traits, which can be learned by deep neural networks and are implicitly represented as the visual prior, e.g., object location and shape, in the model. Such prior potentially impacts many vision tasks. For example, in conditional image synthesis, spatial conditions failing to adhere to the prior can result in visually inaccurate synthetic results. This work aims to explicitly learn the visual prior and enable the customization of sampling. Inspired by advances in language modeling, we propose to learn Visual prior via Generative Pre-Training, dubbed VisorGPT. By discretizing visual locations of objects, e.g., bounding boxes, human pose, and instance masks, into sequences, VisorGPT can model visual prior through likelihood maximization. Besides, prompt engineering is investigated to unify various visual locations and enable customized sampling of sequential outputs from the learned prior. Experimental results demonstrate that VisorGPT can effectively model the visual prior, which can be employed for many vision tasks, such as customizing accurate human pose for conditional image synthesis models like ControlNet. Code will be released at https://github.com/Sierkinhane/VisorGPT. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 07:45:23 GMT'}, {'version': 'v2', 'created': 'Wed, 24 May 2023 07:18:13 GMT'}, {'version': 'v3', 'created': 'Sun, 28 May 2023 11:34:43 GMT'}, {'version': 'v4', 'created': 'Tue, 30 May 2023 15:12:41 GMT'}]",2023-05-31,"[['Xie', 'Jinheng', ''], ['Ye', 'Kai', ''], ['Li', 'Yudong', ''], ['Li', 'Yuexiang', ''], ['Lin', 'Kevin Qinghong', ''], ['Zheng', 'Yefeng', ''], ['Shen', 'Linlin', ''], ['Shou', 'Mike Zheng', '']]",0,1,2023-05-23,4,8,1,0,0,0,0a61802b71aa044cf1fe0e81befec148e0d5001b,258841579.0,https://www.semanticscholar.org/paper/0a61802b71aa044cf1fe0e81befec148e0d5001b,arXiv.org,2023.0,30.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2220635949', 'name': 'Jinheng Xie'}, {'authorId': '2065022067', 'name': 'Kai Ye'}, {'authorId': '2111162634', 'name': 'Yudong Li'}, {'authorId': '1758100', 'name': 'Yuexiang Li'}, {'authorId': '143786724', 'name': 'Kevin Lin'}, {'authorId': '1890905492', 'name': 'Yefeng Zheng'}, {'authorId': '121640365', 'name': 'Linlin Shen'}, {'authorId': '2047358650', 'name': 'Mike Zheng Shou'}]","['National University of Singapore', 'Shenzhen University', 'Tencent']","['China', 'Singapore']",2023-05
2305.13785,Chen Zhang,"Danqing Luo, Chen Zhang, Jiahui Xu, Bin Wang, Yiming Chen, Yan Zhang,
  Haizhou Li",Enhancing Black-Box Few-Shot Text Classification with Prompt-Based Data Augmentation,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Training or finetuning large-scale language models (LLMs) such as GPT-3 requires substantial computation resources, motivating recent efforts to explore parameter-efficient adaptation to downstream tasks. One practical area of research is to treat these models as black boxes and interact with them through their inference APIs. In this paper, we investigate how to optimize few-shot text classification without accessing the gradients of the LLMs. To achieve this, we treat the black-box model as a feature extractor and train a classifier with the augmented text data. Data augmentation is performed using prompt-based finetuning on an auxiliary language model with a much smaller parameter size than the black-box model. Through extensive experiments on eight text classification datasets, we show that our approach, dubbed BT-Classifier, significantly outperforms state-of-the-art black-box few-shot learners and performs on par with methods that rely on full-model tuning. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 07:54:34 GMT'}]",2023-05-24,"[['Luo', 'Danqing', ''], ['Zhang', 'Chen', ''], ['Xu', 'Jiahui', ''], ['Wang', 'Bin', ''], ['Chen', 'Yiming', ''], ['Zhang', 'Yan', ''], ['Li', 'Haizhou', '']]",0,1,2023-05-23,1,7,1,1,0,1,b6787179220b6aea5317d072f409bf4ed776cd62,258841819.0,https://www.semanticscholar.org/paper/b6787179220b6aea5317d072f409bf4ed776cd62,arXiv.org,2023.0,24.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2061548489', 'name': 'Dan Luo'}, {'authorId': '2111574800', 'name': 'Chen Zhang'}, {'authorId': '2110502038', 'name': 'Jiahui Xu'}, {'authorId': '2222324269', 'name': 'Bin Wang'}, {'authorId': '2188651547', 'name': 'Yiming Chen'}, {'authorId': '39509574', 'name': 'Yan Zhang'}, {'authorId': '2119251083', 'name': 'Haizhou Li'}]","['Chinese University of Hong Kong, Shenzhen', 'National University of Singapore', 'Kriston AI Lab, China']","['China', 'Singapore']",2023-05
2305.13888,Xuekai Zhu,"Xuekai Zhu, Biqing Qi, Kaiyan Zhang, Xingwei Long, Bowen Zhou",PaD: Program-aided Distillation Specializes Large Models in Reasoning,work in progress,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While Large Language Models (LLMs) excel in several natural language processing tasks, their size and inaccessibility present challenges for extensive practical application. Previous studies acquire specialized skills through distillation on LLMs, which result in trading generic abilities, called model specialization. As for reasoning ability, chain-of-thought was synthesized to subsequent distillation. However, due to hallucination, synthetic chain-of-thought from LLMs contains faulty reasoning. These incorrect reasoning steps damage the reasoning capability. To tackle above issues, we propose Program-aided Distillation (PaD), which distills LLMs to obtain specialized small models in reasoning tasks. In PaD, we strengthen specialized models with program-aided reasoning, and help them overcome faulty reasoning steps with automated error checking. Experimental results demonstrate that, on the GSM8K benchmark, a 0.06B model using PaD can not only outperform certain LLMs (e.g., LLaMA), but also achieves a 10% improvement over baselines with a significantly smaller scale of parameters and data. Data pruning analysis reveals that PaD possesses higher training efficiency. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 10:11:56 GMT'}]",2023-05-24,"[['Zhu', 'Xuekai', ''], ['Qi', 'Biqing', ''], ['Zhang', 'Kaiyan', ''], ['Long', 'Xingwei', ''], ['Zhou', 'Bowen', '']]",0,0,2023-05-23,1,5,1,1,1,0,bb8f7fbec020675d269ccfa0e6e603f02b664c0d,258840866.0,https://www.semanticscholar.org/paper/bb8f7fbec020675d269ccfa0e6e603f02b664c0d,arXiv.org,2023.0,46.0,6.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2145238612', 'name': 'Xuekai Zhu'}, {'authorId': '66242399', 'name': 'Biqing Qi'}, {'authorId': '2153281320', 'name': 'Kaiyan Zhang'}, {'authorId': '2218130130', 'name': 'Xingwei Long'}, {'authorId': '2218723159', 'name': 'Bowen Zhou'}]","['Tsinghua University', 'Chinese Center For Disease Control and Prevention']",['China'],2023-05
2305.13972,Chuanyuan Tan,"Chuanyuan Tan, Yuehe Chen, Wenbiao Shao, Wenliang Chen",Make a Choice! Knowledge Base Question Answering with In-Context Learning,Work in Progress,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Question answering over knowledge bases (KBQA) aims to answer factoid questions with a given knowledge base (KB). Due to the large scale of KB, annotated data is impossible to cover all fact schemas in KB, which poses a challenge to the generalization ability of methods that require a sufficient amount of annotated data. Recently, LLMs have shown strong few-shot performance in many NLP tasks. We expect LLM can help existing methods improve their generalization ability, especially in low-resource situations. In this paper, we present McL-KBQA, a framework that incorporates the few-shot ability of LLM into the KBQA method via ICL-based multiple choice and then improves the effectiveness of the QA tasks. Experimental results on two KBQA datasets demonstrate the competitive performance of McL-KBQA with strong improvements in generalization. We expect to explore a new way to QA tasks from KBQA in conjunction with LLM, how to generate answers normatively and correctly with strong generalization. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 11:56:03 GMT'}]",2023-05-24,"[['Tan', 'Chuanyuan', ''], ['Chen', 'Yuehe', ''], ['Shao', 'Wenbiao', ''], ['Chen', 'Wenliang', '']]",0,0,2023-05-23,1,4,1,0,0,0,e3519e59a1dd374535d162bf400887e2be7429ab,258841512.0,https://www.semanticscholar.org/paper/e3519e59a1dd374535d162bf400887e2be7429ab,arXiv.org,2023.0,18.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2186374155', 'name': 'Chuanyuan Tan'}, {'authorId': '2186399658', 'name': 'Yuehe Chen'}, {'authorId': '2218326201', 'name': 'Wenbiao Shao'}, {'authorId': '2463750', 'name': 'Wenliang Chen'}]","['Soochow University', 'Huawei Technologies (China)']",['China'],2023-05
2305.13981,Ji Qi,"Ji Qi, Chuchun Zhang, Xiaozhi Wang, Kaisheng Zeng, Jifan Yu, Jinxin
  Liu, Jiuding Sun, Yuxiang Chen, Lei How, Juanzi Li, Bin Xu",Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The robustness to distribution changes ensures that NLP models can be successfully applied in the realistic world, especially for information extraction tasks. However, most prior evaluation benchmarks have been devoted to validating pairwise matching correctness, ignoring the crucial measurement of robustness. In this paper, we present the first benchmark that simulates the evaluation of open information extraction models in the real world, where the syntactic and expressive distributions under the same knowledge meaning may drift variously. We design and annotate a large-scale testbed in which each example is a knowledge-invariant clique that consists of sentences with structured knowledge of the same meaning but with different syntactic and expressive forms. By further elaborating the robustness metric, a model is judged to be robust if its performance is consistently accurate on the overall cliques. We perform experiments on typical models published in the last decade as well as a popular large language model, the results show that the existing successful models exhibit a frustrating degradation, with a maximum drop of 23.43 F1 score. Our resources and code will be publicly available. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 12:05:09 GMT'}]",2023-05-24,"[['Qi', 'Ji', ''], ['Zhang', 'Chuchun', ''], ['Wang', 'Xiaozhi', ''], ['Zeng', 'Kaisheng', ''], ['Yu', 'Jifan', ''], ['Liu', 'Jinxin', ''], ['Sun', 'Jiuding', ''], ['Chen', 'Yuxiang', ''], ['How', 'Lei', ''], ['Li', 'Juanzi', ''], ['Xu', 'Bin', '']]",0,0,2023-05-23,1,11,2,0,0,0,c24c67fc0b7547be5306801c01ee6f9e7bab7ebc,258841244.0,https://www.semanticscholar.org/paper/c24c67fc0b7547be5306801c01ee6f9e7bab7ebc,arXiv.org,2023.0,39.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2091076497', 'name': 'Ji Qi'}, {'authorId': '2209202649', 'name': 'Chuchu Zhang'}, {'authorId': '48631777', 'name': 'Xiaozhi Wang'}, {'authorId': '10673612', 'name': 'Kaisheng Zeng'}, {'authorId': '2116034394', 'name': 'Jifan Yu'}, {'authorId': '2187062668', 'name': 'Jinxi Liu'}, {'authorId': '2175443520', 'name': 'Jiu Sun'}, {'authorId': '2109168537', 'name': 'Yuxiang Chen'}, {'authorId': '2218442246', 'name': 'Lei How'}, {'authorId': '2133353675', 'name': 'Juanzi Li'}, {'authorId': '2113744169', 'name': 'Bin Xu'}]","['Tsinghua University', 'University of International Business and Economics']",['China'],2023-05
2305.14019,Kaiyan Chang,"Kaiyan Chang and Ying Wang and Haimeng Ren and Mengdi Wang and
  Shengwen Liang and Yinhe Han and Huawei Li and Xiaowei Li",ChipGPT: How far are we from natural language hardware design,,,,,cs.AI cs.AR cs.PL,http://creativecommons.org/licenses/by/4.0/,"  As large language models (LLMs) like ChatGPT exhibited unprecedented machine intelligence, it also shows great performance in assisting hardware engineers to realize higher-efficiency logic design via natural language interaction. To estimate the potential of the hardware design process assisted by LLMs, this work attempts to demonstrate an automated design environment that explores LLMs to generate hardware logic designs from natural language specifications. To realize a more accessible and efficient chip development flow, we present a scalable four-stage zero-code logic design framework based on LLMs without retraining or finetuning. At first, the demo, ChipGPT, begins by generating prompts for the LLM, which then produces initial Verilog programs. Second, an output manager corrects and optimizes these programs before collecting them into the final design space. Eventually, ChipGPT will search through this space to select the optimal design under the target metrics. The evaluation sheds some light on whether LLMs can generate correct and complete hardware logic designs described by natural language for some specifications. It is shown that ChipGPT improves programmability, and controllability, and shows broader design optimization space compared to prior work and native LLMs alone. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 12:54:02 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Jun 2023 13:24:11 GMT'}, {'version': 'v3', 'created': 'Mon, 19 Jun 2023 08:28:15 GMT'}]",2023-06-21,"[['Chang', 'Kaiyan', ''], ['Wang', 'Ying', ''], ['Ren', 'Haimeng', ''], ['Wang', 'Mengdi', ''], ['Liang', 'Shengwen', ''], ['Han', 'Yinhe', ''], ['Li', 'Huawei', ''], ['Li', 'Xiaowei', '']]",1,1,2023-05-23,3,8,3,1,0,1,bd0488e0c1bb3ab375f28b3157058101350d3766,258841170.0,https://www.semanticscholar.org/paper/bd0488e0c1bb3ab375f28b3157058101350d3766,arXiv.org,2023.0,27.0,5.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2218999828', 'name': 'Kaiyan Chang'}, {'authorId': '49416173', 'name': 'Y. Wang'}, {'authorId': '2199266536', 'name': 'Haimeng Ren'}, {'authorId': '145731462', 'name': 'Mengdi Wang'}, {'authorId': '48814007', 'name': 'Shengwen Liang'}, {'authorId': '2276854', 'name': 'Yinhe Han'}, {'authorId': '47892692', 'name': 'Huawei Li'}, {'authorId': '40613624', 'name': 'Xiaowei Li'}]","['University of Chinese Academy of Sciences', 'Chinese Academy of Sciences', 'ShanghaiTech University']",['China'],2023-05
2305.14057,Lei Li,"Lei Li, Jingjing Xu, Qingxiu Dong, Ce Zheng, Qi Liu, Lingpeng Kong, Xu
  Sun",Can Language Models Understand Physical Concepts?,,,,,cs.CL cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Language models~(LMs) gradually become general-purpose interfaces in the interactive and embodied world, where the understanding of physical concepts is an essential prerequisite. However, it is not yet clear whether LMs can understand physical concepts in the human world. To investigate this, we design a benchmark VEC that covers the tasks of (i) Visual concepts, such as the shape and material of objects, and (ii) Embodied Concepts, learned from the interaction with the world such as the temperature of objects. Our zero (few)-shot prompting results show that the understanding of certain visual concepts emerges as scaling up LMs, but there are still basic concepts to which the scaling law does not apply. For example, OPT-175B performs close to humans with a zero-shot accuracy of 85\% on the material concept, yet behaves like random guessing on the mass concept. Instead, vision-augmented LMs such as CLIP and BLIP achieve a human-level understanding of embodied concepts. Analysis indicates that the rich semantics in visual representation can serve as a valuable source of embodied knowledge. Inspired by this, we propose a distillation method to transfer embodied knowledge from VLMs to LMs, achieving performance gain comparable with that by scaling up the parameters of LMs 134x. Our dataset is available at \url{https://github.com/TobiasLee/VEC} ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 13:36:55 GMT'}]",2023-05-24,"[['Li', 'Lei', ''], ['Xu', 'Jingjing', ''], ['Dong', 'Qingxiu', ''], ['Zheng', 'Ce', ''], ['Liu', 'Qi', ''], ['Kong', 'Lingpeng', ''], ['Sun', 'Xu', '']]",0,0,2023-05-23,1,7,2,1,1,0,1caa2a29d3ca38d0e5111f4f9ae140727bb7d567,258841417.0,https://www.semanticscholar.org/paper/1caa2a29d3ca38d0e5111f4f9ae140727bb7d567,arXiv.org,2023.0,64.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '49192881', 'name': 'Lei Li'}, {'authorId': '47883405', 'name': 'Jingjing Xu'}, {'authorId': '2047143813', 'name': 'Qingxiu Dong'}, {'authorId': '2113919886', 'name': 'Ce Zheng'}, {'authorId': '2144831944', 'name': 'Qi Liu'}, {'authorId': '47648549', 'name': 'Lingpeng Kong'}, {'authorId': '2116530295', 'name': 'Xu Sun'}]","['Shanghai Artificial Intelligence Laboratory', 'Peking University', 'University of Hong Kong']","['China', 'Hong Kong']",2023-05
2305.14069,Shiqi Chen,"Shiqi Chen, Siyang Gao and Junxian He",Evaluating Factual Consistency of Summaries with Large Language Models,Preprint,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Detecting factual errors in summaries has been an important and challenging subject in summarization research. Inspired by the emergent ability of large language models (LLMs), we explore evaluating factual consistency of summaries by directly prompting LLMs. We present a comprehensive empirical study to assess the ability of LLMs as factual consistency evaluators, which consists of (1) analyzing different LLMs such as the GPT model series and Flan-T5; (2) investigating a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting method to tackle long summaries; and (3) evaluating on diverse summaries generated by multiple summarization systems, ranging from pre-transformer methods to SOTA pretrained models. Our experiments demonstrate that prompting LLMs is able to outperform the previous best factuality systems in all settings, by up to 12.2 absolute points in terms of the binary classification accuracy on inconsistency detection. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 13:48:32 GMT'}]",2023-05-24,"[['Chen', 'Shiqi', ''], ['Gao', 'Siyang', ''], ['He', 'Junxian', '']]",0,1,2023-05-23,1,3,1,3,2,1,152d9a231c00d4495c9bc4a466f42165ce2e2164,258840936.0,https://www.semanticscholar.org/paper/152d9a231c00d4495c9bc4a466f42165ce2e2164,arXiv.org,2023.0,53.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108956946', 'name': 'Shiqi Chen'}, {'authorId': '3134704', 'name': 'Siyang Gao'}, {'authorId': '2109932032', 'name': 'Junxian He'}]","['City University of Hong Kong', 'Hong Kong University of Science and Technology']",['China'],2023-05
2305.14091,Ziyin Zhang,"Hai Hu and Ziyin Zhang and Weifang Huang and Jackie Yan-Ki Lai and
  Aini Li and Yina Patterson and Jiahui Huang and Peng Zhang and Chien-Jer
  Charles Lin and Rui Wang",Revisiting Acceptability Judgements,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  In this work, we revisit linguistic acceptability in the context of large language models. We introduce CoLAC - Corpus of Linguistic Acceptability in Chinese, the first large-scale acceptability dataset for a non-Indo-European language. It is verified by native speakers and is the first acceptability dataset that comes with two sets of labels: a linguist label and a crowd label. Our experiments show that even the largest InstructGPT model performs only at chance level on CoLAC, while ChatGPT's performance (48.30 MCC) is also much below supervised models (59.03 MCC) and human (65.11 MCC). Through cross-lingual transfer experiments and fine-grained linguistic analysis, we provide detailed analysis of the model predictions and demonstrate for the first time that knowledge of linguistic acceptability can be transferred across typologically distinct languages, as well as be traced back to pre-training. Our dataset is publicly available at \url{https://github.com/huhailinguist/CoLAC}. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 14:16:22 GMT'}, {'version': 'v2', 'created': 'Wed, 24 May 2023 11:20:46 GMT'}, {'version': 'v3', 'created': 'Thu, 28 Sep 2023 02:58:48 GMT'}]",2023-09-29,"[['Hu', 'Hai', ''], ['Zhang', 'Ziyin', ''], ['Huang', 'Weifang', ''], ['Lai', 'Jackie Yan-Ki', ''], ['Li', 'Aini', ''], ['Patterson', 'Yina', ''], ['Huang', 'Jiahui', ''], ['Zhang', 'Peng', ''], ['Lin', 'Chien-Jer Charles', ''], ['Wang', 'Rui', '']]",1,1,2023-05-23,3,10,2,2,0,2,e9a0b58b498e4d8b16706f04452e854caca1444a,258841631.0,https://www.semanticscholar.org/paper/e9a0b58b498e4d8b16706f04452e854caca1444a,arXiv.org,2023.0,49.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Linguistics', 'source': 's2-fos-model'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '145309512', 'name': 'Hai Hu'}, {'authorId': '2116462959', 'name': 'Ziyin Zhang'}, {'authorId': '2000111554', 'name': 'Wei Huang'}, {'authorId': '8123477', 'name': 'J. Lai'}, {'authorId': '2112544845', 'name': 'Aini Li'}, {'authorId': '2146275527', 'name': 'Yi Ma'}, {'authorId': '2218080662', 'name': 'Jiahui Huang'}, {'authorId': '2151330499', 'name': 'Peng Zhang'}, {'authorId': '2151038626', 'name': 'Rui Wang'}]","['University of Pennsylvania', 'City University of Hong Kong', 'Shanghai Jiao Tong University', 'Indiana University Bloomington', 'University of Washington', 'Brigham Young University']","['China', 'United States']",2023-05
2305.14167,Renjie Pi,"Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong, Jipeng Zhang,
  Lewei Yao, Jianhua Han, Hang Xu, Lingpeng Kong, Tong Zhang",DetGPT: Detect What You Need via Reasoning,,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In recent years, the field of computer vision has seen significant advancements thanks to the development of large language models (LLMs). These models have enabled more effective and sophisticated interactions between humans and machines, paving the way for novel techniques that blur the lines between human and machine intelligence. In this paper, we introduce a new paradigm for object detection that we call reasoning-based object detection. Unlike conventional object detection methods that rely on specific object names, our approach enables users to interact with the system using natural language instructions, allowing for a higher level of interactivity. Our proposed method, called DetGPT, leverages state-of-the-art multi-modal models and open-vocabulary object detectors to perform reasoning within the context of the user's instructions and the visual scene. This enables DetGPT to automatically locate the object of interest based on the user's expressed desires, even if the object is not explicitly mentioned. For instance, if a user expresses a desire for a cold beverage, DetGPT can analyze the image, identify a fridge, and use its knowledge of typical fridge contents to locate the beverage. This flexibility makes our system applicable across a wide range of fields, from robotics and automation to autonomous driving. Overall, our proposed paradigm and DetGPT demonstrate the potential for more sophisticated and intuitive interactions between humans and machines. We hope that our proposed paradigm and approach will provide inspiration to the community and open the door to more interative and versatile object detection systems. Our project page is launched at detgpt.github.io. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 15:37:28 GMT'}, {'version': 'v2', 'created': 'Wed, 24 May 2023 02:51:37 GMT'}]",2023-05-25,"[['Pi', 'Renjie', ''], ['Gao', 'Jiahui', ''], ['Diao', 'Shizhe', ''], ['Pan', 'Rui', ''], ['Dong', 'Hanze', ''], ['Zhang', 'Jipeng', ''], ['Yao', 'Lewei', ''], ['Han', 'Jianhua', ''], ['Xu', 'Hang', ''], ['Kong', 'Lingpeng', ''], ['Zhang', 'Tong', '']]",0,1,2023-05-23,2,11,2,0,0,0,2ad8183c72a90511383a32ccaeea313eb85f4085,258841764.0,https://www.semanticscholar.org/paper/2ad8183c72a90511383a32ccaeea313eb85f4085,arXiv.org,2023.0,56.0,14.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2066420772', 'name': 'Renjie Pi'}, {'authorId': '144407296', 'name': 'Jiahui Gao'}, {'authorId': '50826757', 'name': 'Shizhe Diao'}, {'authorId': '2192845956', 'name': 'Rui Pan'}, {'authorId': '35279146', 'name': 'Hanze Dong'}, {'authorId': '50561049', 'name': 'Jipeng Zhang'}, {'authorId': '1429192500', 'name': 'Lewei Yao'}, {'authorId': '47180442', 'name': 'Jianhua Han'}, {'authorId': '2143534132', 'name': 'Hang Xu'}, {'authorId': '2218224348', 'name': 'Lingpeng Kong Tong Zhang'}]","['Shanghai Jiao Tong University', 'University of Hong Kong', 'Hong Kong University of Science and Technology']","['China', 'Hong Kong']",2023-05
2305.14169,Naihao Deng,"Naihao Deng, Yikai Liu, Mingye Chen, Winston Wu, Siyang Liu, Yulong
  Chen, Yue Zhang, Rada Mihalcea",EASE: An Easily-Customized Annotation System Powered by Efficiency Enhancement Mechanisms,20 pages,,,,cs.HC cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  The performance of current supervised AI systems is tightly connected to the availability of annotated datasets. Annotations are usually collected through annotation tools, which are often designed for specific tasks and are difficult to customize. Moreover, existing annotation tools with an active learning mechanism often only support limited use cases. To address these limitations, we present EASE, an Easily-Customized Annotation System Powered by Efficiency Enhancement Mechanisms. \sysname provides modular annotation units for building customized annotation interfaces and also provides multiple back-end options that suggest annotations using (1) multi-task active learning; (2) demographic feature based active learning; (3) a prompt system that can query the API of large language models. We conduct multiple experiments and user studies to evaluate our system's flexibility and effectiveness. Our results show that our system can meet the diverse needs of NLP researchers and significantly accelerate the annotation process. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 15:38:37 GMT'}]",2023-05-24,"[['Deng', 'Naihao', ''], ['Liu', 'Yikai', ''], ['Chen', 'Mingye', ''], ['Wu', 'Winston', ''], ['Liu', 'Siyang', ''], ['Chen', 'Yulong', ''], ['Zhang', 'Yue', ''], ['Mihalcea', 'Rada', '']]",0,0,2023-05-23,1,8,2,0,0,0,d36d20f4b64538fcf2c250e3ea11683a7d9ca301,258841189.0,https://www.semanticscholar.org/paper/d36d20f4b64538fcf2c250e3ea11683a7d9ca301,arXiv.org,2023.0,36.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2142468208', 'name': 'Naihao Deng'}, {'authorId': '2154499431', 'name': 'Yikai Liu'}, {'authorId': '3259614', 'name': 'Mingye Chen'}, {'authorId': '2110027458', 'name': 'Winston Wu'}, {'authorId': '2155432827', 'name': 'Siyang Liu'}, {'authorId': '2109404730', 'name': 'Yulong Chen'}, {'authorId': '2218308011', 'name': 'Yue Zhang'}, {'authorId': '145557251', 'name': 'Rada Mihalcea'}]",['Westlake University'],['China'],2023-05
2305.14221,Xinyu Zhu,"Xinyu Zhu, Cheng Yang, Bei Chen, Siheng Li, Jian-Guang Lou, Yujiu Yang",Question Answering as Programming for Solving Time-Sensitive Questions,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work we try to apply Large Language Models (LLMs) to reframe the Question Answering task as Programming (QAaP). Due to the inherent dynamic nature of the real world, factual questions frequently involve a symbolic constraint: time, solving these questions necessitates not only extensive world knowledge, but also advanced reasoning ability to satisfy the temporal constraints. Despite the remarkable intelligence exhibited by LLMs in various NLP tasks, our experiments reveal that the aforementioned problems continue to pose a significant challenge to existing LLMs. To solve these time-sensitive factual questions, considering that modern LLMs possess superior ability in both natural language understanding and programming,we endeavor to leverage LLMs to represent diversely expressed text as well-structured code, and thereby grasp the desired knowledge along with the underlying symbolic constraint. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 16:35:16 GMT'}]",2023-05-24,"[['Zhu', 'Xinyu', ''], ['Yang', 'Cheng', ''], ['Chen', 'Bei', ''], ['Li', 'Siheng', ''], ['Lou', 'Jian-Guang', ''], ['Yang', 'Yujiu', '']]",0,0,2023-05-23,1,6,1,0,0,0,ae80c69872d4af5814d9d7dfa771c794a93697d3,258841488.0,https://www.semanticscholar.org/paper/ae80c69872d4af5814d9d7dfa771c794a93697d3,arXiv.org,2023.0,38.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2116314158', 'name': 'Xinyu Zhu'}, {'authorId': '3443627', 'name': 'Cheng Yang'}, {'authorId': '143876723', 'name': 'B. Chen'}, {'authorId': '47319720', 'name': 'Siheng Li'}, {'authorId': '153249455', 'name': 'Jian-Guang Lou'}, {'authorId': '2108585311', 'name': 'Yujiu Yang'}]","['Tsinghua University', 'Microsoft']","['China', 'United States']",2023-05
2305.14233,Ning Ding,"Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu,
  Zhiyuan Liu, Maosong Sun, Bowen Zhou",Enhancing Chat Language Models by Scaling High-quality Instructional Conversations,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fine-tuning on instruction data has been widely validated as an effective practice for implementing chat language models like ChatGPT. Scaling the diversity and quality of such data, although straightforward, stands a great chance of leading to improved performance. This paper aims to improve the upper bound of open-source models further. We first provide a systematically designed, diverse, informative, large-scale dataset of instructional conversations, UltraChat, which does not involve human queries. Our objective is to capture the breadth of interactions that a human might have with an AI assistant and employs a comprehensive framework to generate multi-turn conversation iteratively. UltraChat contains 1.5 million high-quality multi-turn dialogues and covers a wide range of topics and instructions. Our statistical analysis of UltraChat reveals its superiority in various key metrics, including scale, average length, diversity, coherence, etc., solidifying its position as a leading open-source dataset. Building upon UltraChat, we fine-tune a LLaMA model to create a powerful conversational model, UltraLLaMA. Our evaluations indicate that UltraLLaMA consistently outperforms other open-source models, including Vicuna, the previously recognized state-of-the-art open-source model. The dataset and the model will be publicly released\footnote{\url{https://github.com/thunlp/UltraChat}}. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 16:49:14 GMT'}]",2023-05-24,"[['Ding', 'Ning', ''], ['Chen', 'Yulin', ''], ['Xu', 'Bokai', ''], ['Qin', 'Yujia', ''], ['Zheng', 'Zhi', ''], ['Hu', 'Shengding', ''], ['Liu', 'Zhiyuan', ''], ['Sun', 'Maosong', ''], ['Zhou', 'Bowen', '']]",1,1,2023-05-23,1,9,2,3,2,1,a122863d239643453195424c04067e89406246e1,258840897.0,https://www.semanticscholar.org/paper/a122863d239643453195424c04067e89406246e1,arXiv.org,2023.0,31.0,33.0,11.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '46649145', 'name': 'Ning Ding'}, {'authorId': '2135835258', 'name': 'Yulin Chen'}, {'authorId': '2052218689', 'name': 'Bokai Xu'}, {'authorId': '50625437', 'name': 'Yujia Qin'}, {'authorId': '2115548452', 'name': 'Zhi Zheng'}, {'authorId': '1576223501', 'name': 'Shengding Hu'}, {'authorId': '2141313179', 'name': 'Zhiyuan Liu'}, {'authorId': '1753344', 'name': 'Maosong Sun'}, {'authorId': '2218723159', 'name': 'Bowen Zhou'}]",['Tsinghua University'],['China'],2023-05
2305.14239,Yixin Liu,"Yixin Liu, Alexander R. Fabbri, Pengfei Liu, Dragomir Radev, Arman
  Cohan",On Learning to Summarize with Large Language Models as References,GitHub Repo: https://github.com/yixinL7/SumLLM,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent studies have found that summaries generated by large language models (LLMs) are favored by human annotators over the original reference summaries in commonly used summarization datasets. Therefore, we investigate a new learning paradigm of text summarization models that considers the LLMs as the reference or the gold-standard oracle on commonly used summarization datasets such as the CNN/DailyMail dataset. To examine the standard practices that are aligned with the new learning setting, we propose a novel training method that is based on contrastive learning with LLMs as a summarization quality evaluator. For this reward-based training method, we investigate two different methods of utilizing LLMs for summary quality evaluation, namely GPTScore and GPTRank. Our experiments on the CNN/DailyMail dataset demonstrate that smaller summarization models trained by our proposed method can achieve performance equal to or surpass that of the reference LLMs, as evaluated by the LLMs themselves. This underscores the efficacy of our proposed paradigm in enhancing model performance over the standard maximum likelihood estimation (MLE) training method, and its efficiency since it only requires a small budget to access the LLMs. We release the training scripts, model outputs, and LLM-based evaluation results to facilitate future studies. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 16:56:04 GMT'}]",2023-05-24,"[['Liu', 'Yixin', ''], ['Fabbri', 'Alexander R.', ''], ['Liu', 'Pengfei', ''], ['Radev', 'Dragomir', ''], ['Cohan', 'Arman', '']]",0,1,2023-05-23,1,5,1,0,0,0,d96d6d7f492adb3005aa9371d85bbb882abb6fa4,258841126.0,https://www.semanticscholar.org/paper/d96d6d7f492adb3005aa9371d85bbb882abb6fa4,arXiv.org,2023.0,75.0,14.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108176413', 'name': 'Yixin Liu'}, {'authorId': '46255971', 'name': 'Alexander R. Fabbri'}, {'authorId': '144118452', 'name': 'Pengfei Liu'}, {'authorId': '9215251', 'name': 'Dragomir R. Radev'}, {'authorId': '2527954', 'name': 'Arman Cohan'}]","['Shanghai Jiao Tong University', 'Allen Institute for Artificial Intelligence', 'Yale University']","['China', 'United States']",2023-05
2305.14283,Xinbei Ma,"Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, Nan Duan",Query Rewriting for Retrieval-Augmented Large Language Models,working in progress,,,,cs.CL,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Large Language Models (LLMs) play a powerful \textit{Reader} of the \textit{Retrieve-then-Read} pipeline, making great progress in knowledge-based open-domain tasks. This work introduces a new framework, \textit{Rewrite-Retrieve-Read} that improves the retrieval-augmented method from the perspective of the query rewriting. Prior studies mostly contribute to adapt the retriever or stimulate the reader. Different from them, our approach pay attention of the query adaptation. Because the original query can not be always optimal to retrieve for the LLM, especially in the real world.(1) We first prompt an LLM to rewrite the queries, then conduct retrieval-augmented reading. (2) We further apply a small language model as a trainable rewriter, which rewrite the search query to cater to the frozen retriever and the LLM reader. To fine-tune the rewriter, we first use a pseudo data to conduct supervised warm-up training. Then the \textit{Retrieve-then-Read} pipeline is modeled as a reinforcement learning context. The rewriter is further trained as a policy model by maximize the reward of the pipeline performance. Evaluation is performed on two downstream tasks, open-domain QA and multiple choice. Our framework is proved effective and scalable. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 17:27:50 GMT'}]",2023-05-24,"[['Ma', 'Xinbei', ''], ['Gong', 'Yeyun', ''], ['He', 'Pengcheng', ''], ['Zhao', 'Hai', ''], ['Duan', 'Nan', '']]",0,0,2023-05-23,1,5,1,0,0,0,f743287be3ced6757de7ecb26d03815b22cd737b,258841283.0,https://www.semanticscholar.org/paper/f743287be3ced6757de7ecb26d03815b22cd737b,arXiv.org,2023.0,61.0,3.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2141114505', 'name': 'Xinbei Ma'}, {'authorId': '2171182', 'name': 'Yeyun Gong'}, {'authorId': '50462546', 'name': 'Pengcheng He'}, {'authorId': '2146232510', 'name': 'Hai Zhao'}, {'authorId': '46429989', 'name': 'Nan Duan'}]","['Shanghai Jiao Tong University', 'Microsoft']","['China', 'United States']",2023-05
2305.14303,Yilun Zhao,"Yilun Zhao, Zhenting Qi, Linyong Nan, Boyu Mi, Yixin Liu, Weijin Zou,
  Simeng Han, Xiangru Tang, Yumo Xu, Arman Cohan, Dragomir Radev",QTSumm: A New Benchmark for Query-Focused Table Summarization,work in progress,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  People primarily consult tables to conduct data analysis or answer specific questions. Text generation systems that can provide accurate table summaries tailored to users' information needs can facilitate more efficient access to relevant data insights. However, existing table-to-text generation studies primarily focus on converting tabular data into coherent statements, rather than addressing information-seeking purposes. In this paper, we define a new query-focused table summarization task, where text generation models have to perform human-like reasoning and analysis over the given table to generate a tailored summary, and we introduce a new benchmark named QTSumm for this task. QTSumm consists of 5,625 human-annotated query-summary pairs over 2,437 tables on diverse topics. Moreover, we investigate state-of-the-art models (i.e., text generation, table-to-text generation, and large language models) on the QTSumm dataset. Experimental results and manual analysis reveal that our benchmark presents significant challenges in table-to-text generation for future research. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 17:43:51 GMT'}]",2023-05-24,"[['Zhao', 'Yilun', ''], ['Qi', 'Zhenting', ''], ['Nan', 'Linyong', ''], ['Mi', 'Boyu', ''], ['Liu', 'Yixin', ''], ['Zou', 'Weijin', ''], ['Han', 'Simeng', ''], ['Tang', 'Xiangru', ''], ['Xu', 'Yumo', ''], ['Cohan', 'Arman', ''], ['Radev', 'Dragomir', '']]",0,0,2023-05-23,1,11,1,0,0,0,beeaa3b353f0afeb2387f8336c052042a9b78cc0,258841577.0,https://www.semanticscholar.org/paper/beeaa3b353f0afeb2387f8336c052042a9b78cc0,arXiv.org,2023.0,41.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '46316984', 'name': 'Yilun Zhao'}, {'authorId': '2186056193', 'name': 'Zhenting Qi'}, {'authorId': '51990260', 'name': 'Linyong Nan'}, {'authorId': '2124195718', 'name': 'Boyu Mi'}, {'authorId': '2108176413', 'name': 'Yixin Liu'}, {'authorId': '2166311011', 'name': 'Weijin Zou'}, {'authorId': '3226782', 'name': 'Simeng Han'}, {'authorId': '47274259', 'name': 'Xiangru Tang'}, {'authorId': '115986373', 'name': 'Yumo Xu'}, {'authorId': '2527954', 'name': 'Arman Cohan'}, {'authorId': '9215251', 'name': 'Dragomir R. Radev'}]","['Harvard University', 'Yale University', 'Zhejiang University', 'Allen Institute for Artificial Intelligence', 'University of Edinburgh']","['China', 'United States', 'United Kingdom']",2023-05
2305.14323,Zhipeng Chen,"Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Wayne Xin Zhao and
  Ji-Rong Wen",ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models,"11 pages, working in progress",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although large language models (LLMs) have achieved excellent performance in a variety of evaluation benchmarks, they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning. To improve the reasoning abilities, we propose \textbf{ChatCoT}, a tool-augmented chain-of-thought reasoning framework for chat-based LLMs. In ChatCoT, we model the chain-of-thought~(CoT) reasoning as multi-turn conversations, to utilize tools in a more natural way through chatting. At each turn, LLMs can either interact with tools or perform the reasoning. Our approach can effectively leverage the multi-turn conversation ability of chat-based LLMs, and integrate the thought chain following and tools manipulation in a unified way. Specially, we initialize the early turns of the conversation by the tools, tasks and reasoning format, and propose an iterative \emph{tool-augmented reasoning} step to perform step-by-step tool-augmented reasoning. The experiment results on two complex reasoning datasets (MATH and HotpotQA) have shown the effectiveness of ChatCoT on complex reasoning tasks, achieving a 6.8\% relative improvement over the state-of-the-art baseline. Our code and data are available at: \url{https://github.com/RUCAIBOX/ChatCoT}. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 17:54:33 GMT'}, {'version': 'v2', 'created': 'Wed, 24 May 2023 11:40:59 GMT'}]",2023-05-25,"[['Chen', 'Zhipeng', ''], ['Zhou', 'Kun', ''], ['Zhang', 'Beichen', ''], ['Gong', 'Zheng', ''], ['Zhao', 'Wayne Xin', ''], ['Wen', 'Ji-Rong', '']]",0,0,2023-05-23,2,6,1,0,0,0,95ca67ba607d7859ee8eec457f4b59b115d69bf5,258841374.0,https://www.semanticscholar.org/paper/95ca67ba607d7859ee8eec457f4b59b115d69bf5,arXiv.org,2023.0,31.0,7.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '46842323', 'name': 'Z. Chen'}, {'authorId': '1423651904', 'name': 'Kun Zhou'}, {'authorId': '2107926615', 'name': 'Beichen Zhang'}, {'authorId': '2164092564', 'name': 'Zheng Gong'}, {'authorId': '2542603', 'name': 'Wayne Xin Zhao'}, {'authorId': '153693432', 'name': 'Ji-rong Wen'}]","['Beijing Key Laboratory of Big Data Management and Analysis Methods. {zhipeng_chen,', 'Renmin University of China']",['China'],2023-05
2305.14327,Da Yin,"Da Yin, Xiao Liu, Fan Yin, Ming Zhong, Hritik Bansal, Jiawei Han,
  Kai-Wei Chang",Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation,Work in progress,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Instruction tuning has emerged to enhance the capabilities of large language models (LLMs) in providing appropriate outputs based on input instructions. However, existing methods for collecting instruction-tuning data suffer from limitations in scalability and affordability. In this paper, we propose Dynosaur, a dynamic growth paradigm for instruction-tuning data curation. Built upon the metadata of existing NLP datasets, we generate multiple task instructions applicable to various NLP datasets and determine the relevant data fields for constructing instruction-tuning data with LLMs. Dynosaur offers several advantages: 1) lower generation costs (less than $12 for generating 800K instruction-tuning data), 2) good quality of instruction-tuning data (better performance than Alpaca and Instruction GPT-4 on Super-NI with comparable data sizes), and 3) the ability to grow dynamically by incorporating new datasets from Huggingface Datasets Platform. We further investigate continual learning as an approach to learning with the ever-growing instruction-tuning dataset. We demonstrate that replay methods not only help mitigate forgetting issues but help generalize to unseen tasks better. As a novel continual learning scenario for instruction tuning, selecting tasks based on instruction representations can be an effective replaying strategy. Code and data are released at \url{https://github.com/WadeYin9712/Dynosaur}. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 17:56:26 GMT'}]",2023-05-24,"[['Yin', 'Da', ''], ['Liu', 'Xiao', ''], ['Yin', 'Fan', ''], ['Zhong', 'Ming', ''], ['Bansal', 'Hritik', ''], ['Han', 'Jiawei', ''], ['Chang', 'Kai-Wei', '']]",0,1,2023-05-23,1,7,2,2,0,2,7742233d33da13910d0303e4ec8814a4e26e96e9,258841263.0,https://www.semanticscholar.org/paper/7742233d33da13910d0303e4ec8814a4e26e96e9,arXiv.org,2023.0,39.0,8.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '144508458', 'name': 'Da Yin'}, {'authorId': '49543720', 'name': 'Xiao Liu'}, {'authorId': '2065089223', 'name': 'Fan Yin'}, {'authorId': '1606040932', 'name': 'Ming Zhong'}, {'authorId': '103404553', 'name': 'Hritik Bansal'}, {'authorId': '2111759643', 'name': 'Jiawei Han'}, {'authorId': '2782886', 'name': 'Kai-Wei Chang'}]","['Peking University', 'University of California, Los Angeles', 'University of Illinois Urbana-Champaign']","['China', 'United States']",2023-05
2305.14333,Xu Zhao,"Xu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, Qizhe Xie",Automatic Model Selection with Large Language Models for Reasoning,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Chain-of-Thought and Program-Aided Language Models represent two distinct reasoning methods, each with its own strengths and weaknesses. We demonstrate that it is possible to combine the best of both worlds by using different models for different problems, employing a large language model (LLM) to perform model selection. Through a theoretical analysis, we discover that the performance improvement is determined by the differences between the combined methods and the success rate of choosing the correct model. On eight reasoning datasets, our proposed approach shows significant improvements. Furthermore, we achieve new state-of-the-art results on GSM8K and SVAMP with accuracies of 96.5% and 93.7%, respectively. Our code is publicly available at https://github.com/XuZhao0/Model-Selection-Reasoning. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 17:57:59 GMT'}]",2023-05-24,"[['Zhao', 'Xu', ''], ['Xie', 'Yuxi', ''], ['Kawaguchi', 'Kenji', ''], ['He', 'Junxian', ''], ['Xie', 'Qizhe', '']]",0,0,2023-05-23,1,5,2,0,0,0,6e0def929c2ca3109f87bc0d09c4ab5dbf8e7ba3,258840882.0,https://www.semanticscholar.org/paper/6e0def929c2ca3109f87bc0d09c4ab5dbf8e7ba3,arXiv.org,2023.0,33.0,11.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1410132011', 'name': 'Xu Zhao'}, {'authorId': '46268944', 'name': 'Yuxi Xie'}, {'authorId': '1392876047', 'name': 'Kenji Kawaguchi'}, {'authorId': '2109932032', 'name': 'Junxian He'}, {'authorId': '1912046', 'name': 'Qizhe Xie'}]","['National University of Singapore', 'Hong Kong University of Science and Technology']","['China', 'Singapore']",2023-05
2305.14483,Yang Yu,"Jing-Cheng Pang, Pengyuan Wang, Kaiyuan Li, Xiong-Hui Chen, Jiacheng
  Xu, Zongzhang Zhang, and Yang Yu",Language Model Self-improvement by Reinforcement Learning Contemplation,,,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) have exhibited remarkable performance across various natural language processing (NLP) tasks. However, fine-tuning these models often necessitates substantial supervision, which can be expensive and time-consuming to obtain. This paper introduces a novel unsupervised method called LanguageModel Self-Improvement by Reinforcement Learning Contemplation (SIRLC) that improves LLMs without reliance on external labels. Our approach is grounded in the observation that it is simpler for language models to assess text quality than to generate text. Building on this insight, SIRLC assigns LLMs dual roles as both student and teacher. As a student, the LLM generates answers to unlabeled questions, while as a teacher, it evaluates the generated text and assigns scores accordingly. The model parameters are updated using reinforcement learning to maximize the evaluation score. We demonstrate that SIRLC can be applied to various NLP tasks, such as reasoning problems, text generation, and machine translation. Our experiments show that SIRLC effectively improves LLM performance without external supervision, resulting in a 5.6% increase in answering accuracy for reasoning tasks and a rise in BERTScore from 0.82 to 0.86 for translation tasks. Furthermore, SIRLC can be applied to models of different sizes, showcasing its broad applicability. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 19:25:52 GMT'}]",2023-05-25,"[['Pang', 'Jing-Cheng', ''], ['Wang', 'Pengyuan', ''], ['Li', 'Kaiyuan', ''], ['Chen', 'Xiong-Hui', ''], ['Xu', 'Jiacheng', ''], ['Zhang', 'Zongzhang', ''], ['Yu', 'Yang', '']]",0,0,2023-05-23,1,7,2,0,0,0,c226a4acb42912054d498bcf771023b0ba2da001,258865735.0,https://www.semanticscholar.org/paper/c226a4acb42912054d498bcf771023b0ba2da001,arXiv.org,2023.0,43.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1432234123', 'name': 'Jing-Cheng Pang'}, {'authorId': '2218364703', 'name': 'Pengyuan Wang'}, {'authorId': '2158260214', 'name': 'Kaiyuan Li'}, {'authorId': '2108968873', 'name': 'Xiong-Hui Chen'}, {'authorId': '2121525724', 'name': 'Jiacheng Xu'}, {'authorId': '2079174', 'name': 'Zongzhang Zhang'}, {'authorId': '2152850415', 'name': 'Yang Yu'}]",['Nanjing University'],['China'],2023-05
2305.14497,Zhiheng Xi,"Zhiheng Xi, Senjie Jin, Yuhao Zhou, Rui Zheng, Songyang Gao, Tao Gui,
  Qi Zhang, Xuanjing Huang",Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement,Preprint,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Prompting methods such as Chain-of-Thought (CoT) have shed new light on enhancing the reasoning capabilities of large language models, and researchers have extensively explored the generation process of rationales and answers. However, they have overlooked the potential challenges posed by the poor quality of reasoning problems, which may influence the reasoning performance significantly. In this work, we propose Self-Polish (SP), a novel method that facilitates the model's problem-solving process by prompting them to progressively refine the given problems to be more comprehensible and solvable. Specifically, the method teaches models to eliminate irrelevant information, rearrange the logic structure and organize local conditions into new ones parallelly. SP is orthogonal to all other prompting methods, making it convenient to integrate with state-of-the-art techniques for further improvement. We conduct thorough experiments on five benchmarks to illustrate the effectiveness of the proposed method. For example, with Text-davinci-003, our method boosts the performance of standard few-shot prompting by $8.0\%$ on GSM8K and $17.8\%$ on MultiArith; it also improves the performance of CoT by $6.0\%$ on GSM8K and $6.0\%$ on MathQA, respectively. Furthermore, our method also showcases impressive performance on robustness evaluation. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 19:58:30 GMT'}]",2023-05-25,"[['Xi', 'Zhiheng', ''], ['Jin', 'Senjie', ''], ['Zhou', 'Yuhao', ''], ['Zheng', 'Rui', ''], ['Gao', 'Songyang', ''], ['Gui', 'Tao', ''], ['Zhang', 'Qi', ''], ['Huang', 'Xuanjing', '']]",0,0,2023-05-23,1,8,2,0,0,0,9a9b1e2968302eb882870537d4af6e2c722dfd1a,258865576.0,https://www.semanticscholar.org/paper/9a9b1e2968302eb882870537d4af6e2c722dfd1a,arXiv.org,2023.0,47.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2190751523', 'name': 'Zhiheng Xi'}, {'authorId': '2219131195', 'name': 'Senjie Jin'}, {'authorId': '2212175381', 'name': 'Yuhao Zhou'}, {'authorId': '2058585152', 'name': 'Rui Zheng'}, {'authorId': '2181306462', 'name': 'Songyang Gao'}, {'authorId': '2067331064', 'name': 'Tao Gui'}, {'authorId': '47835189', 'name': 'Qi Zhang'}, {'authorId': '1790227', 'name': 'Xuanjing Huang'}]",['Fudan University'],['China'],2023-05
2305.14688,Benfeng Xu,"Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong
  Zhang, Zhendong Mao",ExpertPrompting: Instructing Large Language Models to be Distinguished Experts,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The answering quality of an aligned large language model (LLM) can be drastically improved if treated with proper crafting of prompts. In this paper, we propose ExpertPrompting to elicit the potential of LLMs to answer as distinguished experts. We first utilize In-Context Learning to automatically synthesize detailed and customized descriptions of the expert identity for each specific instruction, and then ask LLMs to provide answer conditioned on such agent background. Based on this augmented prompting strategy, we produce a new set of instruction-following data using GPT-3.5, and train a competitive open-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation to show that 1) the expert data is of significantly higher quality than vanilla answers, and 2) ExpertLLaMA outperforms existing open-source opponents and achieves 96\% of the original ChatGPT's capability. All data and the ExpertLLaMA model will be made publicly available at \url{https://github.com/OFA-Sys/ExpertLLaMA}. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 03:51:31 GMT'}]",2023-05-25,"[['Xu', 'Benfeng', ''], ['Yang', 'An', ''], ['Lin', 'Junyang', ''], ['Wang', 'Quan', ''], ['Zhou', 'Chang', ''], ['Zhang', 'Yongdong', ''], ['Mao', 'Zhendong', '']]",1,1,2023-05-24,1,7,2,3,0,3,763eb8d43e2f8a5d9da26269a4985efd1c099a5b,258865458.0,https://www.semanticscholar.org/paper/763eb8d43e2f8a5d9da26269a4985efd1c099a5b,arXiv.org,2023.0,16.0,20.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1754285124', 'name': 'Benfeng Xu'}, {'authorId': '143936592', 'name': 'An Yang'}, {'authorId': '35996608', 'name': 'Junyang Lin'}, {'authorId': '2145348814', 'name': 'Quang Wang'}, {'authorId': '2192678144', 'name': 'Chang Zhou'}, {'authorId': '2145050375', 'name': 'Yongdong Zhang'}, {'authorId': '1855978', 'name': 'Zhendong Mao'}]","['Beijing University of Posts and Telecommunications', 'Alibaba', 'University of Science and Technology of China']",['China'],2023-05
2305.14742,Dongxu Yue,"Dongxu Yue, Qin Guo, Munan Ning, Jiaxi Cui, Yuesheng Zhu, Li Yuan",ChatFace: Chat-Guided Real Face Editing via Diffusion Latent Space Manipulation,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Editing real facial images is a crucial task in computer vision with significant demand in various real-world applications. While GAN-based methods have showed potential in manipulating images especially when combined with CLIP, these methods are limited in their ability to reconstruct real images due to challenging GAN inversion capability. Despite the successful image reconstruction achieved by diffusion-based methods, there are still challenges in effectively manipulating fine-gained facial attributes with textual instructions.To address these issues and facilitate convenient manipulation of real facial images, we propose a novel approach that conduct text-driven image editing in the semantic latent space of diffusion model. By aligning the temporal feature of the diffusion model with the semantic condition at generative process, we introduce a stable manipulation strategy, which perform precise zero-shot manipulation effectively. Furthermore, we develop an interactive system named ChatFace, which combines the zero-shot reasoning ability of large language models to perform efficient manipulations in diffusion semantic latent space. This system enables users to perform complex multi-attribute manipulations through dialogue, opening up new possibilities for interactive image editing. Extensive experiments confirmed that our approach outperforms previous methods and enables precise editing of real facial images, making it a promising candidate for real-world applications. Project page: https://dongxuyue.github.io/chatface/ ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 05:28:37 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Jun 2023 10:34:05 GMT'}]",2023-06-06,"[['Yue', 'Dongxu', ''], ['Guo', 'Qin', ''], ['Ning', 'Munan', ''], ['Cui', 'Jiaxi', ''], ['Zhu', 'Yuesheng', ''], ['Yuan', 'Li', '']]",0,0,2023-05-24,2,6,1,0,0,0,7c4f6fd4c7eadcc7189a6797db215895340f93c7,258865947.0,https://www.semanticscholar.org/paper/7c4f6fd4c7eadcc7189a6797db215895340f93c7,arXiv.org,2023.0,54.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '22280052', 'name': 'Dongxu Yue'}, {'authorId': '2219068504', 'name': 'Qin Guo'}, {'authorId': '35480371', 'name': 'Munan Ning'}, {'authorId': '35119585', 'name': 'Jiaxi Cui'}, {'authorId': '2073517', 'name': 'Yuesheng Zhu'}, {'authorId': '2152193547', 'name': 'Liuliang Yuan'}]",['Peking University'],['China'],2023-05
2305.14791,Yongqi Li,"Yongqi Li, Mayi Xu, Xin Miao, Shen Zhou, Tieyun Qian",Large Language Models as Counterfactual Generator: Strengths and Weaknesses,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have demonstrated remarkable performance in a range of natural language understanding and generation tasks. Yet, their ability to generate counterfactuals, which can be used for areas like data augmentation, remains under-explored. This study aims to investigate the counterfactual generation capabilities of LLMs and analysis factors that influence this ability. First, we evaluate how effective are LLMs in counterfactual generation through data augmentation experiments for small language models (SLMs) across four tasks: sentiment analysis, natural language inference, named entity recognition, and relation extraction. While LLMs show promising enhancements in various settings, they struggle in complex tasks due to their self-limitations and the lack of logical guidance to produce counterfactuals that align with commonsense. Second, our analysis reveals the pivotal role of providing accurate task definitions and detailed step-by-step instructions to LLMs in generating counterfactuals. Interestingly, we also find that LLMs can generate reasonable counterfactuals even with unreasonable demonstrations, which illustrates that demonstrations are primarily to regulate the output format.This study provides the first comprehensive insight into counterfactual generation abilities of LLMs, and offers a novel perspective on utilizing LLMs for data augmentation to enhance SLMs. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 06:44:32 GMT'}]",2023-05-25,"[['Li', 'Yongqi', ''], ['Xu', 'Mayi', ''], ['Miao', 'Xin', ''], ['Zhou', 'Shen', ''], ['Qian', 'Tieyun', '']]",0,0,2023-05-24,1,5,1,0,0,0,f1904aadbccfc53735223375016f4b13b6149271,258866143.0,https://www.semanticscholar.org/paper/f1904aadbccfc53735223375016f4b13b6149271,arXiv.org,2023.0,48.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2220318873', 'name': 'Yongqian Li'}, {'authorId': '2110520130', 'name': 'Mayi Xu'}, {'authorId': '2215546811', 'name': 'Xin Miao'}, {'authorId': '2218563624', 'name': 'Shen Zhou'}, {'authorId': '34559283', 'name': 'T. Qian'}]",['Wuhan University'],['China'],2023-05
2305.14825,Xiaojuan Tang,"Xiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu,
  Yitao Liang, Muhan Zhang",Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  The emergent few-shot reasoning capabilities of Large Language Models (LLMs) have excited the natural language and machine learning community over recent years. Despite of numerous successful applications, the underlying mechanism of such in-context capabilities still remains unclear. In this work, we hypothesize that the learned \textit{semantics} of language tokens do the most heavy lifting during the reasoning process. Different from human's symbolic reasoning process, the semantic representations of LLMs could create strong connections among tokens, thus composing a superficial logical chain. To test our hypothesis, we decouple semantics from the language reasoning process and evaluate three kinds of reasoning abilities, i.e., deduction, induction and abduction. Our findings reveal that semantics play a vital role in LLMs' in-context reasoning -- LLMs perform significantly better when semantics are consistent with commonsense but struggle to solve symbolic or counter-commonsense reasoning tasks by leveraging in-context new knowledge. The surprising observations question whether modern LLMs have mastered the inductive, deductive and abductive reasoning abilities as in human intelligence, and motivate research on unveiling the magic existing within the black-box LLMs. On the whole, our analysis provides a novel perspective on the role of semantics in developing and evaluating language models' reasoning abilities. Code is available at {\url{https://github.com/XiaojuanTang/ICSR}}. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 07:33:34 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Jun 2023 16:38:51 GMT'}]",2023-06-09,"[['Tang', 'Xiaojuan', ''], ['Zheng', 'Zilong', ''], ['Li', 'Jiaqi', ''], ['Meng', 'Fanxu', ''], ['Zhu', 'Song-Chun', ''], ['Liang', 'Yitao', ''], ['Zhang', 'Muhan', '']]",0,0,2023-05-24,2,7,2,0,0,0,768850402f991a97ce7a7d063d07050add0254ca,258865899.0,https://www.semanticscholar.org/paper/768850402f991a97ce7a7d063d07050add0254ca,arXiv.org,2023.0,57.0,13.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2189014540', 'name': 'Xiaojuan Tang'}, {'authorId': '49774254', 'name': 'Zilong Zheng'}, {'authorId': None, 'name': 'Jiaqi Li'}, {'authorId': '14788211', 'name': 'Fanxu Meng'}, {'authorId': '145380991', 'name': 'Song-Chun Zhu'}, {'authorId': '2141192339', 'name': 'Yitao Liang'}, {'authorId': '1390814008', 'name': 'Muhan Zhang'}]","['Peking University', 'Tsinghua University', 'The University of Tokyo']","['China', 'Japan']",2023-05
2305.14869,Weiqi Wang Mr.,"Weiqi Wang, Tianqing Fang, Wenxuan Ding, Baixuan Xu, Xin Liu, Yangqiu
  Song, Antoine Bosselut",CAR: Conceptualization-Augmented Reasoner for Zero-Shot Commonsense Question Answering,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The task of zero-shot commonsense question answering evaluates models on their capacity to reason about general scenarios beyond those presented in specific datasets. Existing approaches for tackling this task leverage external knowledge from CommonSense Knowledge Bases (CSKBs) by pretraining the model on synthetic QA pairs constructed from CSKBs. In these approaches, negative examples (distractors) are formulated by randomly sampling from CSKBs using fairly primitive keyword constraints. However, two bottlenecks limit these approaches: the inherent incompleteness of CSKBs limits the semantic coverage of synthetic QA pairs, and the lack of human annotations makes the sampled negative examples potentially uninformative and contradictory. To tackle these limitations above, we propose Conceptualization-Augmented Reasoner (CAR), a zero-shot commonsense question-answering framework that fully leverages the power of conceptualization. Specifically, CAR abstracts a commonsense knowledge triple to many higher-level instances, which increases the coverage of CSKB and expands the ground-truth answer space, reducing the likelihood of selecting false-negative distractors. Extensive experiments demonstrate that CAR more robustly generalizes to answering questions about zero-shot commonsense scenarios than existing methods, including large language models, such as GPT3.5 and ChatGPT. Our codes, data, and model checkpoints are available at https://github.com/HKUST-KnowComp/CAR. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 08:21:31 GMT'}]",2023-05-25,"[['Wang', 'Weiqi', ''], ['Fang', 'Tianqing', ''], ['Ding', 'Wenxuan', ''], ['Xu', 'Baixuan', ''], ['Liu', 'Xin', ''], ['Song', 'Yangqiu', ''], ['Bosselut', 'Antoine', '']]",1,1,2023-05-24,1,7,1,2,0,2,26f5a7259bad9ff05f481aeb57d7431d499fb100,258866021.0,https://www.semanticscholar.org/paper/26f5a7259bad9ff05f481aeb57d7431d499fb100,arXiv.org,2023.0,113.0,11.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1587728690', 'name': 'Weiqi Wang'}, {'authorId': '2044202073', 'name': 'Tianqing Fang'}, {'authorId': '2037491', 'name': 'Wenxuan Ding'}, {'authorId': '7576048', 'name': 'Baixuan Xu'}, {'authorId': '89121677', 'name': 'Xin Liu'}, {'authorId': '1809614', 'name': 'Yangqiu Song'}, {'authorId': '2691021', 'name': 'Antoine Bosselut'}]","['cole Polytechnique Fdrale de Lausanne', 'Hong Kong University of Science and Technology']","['China', 'Switzerland']",2023-05
2305.14898,Keming Lu,"Keming Lu, Xiaoman Pan, Kaiqiang Song, Hongming Zhang, Dong Yu,
  Jianshu Chen",PIVOINE: Instruction Tuning for Open-world Information Extraction,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We consider the problem of Open-world Information Extraction (Open-world IE), which extracts comprehensive entity profiles from unstructured texts. Different from the conventional closed-world setting of Information Extraction (IE), Open-world IE considers a more general situation where entities and relations could be beyond a predefined ontology. More importantly, we seek to develop a large language model (LLM) that is able to perform Open-world IE to extract desirable entity profiles characterized by (possibly fine-grained) natural language instructions. We achieve this by finetuning LLMs using instruction tuning. In particular, we construct INSTRUCTOPENWIKI, a substantial instruction tuning dataset for Open-world IE enriched with a comprehensive corpus, extensive annotations, and diverse instructions. We finetune the pretrained BLOOM models on INSTRUCTOPENWIKI and obtain PIVOINE, an LLM for Open-world IE with strong instruction-following capabilities. Our experiments demonstrate that PIVOINE significantly outperforms traditional closed-world methods and other LLM baselines, displaying impressive generalization capabilities on both unseen instructions and out-of-ontology cases. Consequently, PIVOINE emerges as a promising solution to tackle the open-world challenge in IE effectively. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 08:52:08 GMT'}]",2023-05-25,"[['Lu', 'Keming', ''], ['Pan', 'Xiaoman', ''], ['Song', 'Kaiqiang', ''], ['Zhang', 'Hongming', ''], ['Yu', 'Dong', ''], ['Chen', 'Jianshu', '']]",0,0,2023-05-24,1,6,1,1,1,0,0042c4f35d4a3ea947de8374e298099d476dad7f,258866183.0,https://www.semanticscholar.org/paper/0042c4f35d4a3ea947de8374e298099d476dad7f,arXiv.org,2023.0,36.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1515662094', 'name': 'K. Lu'}, {'authorId': '34741133', 'name': 'Xiaoman Pan'}, {'authorId': '50982080', 'name': 'Kaiqiang Song'}, {'authorId': '49723569', 'name': 'Hongming Zhang'}, {'authorId': '144580027', 'name': 'Dong Yu'}, {'authorId': '2108276402', 'name': 'Jianshu Chen'}]","['University of Southern California', 'Tencent']","['China', 'United States']",2023-05
2305.15005,Wenxuan Zhang,"Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, Lidong Bing",Sentiment Analysis in the Era of Large Language Models: A Reality Check,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Sentiment analysis (SA) has been a long-standing research area in natural language processing. It can offer rich insights into human sentiments and opinions and has thus seen considerable interest from both academia and industry. With the advent of large language models (LLMs) such as ChatGPT, there is a great potential for their employment on SA problems. However, the extent to which existing LLMs can be leveraged for different sentiment analysis tasks remains unclear. This paper aims to provide a comprehensive investigation into the capabilities of LLMs in performing various sentiment analysis tasks, from conventional sentiment classification to aspect-based sentiment analysis and multifaceted analysis of subjective texts. We evaluate performance across 13 tasks on 26 datasets and compare the results against small language models (SLMs) trained on domain-specific datasets. Our study reveals that while LLMs demonstrate satisfactory performance in simpler tasks, they lag behind in more complex tasks requiring deeper understanding or structured sentiment information. However, LLMs significantly outperform SLMs in few-shot learning settings, suggesting their potential when annotation resources are limited. We also highlight the limitations of current evaluation practices in assessing LLMs' SA abilities and propose a novel benchmark, \textsc{SentiEval}, for a more comprehensive and realistic evaluation. Data and code during our investigations are available at \url{https://github.com/DAMO-NLP-SG/LLM-Sentiment}. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 10:45:25 GMT'}]",2023-05-25,"[['Zhang', 'Wenxuan', ''], ['Deng', 'Yue', ''], ['Liu', 'Bing', ''], ['Pan', 'Sinno Jialin', ''], ['Bing', 'Lidong', '']]",1,1,2023-05-24,1,5,1,1,0,1,c589ddc6c6fb07189af7c1212f6eb15c5ff72cde,258866189.0,https://www.semanticscholar.org/paper/c589ddc6c6fb07189af7c1212f6eb15c5ff72cde,arXiv.org,2023.0,67.0,30.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '150341144', 'name': 'Wenxuan Zhang'}, {'authorId': '2162024594', 'name': 'Yue Deng'}, {'authorId': '49166962', 'name': 'Bing-Quan Liu'}, {'authorId': '1746914', 'name': 'Sinno Jialin Pan'}, {'authorId': '1996394', 'name': 'Lidong Bing'}]","['Alibaba', 'University of Illinois at Chicago', 'Chinese University of Hong Kong', 'Nanyang Technological University']","['China', 'United States', 'Singapore']",2023-05
2305.15014,Xingxuan Li,"Xingxuan Li, Liying Cheng, Qingyu Tan, Hwee Tou Ng, Shafiq Joty,
  Lidong Bing",Unlocking Temporal Question Answering for Large Language Models Using Code Execution,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have made significant progress in natural language processing (NLP), and are utilized extensively in various applications. Recent works, such as chain-of-thought (CoT), have shown that intermediate reasoning steps can improve the performance of LLMs for complex reasoning tasks, such as math problems and symbolic question-answering tasks. However, we notice the challenge that LLMs face when it comes to temporal reasoning. Our preliminary experiments show that generating intermediate reasoning steps does not always boost the performance of complex temporal question-answering tasks. Therefore, we propose a novel framework that combines the extraction capability of LLMs and the logical reasoning capability of a Python solver to tackle this issue. Extensive experiments and analysis demonstrate the effectiveness of our framework in handling intricate time-bound reasoning tasks. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 10:57:53 GMT'}]",2023-05-25,"[['Li', 'Xingxuan', ''], ['Cheng', 'Liying', ''], ['Tan', 'Qingyu', ''], ['Ng', 'Hwee Tou', ''], ['Joty', 'Shafiq', ''], ['Bing', 'Lidong', '']]",0,0,2023-05-24,1,6,1,0,0,0,220cb18e8e005024a7ed1c1d41b4b6fa4774847f,258865257.0,https://www.semanticscholar.org/paper/220cb18e8e005024a7ed1c1d41b4b6fa4774847f,arXiv.org,2023.0,31.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2155447436', 'name': 'Xingxuan Li'}, {'authorId': '123962152', 'name': 'Liying Cheng'}, {'authorId': '118358816', 'name': 'Qingyu Tan'}, {'authorId': '34789794', 'name': 'H. Ng'}, {'authorId': '2708940', 'name': 'Shafiq R. Joty'}, {'authorId': '1996394', 'name': 'Lidong Bing'}]","['Alibaba', 'National University of Singapore', 'United States Department of State', 'Nanyang Technological University', 'Salesforce AI']","['China', 'United States', 'Singapore']",2023-05
2305.15021,Yao Mu Mark,"Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun
  Jin, Bin Wang, Jifeng Dai, Yu Qiao, Ping Luo",EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought,,,,,cs.RO cs.AI cs.CV cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Embodied AI is a crucial frontier in robotics, capable of planning and executing action sequences for robots to accomplish long-horizon tasks in physical environments. In this work, we introduce EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi-modal understanding and execution capabilities. To achieve this, we have made the following efforts: (i) We craft a large-scale embodied planning dataset, termed EgoCOT. The dataset consists of carefully selected videos from the Ego4D dataset, along with corresponding high-quality language instructions. Specifically, we generate a sequence of sub-goals with the ""Chain of Thoughts"" mode for effective embodied planning. (ii) We introduce an efficient training approach to EmbodiedGPT for high-quality plan generation, by adapting a 7B large language model (LLM) to the EgoCOT dataset via prefix tuning. (iii) We introduce a paradigm for extracting task-related features from LLM-generated planning queries to form a closed loop between high-level planning and low-level control. Extensive experiments show the effectiveness of EmbodiedGPT on embodied tasks, including embodied planning, embodied control, visual captioning, and visual question answering. Notably, EmbodiedGPT significantly enhances the success rate of the embodied control task by extracting more effective features. It has achieved a remarkable 1.6 times increase in success rate on the Franka Kitchen benchmark and a 1.3 times increase on the Meta-World benchmark, compared to the BLIP-2 baseline fine-tuned with the Ego4D dataset. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 11:04:30 GMT'}, {'version': 'v2', 'created': 'Wed, 13 Sep 2023 23:46:22 GMT'}]",2023-09-15,"[['Mu', 'Yao', ''], ['Zhang', 'Qinglong', ''], ['Hu', 'Mengkang', ''], ['Wang', 'Wenhai', ''], ['Ding', 'Mingyu', ''], ['Jin', 'Jun', ''], ['Wang', 'Bin', ''], ['Dai', 'Jifeng', ''], ['Qiao', 'Yu', ''], ['Luo', 'Ping', '']]",0,1,2023-05-24,2,10,4,0,0,0,00cb69a9f280317d1c59ac5827551ee9b10642b8,258865718.0,https://www.semanticscholar.org/paper/00cb69a9f280317d1c59ac5827551ee9b10642b8,arXiv.org,2023.0,73.0,32.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Engineering', 'source': 's2-fos-model'}]","[{'authorId': '1675357512', 'name': 'Yao Mu'}, {'authorId': '2135294932', 'name': 'Qinglong Zhang'}, {'authorId': '2166455111', 'name': 'Mengkang Hu'}, {'authorId': '47825073', 'name': 'Wen Wang'}, {'authorId': '48876151', 'name': 'Mingyu Ding'}, {'authorId': '2110827238', 'name': 'Jun Jin'}, {'authorId': '37722675', 'name': 'Bin Wang'}, {'authorId': '3304536', 'name': 'Jifeng Dai'}, {'authorId': '145858545', 'name': 'Y. Qiao'}, {'authorId': '2143481782', 'name': 'Ping Luo'}]","['Shanghai Artificial Intelligence Laboratory', 'University of Hong Kong', ""Noah's Ark Laboratory""]","['China', 'Hong Kong']",2023-05
2305.15023,Gen Luo,"Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, Rongrong
  Ji",Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models,,,,,cs.CV,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Recently, growing interest has been aroused in extending the multimodal capability of large language models (LLMs), e.g., vision-language (VL) learning, which is regarded as the next milestone of artificial general intelligence. However, existing solutions are prohibitively expensive, which not only need to optimize excessive parameters, but also require another large-scale pre-training before VL instruction tuning. In this paper, we propose a novel and affordable solution for the effective VL adaption of LLMs, called Mixture-of-Modality Adaptation (MMA). Instead of using large neural networks to connect the image encoder and LLM, MMA adopts lightweight modules, i.e., adapters, to bridge the gap between LLMs and VL tasks, which also enables the joint optimization of the image and language models. Meanwhile, MMA is also equipped with a routing algorithm to help LLMs achieve an automatic shift between single- and multi-modal instructions without compromising their ability of natural language understanding. To validate MMA, we apply it to a recent LLM called LLaMA and term this formed large vision-language instructed model as LaVIN. To validate MMA and LaVIN, we conduct extensive experiments under two setups, namely multimodal science question answering and multimodal dialogue. The experimental results not only demonstrate the competitive performance and the superior training efficiency of LaVIN than existing multimodal LLMs, but also confirm its great potential as a general-purpose chatbot. More importantly, the actual expenditure of LaVIN is extremely cheap, e.g., only 1.4 training hours with 3.8M trainable parameters, greatly confirming the effectiveness of MMA. Our project is released at https://luogen1996.github.io/lavin. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 11:06:15 GMT'}, {'version': 'v2', 'created': 'Thu, 15 Jun 2023 07:02:41 GMT'}]",2023-06-16,"[['Luo', 'Gen', ''], ['Zhou', 'Yiyi', ''], ['Ren', 'Tianhe', ''], ['Chen', 'Shengxin', ''], ['Sun', 'Xiaoshuai', ''], ['Ji', 'Rongrong', '']]",0,0,2023-05-24,2,6,1,1,1,0,9c3a9b4821daa03cb5369041d59d2714329a3811,258865326.0,https://www.semanticscholar.org/paper/9c3a9b4821daa03cb5369041d59d2714329a3811,arXiv.org,2023.0,53.0,21.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2056100172', 'name': 'Gen Luo'}, {'authorId': '2110191063', 'name': 'Yiyi Zhou'}, {'authorId': '2143150727', 'name': 'Tianhe Ren'}, {'authorId': '2118529267', 'name': 'Shen Chen'}, {'authorId': '1759841', 'name': 'Xiaoshuai Sun'}, {'authorId': '1572139630', 'name': 'Rongrong Ji'}]","['Xiamen University', 'Peng Cheng Laboratory']",['China'],2023-05
2305.15024,Weiqiang Jin,"Biao Zhao, Weiqiang Jin, Javier Del Ser, Guang Yang",ChatAgri: Exploring Potentials of ChatGPT on Cross-linguistic Agricultural Text Classification,"24 pages,10+figures,46references.Both the first two authors, Biao
  Zhao and Weiqiang Jin, made equal contributions to this work. Corresponding
  author: Guang Yang",,,,cs.CL cs.AI,http://creativecommons.org/publicdomain/zero/1.0/,"  In the era of sustainable smart agriculture, a massive amount of agricultural news text is being posted on the Internet, in which massive agricultural knowledge has been accumulated. In this context, it is urgent to explore effective text classification techniques for users to access the required agricultural knowledge with high efficiency. Mainstream deep learning approaches employing fine-tuning strategies on pre-trained language models (PLMs), have demonstrated remarkable performance gains over the past few years. Nonetheless, these methods still face many drawbacks that are complex to solve, including: 1. Limited agricultural training data due to the expensive-cost and labour-intensive annotation; 2. Poor domain transferability, especially of cross-linguistic ability; 3. Complex and expensive large models deployment.Inspired by the extraordinary success brought by the recent ChatGPT (e.g. GPT-3.5, GPT-4), in this work, we systematically investigate and explore the capability and utilization of ChatGPT applying to the agricultural informatization field. ....(shown in article).... Code has been released on Github https://github.com/albert-jin/agricultural_textual_classification_ChatGPT. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 11:06:23 GMT'}]",2023-05-25,"[['Zhao', 'Biao', ''], ['Jin', 'Weiqiang', ''], ['Del Ser', 'Javier', ''], ['Yang', 'Guang', '']]",1,1,2023-05-24,1,4,2,3,0,3,a2cae38a542a77c8fdb1d38350b58efd60cf12eb,258865467.0,https://www.semanticscholar.org/paper/a2cae38a542a77c8fdb1d38350b58efd60cf12eb,Neurocomputing,2023.0,53.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Agricultural and Food Sciences', 'source': 's2-fos-model'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2200563019', 'name': 'Biao Zhao'}, {'authorId': '2151509980', 'name': 'Weiqiang Jin'}, {'authorId': '9221552', 'name': 'J. Ser'}, {'authorId': '2149522790', 'name': 'Guangyao Yang'}]","['Imperial College London', ""Xi'an Jiaotong University"", 'TECNALIA, Basque Research & Technology Alliance (BRTA), Derio, 48160, Spain']","['China', 'United Kingdom', 'Spain']",2023-05
2305.15038,Liying Cheng,"Liying Cheng, Xingxuan Li, Lidong Bing",Is GPT-4 a Good Data Analyst?,"11 pages, 2 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As large language models (LLMs) have demonstrated their powerful capabilities in plenty of domains and tasks, including context understanding, code generation, language generation, data storytelling, etc., many data analysts may raise concerns if their jobs will be replaced by AI. This controversial topic has drawn a lot of attention in public. However, we are still at a stage of divergent opinions without any definitive conclusion. Motivated by this, we raise the research question of ""is GPT-4 a good data analyst?"" in this work and aim to answer it by conducting head-to-head comparative studies. In detail, we regard GPT-4 as a data analyst to perform end-to-end data analysis with databases from a wide range of domains. We propose a framework to tackle the problems by carefully designing the prompts for GPT-4 to conduct experiments. We also design several task-specific evaluation metrics to systematically compare the performance between several professional human data analysts and GPT-4. Experimental results show that GPT-4 can achieve comparable performance to humans. We also provide in-depth discussions about our results to shed light on further studies before we reach the conclusion that GPT-4 can replace data analysts. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 11:26:59 GMT'}]",2023-05-25,"[['Cheng', 'Liying', ''], ['Li', 'Xingxuan', ''], ['Bing', 'Lidong', '']]",0,1,2023-05-24,1,3,1,1,0,1,c4235f0cd2011583703a70a27b60b62a85b43ea4,258866019.0,https://www.semanticscholar.org/paper/c4235f0cd2011583703a70a27b60b62a85b43ea4,arXiv.org,2023.0,33.0,22.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '123962152', 'name': 'Liying Cheng'}, {'authorId': '2155447436', 'name': 'Xingxuan Li'}, {'authorId': '1996394', 'name': 'Lidong Bing'}]","['Alibaba', 'Hupan Lab, 310023, Hangzhou, China', 'Nanyang Technological University']","['China', 'Singapore']",2023-05
2305.15062,Quzhe Huang,"Quzhe Huang, Mingxu Tao, Zhenwei An, Chen Zhang, Cong Jiang, Zhibin
  Chen, Zirui Wu, Yansong Feng",Lawyer LLaMA Technical Report,Work in progress,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs), like LLaMA, have exhibited remarkable performances across various tasks. Nevertheless, when deployed to specific domains such as law or medicine, the models still confront the challenge of a deficiency in domain-specific knowledge and an inadequate capability to leverage that knowledge to resolve domain-related problems. In this paper, we focus on the legal domain and explore how to inject domain knowledge during the continual training stage and how to design proper supervised finetune tasks to help the model tackle practical issues. Moreover, to alleviate the hallucination problem during model's generation, we add a retrieval module and extract relevant articles before the model answers any queries. Augmenting with the extracted evidence, our model could generate more reliable responses. We release our data and model at https://github.com/AndrewZhe/lawyer-llama. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 11:52:07 GMT'}]",2023-05-25,"[['Huang', 'Quzhe', ''], ['Tao', 'Mingxu', ''], ['An', 'Zhenwei', ''], ['Zhang', 'Chen', ''], ['Jiang', 'Cong', ''], ['Chen', 'Zhibin', ''], ['Wu', 'Zirui', ''], ['Feng', 'Yansong', '']]",0,0,2023-05-24,1,8,2,1,1,0,4fd05237af737c58c60e4ca8a745f85013681604,258865862.0,https://www.semanticscholar.org/paper/4fd05237af737c58c60e4ca8a745f85013681604,,2023.0,43.0,9.0,2.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2007771781', 'name': 'Quzhe Huang'}, {'authorId': '2053641126', 'name': 'Mingxu Tao'}, {'authorId': '1961228', 'name': 'Zhenwei An'}, {'authorId': '2111574159', 'name': 'Chen Zhang'}, {'authorId': '2189380054', 'name': 'Cong Jiang'}, {'authorId': '2136360202', 'name': 'Zhibin Chen'}, {'authorId': '2144021603', 'name': 'Zirui Wu'}, {'authorId': '2115387922', 'name': 'Yansong Feng'}]",['Peking University'],['China'],2023-05
2305.15066,Jiayan Guo,"Jiayan Guo, Lun Du, Hengyu Liu, Mengyu Zhou, Xinyi He, Shi Han",GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking,,,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models~(LLM) like ChatGPT have become indispensable to artificial general intelligence~(AGI), demonstrating excellent performance in various natural language processing tasks. In the real world, graph data is ubiquitous and an essential part of AGI and prevails in domains like social network analysis, bioinformatics and recommender systems. The training corpus of large language models often includes some algorithmic components, which allows them to achieve certain effects on some graph data-related problems. However, there is still little research on their performance on a broader range of graph-structured data. In this study, we conduct an extensive investigation to assess the proficiency of LLMs in comprehending graph data, employing a diverse range of structural and semantic-related tasks. Our analysis encompasses 10 distinct tasks that evaluate the LLMs' capabilities in graph understanding. Through our study, we not only uncover the current limitations of language models in comprehending graph structures and performing associated reasoning tasks but also emphasize the necessity for further advancements and novel approaches to enhance their graph processing capabilities. Our findings contribute valuable insights towards bridging the gap between language models and graph understanding, paving the way for more effective graph mining and knowledge extraction. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 11:53:19 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Jul 2023 15:08:00 GMT'}]",2023-07-12,"[['Guo', 'Jiayan', ''], ['Du', 'Lun', ''], ['Liu', 'Hengyu', ''], ['Zhou', 'Mengyu', ''], ['He', 'Xinyi', ''], ['Han', 'Shi', '']]",1,1,2023-05-24,2,6,2,1,0,1,2b967d82b25088566980aaaf5a7062d90b2fb14f,258865990.0,https://www.semanticscholar.org/paper/2b967d82b25088566980aaaf5a7062d90b2fb14f,arXiv.org,2023.0,44.0,24.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2218911056', 'name': 'Jiayan Guo'}, {'authorId': '12723949', 'name': 'Lun Du'}, {'authorId': '2139794154', 'name': 'Hengyu Liu'}]","['Peking University', ""Xi'an Jiaotong University"", 'Microsoft', 'University of Technology Sydney']","['China', 'United States', 'Australia']",2023-05
2305.15067,Tianyi Tang,"Tianyi Tang, Hongyuan Lu, Yuchen Eleanor Jiang, Haoyang Huang,
  Dongdong Zhang, Wayne Xin Zhao, Furu Wei",Not All Metrics Are Guilty: Improving NLG Evaluation with LLM Paraphrasing,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Most research about natural language generation (NLG) relies on evaluation benchmarks with limited references for a sample, which may result in poor correlations with human judgements. The underlying reason is that one semantic meaning can actually be expressed in different forms, and the evaluation with a single or few references may not accurately reflect the quality of the model's hypotheses. To address this issue, this paper presents a novel method, named Para-Ref, to enhance existing evaluation benchmarks by enriching the number of references. We leverage large language models (LLMs) to paraphrase a single reference into multiple high-quality ones in diverse expressions. Experimental results on representative NLG tasks of machine translation, text summarization, and image caption demonstrate that our method can effectively improve the correlation with human evaluation for sixteen automatic evaluation metrics by +7.82% in ratio. We release the code and data at https://github.com/RUCAIBox/Para-Ref. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 11:53:29 GMT'}]",2023-05-25,"[['Tang', 'Tianyi', ''], ['Lu', 'Hongyuan', ''], ['Jiang', 'Yuchen Eleanor', ''], ['Huang', 'Haoyang', ''], ['Zhang', 'Dongdong', ''], ['Zhao', 'Wayne Xin', ''], ['Wei', 'Furu', '']]",0,0,2023-05-24,1,7,1,0,0,0,0a0ba2a90a4a5871d58e78b333f805eac896a1b1,258865493.0,https://www.semanticscholar.org/paper/0a0ba2a90a4a5871d58e78b333f805eac896a1b1,arXiv.org,2023.0,42.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1997234792', 'name': 'Tianyi Tang'}, {'authorId': '2156273800', 'name': 'Hongyuan Lu'}, {'authorId': '2134457930', 'name': 'Yuchen Jiang'}, {'authorId': '15086992', 'name': 'Haoyang Huang'}, {'authorId': '40232931', 'name': 'Dongdong Zhang'}, {'authorId': '2542603', 'name': 'Wayne Xin Zhao'}, {'authorId': '49807919', 'name': 'Furu Wei'}]","['Beijing Key Laboratory of Big Data Management and Analysis Methods', 'Chinese University of Hong Kong', 'Microsoft', 'Renmin University of China']",['China'],2023-05
2305.15072,Yuxuan Sun,"Yuxuan Sun, Chenglu Zhu, Sunyi Zheng, Kai Zhang, Zhongyi Shui,
  Xiaoxuan Yu, Yizhi Zhao, Honglin Li, Yunlong Zhang, Ruojia Zhao, Xinheng Lyu,
  Lin Yang",PathAsst: Redefining Pathology through Generative Foundation AI Assistant for Pathology,"13 pages, 5 figures, conference",,,,cs.CV cs.MM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, with significant applications in natural image interpretation. However, the field of pathology has largely remained untapped in this regard, despite the growing need for accurate, timely, and personalized diagnostics. To bridge the gap in pathology MLLMs, we present the PathAsst in this study, which is a generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. To develop PathAsst, we collect over 142K high-quality pathology image-text pairs from a variety of reliable sources, including PubMed, comprehensive pathology textbooks, reputable pathology websites, and private data annotated by pathologists. Leveraging the advanced capabilities of ChatGPT/GPT-4, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data, specifically tailored for the invocation of the pathology-specific models, allowing the PathAsst to effectively interact with these models based on the input image and user intent, consequently enhancing the model's diagnostic capabilities. Subsequently, our PathAsst is trained based on Vicuna-13B language model in coordination with the CLIP vision encoder. The results of PathAsst show the potential of harnessing the AI-powered generative foundation model to improve pathology diagnosis and treatment processes. We are committed to open-sourcing our meticulously curated dataset, as well as a comprehensive toolkit designed to aid researchers in the extensive collection and preprocessing of their own datasets. Resources can be obtained at https://github.com/superjamessyx/Generative-Foundation-AI-Assistant-for-Pathology. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 11:55:50 GMT'}]",2023-05-25,"[['Sun', 'Yuxuan', ''], ['Zhu', 'Chenglu', ''], ['Zheng', 'Sunyi', ''], ['Zhang', 'Kai', ''], ['Shui', 'Zhongyi', ''], ['Yu', 'Xiaoxuan', ''], ['Zhao', 'Yizhi', ''], ['Li', 'Honglin', ''], ['Zhang', 'Yunlong', ''], ['Zhao', 'Ruojia', ''], ['Lyu', 'Xinheng', ''], ['Yang', 'Lin', '']]",1,1,2023-05-24,1,12,2,3,1,2,86cbd30d1096b0c7e4ac6b03d97a8df12fd21457,258865639.0,https://www.semanticscholar.org/paper/86cbd30d1096b0c7e4ac6b03d97a8df12fd21457,arXiv.org,2023.0,47.0,7.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2163660875', 'name': 'Yuxuan Sun'}, {'authorId': '50812118', 'name': 'Chenglu Zhu'}, {'authorId': '102500363', 'name': 'S. Zheng'}, {'authorId': '145086492', 'name': 'Kai Zhang'}, {'authorId': '2174668671', 'name': 'Zhongyi Shui'}, {'authorId': '2164321508', 'name': 'Xiaoxuan Yu'}, {'authorId': '2206994095', 'name': 'Yi-Lei Zhao'}, {'authorId': '2115564011', 'name': 'Honglin Li'}, {'authorId': '153533583', 'name': 'Yunlong Zhang'}, {'authorId': '2218101317', 'name': 'Ruojia Zhao'}, {'authorId': '2218462095', 'name': 'Xinheng Lyu'}, {'authorId': '2155558305', 'name': 'Lin Yang'}]","['Westlake University', 'Zhejiang University', 'The Ohio State University']","['China', 'United States']",2023-05
2305.15075,Benyou Wang,"Hongbo Zhang and Junying Chen and Feng Jiang and Fei Yu and Zhihong
  Chen and Jianquan Li and Guiming Chen and Xiangbo Wu and Zhiyi Zhang and
  Qingying Xiao and Xiang Wan and Benyou Wang and Haizhou Li","HuatuoGPT, towards Taming Language Model to Be a Doctor",,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we present HuatuoGPT, a large language model (LLM) for medical consultation. The core recipe of HuatuoGPT is to leverage both \textit{distilled data from ChatGPT} and \textit{real-world data from doctors} in the supervised fine-tuned stage. The responses of ChatGPT are usually detailed, well-presented and informative while it cannot perform like a doctor in many aspects, e.g. for integrative diagnosis. We argue that real-world data from doctors would be complementary to distilled data in the sense the former could tame a distilled language model to perform like doctors. To better leverage the strengths of both data, we train a reward model to align the language model with the merits that both data bring, following an RLAIF (reinforced learning from AI feedback) fashion. To evaluate and benchmark the models, we propose a comprehensive evaluation scheme (including automatic and manual metrics). Experimental results demonstrate that HuatuoGPT achieves state-of-the-art results in performing medical consultation among open-source LLMs in GPT-4 evaluation, human evaluation, and medical benchmark datasets. It is worth noting that by using additional real-world data and RLAIF, the distilled language model (i.e., HuatuoGPT) outperforms its teacher model ChatGPT in most cases. Our code, data, and models are publicly available at \url{https://github.com/FreedomIntelligence/HuatuoGPT}. The online demo is available at \url{https://www.HuatuoGPT.cn/}. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 11:56:01 GMT'}]",2023-05-25,"[['Zhang', 'Hongbo', ''], ['Chen', 'Junying', ''], ['Jiang', 'Feng', ''], ['Yu', 'Fei', ''], ['Chen', 'Zhihong', ''], ['Li', 'Jianquan', ''], ['Chen', 'Guiming', ''], ['Wu', 'Xiangbo', ''], ['Zhang', 'Zhiyi', ''], ['Xiao', 'Qingying', ''], ['Wan', 'Xiang', ''], ['Wang', 'Benyou', ''], ['Li', 'Haizhou', '']]",1,1,2023-05-24,1,13,2,2,0,2,5459cab5dcf3c65c6b4f63b3d9f1e376f722bbcb,258865566.0,https://www.semanticscholar.org/paper/5459cab5dcf3c65c6b4f63b3d9f1e376f722bbcb,arXiv.org,2023.0,28.0,27.0,6.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2116271777', 'name': 'Hongbo Zhang'}, {'authorId': '2108170007', 'name': 'Junying Chen'}, {'authorId': '2214807050', 'name': 'Feng Jiang'}, {'authorId': '40471592', 'name': 'Fei Yu'}, {'authorId': '46843171', 'name': 'Zhihong Chen'}, {'authorId': '2130169642', 'name': 'Jianquan Li'}, {'authorId': '2116379168', 'name': 'Guimin Chen'}, {'authorId': '2108406947', 'name': 'Xiangbo Wu'}, {'authorId': '2214794954', 'name': 'Zhiyi Zhang'}, {'authorId': '2218223021', 'name': 'Qingying Xiao'}, {'authorId': '2101317304', 'name': 'Xiang Wan'}, {'authorId': '2894465', 'name': 'Benyou Wang'}, {'authorId': '2218230700', 'name': 'Haizhou Li'}]","['Chinese University of Hong Kong, Shenzhen']",['China'],2023-05
2305.15077,Junxian He,"Junlei Zhang, Zhenzhong Lan, Junxian He",Contrastive Learning of Sentence Embeddings from Scratch,Preprint,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Contrastive learning has been the dominant approach to train state-of-the-art sentence embeddings. Previous studies have typically learned sentence embeddings either through the use of human-annotated natural language inference (NLI) data or via large-scale unlabeled sentences in an unsupervised manner. However, even in the case of unlabeled data, their acquisition presents challenges in certain domains due to various reasons. To address these issues, we present SynCSE, a contrastive learning framework that trains sentence embeddings with synthesized data. Specifically, we explore utilizing large language models to synthesize the required data samples for contrastive learning, including (1) producing positive and negative annotations given unlabeled sentences (SynCSE-partial), and (2) generating sentences along with their corresponding annotations from scratch (SynCSE-scratch). Experimental results on sentence similarity and reranking tasks indicate that both SynCSE-partial and SynCSE-scratch greatly outperform unsupervised baselines, and SynCSE-partial even achieves comparable performance to the supervised models in most settings. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 11:56:21 GMT'}]",2023-05-25,"[['Zhang', 'Junlei', ''], ['Lan', 'Zhenzhong', ''], ['He', 'Junxian', '']]",0,0,2023-05-24,1,3,1,0,0,0,65e5f2c5fafa70088169bc4d27abc40406f08b1f,258865824.0,https://www.semanticscholar.org/paper/65e5f2c5fafa70088169bc4d27abc40406f08b1f,arXiv.org,2023.0,58.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108012146', 'name': 'Junlei Zhang'}, {'authorId': '2362534', 'name': 'Zhenzhong Lan'}, {'authorId': '2109932032', 'name': 'Junxian He'}]","['Westlake University', 'Zhejiang University', 'Hong Kong University of Science and Technology']",['China'],2023-05
2305.15083,Jiahuan Li,"Jiahuan Li, Hao Zhou, Shujian Huang, Shanbo Cheng, Jiajun Chen",Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large-scale Pretrained Language Models (LLMs), such as ChatGPT and GPT4, have shown strong abilities in multilingual translations, without being explicitly trained on parallel corpora. It is interesting how the LLMs obtain their ability to carry out translation instructions for different languages. In this paper, we present a detailed analysis by finetuning a multilingual pretrained language model, XGLM-7B, to perform multilingual translation following given instructions. Firstly, we show that multilingual LLMs have stronger translation abilities than previously demonstrated. For a certain language, the performance depends on its similarity to English and the amount of data used in the pretraining phase. Secondly, we find that LLMs' ability to carry out translation instructions relies on the understanding of translation instructions and the alignment among different languages. With multilingual finetuning, LLMs could learn to perform the translation task well even for those language pairs unseen during the instruction tuning phase. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 12:00:24 GMT'}, {'version': 'v2', 'created': 'Fri, 30 Jun 2023 02:32:11 GMT'}]",2023-07-03,"[['Li', 'Jiahuan', ''], ['Zhou', 'Hao', ''], ['Huang', 'Shujian', ''], ['Cheng', 'Shanbo', ''], ['Chen', 'Jiajun', '']]",1,1,2023-05-24,2,5,1,2,0,2,5df78a86dc942d3c53aec5ed2e8ac141cb41aa61,258865882.0,https://www.semanticscholar.org/paper/5df78a86dc942d3c53aec5ed2e8ac141cb41aa61,arXiv.org,2023.0,34.0,9.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '2108959287', 'name': 'Jiahuan Li'}, {'authorId': '2111824520', 'name': 'Hao Zhou'}, {'authorId': '2046010', 'name': 'Shujian Huang'}, {'authorId': '2118436421', 'name': 'Shan Chen'}, {'authorId': '1838162', 'name': 'Jiajun Chen'}]","['ByteDance', 'Nanjing University']",['China'],2023-05
2305.15225,Hongyin Luo,"Hongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim,
  Xixin Wu, Danny Fox, Helen Meng, James Glass",SAIL: Search-Augmented Instruction Learning,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) have been significantly improved by instruction fine-tuning, but still lack transparency and the ability to utilize up-to-date knowledge and information. In this work, we propose search-augmented instruction learning (SAIL), which grounds the language generation and instruction following abilities on complex search results generated by in-house and external search engines. With an instruction tuning corpus, we collect search results for each training case from different search APIs and domains, and construct a new search-grounded training set containing \textit{(instruction, grounding information, response)} triplets. We then fine-tune the LLaMA-7B model on the constructed training set. Since the collected results contain unrelated and disputing languages, the model needs to learn to ground on trustworthy search results, filter out distracting passages, and generate the target response. The search result-denoising process entails explicit trustworthy information selection and multi-hop reasoning, since the retrieved passages might be informative but not contain the instruction-following answer. Experiments show that the fine-tuned SAIL-7B model has a strong instruction-following ability, and it performs significantly better on transparency-sensitive tasks, including open-ended question answering and fact checking. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 15:07:30 GMT'}, {'version': 'v2', 'created': 'Sun, 25 Jun 2023 17:56:37 GMT'}]",2023-06-27,"[['Luo', 'Hongyin', ''], ['Chuang', 'Yung-Sung', ''], ['Gong', 'Yuan', ''], ['Zhang', 'Tianhua', ''], ['Kim', 'Yoon', ''], ['Wu', 'Xixin', ''], ['Fox', 'Danny', ''], ['Meng', 'Helen', ''], ['Glass', 'James', '']]",0,0,2023-05-24,2,9,1,1,1,0,a7977870b58e716cd93f571a3b75a610167a75bd,258865283.0,https://www.semanticscholar.org/paper/a7977870b58e716cd93f571a3b75a610167a75bd,arXiv.org,2023.0,42.0,6.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1944274', 'name': 'Hongyin Luo'}, {'authorId': '2475831', 'name': 'Yung-Sung Chuang'}, {'authorId': '145802952', 'name': 'Yuan Gong'}, {'authorId': '2146333115', 'name': 'Tianhua Zhang'}, {'authorId': '143827730', 'name': 'Yoon Kim'}, {'authorId': '1847260', 'name': 'Xixin Wu'}, {'authorId': '31997718', 'name': 'D. Fox'}, {'authorId': '145199941', 'name': 'H. Meng'}, {'authorId': '145898106', 'name': 'James R. Glass'}]","['MIT Computer Science and Artificial Intelligence Lab, Cambridge MA, USA', 'Massachusetts Institute of Technology', 'Chinese University of Hong Kong']","['China', 'United States']",2023-05
2305.15262,Kejuan Yang,"Kejuan Yang, Xiao Liu, Kaiwen Men, Aohan Zeng, Yuxiao Dong, Jie Tang",Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  We identify two crucial limitations in the evaluation of recent parallel-integrated method Parallel Context Windows (PCW), which extends the maximum context lengths of language models, e.g., 2048 for LLaMA, by harnessing window-wise attention and positional embedding techniques. We first show that a simple yet strong baseline, weighted sum ensemble, is missing for the in-context few-shot classification. Moreover, on more challenging Chain-of-Thought (CoT) reasoning (e.g., HotpotQA), PCW would present unexpected deterioration regarding question miscomprehension and false inference. Based on our findings, we suggest that the existing PCW design may not guarantee sufficient improvement and practicality in handling lengthy documents in real-world applications. More community efforts on enabling language models' long context understanding ability should be paid. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 15:48:29 GMT'}]",2023-05-25,"[['Yang', 'Kejuan', ''], ['Liu', 'Xiao', ''], ['Men', 'Kaiwen', ''], ['Zeng', 'Aohan', ''], ['Dong', 'Yuxiao', ''], ['Tang', 'Jie', '']]",0,0,2023-05-24,1,6,1,1,1,0,143a5cd40df8b9edd3bdccc6d7b2245150be8616,258865693.0,https://www.semanticscholar.org/paper/143a5cd40df8b9edd3bdccc6d7b2245150be8616,arXiv.org,2023.0,27.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2218347104', 'name': 'Kejuan Yang'}, {'authorId': '2111312892', 'name': 'Xiao Liu'}, {'authorId': '2218167927', 'name': 'Kaiwen Men'}, {'authorId': '2051712753', 'name': 'Aohan Zeng'}, {'authorId': '2047998', 'name': 'Yuxiao Dong'}, {'authorId': '2148911956', 'name': 'Jie Tang'}]",['Tsinghua University'],['China'],2023-05
2305.15268,Zhengwei Tao,"Zhengwei Tao, Zhi Jin, Xiaoying Bai, Haiyan Zhao, Yanlin Feng, Jia Li,
  Wenpeng Hu",EvEval: A Comprehensive Evaluation of Event Semantics for Large Language Models,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Events serve as fundamental units of occurrence within various contexts. The processing of event semantics in textual information forms the basis of numerous natural language processing (NLP) applications. Recent studies have begun leveraging large language models (LLMs) to address event semantic processing. However, the extent that LLMs can effectively tackle these challenges remains uncertain. Furthermore, the lack of a comprehensive evaluation framework for event semantic processing poses a significant challenge in evaluating these capabilities. In this paper, we propose an overarching framework for event semantic processing, encompassing understanding, reasoning, and prediction, along with their fine-grained aspects. To comprehensively evaluate the event semantic processing abilities of models, we introduce a novel benchmark called EVEVAL. We collect 8 datasets that cover all aspects of event semantic processing. Extensive experiments are conducted on EVEVAL, leading to several noteworthy findings based on the obtained results. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 15:55:40 GMT'}]",2023-05-25,"[['Tao', 'Zhengwei', ''], ['Jin', 'Zhi', ''], ['Bai', 'Xiaoying', ''], ['Zhao', 'Haiyan', ''], ['Feng', 'Yanlin', ''], ['Li', 'Jia', ''], ['Hu', 'Wenpeng', '']]",0,0,2023-05-24,1,7,2,0,0,0,d723193c0453223803ffb926354cd6d2dee32b06,258866165.0,https://www.semanticscholar.org/paper/d723193c0453223803ffb926354cd6d2dee32b06,arXiv.org,2023.0,51.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '148437893', 'name': 'Zhengwei Tao'}, {'authorId': '1700880', 'name': 'Zhi Jin'}, {'authorId': '2219053675', 'name': 'Xiaoying Bai'}, {'authorId': '2187705776', 'name': 'Haiyan Zhao'}, {'authorId': '2115389088', 'name': 'Yanlin Feng'}, {'authorId': '2118373749', 'name': 'Jia Li'}, {'authorId': '7849217', 'name': 'Wenpeng Hu'}]",['Peking University'],['China'],2023-05
2305.15294,Zhihong Shao,"Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu
  Chen",Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy,Preprint,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which interleaves retrieval with generation when producing an output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 16:17:36 GMT'}]",2023-05-25,"[['Shao', 'Zhihong', ''], ['Gong', 'Yeyun', ''], ['Shen', 'Yelong', ''], ['Huang', 'Minlie', ''], ['Duan', 'Nan', ''], ['Chen', 'Weizhu', '']]",0,0,2023-05-24,1,6,1,0,0,0,a1675f47125aa409525c5f759b5e6bcc1c8831aa,258866037.0,https://www.semanticscholar.org/paper/a1675f47125aa409525c5f759b5e6bcc1c8831aa,arXiv.org,2023.0,40.0,10.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '144485528', 'name': 'Zhihong Shao'}, {'authorId': '2171182', 'name': 'Yeyun Gong'}, {'authorId': '1752875', 'name': 'Yelong Shen'}, {'authorId': '1730108', 'name': 'Minlie Huang'}, {'authorId': '46429989', 'name': 'Nan Duan'}, {'authorId': '2109136147', 'name': 'Weizhu Chen'}]","['State Key Lab of Intelligent Technology and Systems,', 'Artificial Intelligence Research Institute', 'Beijing Information Science & Technology University', 'Tsinghua University', 'Microsoft']","['China', 'United States', 'Spain']",2023-05
2305.15377,Yan Liu,"Yan Liu, Xiaokang Chen, Yan Gao, Zhe Su, Fengji Zhang, Daoguang Zan,
  Jian-Guang Lou, Pin-Yu Chen, Tsung-Yi Ho",Uncovering and Quantifying Social Biases in Code Generation,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  With the popularity of automatic code generation tools, such as Copilot, the study of the potential hazards of these tools is gaining importance. In this work, we explore the social bias problem in pre-trained code generation models. We propose a new paradigm to construct code prompts and successfully uncover social biases in code generation models. To quantify the severity of social biases in generated code, we develop a dataset along with three metrics to evaluate the overall social bias and fine-grained unfairness across different demographics. Experimental results on three pre-trained code generation models (Codex, InCoder, and CodeGen) with varying sizes, reveal severe social biases. Moreover, we conduct analysis to provide useful insights for further choice of code generation models with low social bias. (This work contains examples that potentially implicate stereotypes, associations, and other harms that could be offensive to individuals in certain social groups.) ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 17:37:33 GMT'}]",2023-05-25,"[['Liu', 'Yan', ''], ['Chen', 'Xiaokang', ''], ['Gao', 'Yan', ''], ['Su', 'Zhe', ''], ['Zhang', 'Fengji', ''], ['Zan', 'Daoguang', ''], ['Lou', 'Jian-Guang', ''], ['Chen', 'Pin-Yu', ''], ['Ho', 'Tsung-Yi', '']]",0,0,2023-05-24,1,9,1,2,1,1,e43c46f2c00df489e3a68b58bfcd3f87a84a3f59,258866038.0,https://www.semanticscholar.org/paper/e43c46f2c00df489e3a68b58bfcd3f87a84a3f59,arXiv.org,2023.0,60.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1679704', 'name': 'Y. Liu'}, {'authorId': '2212169879', 'name': 'Xiaokang Chen'}, {'authorId': '152673873', 'name': 'Yan Gao'}, {'authorId': '2072382595', 'name': 'Zhe Su'}, {'authorId': '2158120018', 'name': 'Fengji Zhang'}, {'authorId': '2134434187', 'name': 'Daoguang Zan'}, {'authorId': '153249455', 'name': 'Jian-Guang Lou'}, {'authorId': '2218144207', 'name': 'Pin-Yu Chen'}, {'authorId': '2103197703', 'name': 'Tsung-Yi Ho'}]","['Peking University', 'IBM Research - China', 'Chinese University of Hong Kong', 'Microsoft']","['China', 'India']",2023-05
2305.15645,Fengran Mo,"Fengran Mo, Kelong Mao, Yutao Zhu, Yihong Wu, Kaiyu Huang, Jian-Yun
  Nie",ConvGQR: Generative Query Reformulation for Conversational Search,Accepted at ACL 2023,,,,cs.IR cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In conversational search, the user's real search intent for the current turn is dependent on the previous conversation history. It is challenging to determine a good search query from the whole conversation context. To avoid the expensive re-training of the query encoder, most existing methods try to learn a rewriting model to de-contextualize the current query by mimicking the manual query rewriting. However, manually rewritten queries are not always the best search queries. Training a rewriting model on them would limit the model's ability to produce good search queries. Another useful hint is the potential answer to the question. In this paper, we propose ConvGQR, a new framework to reformulate conversational queries based on generative pre-trained language models (PLMs), one for query rewriting and another for generating potential answers. By combining both, ConvGQR can produce better search queries. In addition, to relate query reformulation to retrieval performance, we propose a knowledge infusion mechanism to optimize both query reformulation and retrieval. Extensive experiments on four conversational search datasets demonstrate the effectiveness of ConvGQR. ","[{'version': 'v1', 'created': 'Thu, 25 May 2023 01:45:06 GMT'}, {'version': 'v2', 'created': 'Fri, 26 May 2023 02:32:18 GMT'}]",2023-05-29,"[['Mo', 'Fengran', ''], ['Mao', 'Kelong', ''], ['Zhu', 'Yutao', ''], ['Wu', 'Yihong', ''], ['Huang', 'Kaiyu', ''], ['Nie', 'Jian-Yun', '']]",0,1,2023-05-25,2,6,2,0,0,0,b46594805f241fe787b3b03b80e9796653f17fb1,258887946.0,https://www.semanticscholar.org/paper/b46594805f241fe787b3b03b80e9796653f17fb1,Annual Meeting of the Association for Computational Linguistics,2023.0,50.0,5.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2007643794', 'name': 'Fengran Mo'}, {'authorId': '1580228663', 'name': 'Kelong Mao'}, {'authorId': '1900406', 'name': 'Yutao Zhu'}, {'authorId': '2115665285', 'name': 'Yihong Wu'}, {'authorId': '2112768206', 'name': 'Kaiyu Huang'}, {'authorId': '50204644', 'name': 'J. Nie'}]","['Tsinghua University', 'Universit de Montral', 'Renmin University of China']","['Canada', 'China']",2023-05
2305.15673,Yanfang Chen,"Aakas Zhiyuli, Yanfang Chen, Xuan Zhang, Xun Liang",BookGPT: A General Framework for Book Recommendation Empowered by Large Language Model,Under Review,,,,cs.IR cs.CL,http://creativecommons.org/licenses/by/4.0/,"  With the continuous development and change exhibited by large language model (LLM) technology, represented by generative pretrained transformers (GPTs), many classic scenarios in various fields have re-emerged with new opportunities. This paper takes ChatGPT as the modeling object, incorporates LLM technology into the typical book resource understanding and recommendation scenario for the first time, and puts it into practice. By building a ChatGPT-like book recommendation system (BookGPT) framework based on ChatGPT, this paper attempts to apply ChatGPT to recommendation modeling for three typical tasks, book rating recommendation, user rating recommendation, and book summary recommendation, and explores the feasibility of LLM technology in book recommendation scenarios. At the same time, based on different evaluation schemes for book recommendation tasks and the existing classic recommendation models, this paper discusses the advantages and disadvantages of the BookGPT in book recommendation scenarios and analyzes the opportunities and improvement directions for subsequent LLMs in these scenarios. ","[{'version': 'v1', 'created': 'Thu, 25 May 2023 02:45:22 GMT'}]",2023-05-26,"[['Zhiyuli', 'Aakas', ''], ['Chen', 'Yanfang', ''], ['Zhang', 'Xuan', ''], ['Liang', 'Xun', '']]",1,1,2023-05-25,1,4,2,1,0,1,af067c6f1c12941625e4c3b49b002c7c7c0b2542,258887872.0,https://www.semanticscholar.org/paper/af067c6f1c12941625e4c3b49b002c7c7c0b2542,Electronics,2023.0,48.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2773700', 'name': 'Aakas Zhiyuli'}, {'authorId': '2109261285', 'name': 'YanFang Chen'}, {'authorId': '2108233132', 'name': 'Xuan Zhang'}, {'authorId': '2216707729', 'name': 'Xun Liang'}]","['Shanghai Institute for Science of Science', 'Renmin University of China']",['China'],2023-05
2305.15695,Xiaoyu Chen,"Xiaoyu Chen, Shenao Zhang, Pushi Zhang, Li Zhao, Jianyu Chen",Asking Before Action: Gather Information in Embodied Decision Making with Language Models,,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With strong capabilities of reasoning and a generic understanding of the world, Large Language Models (LLMs) have shown great potential in building versatile embodied decision making agents capable of performing diverse tasks. However, when deployed to unfamiliar environments, we show that LLM agents face challenges in efficiently gathering necessary information, leading to suboptimal performance. On the other hand, in unfamiliar scenarios, human individuals often seek additional information from their peers before taking action, leveraging external knowledge to avoid unnecessary trial and error. Building upon this intuition, we propose \textit{Asking Before Action} (ABA), a method that empowers the agent to proactively query external sources for pertinent information using natural language during their interactions in the environment. In this way, the agent is able to enhance its efficiency and performance by mitigating wasteful steps and circumventing the difficulties associated with exploration in unfamiliar environments. We empirically evaluate our method on an embodied decision making benchmark, ALFWorld, and demonstrate that despite modest modifications in prompts, our method exceeds baseline LLM agents by more than $40$%. Further experiments on two variants of ALFWorld illustrate that by imitation learning, ABA effectively retains and reuses queried and known information in subsequent tasks, mitigating the need for repetitive inquiries. Both qualitative and quantitative results exhibit remarkable performance on tasks that previous methods struggle to solve. ","[{'version': 'v1', 'created': 'Thu, 25 May 2023 04:05:08 GMT'}]",2023-05-26,"[['Chen', 'Xiaoyu', ''], ['Zhang', 'Shenao', ''], ['Zhang', 'Pushi', ''], ['Zhao', 'Li', ''], ['Chen', 'Jianyu', '']]",0,0,2023-05-25,1,5,1,0,0,0,d7a74d22139b8ff806f9dbcb5242b216eeac50c7,258887626.0,https://www.semanticscholar.org/paper/d7a74d22139b8ff806f9dbcb5242b216eeac50c7,arXiv.org,2023.0,51.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2218623338', 'name': 'Xiaoyu Chen'}, {'authorId': '2145522248', 'name': 'Shenao Zhang'}, {'authorId': '1570021289', 'name': 'Pushi Zhang'}, {'authorId': '2218154011', 'name': 'Li Zhao'}, {'authorId': '1391201846', 'name': 'Jianyu Chen'}]","['Tsinghua University', 'Northwestern University', 'Microsoft']","['China', 'United States']",2023-05
2305.15808,Yiqi Lin,"Yiqi Lin, Hao Wu, Ruichen Wang, Haonan Lu, Xiaodong Lin, Hui Xiong,
  Lin Wang",Towards Language-guided Interactive 3D Generation: LLMs as Layout Interpreter with Generative Feedback,Preprint. Work in Progres,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generating and editing a 3D scene guided by natural language poses a challenge, primarily due to the complexity of specifying the positional relations and volumetric changes within the 3D space. Recent advancements in Large Language Models (LLMs) have demonstrated impressive reasoning, conversational, and zero-shot generation abilities across various domains. Surprisingly, these models also show great potential in realizing and interpreting the 3D space. In light of this, we propose a novel language-guided interactive 3D generation system, dubbed LI3D, that integrates LLMs as a 3D layout interpreter into the off-the-shelf layout-to-3D generative models, allowing users to flexibly and interactively generate visual content. Specifically, we design a versatile layout structure base on the bounding boxes and semantics to prompt the LLMs to model the spatial generation and reasoning from language. Our system also incorporates LLaVA, a large language and vision assistant, to provide generative feedback from the visual aspect for improving the visual quality of generated content. We validate the effectiveness of LI3D, primarily in 3D generation and editing through multi-round interactions, which can be flexibly extended to 2D generation and editing. Various experiments demonstrate the potential benefits of incorporating LLMs in generative AI for applications, e.g., metaverse. Moreover, we benchmark the layout reasoning performance of LLMs with neural visual artist tasks, revealing their emergent ability in the spatial layout domain. ","[{'version': 'v1', 'created': 'Thu, 25 May 2023 07:43:39 GMT'}]",2023-05-26,"[['Lin', 'Yiqi', ''], ['Wu', 'Hao', ''], ['Wang', 'Ruichen', ''], ['Lu', 'Haonan', ''], ['Lin', 'Xiaodong', ''], ['Xiong', 'Hui', ''], ['Wang', 'Lin', '']]",0,0,2023-05-25,1,7,1,0,0,0,ef8c21e1f574495f0c80b8c1037dbdb886f0808d,258888229.0,https://www.semanticscholar.org/paper/ef8c21e1f574495f0c80b8c1037dbdb886f0808d,arXiv.org,2023.0,42.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '46396519', 'name': 'Yiqi Lin'}, {'authorId': '1664776313', 'name': 'Hao Wu'}, {'authorId': '2213709086', 'name': 'Ruichen Wang'}, {'authorId': '2130373', 'name': 'H. Lu'}, {'authorId': '2117690698', 'name': 'Xiaodong Lin'}, {'authorId': '2217886184', 'name': 'Hui Xiong'}, {'authorId': '2168616303', 'name': 'Lin Wang'}]","['Rutgers, The State University of New Jersey', 'Hong Kong University of Science and Technology']","['China', 'United States']",2023-05
2305.15964,Zihao Zhao,"Zihao Zhao, Sheng Wang, Jinchen Gu, Yitao Zhu, Lanzhuju Mei, Zixu
  Zhuang, Zhiming Cui, Qian Wang, Dinggang Shen",ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs,"Authors Zihao Zhao, Sheng Wang, Jinchen Gu, Yitao Zhu contributed
  equally to this work and should be considered co-first authors",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The integration of Computer-Assisted Diagnosis (CAD) with Large Language Models (LLMs) holds great potential in clinical applications, specifically in the roles of virtual family doctors and clinic assistants. However, current works in this field are plagued by limitations, specifically a restricted scope of applicable image domains and the provision of unreliable medical advice. This restricts their overall processing capabilities. Furthermore, the mismatch in writing style between LLMs and radiologists undermines their practical usefulness. To tackle these challenges, we introduce ChatCAD+, which is designed to be universal and reliable. It is capable of handling medical images from diverse domains and leveraging up-to-date information from reputable medical websites to provide reliable medical advice. Additionally, it incorporates a template retrieval system that improves report generation performance via exemplar reports. This approach ensures greater consistency with the expertise of human professionals. The source code is available at https://github.com/zhaozh10/ChatCAD. ","[{'version': 'v1', 'created': 'Thu, 25 May 2023 12:03:31 GMT'}, {'version': 'v2', 'created': 'Fri, 26 May 2023 02:53:58 GMT'}, {'version': 'v3', 'created': 'Thu, 29 Jun 2023 02:57:48 GMT'}, {'version': 'v4', 'created': 'Fri, 7 Jul 2023 16:16:12 GMT'}]",2023-07-10,"[['Zhao', 'Zihao', ''], ['Wang', 'Sheng', ''], ['Gu', 'Jinchen', ''], ['Zhu', 'Yitao', ''], ['Mei', 'Lanzhuju', ''], ['Zhuang', 'Zixu', ''], ['Cui', 'Zhiming', ''], ['Wang', 'Qian', ''], ['Shen', 'Dinggang', '']]",0,0,2023-05-25,4,9,1,0,0,0,06091944b864d6dc473cab63321a95fb9c4067cc,258887821.0,https://www.semanticscholar.org/paper/06091944b864d6dc473cab63321a95fb9c4067cc,arXiv.org,2023.0,51.0,6.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '15594254', 'name': 'Zihao Zhao'}, {'authorId': '2151487856', 'name': 'Sheng Wang'}, {'authorId': '2219370455', 'name': 'Jinchen Gu'}, {'authorId': '2212244316', 'name': 'Yitao Zhu'}, {'authorId': '2162946126', 'name': 'Lanzhuju Mei'}, {'authorId': '2124742549', 'name': 'Zixu Zhuang'}, {'authorId': '2089582101', 'name': 'Zhiming Cui'}, {'authorId': '2215061296', 'name': 'Qian Wang'}, {'authorId': '2150038187', 'name': 'Dinggang Shen'}]","['ShanghaiTech University', 'Shanghai Jiao Tong University', 'Shanghai Clinical Research Center', 'Shanghai United Imaging Intelligence Co., Ltd., Shanghai 200230, China, and also', 'Shanghai United Imaging In-telligence Co. Ltd., Shanghai 200230, China.']",['China'],2023-05
2305.16048,Zhifeng Li,Zhifeng Li and Yifan Fan and Bowei Zou and Yu Hong,UFO: Unified Fact Obtaining for Commonsense Question Answering,IJCNN 2023,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Leveraging external knowledge to enhance the reasoning ability is crucial for commonsense question answering. However, the existing knowledge bases heavily rely on manual annotation which unavoidably causes deficiency in coverage of world-wide commonsense knowledge. Accordingly, the knowledge bases fail to be flexible enough to support the reasoning over diverse questions. Recently, large-scale language models (LLMs) have dramatically improved the intelligence in capturing and leveraging knowledge, which opens up a new way to address the issue of eliciting knowledge from language models. We propose a Unified Facts Obtaining (UFO) approach. UFO turns LLMs into knowledge sources and produces relevant facts (knowledge statements) for the given question. We first develop a unified prompt consisting of demonstrations that cover different aspects of commonsense and different question styles. On this basis, we instruct the LLMs to generate question-related supporting facts for various commonsense questions via prompting. After facts generation, we apply a dense retrieval-based fact selection strategy to choose the best-matched fact. This kind of facts will be fed into the answer inference model along with the question. Notably, due to the design of unified prompts, UFO can support reasoning in various commonsense aspects (including general commonsense, scientific commonsense, and social commonsense). Extensive experiments on CommonsenseQA 2.0, OpenBookQA, QASC, and Social IQA benchmarks show that UFO significantly improves the performance of the inference model and outperforms manually constructed knowledge sources. ","[{'version': 'v1', 'created': 'Thu, 25 May 2023 13:25:49 GMT'}]",2023-05-26,"[['Li', 'Zhifeng', ''], ['Fan', 'Yifan', ''], ['Zou', 'Bowei', ''], ['Hong', 'Yu', '']]",0,0,2023-05-25,1,4,2,0,0,0,da01aecbfd18b44fb274fc082cd9d36534486013,258887773.0,https://www.semanticscholar.org/paper/da01aecbfd18b44fb274fc082cd9d36534486013,IEEE International Joint Conference on Neural Network,2023.0,34.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2217233875', 'name': 'Zhifeng Li'}, {'authorId': '2042646882', 'name': 'Yifan Fan'}, {'authorId': '3078054', 'name': 'Bowei Zou'}, {'authorId': '144873792', 'name': 'Yu Hong'}]","['Soochow University', 'Institute for Infocomm Research']","['China', 'Singapore']",2023-05
2305.16103,Zijia Zhao,"Zijia Zhao, Longteng Guo, Tongtian Yue, Sihan Chen, Shuai Shao, Xinxin
  Zhu, Zehuan Yuan, Jing Liu",ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst,,,,,cs.CV cs.AI cs.CL cs.MM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Building general-purpose models that can perceive diverse real-world modalities and solve various tasks is an appealing target in artificial intelligence. In this paper, we present ChatBridge, a novel multimodal language model that leverages the expressive capabilities of language as the catalyst to bridge the gap between various modalities. We show that only language-paired two-modality data is sufficient to connect all modalities. ChatBridge leverages recent large language models (LLM) and extends their zero-shot capabilities to incorporate diverse multimodal inputs. ChatBridge undergoes a two-stage training. The first stage aligns each modality with language, which brings emergent multimodal correlation and collaboration abilities. The second stage instruction-finetunes ChatBridge to align it with user intent with our newly proposed multimodal instruction tuning dataset, named MULTIS, which covers a wide range of 16 multimodal tasks of text, image, video, and audio modalities. We show strong quantitative and qualitative results on zero-shot multimodal tasks covering text, image, video, and audio modalities. All codes, data, and models of ChatBridge will be open-sourced. ","[{'version': 'v1', 'created': 'Thu, 25 May 2023 14:34:08 GMT'}]",2023-05-26,"[['Zhao', 'Zijia', ''], ['Guo', 'Longteng', ''], ['Yue', 'Tongtian', ''], ['Chen', 'Sihan', ''], ['Shao', 'Shuai', ''], ['Zhu', 'Xinxin', ''], ['Yuan', 'Zehuan', ''], ['Liu', 'Jing', '']]",0,0,2023-05-25,1,8,4,0,0,0,c6ac708b65b24c20f80831d518c1795ce8133ad5,258887944.0,https://www.semanticscholar.org/paper/c6ac708b65b24c20f80831d518c1795ce8133ad5,arXiv.org,2023.0,72.0,12.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2117928203', 'name': 'Zijia Zhao'}, {'authorId': '26982950', 'name': 'Longteng Guo'}, {'authorId': '2218233836', 'name': 'Tongtian Yue'}, {'authorId': '2111638099', 'name': 'Si-Qing Chen'}, {'authorId': '2055776155', 'name': 'Shuai Shao'}, {'authorId': '48386951', 'name': 'Xinxin Zhu'}, {'authorId': '51305314', 'name': 'Zehuan Yuan'}, {'authorId': '40478971', 'name': 'Jing Liu'}]","['ByteDance', 'University of Chinese Academy of Sciences', 'Chinese Academy of Sciences']",['China'],2023-05
2305.16334,Yuanzhen Xie,"Yuanzhen Xie, Tao Xie, Mingxiong Lin, WenTao Wei, Chenglin Li, Beibei
  Kong, Lei Chen, Chengxiang Zhuo, Bo Hu, Zang Li",OlaGPT: Empowering LLMs With Human-like Problem-Solving Abilities,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In most current research, large language models (LLMs) are able to perform reasoning tasks by generating chains of thought through the guidance of specific prompts. However, there still exists a significant discrepancy between their capability in solving complex reasoning problems and that of humans. At present, most approaches focus on chains of thought (COT) and tool use, without considering the adoption and application of human cognitive frameworks. It is well-known that when confronting complex reasoning challenges, humans typically employ various cognitive abilities, and necessitate interaction with all aspects of tools, knowledge, and the external environment information to accomplish intricate tasks. This paper introduces a novel intelligent framework, referred to as OlaGPT. OlaGPT carefully studied a cognitive architecture framework, and propose to simulate certain aspects of human cognition. The framework involves approximating different cognitive modules, including attention, memory, reasoning, learning, and corresponding scheduling and decision-making mechanisms. Inspired by the active learning mechanism of human beings, it proposes a learning unit to record previous mistakes and expert opinions, and dynamically refer to them to strengthen their ability to solve similar problems. The paper also outlines common effective reasoning frameworks for human problem-solving and designs Chain-of-Thought (COT) templates accordingly. A comprehensive decision-making mechanism is also proposed to maximize model accuracy. The efficacy of OlaGPT has been stringently evaluated on multiple reasoning datasets, and the experimental outcomes reveal that OlaGPT surpasses state-of-the-art benchmarks, demonstrating its superior performance. Our implementation of OlaGPT is available on GitHub: \url{https://github.com/oladata-team/OlaGPT}. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 09:36:51 GMT'}]",2023-05-29,"[['Xie', 'Yuanzhen', ''], ['Xie', 'Tao', ''], ['Lin', 'Mingxiong', ''], ['Wei', 'WenTao', ''], ['Li', 'Chenglin', ''], ['Kong', 'Beibei', ''], ['Chen', 'Lei', ''], ['Zhuo', 'Chengxiang', ''], ['Hu', 'Bo', ''], ['Li', 'Zang', '']]",0,1,2023-05-23,1,10,2,0,0,0,aee79d072d627939ce1382c0af8a085ed96c5269,258947120.0,https://www.semanticscholar.org/paper/aee79d072d627939ce1382c0af8a085ed96c5269,arXiv.org,2023.0,35.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1563754185', 'name': 'Yuanzhen Xie'}, {'authorId': '2218515877', 'name': 'Tao Xie'}, {'authorId': '2218721138', 'name': 'Mingxiong Lin'}, {'authorId': '2212773394', 'name': 'Wen-Ke Wei'}, {'authorId': '2128672010', 'name': 'Chenglin Li'}, {'authorId': '2056613156', 'name': 'Beibei Kong'}, {'authorId': '2214406355', 'name': 'Lei Chen'}, {'authorId': '51053175', 'name': 'Chengxiang Zhuo'}, {'authorId': '2118094683', 'name': 'Bo Hu'}, {'authorId': '2109614297', 'name': 'Zang Li'}]",['Tencent'],['China'],2023-05
2305.16344,Chongjian Yue,"Chongjian Yue, Xinrun Xu, Xiaojun Ma, Lun Du, Hengyu Liu, Zhiming
  Ding, Yanbing Jiang, Shi Han, Dongmei Zhang",Leveraging LLMs for KPIs Retrieval from Hybrid Long-Document: A Comprehensive Framework and Dataset,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) demonstrate exceptional performance in textual understanding and tabular reasoning tasks. However, their ability to comprehend and analyze hybrid text, containing textual and tabular data, remains underexplored. In this research, we specialize in harnessing the potential of LLMs to comprehend critical information from financial reports, which are hybrid long-documents. We propose an Automated Financial Information Extraction (AFIE) framework that enhances LLMs' ability to comprehend and extract information from financial reports. To evaluate AFIE, we develop a Financial Reports Numerical Extraction (FINE) dataset and conduct an extensive experimental analysis. Our framework is effectively validated on GPT-3.5 and GPT-4, yielding average accuracy increases of 53.94% and 33.77%, respectively, compared to a naive method. These results suggest that the AFIE framework offers accuracy for automated numerical extraction from complex, hybrid documents. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 10:35:58 GMT'}]",2023-05-29,"[['Yue', 'Chongjian', ''], ['Xu', 'Xinrun', ''], ['Ma', 'Xiaojun', ''], ['Du', 'Lun', ''], ['Liu', 'Hengyu', ''], ['Ding', 'Zhiming', ''], ['Jiang', 'Yanbing', ''], ['Han', 'Shi', ''], ['Zhang', 'Dongmei', '']]",0,1,2023-05-24,1,9,2,2,0,2,520628a4d5b0d609586292c78871ab6b9504a501,258947744.0,https://www.semanticscholar.org/paper/520628a4d5b0d609586292c78871ab6b9504a501,arXiv.org,2023.0,49.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2084608753', 'name': 'C. Yue'}, {'authorId': '2197205915', 'name': 'Xinru Xu'}, {'authorId': '2115573502', 'name': 'Xiao Ma'}, {'authorId': '12723949', 'name': 'Lun Du'}, {'authorId': '2139794154', 'name': 'Hengyu Liu'}, {'authorId': '2112354246', 'name': 'Zhiming Ding'}, {'authorId': '2219041182', 'name': 'Yanbing Jiang'}, {'authorId': '2109750123', 'name': 'Shi Han'}, {'authorId': '2140415600', 'name': 'Dongmei Zhang'}]","['Peking University', 'University of Technology', 'Chinese Academy of Sciences', 'Microsoft']","['China', 'United States', 'Russia']",2023-05
2305.16355,Yixuan Su,"Yixuan Su and Tian Lan and Huayang Li and Jialu Xu and Yan Wang and
  Deng Cai",PandaGPT: One Model To Instruction-Follow Them All,"Technical report, work in progress. Our project page is at
  https://panda-gpt.github.io/",,,,cs.CL cs.CV,http://creativecommons.org/licenses/by/4.0/,"  We present PandaGPT, an approach to emPower large lANguage moDels with visual and Auditory instruction-following capabilities. Our pilot experiments show that PandaGPT can perform complex tasks such as detailed image description generation, writing stories inspired by videos, and answering questions about audios. More interestingly, PandaGPT can take multimodal inputs simultaneously and compose their semantics naturally. For example, PandaGPT can connect how objects look in an image/video and how they sound in an audio. To do so, PandaGPT combines the multimodal encoders from ImageBind and the large language models from Vicuna. Notably, only aligned image-text pairs are required for the training of PandaGPT. Thanks to the strong capability of ImageBind in embedding data from different modalities into the same space, PandaGPT displays emergent, i.e. zero-shot, cross-modal behaviors for data other than image and text (e.g., video, audio, depth, thermal, and IMU). We hope that PandaGPT serves as an initial step toward building AGI that can perceive and understand inputs in different modalities holistically, as we humans do. Our project page is at https://panda-gpt.github.io/. ","[{'version': 'v1', 'created': 'Thu, 25 May 2023 04:16:07 GMT'}]",2023-05-29,"[['Su', 'Yixuan', ''], ['Lan', 'Tian', ''], ['Li', 'Huayang', ''], ['Xu', 'Jialu', ''], ['Wang', 'Yan', ''], ['Cai', 'Deng', '']]",0,1,2023-05-25,1,6,2,1,1,0,d3f79210b54e168c76b8c311488f42d7d1048b81,258947721.0,https://www.semanticscholar.org/paper/d3f79210b54e168c76b8c311488f42d7d1048b81,arXiv.org,2023.0,29.0,58.0,10.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '50087162', 'name': 'Yixuan Su'}, {'authorId': '2113225964', 'name': 'Tian Lan'}, {'authorId': '91956362', 'name': 'Huayang Li'}, {'authorId': '2180639044', 'name': 'Jialu Xu'}, {'authorId': '2152546690', 'name': 'Yan Wang'}, {'authorId': '2053327987', 'name': 'Deng Cai'}]","['University of Cambridge', 'Tencent', 'Nara Institute of Science and Technology']","['China', 'Japan', 'United Kingdom']",2023-05
2305.16582,Yao Yao,"Yao Yao, Zuchao Li, Hai Zhao","Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models",,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  With the widespread use of large language models (LLMs) in NLP tasks, researchers have discovered the potential of Chain-of-thought (CoT) to assist LLMs in accomplishing complex reasoning tasks by generating intermediate steps. However, human thought processes are often non-linear, rather than simply sequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT) reasoning, which models human thought processes not only as a chain but also as a graph. By representing thought units as nodes and connections between them as edges, our approach captures the non-sequential nature of human thinking and allows for a more realistic modeling of thought processes. Similar to Multimodal-CoT, we modeled GoT reasoning as a two-stage framework, generating rationales first and then producing the final answer. Specifically, we employ an additional graph-of-thoughts encoder for GoT representation learning and fuse the GoT representation with the original input representation through a gated fusion mechanism. We implement a GoT reasoning model on the T5 pre-trained model and evaluate its performance on a text-only reasoning task (GSM8K) and a multimodal reasoning task (ScienceQA). Our model achieves significant improvement over the strong CoT baseline with 3.41% and 5.08% on the GSM8K test set with T5-base and T5-large architectures, respectively. Additionally, our model boosts accuracy from 84.91% to 91.54% using the T5-base model and from 91.68% to 92.77% using the T5-large model over the state-of-the-art Multimodal-CoT on the ScienceQA test set. Experiments have shown that GoT achieves comparable results to Multimodal-CoT(large) with over 700M parameters, despite having fewer than 250M backbone model parameters, demonstrating the effectiveness of GoT. ","[{'version': 'v1', 'created': 'Fri, 26 May 2023 02:15:09 GMT'}]",2023-05-29,"[['Yao', 'Yao', ''], ['Li', 'Zuchao', ''], ['Zhao', 'Hai', '']]",0,0,2023-05-26,1,3,1,1,1,0,adb9acaf9184bdbd23105f1a383848eed9bc82fc,258947684.0,https://www.semanticscholar.org/paper/adb9acaf9184bdbd23105f1a383848eed9bc82fc,arXiv.org,2023.0,33.0,18.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2154857867', 'name': 'Yao Yao'}, {'authorId': '30658665', 'name': 'Z. Li'}, {'authorId': '2146232510', 'name': 'Hai Zhao'}]","['Wuhan University', 'Shanghai Jiao Tong University', 'National Engineering Research Center for Multimedia Software,']",['China'],2023-05
2305.16617,Zhijie Deng,"Zhijie Deng, Hongcheng Gao, Yibo Miao, Hao Zhang",Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model,,,,,cs.LG cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The detection of machine-generated text, especially from large language models (LLMs), is crucial in preventing serious social problems resulting from their misuse. Some methods train dedicated detectors on specific datasets but fall short in generalizing to unseen test data, while other zero-shot ones often yield suboptimal performance. Although the recent DetectGPT has shown promising detection performance, it suffers from significant inefficiency issues, as detecting a single candidate requires scoring hundreds of its perturbations with the source LLM. This paper aims to bridge this gap. Technically, we propose to incorporate a Bayesian surrogate model, which allows us to select typical samples based on Bayesian uncertainty and interpolate scores from typical samples to other ones, to improve query efficiency. Our empirical results demonstrate that our method significantly outperforms existing approaches under a low query budget. Notably, our method achieves similar performance with up to 2 times fewer queries than DetectGPT and 3.7% higher AUROC at a query number of 5. ","[{'version': 'v1', 'created': 'Fri, 26 May 2023 04:23:10 GMT'}]",2023-05-29,"[['Deng', 'Zhijie', ''], ['Gao', 'Hongcheng', ''], ['Miao', 'Yibo', ''], ['Zhang', 'Hao', '']]",0,1,2023-05-26,1,4,3,0,0,0,2eeff1edde4fc66284e7bedeb8eb8878ed4560f4,258947640.0,https://www.semanticscholar.org/paper/2eeff1edde4fc66284e7bedeb8eb8878ed4560f4,arXiv.org,2023.0,36.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '145114723', 'name': 'Zhijie Deng'}, {'authorId': '2162081759', 'name': 'Hongcheng Gao'}, {'authorId': '2188993538', 'name': 'Yibo Miao'}, {'authorId': '46702482', 'name': 'Hao Zhang'}]","['Shanghai Jiao Tong University', 'University of California, San Diego']","['China', 'United States']",2023-05
2305.16934,Tianyu Pang,"Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man
  Cheung, Min Lin",On Evaluating Adversarial Robustness of Large Vision-Language Models,,,,,cs.CV cs.CL cs.CR cs.LG cs.MM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses. In particular, we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we observe that black-box queries on these VLMs can further improve the effectiveness of targeted evasion, resulting in a surprisingly high success rate for generating targeted responses. Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice. Code is at https://github.com/yunqing-me/AttackVLM. ","[{'version': 'v1', 'created': 'Fri, 26 May 2023 13:49:44 GMT'}]",2023-05-29,"[['Zhao', 'Yunqing', ''], ['Pang', 'Tianyu', ''], ['Du', 'Chao', ''], ['Yang', 'Xiao', ''], ['Li', 'Chongxuan', ''], ['Cheung', 'Ngai-Man', ''], ['Lin', 'Min', '']]",1,1,2023-05-26,1,7,5,2,0,2,8ecdbfe011b7189fa0ee49ffc4e42a93d728a371,258947177.0,https://www.semanticscholar.org/paper/8ecdbfe011b7189fa0ee49ffc4e42a93d728a371,arXiv.org,2023.0,108.0,19.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '30829388', 'name': 'Yunqing Zhao'}, {'authorId': '19201674', 'name': 'Tianyu Pang'}, {'authorId': '144369497', 'name': 'Chao Du'}, {'authorId': '2109457982', 'name': 'Xiao Yang'}, {'authorId': '2399563', 'name': 'Chongxuan Li'}, {'authorId': '145240703', 'name': 'Ngai-Man Cheung'}, {'authorId': '2115913164', 'name': 'Min Lin'}]","['Singapore University of Technology and Design', 'Tsinghua University', 'Sea AI Lab, Singapore', 'Renmin University of China']","['China', 'Singapore']",2023-05
2305.16944,Tianyi Tang,"Tianyi Tang, Yushuo Chen, Yifan Du, Junyi Li, Wayne Xin Zhao, and
  Ji-Rong Wen",Learning to Imagine: Visually-Augmented Natural Language Generation,Accepted by ACL 2023,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  People often imagine relevant scenes to aid in the writing process. In this work, we aim to utilize visual information for composition in the same manner as humans. We propose a method, LIVE, that makes pre-trained language models (PLMs) Learn to Imagine for Visuallyaugmented natural language gEneration. First, we imagine the scene based on the text: we use a diffusion model to synthesize high-quality images conditioned on the input texts. Second, we use CLIP to determine whether the text can evoke the imagination in a posterior way. Finally, our imagination is dynamic, and we conduct synthesis for each sentence rather than generate only one image for an entire paragraph. Technically, we propose a novel plug-and-play fusion layer to obtain visually-augmented representations for each text. Our vision-text fusion layer is compatible with Transformerbased architecture. We have conducted extensive experiments on four generation tasks using BART and T5, and the automatic results and human evaluation demonstrate the effectiveness of our proposed method. We will release the code, model, and data at the link: https://github.com/RUCAIBox/LIVE. ","[{'version': 'v1', 'created': 'Fri, 26 May 2023 13:59:45 GMT'}, {'version': 'v2', 'created': 'Sun, 4 Jun 2023 14:30:19 GMT'}, {'version': 'v3', 'created': 'Thu, 15 Jun 2023 06:25:34 GMT'}]",2023-06-16,"[['Tang', 'Tianyi', ''], ['Chen', 'Yushuo', ''], ['Du', 'Yifan', ''], ['Li', 'Junyi', ''], ['Zhao', 'Wayne Xin', ''], ['Wen', 'Ji-Rong', '']]",0,0,2023-05-26,3,6,1,1,1,0,9f384b4af6e6a001c6ef6a49f933a2094c5ee43b,258947060.0,https://www.semanticscholar.org/paper/9f384b4af6e6a001c6ef6a49f933a2094c5ee43b,Annual Meeting of the Association for Computational Linguistics,2023.0,62.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1997234792', 'name': 'Tianyi Tang'}, {'authorId': '2109315001', 'name': 'Yushuo Chen'}, {'authorId': '2111895473', 'name': 'Yifan Du'}, {'authorId': '2018027', 'name': 'Junyi Li'}, {'authorId': '2542603', 'name': 'Wayne Xin Zhao'}, {'authorId': '153693432', 'name': 'Ji-rong Wen'}]","['Beijing Key Laboratory of Big Data Management and Analysis Methods', 'Universit de Montral', 'Renmin University of China']","['Canada', 'China']",2023-05
2305.17066,Dylan Ashley,"Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R. Ashley,
  R\'obert Csord\'as, Anand Gopalakrishnan, Abdullah Hamdi, Hasan Abed Al Kader
  Hammoud, Vincent Herrmann, Kazuki Irie, Louis Kirsch, Bing Li, Guohao Li,
  Shuming Liu, Jinjie Mai, Piotr Pi\k{e}kos, Aditya Ramesh, Imanol Schlag,
  Weimin Shi, Aleksandar Stani\'c, Wenyi Wang, Yuhui Wang, Mengmeng Xu,
  Deng-Ping Fan, Bernard Ghanem, J\""urgen Schmidhuber",Mindstorms in Natural Language-Based Societies of Mind,"9 pages in main text + 7 pages of references + 38 pages of
  appendices, 14 figures in main text + 13 in appendices, 7 tables in
  appendices",,,,cs.AI cs.CL cs.CV cs.LG cs.MA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Both Minsky's ""society of mind"" and Schmidhuber's ""learning to think"" inspire diverse societies of large multimodal neural networks (NNs) that solve problems by interviewing each other in a ""mindstorm."" Recent implementations of NN-based societies of minds consist of large language models (LLMs) and other NN-based experts communicating through a natural language interface. In doing so, they overcome the limitations of single LLMs, improving multimodal zero-shot reasoning. In these natural language-based societies of mind (NLSOMs), new agents -- all communicating through the same universal symbolic language -- are easily added in a modular fashion. To demonstrate the power of NLSOMs, we assemble and experiment with several of them (having up to 129 members), leveraging mindstorms in them to solve some practical AI tasks: visual question answering, image captioning, text-to-image synthesis, 3D generation, egocentric retrieval, embodied AI, and general language-based task solving. We view this as a starting point towards much larger NLSOMs with billions of agents-some of which may be humans. And with this emergence of great societies of heterogeneous minds, many new research questions have suddenly become paramount to the future of artificial intelligence. What should be the social structure of an NLSOM? What would be the (dis)advantages of having a monarchical rather than a democratic structure? How can principles of NN economies be used to maximize the total reward of a reinforcement learning NLSOM? In this work, we identify, discuss, and try to answer some of these questions. ","[{'version': 'v1', 'created': 'Fri, 26 May 2023 16:21:25 GMT'}]",2023-05-29,"[['Zhuge', 'Mingchen', ''], ['Liu', 'Haozhe', ''], ['Faccio', 'Francesco', ''], ['Ashley', 'Dylan R.', ''], ['Csords', 'Rbert', ''], ['Gopalakrishnan', 'Anand', ''], ['Hamdi', 'Abdullah', ''], ['Hammoud', 'Hasan Abed Al Kader', ''], ['Herrmann', 'Vincent', ''], ['Irie', 'Kazuki', ''], ['Kirsch', 'Louis', ''], ['Li', 'Bing', ''], ['Li', 'Guohao', ''], ['Liu', 'Shuming', ''], ['Mai', 'Jinjie', ''], ['Pikos', 'Piotr', ''], ['Ramesh', 'Aditya', ''], ['Schlag', 'Imanol', ''], ['Shi', 'Weimin', ''], ['Stani', 'Aleksandar', ''], ['Wang', 'Wenyi', ''], ['Wang', 'Yuhui', ''], ['Xu', 'Mengmeng', ''], ['Fan', 'Deng-Ping', ''], ['Ghanem', 'Bernard', ''], ['Schmidhuber', 'Jrgen', '']]",0,0,2023-05-26,1,26,5,0,0,0,de87c9522ce0c70dc39ce66874b06b34a9fe74eb,258947227.0,https://www.semanticscholar.org/paper/de87c9522ce0c70dc39ce66874b06b34a9fe74eb,arXiv.org,2023.0,0.0,8.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1722264111', 'name': 'Mingchen Zhuge'}, {'authorId': '46935952', 'name': 'Haozhe Liu'}, {'authorId': '79787170', 'name': 'Francesco Faccio'}, {'authorId': '35497986', 'name': 'Dylan R. Ashley'}, {'authorId': '2258963332', 'name': ""R'obert Csord'as""}, {'authorId': '31534667', 'name': 'Anand Gopalakrishnan'}, {'authorId': '24029368', 'name': 'Abdullah Hamdi'}, {'authorId': '1500194870', 'name': 'H. Hammoud'}, {'authorId': '2074116613', 'name': 'Vincent Herrmann'}, {'authorId': '2350348', 'name': 'Kazuki Irie'}, {'authorId': '3031520', 'name': 'Louis Kirsch'}, {'authorId': '114486625', 'name': 'Bing-chuan Li'}, {'authorId': '49461641', 'name': 'G. Li'}, {'authorId': '2107935382', 'name': 'Shuming Liu'}, {'authorId': '2005711751', 'name': 'Jinjie Mai'}, {'authorId': '2107705802', 'name': 'Piotr Pikekos'}, {'authorId': '1992922591', 'name': 'A. Ramesh'}, {'authorId': '35328044', 'name': 'Imanol Schlag'}, {'authorId': '2154581258', 'name': 'Weimin Shi'}, {'authorId': '2218538693', 'name': ""Aleksandar Stani'c""}, {'authorId': '2218962876', 'name': 'Wenyi Wang'}, {'authorId': '2208969427', 'name': 'YuHan Wang'}, {'authorId': '97375393', 'name': 'Mengmeng Xu'}, {'authorId': '23999143', 'name': 'Deng-Ping Fan'}, {'authorId': '2931652', 'name': 'Bernard Ghanem'}, {'authorId': '145341374', 'name': 'J. Schmidhuber'}]","[""Institut Suprieur de l'lectronique et du Numrique"", 'King Abdullah University of Science and Technology', 'Beihang University', 'ETH Zurich', 'Dalle Molle Institute for Artificial Intelligence Research', 'University of Oxford', 'Universit della Svizzera italiana', 'University of Applied Sciences and Arts of Southern Switzerland']","['Saudi Arabia', 'United Kingdom', 'France', 'Switzerland', 'China']",2023-05
2305.17144,Jifeng Dai,"Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu
  Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang,
  Jifeng Dai",Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory,,,,,cs.AI cs.CL cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The captivating realm of Minecraft has attracted substantial research interest in recent years, serving as a rich platform for developing intelligent agents capable of functioning in open-world environments. However, the current research landscape predominantly focuses on specific objectives, such as the popular ""ObtainDiamond"" task, and has not yet shown effective generalization to a broader spectrum of tasks. Furthermore, the current leading success rate for the ""ObtainDiamond"" task stands at around 20%, highlighting the limitations of Reinforcement Learning (RL) based controllers used in existing methods. To tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel framework integrates Large Language Models (LLMs) with text-based knowledge and memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These agents, equipped with the logic and common sense capabilities of LLMs, can skillfully navigate complex, sparse-reward environments with text-based interactions. We develop a set of structured actions and leverage LLMs to generate action plans for the agents to execute. The resulting LLM-based agent markedly surpasses previous methods, achieving a remarkable improvement of +47.5% in success rate on the ""ObtainDiamond"" task, demonstrating superior robustness compared to traditional RL-based controllers. Notably, our agent is the first to procure all items in the Minecraft Overworld technology tree, demonstrating its extensive capabilities. GITM does not need any GPU for training, but a single CPU node with 32 CPU cores is enough. This research shows the potential of LLMs in developing capable agents for handling long-horizon, complex tasks and adapting to uncertainties in open-world environments. See the project website at https://github.com/OpenGVLab/GITM. ","[{'version': 'v1', 'created': 'Thu, 25 May 2023 17:59:49 GMT'}, {'version': 'v2', 'created': 'Thu, 1 Jun 2023 09:18:01 GMT'}]",2023-06-02,"[['Zhu', 'Xizhou', ''], ['Chen', 'Yuntao', ''], ['Tian', 'Hao', ''], ['Tao', 'Chenxin', ''], ['Su', 'Weijie', ''], ['Yang', 'Chenyu', ''], ['Huang', 'Gao', ''], ['Li', 'Bin', ''], ['Lu', 'Lewei', ''], ['Wang', 'Xiaogang', ''], ['Qiao', 'Yu', ''], ['Zhang', 'Zhaoxiang', ''], ['Dai', 'Jifeng', '']]",0,0,2023-05-25,2,13,4,0,0,0,c695c4e68561347564ea0daa50dc339dff73d8c5,258959262.0,https://www.semanticscholar.org/paper/c695c4e68561347564ea0daa50dc339dff73d8c5,arXiv.org,2023.0,27.0,36.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2578924', 'name': 'Xizhou Zhu'}, {'authorId': '2798406', 'name': 'Yuntao Chen'}, {'authorId': '50007795', 'name': 'Hao Tian'}, {'authorId': '150959424', 'name': 'Chenxin Tao'}, {'authorId': '145499378', 'name': 'Weijie Su'}, {'authorId': '2154173465', 'name': 'Chenyuan Yang'}, {'authorId': '2115218570', 'name': 'Gao Huang'}, {'authorId': '2218579598', 'name': 'Bin Li'}, {'authorId': '152309485', 'name': 'Lewei Lu'}, {'authorId': '2137313794', 'name': 'Xiaogang Wang'}, {'authorId': '145858545', 'name': 'Y. Qiao'}, {'authorId': '2175272847', 'name': 'Zhaoxiang Zhang'}, {'authorId': '3304536', 'name': 'Jifeng Dai'}]","['SenseTime Research', 'Centre for Artificial Intelligence and Robotics', 'Chinese Academy of Sciences', 'Shanghai Artificial Intelligence Laboratory', 'Tsinghua University', 'Chinese University of Hong Kong', 'University of Science and Technology of China']","['China', 'India']",2023-05
2305.17147,Zhaowei Zhang,"Zhaowei Zhang, Nian Liu, Siyuan Qi, Ceyao Zhang, Ziqi Rong, Song-Chun
  Zhu, Shuguang Cui, Yaodong Yang",Heterogeneous Value Evaluation for Large Language Models,"Our full prompts are released in the repo:
  https://github.com/zowiezhang/A2EHV",,,,cs.CL cs.AI cs.HC cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The emergent capabilities of Large Language Models (LLMs) have made it crucial to align their values with those of humans. Current methodologies typically attempt alignment with a homogeneous human value and requires human verification, yet lack consensus on the desired aspect and depth of alignment and resulting human biases. In this paper, we propose A2EHV, an Automated Alignment Evaluation with a Heterogeneous Value system that (1) is automated to minimize individual human biases, and (2) allows assessments against various target values to foster heterogeneous agents. Our approach pivots on the concept of value rationality, which represents the ability for agents to execute behaviors that satisfy a target value the most. The quantification of value rationality is facilitated by the Social Value Orientation framework from social psychology, which partitions the value space into four categories to assess social preferences from agents' behaviors. We evaluate the value rationality of eight mainstream LLMs and observe that large models are more inclined to align neutral values compared to those with strong personal values. By examining the behavior of these LLMs, we contribute to a deeper understanding of value alignment within a heterogeneous value system. ","[{'version': 'v1', 'created': 'Fri, 26 May 2023 02:34:20 GMT'}, {'version': 'v2', 'created': 'Thu, 1 Jun 2023 17:00:50 GMT'}]",2023-06-02,"[['Zhang', 'Zhaowei', ''], ['Liu', 'Nian', ''], ['Qi', 'Siyuan', ''], ['Zhang', 'Ceyao', ''], ['Rong', 'Ziqi', ''], ['Zhu', 'Song-Chun', ''], ['Cui', 'Shuguang', ''], ['Yang', 'Yaodong', '']]",0,0,2023-05-26,2,8,4,0,0,0,0cf9616021a281bf6cd4119c5cdd386f5a764481,258959189.0,https://www.semanticscholar.org/paper/0cf9616021a281bf6cd4119c5cdd386f5a764481,arXiv.org,2023.0,30.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Psychology', 'source': 's2-fos-model'}]","[{'authorId': '2174174943', 'name': 'Zhaowei Zhang'}, {'authorId': '2106437037', 'name': 'N. Liu'}, {'authorId': '3390244', 'name': 'Siyuan Qi'}, {'authorId': '2000868582', 'name': 'Ceyao Zhang'}, {'authorId': '2218455070', 'name': 'Ziqi Rong'}, {'authorId': '47796324', 'name': 'Yaodong Yang'}, {'authorId': '1745056', 'name': 'Shuguang Cui'}]","['Peking University', 'National Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI)', 'Chinese University of Hong Kong, Shenzhen', 'University of MichiganAnn Arbor']","['China', 'United States']",2023-05
2305.17197,Hongyin Luo,"Jiaxin Ge, Hongyin Luo, Yoon Kim, James Glass",Entailment as Robust Self-Learner,Accepted by ACL 2023 main conference,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Entailment has been recognized as an important metric for evaluating natural language understanding (NLU) models, and recent studies have found that entailment pretraining benefits weakly supervised fine-tuning. In this work, we design a prompting strategy that formulates a number of different NLU tasks as contextual entailment. This approach improves the zero-shot adaptation of pretrained entailment models. Secondly, we notice that self-training entailment-based models with unlabeled data can significantly improve the adaptation performance on downstream tasks. To achieve more stable improvement, we propose the Simple Pseudo-Label Editing (SimPLE) algorithm for better pseudo-labeling quality in self-training. We also found that both pretrained entailment-based models and the self-trained models are robust against adversarial evaluation data. Experiments on binary and multi-class classification tasks show that SimPLE leads to more robust self-training results, indicating that the self-trained entailment models are more efficient and trustworthy than large language models on language understanding tasks. ","[{'version': 'v1', 'created': 'Fri, 26 May 2023 18:41:23 GMT'}]",2023-05-30,"[['Ge', 'Jiaxin', ''], ['Luo', 'Hongyin', ''], ['Kim', 'Yoon', ''], ['Glass', 'James', '']]",0,0,2023-05-26,1,4,1,0,0,0,4f2dfe2c224ff00a5ff6f0a852ffedb8d1209fb6,258960342.0,https://www.semanticscholar.org/paper/4f2dfe2c224ff00a5ff6f0a852ffedb8d1209fb6,Annual Meeting of the Association for Computational Linguistics,2023.0,80.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2214584825', 'name': 'Jiaxin Ge'}, {'authorId': '1944274', 'name': 'Hongyin Luo'}, {'authorId': '38367242', 'name': 'Yoon Kim'}, {'authorId': '145898106', 'name': 'James R. Glass'}]","['Peking University', 'Massachusetts Institute of Technology']","['China', 'United States']",2023-05
2305.17331,Zichun Yu,"Zichun Yu, Chenyan Xiong, Shi Yu and Zhiyuan Liu",Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In,Accepted to ACL 2023,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Retrieval augmentation can aid language models (LMs) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target LMs, we propose augmentation-adapted retriever (AAR), which learns LM's preferences obtained from a known source LM. Experiments on the MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM is able to significantly improve the zero-shot generalization of larger target LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates that the preferences of different LMs overlap, enabling AAR trained with a single source LM to serve as a generic plug-in for various target LMs. Our code is open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever. ","[{'version': 'v1', 'created': 'Sat, 27 May 2023 02:26:52 GMT'}]",2023-05-30,"[['Yu', 'Zichun', ''], ['Xiong', 'Chenyan', ''], ['Yu', 'Shi', ''], ['Liu', 'Zhiyuan', '']]",0,1,2023-05-27,1,4,2,4,2,2,24811cadf16519910f643b6084107164e6ca4219,258960666.0,https://www.semanticscholar.org/paper/24811cadf16519910f643b6084107164e6ca4219,Annual Meeting of the Association for Computational Linguistics,2023.0,52.0,5.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '103985655', 'name': 'Zichun Yu'}, {'authorId': '2139787803', 'name': 'Chenyan Xiong'}, {'authorId': '150311558', 'name': 'S. Yu'}, {'authorId': '2109232579', 'name': 'Zhiyuan Liu'}]","['Tsinghua University', 'Microsoft', 'University of Science and Technology Beijing']","['China', 'United States']",2023-05
2305.17367,Yongyu Mu,"Yongyu Mu, Abudurexiti Reheman, Zhiquan Cao, Yuchun Fan, Bei Li,
  Yinqiao Li, Tong Xiao, Chunliang Zhang, Jingbo Zhu",Augmenting Large Language Model Translators via Translation Memories,Accepted to Findings of ACL 2023,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Using translation memories (TMs) as prompts is a promising approach to in-context learning of machine translation models. In this work, we take a step towards prompting large language models (LLMs) with TMs and making them better translators. We find that the ability of LLMs to ``understand'' prompts is indeed helpful for making better use of TMs. Experiments show that the results of a pre-trained LLM translator can be greatly improved by using high-quality TM-based prompts. These results are even comparable to those of the state-of-the-art NMT systems which have access to large-scale in-domain bilingual data and are well tuned on the downstream tasks. ","[{'version': 'v1', 'created': 'Sat, 27 May 2023 04:47:09 GMT'}]",2023-05-30,"[['Mu', 'Yongyu', ''], ['Reheman', 'Abudurexiti', ''], ['Cao', 'Zhiquan', ''], ['Fan', 'Yuchun', ''], ['Li', 'Bei', ''], ['Li', 'Yinqiao', ''], ['Xiao', 'Tong', ''], ['Zhang', 'Chunliang', ''], ['Zhu', 'Jingbo', '']]",0,0,2023-05-27,1,9,1,0,0,0,3ed538484f8ded6b2ffd29bcd19972504909cebf,258960135.0,https://www.semanticscholar.org/paper/3ed538484f8ded6b2ffd29bcd19972504909cebf,Annual Meeting of the Association for Computational Linguistics,2023.0,32.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2040853265', 'name': 'Yongyu Mu'}, {'authorId': '2040853045', 'name': 'Abudurexiti Reheman'}, {'authorId': '2203807692', 'name': 'Zhiquan Cao'}, {'authorId': '2218552428', 'name': 'Yuchun Fan'}, {'authorId': '49730090', 'name': 'Bei Li'}, {'authorId': '2110447363', 'name': 'Yinqiao Li'}, {'authorId': '1391183811', 'name': 'Tong Xiao'}, {'authorId': '1390468289', 'name': 'Chunliang Zhang'}, {'authorId': '1728004', 'name': 'Jingbo Zhu'}]","['NiuTrans Research, Shenyang, China', 'Northeastern University']",['China'],2023-05
2305.17455,Dachuan Shi,"Dachuan Shi, Chaofan Tao, Anyi Rao, Zhendong Yang, Chun Yuan, Jiaqi
  Wang",CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers,Technical Report,,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent vision-language models have achieved tremendous progress far beyond what we ever expected. However, their computational costs are also dramatically growing with rapid development, especially for the large models. It makes model acceleration exceedingly critical in a scenario of limited resources. Although extensively studied for unimodal models, the acceleration for multimodal models, especially the vision-language Transformers, is relatively under-explored. To pursue more efficient and accessible vision-language Transformers, this paper introduces \textbf{Cross}-\textbf{G}uided \textbf{E}nsemble of \textbf{T}okens (\textbf{\emph{CrossGET}}), a universal acceleration framework for vision-language Transformers. This framework adaptively combines tokens through real-time, cross-modal guidance, thereby achieving substantial acceleration while keeping high performance. \textit{CrossGET} has two key innovations: 1) \textit{Cross-Guided Matching and Ensemble}. \textit{CrossGET} incorporates cross-modal guided token matching and ensemble to exploit cross-modal information effectively, only introducing cross-modal tokens with negligible extra parameters. 2) \textit{Complete-Graph Soft Matching}. In contrast to the existing bipartite soft matching approach, \textit{CrossGET} introduces a complete-graph soft matching policy to achieve more reliable token-matching results while maintaining parallelizability and high efficiency. Extensive experiments are conducted on various vision-language tasks, including image-text retrieval, visual reasoning, image captioning, and visual question answering. Performance on both classic multimodal architectures and emerging multimodal LLMs demonstrate the effectiveness and versatility of the proposed \textit{CrossGET} framework. The code will be at \url{https://github.com/sdc17/CrossGET}. ","[{'version': 'v1', 'created': 'Sat, 27 May 2023 12:07:21 GMT'}, {'version': 'v2', 'created': 'Wed, 4 Oct 2023 22:11:50 GMT'}]",2023-10-06,"[['Shi', 'Dachuan', ''], ['Tao', 'Chaofan', ''], ['Rao', 'Anyi', ''], ['Yang', 'Zhendong', ''], ['Yuan', 'Chun', ''], ['Wang', 'Jiaqi', '']]",0,0,2023-05-27,2,6,2,0,0,0,1fed19184785b2b50163b3d8ccb7bfaa0321d1aa,258960368.0,https://www.semanticscholar.org/paper/1fed19184785b2b50163b3d8ccb7bfaa0321d1aa,arXiv.org,2023.0,95.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2004872473', 'name': 'Dachuan Shi'}, {'authorId': '144259094', 'name': 'Chaofan Tao'}, {'authorId': '36290866', 'name': 'Anyi Rao'}, {'authorId': '2149234038', 'name': 'Zhendong Yang'}, {'authorId': '2117728946', 'name': 'Chun Yuan'}, {'authorId': '2156546701', 'name': 'Jiaqi Wang'}]","['Shanghai Artificial Intelligence Laboratory', 'Stanford University', 'Tsinghua University', 'University of Hong Kong']","['China', 'United States', 'Hong Kong']",2023-05
2305.17727,Hang Chen,"Hang Chen, Bingyu Liao, Jing Luo, Wenjing Zhu, Xinyu Yang",Learning a Structural Causal Model for Intuition Reasoning in Conversation,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Reasoning, a crucial aspect of NLP research, has not been adequately addressed by prevailing models including Large Language Model. Conversation reasoning, as a critical component of it, remains largely unexplored due to the absence of a well-designed cognitive model. In this paper, inspired by intuition theory on conversation cognition, we develop a conversation cognitive model (CCM) that explains how each utterance receives and activates channels of information recursively. Besides, we algebraically transformed CCM into a structural causal model (SCM) under some mild assumptions, rendering it compatible with various causal discovery methods. We further propose a probabilistic implementation of the SCM for utterance-level relation reasoning. By leveraging variational inference, it explores substitutes for implicit causes, addresses the issue of their unobservability, and reconstructs the causal representations of utterances through the evidence lower bounds. Moreover, we constructed synthetic and simulated datasets incorporating implicit causes and complete cause labels, alleviating the current situation where all available datasets are implicit-causes-agnostic. Extensive experiments demonstrate that our proposed method significantly outperforms existing methods on synthetic, simulated, and real-world datasets. Finally, we analyze the performance of CCM under latent confounders and propose theoretical ideas for addressing this currently unresolved issue. ","[{'version': 'v1', 'created': 'Sun, 28 May 2023 13:54:09 GMT'}]",2023-05-30,"[['Chen', 'Hang', ''], ['Liao', 'Bingyu', ''], ['Luo', 'Jing', ''], ['Zhu', 'Wenjing', ''], ['Yang', 'Xinyu', '']]",0,0,2023-05-28,1,5,1,0,0,0,0457c6c4f3512beb0237e62292d905e962b391a9,258959446.0,https://www.semanticscholar.org/paper/0457c6c4f3512beb0237e62292d905e962b391a9,arXiv.org,2023.0,100.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2183269339', 'name': 'Hang Chen'}, {'authorId': '2955335', 'name': 'Bingyu Liao'}, {'authorId': '2116740369', 'name': 'Jing Luo'}, {'authorId': '2111465278', 'name': 'Wenjing Zhu'}, {'authorId': None, 'name': 'Xinyu Yang'}]","['Du Xiao Man Inc, Beijing, 100089.', 'Chinese University of Hong Kong', ""Xi'an Jiaotong University""]",['China'],2023-05
2305.17891,Linhao Qu,"Linhao Qu, Xiaoyuan Luo, Kexue Fu, Manning Wang, Zhijian Song",The Rise of AI Language Pathologists: Exploring Two-level Prompt Learning for Few-shot Weakly-supervised Whole Slide Image Classification,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper introduces the novel concept of few-shot weakly supervised learning for pathology Whole Slide Image (WSI) classification, denoted as FSWC. A solution is proposed based on prompt learning and the utilization of a large language model, GPT-4. Since a WSI is too large and needs to be divided into patches for processing, WSI classification is commonly approached as a Multiple Instance Learning (MIL) problem. In this context, each WSI is considered a bag, and the obtained patches are treated as instances. The objective of FSWC is to classify both bags and instances with only a limited number of labeled bags. Unlike conventional few-shot learning problems, FSWC poses additional challenges due to its weak bag labels within the MIL framework. Drawing inspiration from the recent achievements of vision-language models (V-L models) in downstream few-shot classification tasks, we propose a two-level prompt learning MIL framework tailored for pathology, incorporating language prior knowledge. Specifically, we leverage CLIP to extract instance features for each patch, and introduce a prompt-guided pooling strategy to aggregate these instance features into a bag feature. Subsequently, we employ a small number of labeled bags to facilitate few-shot prompt learning based on the bag features. Our approach incorporates the utilization of GPT-4 in a question-and-answer mode to obtain language prior knowledge at both the instance and bag levels, which are then integrated into the instance and bag level language prompts. Additionally, a learnable component of the language prompts is trained using the available few-shot labeled data. We conduct extensive experiments on three real WSI datasets encompassing breast cancer, lung cancer, and cervical cancer, demonstrating the notable performance of the proposed method in bag and instance classification. All codes will be made publicly accessible. ","[{'version': 'v1', 'created': 'Mon, 29 May 2023 05:35:44 GMT'}]",2023-05-30,"[['Qu', 'Linhao', ''], ['Luo', 'Xiaoyuan', ''], ['Fu', 'Kexue', ''], ['Wang', 'Manning', ''], ['Song', 'Zhijian', '']]",0,1,2023-05-29,1,5,1,1,0,1,5d0fd58a69be946096516d723b4bf6326ebb319e,258959507.0,https://www.semanticscholar.org/paper/5d0fd58a69be946096516d723b4bf6326ebb319e,arXiv.org,2023.0,43.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Medicine', 'source': 's2-fos-model'}]","[{'authorId': '2146989657', 'name': 'Linhao Qu'}, {'authorId': '143967633', 'name': 'X. Luo'}, {'authorId': '123724620', 'name': 'Kexue Fu'}, {'authorId': '40532896', 'name': 'Manning Wang'}, {'authorId': '143789515', 'name': 'Zhijian Song'}]","['Fudan University', 'Shanghai Key Laboratory of Trustworthy Computing']",['China'],2023-05
2305.17926,Peiyi Wang,"Peiyi Wang and Lei Li and Liang Chen and Zefan Cai and Dawei Zhu and
  Binghuai Lin and Yunbo Cao and Qi Liu and Tianyu Liu and Zhifang Sui",Large Language Models are not Fair Evaluators,,,,,cs.CL cs.AI cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. To address this issue, we propose a calibration framework with three simple yet effective strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score; 3) Human-in-the-Loop Calibration, which introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. We also manually annotate the ""win/tie/lose"" outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna Benchmark's question prompt, and extensive experiments demonstrate that our approach successfully mitigates evaluation bias, resulting in closer alignment with human judgments. We release our code and human annotation at \url{https://github.com/i-Eval/FairEval} to facilitate future research. ","[{'version': 'v1', 'created': 'Mon, 29 May 2023 07:41:03 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Aug 2023 13:22:35 GMT'}]",2023-08-31,"[['Wang', 'Peiyi', ''], ['Li', 'Lei', ''], ['Chen', 'Liang', ''], ['Cai', 'Zefan', ''], ['Zhu', 'Dawei', ''], ['Lin', 'Binghuai', ''], ['Cao', 'Yunbo', ''], ['Liu', 'Qi', ''], ['Liu', 'Tianyu', ''], ['Sui', 'Zhifang', '']]",1,1,2023-05-29,2,10,3,3,1,2,38d64919ba526868a850a0e5f6239d4c474b7e7e,258960339.0,https://www.semanticscholar.org/paper/38d64919ba526868a850a0e5f6239d4c474b7e7e,arXiv.org,2023.0,44.0,102.0,10.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '144202874', 'name': 'Peiyi Wang'}, {'authorId': '49192881', 'name': 'Lei Li'}, {'authorId': '2146034504', 'name': 'Liang Chen'}, {'authorId': '2116276849', 'name': 'Dawei Zhu'}, {'authorId': '3186130', 'name': 'Binghuai Lin'}, {'authorId': '2154235', 'name': 'Yunbo Cao'}, {'authorId': '2144831944', 'name': 'Qi Liu'}, {'authorId': '1701889', 'name': 'Tianyu Liu'}, {'authorId': '3335836', 'name': 'Zhifang Sui'}]","['Peking University', 'Tencent', 'University of Hong Kong']","['China', 'Hong Kong']",2023-05
2305.18072,Feipeng Ma,"Feipeng Ma, Yizhou Zhou, Fengyun Rao, Yueyi Zhang, Xiaoyan Sun",Text-Only Image Captioning with Multi-Context Data Generation,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text-only Image Captioning (TIC) is an approach that aims to construct a model solely based on text that can accurately describe images. Recently, diffusion models have demonstrated remarkable capabilities in generating high-quality images that are semantically coherent with given texts. This presents an opportunity to generate synthetic training images for TIC. However, we have identified a challenge that the images generated from simple descriptions typically exhibit a single perspective with one or limited contexts, which is not aligned with the complexity of real-world scenes in the image domain. In this paper, we propose a novel framework that addresses this issue by introducing multi-context data generation. Starting with an initial text corpus, our framework employs a large language model to select multiple sentences that describe the same scene from various perspectives. These sentences are then summarized into a single sentence with multiple contexts. We generate simple images using the straightforward sentences and complex images using the summarized sentences through diffusion models. Finally, we train the model exclusively using the synthetic image-text pairs obtained from this process. Experimental results demonstrate that our proposed framework effectively tackles the central challenge we have identified, achieving the state-of-the-art performance on popular datasets such as MSCOCO, Flickr30k, and SS1M. ","[{'version': 'v1', 'created': 'Mon, 29 May 2023 13:18:59 GMT'}]",2023-05-30,"[['Ma', 'Feipeng', ''], ['Zhou', 'Yizhou', ''], ['Rao', 'Fengyun', ''], ['Zhang', 'Yueyi', ''], ['Sun', 'Xiaoyan', '']]",0,0,2023-05-29,1,5,1,0,0,0,6d589d44957e8e4ce4544f11e2a5ac589c7a4f00,258960145.0,https://www.semanticscholar.org/paper/6d589d44957e8e4ce4544f11e2a5ac589c7a4f00,arXiv.org,2023.0,51.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2218044713', 'name': 'Feipeng Ma'}, {'authorId': '49455479', 'name': 'Y. Zhou'}, {'authorId': '102827958', 'name': 'Fengyun Rao'}, {'authorId': '2145912767', 'name': 'Yueyi Zhang'}, {'authorId': '2125995569', 'name': 'Xiaoyan Sun'}]","['Tencent', 'University of Science and Technology of China', 'Inception Institute of Artificial Intelligence']","['China', 'United Arab Emirates']",2023-05
2305.18084,Pengxiang Jin,"Pengxiang Jin, Shenglin Zhang, Minghua Ma, Haozhe Li, Yu Kang, Liqun
  Li, Yudong Liu, Bo Qiao, Chaoyun Zhang, Pu Zhao, Shilin He, Federica Sarro,
  Yingnong Dang, Saravan Rajmohan, Qingwei Lin, Dongmei Zhang",Assess and Summarize: Improve Outage Understanding with Large Language Models,,,,,cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Cloud systems have become increasingly popular in recent years due to their flexibility and scalability. Each time cloud computing applications and services hosted on the cloud are affected by a cloud outage, users can experience slow response times, connection issues or total service disruption, resulting in a significant negative business impact. Outages are usually comprised of several concurring events/source causes, and therefore understanding the context of outages is a very challenging yet crucial first step toward mitigating and resolving outages. In current practice, on-call engineers with in-depth domain knowledge, have to manually assess and summarize outages when they happen, which is time-consuming and labor-intensive. In this paper, we first present a large-scale empirical study investigating the way on-call engineers currently deal with cloud outages at Microsoft, and then present and empirically validate a novel approach (dubbed Oasis) to help the engineers in this task. Oasis is able to automatically assess the impact scope of outages as well as to produce human-readable summarization. Specifically, Oasis first assesses the impact scope of an outage by aggregating relevant incidents via multiple techniques. Then, it generates a human-readable summary by leveraging fine-tuned large language models like GPT-3.x. The impact assessment component of Oasis was introduced in Microsoft over three years ago, and it is now widely adopted, while the outage summarization component has been recently introduced, and in this article we present the results of an empirical evaluation we carried out on 18 real-world cloud systems as well as a human-based evaluation with outage owners. The results show that Oasis can effectively and efficiently summarize outages, and lead Microsoft to deploy its first prototype which is currently under experimental adoption by some of the incident teams. ","[{'version': 'v1', 'created': 'Mon, 29 May 2023 13:36:19 GMT'}]",2023-05-30,"[['Jin', 'Pengxiang', ''], ['Zhang', 'Shenglin', ''], ['Ma', 'Minghua', ''], ['Li', 'Haozhe', ''], ['Kang', 'Yu', ''], ['Li', 'Liqun', ''], ['Liu', 'Yudong', ''], ['Qiao', 'Bo', ''], ['Zhang', 'Chaoyun', ''], ['Zhao', 'Pu', ''], ['He', 'Shilin', ''], ['Sarro', 'Federica', ''], ['Dang', 'Yingnong', ''], ['Rajmohan', 'Saravan', ''], ['Lin', 'Qingwei', ''], ['Zhang', 'Dongmei', '']]",0,1,2023-05-29,1,16,1,1,0,1,9943bbb97a48d10b70453e62307c1c797ed64012,258959521.0,https://www.semanticscholar.org/paper/9943bbb97a48d10b70453e62307c1c797ed64012,arXiv.org,2023.0,31.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2450948', 'name': 'Pengxiang Jin'}, {'authorId': '2841424', 'name': 'Shenglin Zhang'}, {'authorId': '3152770', 'name': 'Minghua Ma'}, {'authorId': '2145539091', 'name': 'Haozhe Li'}, {'authorId': '2110042497', 'name': 'Yu Kang'}, {'authorId': '2007715136', 'name': 'Liqun Li'}, {'authorId': '2181323068', 'name': 'Yudong Liu'}, {'authorId': '2007693904', 'name': 'Bo Qiao'}, {'authorId': '3194878', 'name': 'Chaoyun Zhang'}, {'authorId': '2007757792', 'name': 'Pu Zhao'}, {'authorId': '3470504', 'name': 'Shilin He'}, {'authorId': '2103653', 'name': 'Federica Sarro'}, {'authorId': '34402895', 'name': 'Yingnong Dang'}, {'authorId': '148121358', 'name': 'S. Rajmohan'}, {'authorId': '2793487', 'name': 'Qingwei Lin'}, {'authorId': '2109581369', 'name': 'Dongmei Zhang'}]","['University College London', '2023, San Francisco, USA', 'Microsoft', 'Peking University', 'Nankai University']","['China', 'United States', 'United Kingdom']",2023-05
2305.18098,Wen Yang,"Wen Yang, Chong Li, Jiajun Zhang, Chengqing Zong",BigTranslate: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages,"12 pages, 4 figures. Our model is available at
  https://github.com/ZNLP/BigTranslate",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) demonstrate promising translation performance among various natural languages. However, many LLMs especially the open-sourced ones, such as BLOOM and LLaMA, are English-dominant and support only dozens of natural languages, making the potential of LLMs on language translation less explored. In this work, we present BigTranslate which adapts LLaMA that covers only 20 languages and enhances it with multilingual translation capability on more than 100 languages. BigTranslate is built upon LLaMA-13B and it is optimized in three steps. First, we continue training LLaMA with massive Chinese monolingual data. Second, we continue training the model with a large-scale parallel dataset that covers 102 natural languages. Third, we instruct-tune the foundation model with multilingual translation instructions, leading to our BigTranslate model. The preliminary experiments on multilingual translation show that BigTranslate performs comparably with ChatGPT and Google Translate in many languages and even outperforms ChatGPT in 8 language pairs. We release the BigTranslate model and hope it can advance the research progress. ","[{'version': 'v1', 'created': 'Mon, 29 May 2023 14:07:52 GMT'}, {'version': 'v2', 'created': 'Fri, 7 Jul 2023 08:45:42 GMT'}]",2023-07-10,"[['Yang', 'Wen', ''], ['Li', 'Chong', ''], ['Zhang', 'Jiajun', ''], ['Zong', 'Chengqing', '']]",1,1,2023-05-29,2,4,1,3,2,1,7ff2a68167be556de7eaa7da049d877a7fe05406,258960497.0,https://www.semanticscholar.org/paper/7ff2a68167be556de7eaa7da049d877a7fe05406,,2023.0,41.0,5.0,2.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '2218735807', 'name': 'Wen Yang'}, {'authorId': '2109665081', 'name': 'Chong Li'}, {'authorId': '2124819243', 'name': 'Jiajun Zhang'}, {'authorId': '2064100826', 'name': 'Chengqing Zong'}]","['https://github.com/ZNLP/BigTranslate', 'University of Chinese Academy of Sciences', 'Wuhan AI Research', 'Chinese Academy of Sciences']",['China'],2023-05
2305.18149,Yuchuan Tian,"Yuchuan Tian, Hanting Chen, Xutao Wang, Zheyuan Bai, Qinghua Zhang,
  Ruifeng Li, Chao Xu, Yunhe Wang",Multiscale Positive-Unlabeled Detection of AI-Generated Texts,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are astonishing at generating human-like texts, but they may impact the authenticity of texts. Previous works proposed methods to detect these AI-generated texts, including simple ML classifiers, pretrained-model-based zero-shot methods, and finetuned language classification models. However, mainstream detectors always fail on short texts, like SMSes, Tweets, and reviews. In this paper, a Multiscale Positive-Unlabeled (MPU) training framework is proposed to address the difficulty of short-text detection without sacrificing long-texts. Firstly, we acknowledge the human-resemblance property of short machine texts, and rephrase AI text detection as a partial Positive-Unlabeled (PU) problem by regarding these short machine texts as partially ""unlabeled"". Then in this PU context, we propose the length-sensitive Multiscale PU Loss, where a recurrent model in abstraction is used to estimate positive priors of scale-variant corpora. Additionally, we introduce a Text Multiscaling module to enrich training corpora. Experiments show that our MPU method augments detection performance on long AI-generated texts, and significantly improves short-text detection of language model detectors. Language Models trained with MPU could outcompete existing detectors on various short-text and long-text detection benchmarks. The codes are available at https://github.com/mindspore-lab/mindone/tree/master/examples/detect_chatgpt and https://github.com/YuchuanTian/AIGC_text_detector. ","[{'version': 'v1', 'created': 'Mon, 29 May 2023 15:25:00 GMT'}, {'version': 'v2', 'created': 'Fri, 2 Jun 2023 04:50:54 GMT'}, {'version': 'v3', 'created': 'Fri, 29 Sep 2023 14:23:22 GMT'}]",2023-10-02,"[['Tian', 'Yuchuan', ''], ['Chen', 'Hanting', ''], ['Wang', 'Xutao', ''], ['Bai', 'Zheyuan', ''], ['Zhang', 'Qinghua', ''], ['Li', 'Ruifeng', ''], ['Xu', 'Chao', ''], ['Wang', 'Yunhe', '']]",1,1,2023-05-29,3,8,2,1,0,1,f8c6cb00ab9775f90ded5025b49cc260cede9350,258960584.0,https://www.semanticscholar.org/paper/f8c6cb00ab9775f90ded5025b49cc260cede9350,arXiv.org,2023.0,45.0,7.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2218677105', 'name': 'Yuchuan Tian'}, {'authorId': '2118023932', 'name': 'Hanting Chen'}, {'authorId': '1519959128', 'name': 'Xutao Wang'}, {'authorId': '2218622181', 'name': 'Zheyuan Bai'}, {'authorId': '2112221594', 'name': 'Qinghua Zhang'}, {'authorId': '2204771561', 'name': 'Ruifeng Li'}, {'authorId': '46200183', 'name': 'Chaoxi Xu'}, {'authorId': '2161243', 'name': 'Yunhe Wang'}]","['Peking University', 'Huawei Technologies (China)']",['China'],2023-05
2305.18153,Zhangyue Yin,"Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, Xuanjing
  Huang",Do Large Language Models Know What They Don't Know?,"10 pages, 9 figures, accepted by Findings of ACL2023",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have a wealth of knowledge that allows them to excel in various Natural Language Processing (NLP) tasks. Current research focuses on enhancing their performance within their existing knowledge. Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend. Therefore, the ability to understand their own limitations on the unknows, referred to as self-knowledge, is of paramount importance. This study aims to evaluate LLMs' self-knowledge by assessing their ability to identify unanswerable or unknowable questions. We introduce an automated methodology to detect uncertainty in the responses of these models, providing a novel measure of their self-knowledge. We further introduce a unique dataset, SelfAware, consisting of unanswerable questions from five diverse categories and their answerable counterparts. Our extensive analysis, involving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering an intrinsic capacity for self-knowledge within these models. Moreover, we demonstrate that in-context learning and instruction tuning can further enhance this self-knowledge. Despite this promising insight, our findings also highlight a considerable gap between the capabilities of these models and human proficiency in recognizing the limits of their knowledge. ","[{'version': 'v1', 'created': 'Mon, 29 May 2023 15:30:13 GMT'}, {'version': 'v2', 'created': 'Tue, 30 May 2023 15:14:06 GMT'}]",2023-05-31,"[['Yin', 'Zhangyue', ''], ['Sun', 'Qiushi', ''], ['Guo', 'Qipeng', ''], ['Wu', 'Jiawen', ''], ['Qiu', 'Xipeng', ''], ['Huang', 'Xuanjing', '']]",0,1,2023-05-29,2,6,1,3,1,2,eb971944bccf9793ac463c3e2f4d4251d4e8e071,258959258.0,https://www.semanticscholar.org/paper/eb971944bccf9793ac463c3e2f4d4251d4e8e071,Annual Meeting of the Association for Computational Linguistics,2023.0,30.0,19.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2155273086', 'name': 'Zhangyue Yin'}, {'authorId': '2112455065', 'name': 'Qiushi Sun'}, {'authorId': '153683057', 'name': 'Qipeng Guo'}, {'authorId': '2111123834', 'name': 'Jiawen Wu'}, {'authorId': '1767521', 'name': 'Xipeng Qiu'}, {'authorId': '1790227', 'name': 'Xuanjing Huang'}]","['Fudan University', 'National University of Singapore', 'Unknows Knows Unknows Knows Known Knows Known Unknows Unknown Unknows Unknown Knows Unlock']","['China', 'Singapore']",2023-05
2305.18346,Qichao Wang,"Qichao Wang, Huan Ma, Wentao Wei, Hangyu Li, Liang Chen, Peilin Zhao,
  Binwen Zhao, Bo Hu, Shu Zhang, Zibin Zheng, Bingzhe Wu",Attention Paper: How Generative AI Reshapes Digital Shadow Industry?,,,,,cs.CY cs.AI,http://creativecommons.org/licenses/by/4.0/,"  The rapid development of digital economy has led to the emergence of various black and shadow internet industries, which pose potential risks that can be identified and managed through digital risk management (DRM) that uses different techniques such as machine learning and deep learning. The evolution of DRM architecture has been driven by changes in data forms. However, the development of AI-generated content (AIGC) technology, such as ChatGPT and Stable Diffusion, has given black and shadow industries powerful tools to personalize data and generate realistic images and conversations for fraudulent activities. This poses a challenge for DRM systems to control risks from the source of data generation and to respond quickly to the fast-changing risk environment. This paper aims to provide a technical analysis of the challenges and opportunities of AIGC from upstream, midstream, and downstream paths of black/shadow industries and suggest future directions for improving existing risk control systems. The paper will explore the new black and shadow techniques triggered by generative AI technology and provide insights for building the next-generation DRM system. ","[{'version': 'v1', 'created': 'Fri, 26 May 2023 08:03:50 GMT'}]",2023-05-31,"[['Wang', 'Qichao', ''], ['Ma', 'Huan', ''], ['Wei', 'Wentao', ''], ['Li', 'Hangyu', ''], ['Chen', 'Liang', ''], ['Zhao', 'Peilin', ''], ['Zhao', 'Binwen', ''], ['Hu', 'Bo', ''], ['Zhang', 'Shu', ''], ['Zheng', 'Zibin', ''], ['Wu', 'Bingzhe', '']]",1,1,2023-05-26,1,11,2,1,0,1,a438edd03df28f1c51144ebe9d47c21a33a5dedc,258967422.0,https://www.semanticscholar.org/paper/a438edd03df28f1c51144ebe9d47c21a33a5dedc,ACM Turing Celebration Conference,2023.0,29.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2143466459', 'name': 'Qichao Wang'}, {'authorId': '2110817620', 'name': 'Huan Ma'}, {'authorId': '2212773394', 'name': 'Wen-Ke Wei'}, {'authorId': '2179851049', 'name': 'Hang Li'}, {'authorId': '1853048147', 'name': 'Liang Chen'}, {'authorId': '144259957', 'name': 'P. Zhao'}, {'authorId': '2219035983', 'name': 'Binwen Zhao'}, {'authorId': '2218788071', 'name': 'Bo Hu'}, {'authorId': '2108088736', 'name': 'Shu Zhang'}, {'authorId': '144291579', 'name': 'Zibin Zheng'}, {'authorId': '2152564746', 'name': 'Bing Wu'}]","['Tencent', 'Sun Yat-sen University']",['China'],2023-05
2305.18363,Shuyu Guo,"Shuyu Guo, Shuo Zhang, Weiwei Sun, Pengjie Ren, Zhumin Chen, Zhaochun
  Ren",Towards Explainable Conversational Recommender Systems,,,10.1145/3539618.3591884,,cs.IR cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Explanations in conventional recommender systems have demonstrated benefits in helping the user understand the rationality of the recommendations and improving the system's efficiency, transparency, and trustworthiness. In the conversational environment, multiple contextualized explanations need to be generated, which poses further challenges for explanations. To better measure explainability in conversational recommender systems (CRS), we propose ten evaluation perspectives based on concepts from conventional recommender systems together with the characteristics of CRS. We assess five existing CRS benchmark datasets using these metrics and observe the necessity of improving the explanation quality of CRS. To achieve this, we conduct manual and automatic approaches to extend these dialogues and construct a new CRS dataset, namely Explainable Recommendation Dialogues (E-ReDial). It includes 756 dialogues with over 2,000 high-quality rewritten explanations. We compare two baseline approaches to perform explanation generation based on E-ReDial. Experimental results suggest that models trained on E-ReDial can significantly improve explainability while introducing knowledge into the models can further improve the performance. GPT-3 in the in-context learning setting can generate more realistic and diverse movie descriptions. In contrast, T5 training on E-ReDial can better generate clear reasons for recommendations based on user preferences. E-ReDial is available at https://github.com/Superbooming/E-ReDial. ","[{'version': 'v1', 'created': 'Sat, 27 May 2023 07:36:08 GMT'}]",2023-05-31,"[['Guo', 'Shuyu', ''], ['Zhang', 'Shuo', ''], ['Sun', 'Weiwei', ''], ['Ren', 'Pengjie', ''], ['Chen', 'Zhumin', ''], ['Ren', 'Zhaochun', '']]",0,1,2023-05-27,1,6,2,2,1,1,72986cada4a8269041f1b153b08c51f8707afd91,258967706.0,https://www.semanticscholar.org/paper/72986cada4a8269041f1b153b08c51f8707afd91,Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,2023.0,47.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2119112577', 'name': 'Shuyu Guo'}, {'authorId': '2108032328', 'name': 'Shuo Zhang'}, {'authorId': '2153198380', 'name': 'Weiwei Sun'}, {'authorId': '1749477', 'name': 'Pengjie Ren'}, {'authorId': '1721165', 'name': 'Zhumin Chen'}, {'authorId': '2780667', 'name': 'Z. Ren'}]",['Shandong University'],['China'],2023-05
2305.18396,Xuanqi Liu,Xuanqi Liu and Zhuotao Liu,LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers,,,,,cs.LG cs.CL cs.CR,http://creativecommons.org/licenses/by-sa/4.0/,"  Prior works have attempted to build private inference frameworks for transformer-based large language models (LLMs) in a server-client setting, where the server holds the model parameters and the client inputs the private data for inference. However, these frameworks impose significant overhead when the private inputs are forward propagated through the original LLMs. In this paper, we show that substituting the computation- and communication-heavy operators in the transformer architecture with privacy-computing friendly approximations can greatly reduce the private inference costs with minor impact on model performance. Compared to the state-of-the-art Iron (NeurIPS 2022), our privacy-computing friendly model inference pipeline achieves a $5\times$ acceleration in computation and an 80\% reduction in communication overhead, while retaining nearly identical accuracy. ","[{'version': 'v1', 'created': 'Sun, 28 May 2023 13:08:13 GMT'}]",2023-05-31,"[['Liu', 'Xuanqi', ''], ['Liu', 'Zhuotao', '']]",0,0,2023-05-28,1,2,3,0,0,0,f8fd2b44f4597848a6ffadefd7116e62924fd12f,258967500.0,https://www.semanticscholar.org/paper/f8fd2b44f4597848a6ffadefd7116e62924fd12f,arXiv.org,2023.0,46.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '23979212', 'name': 'Xuanqing Liu'}, {'authorId': '2144264353', 'name': 'Zhuotao Liu'}]",['Tsinghua University'],['China'],2023-05
2305.18403,Mingyang Zhang,"Mingyang Zhang and Hao Chen and Chunhua Shen and Zhen Yang and Linlin
  Ou and Xinyi Yu and Bohan Zhuang",LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning,,,,,cs.LG cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large pre-trained models (LPMs), such as LLaMA and GLM, have shown exceptional performance across various tasks through fine-tuning. Although low-rank adaption (LoRA) has emerged to cheaply fine-tune these LPMs on downstream tasks, their deployment is still hindered by the vast model scale and computational costs. Neural network pruning offers a way to compress LPMs. However, the current pruning methods designed for LPMs are not compatible with LoRA. This is due to their utilization of unstructured pruning on LPMs, impeding the merging of LoRA weights, or their dependence on the gradients of pre-trained weights to guide pruning, which can impose significant memory overhead. To this end, we propose LoRAPrune, a new framework that delivers an accurate, compact model for efficient inference in a highly memory-effective manner. Specifically, we first design a LoRA-guided pruning criterion, which uses the weights and gradients of LoRA, rather than the gradients of pre-trained weights for importance estimation. We then propose a structured iterative pruning procedure, to remove redundant channels and heads. Extensive experimental results demonstrate the superior performance of our LoRAPrune over existing approaches on the LLaMA series models. For instance, at a 50\% compression rate, LoRAPrune outperforms LLM-Pruner by a perplexity reduction of 8.0 on WikiText2 and 16.05 on PTB datasets, while concurrently reducing memory usage by 52.6\%. The code will be released after review ","[{'version': 'v1', 'created': 'Sun, 28 May 2023 15:15:48 GMT'}, {'version': 'v2', 'created': 'Wed, 31 May 2023 22:32:19 GMT'}, {'version': 'v3', 'created': 'Tue, 3 Oct 2023 12:51:55 GMT'}]",2023-10-04,"[['Zhang', 'Mingyang', ''], ['Chen', 'Hao', ''], ['Shen', 'Chunhua', ''], ['Yang', 'Zhen', ''], ['Ou', 'Linlin', ''], ['Yu', 'Xinyi', ''], ['Zhuang', 'Bohan', '']]",0,0,2023-05-28,3,7,2,2,2,0,5fc366d7301f147883ee985cff008839abe19cc7,258967906.0,https://www.semanticscholar.org/paper/5fc366d7301f147883ee985cff008839abe19cc7,,2023.0,60.0,3.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1791201', 'name': 'Mingyang Zhang'}, {'authorId': '2029503517', 'name': 'Hao Chen'}, {'authorId': '12459603', 'name': 'Chunhua Shen'}, {'authorId': '1698784963', 'name': 'Zhenyi Yang'}, {'authorId': '2062580704', 'name': 'Linlin Ou'}, {'authorId': '3263719', 'name': 'Xinyi Yu'}, {'authorId': '3194022', 'name': 'Bohan Zhuang'}]",['Zhejiang University of Technology'],['China'],2023-05
2305.18474,Jiawei Huang,"Jiawei Huang, Yi Ren, Rongjie Huang, Dongchao Yang, Zhenhui Ye, Chen
  Zhang, Jinglin Liu, Xiang Yin, Zejun Ma, Zhou Zhao",Make-An-Audio 2: Temporal-Enhanced Text-to-Audio Generation,,,,,cs.SD cs.LG cs.MM eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large diffusion models have been successful in text-to-audio (T2A) synthesis tasks, but they often suffer from common issues such as semantic misalignment and poor temporal consistency due to limited natural language understanding and data scarcity. Additionally, 2D spatial structures widely used in T2A works lead to unsatisfactory audio quality when generating variable-length audio samples since they do not adequately prioritize temporal information. To address these challenges, we propose Make-an-Audio 2, a latent diffusion-based T2A method that builds on the success of Make-an-Audio. Our approach includes several techniques to improve semantic alignment and temporal consistency: Firstly, we use pre-trained large language models (LLMs) to parse the text into structured <event & order> pairs for better temporal information capture. We also introduce another structured-text encoder to aid in learning semantic alignment during the diffusion denoising process. To improve the performance of variable length generation and enhance the temporal information extraction, we design a feed-forward Transformer-based diffusion denoiser. Finally, we use LLMs to augment and transform a large amount of audio-label data into audio-text datasets to alleviate the problem of scarcity of temporal data. Extensive experiments show that our method outperforms baseline models in both objective and subjective metrics, and achieves significant gains in temporal information understanding, semantic consistency, and sound quality. ","[{'version': 'v1', 'created': 'Mon, 29 May 2023 10:41:28 GMT'}]",2023-05-31,"[['Huang', 'Jiawei', ''], ['Ren', 'Yi', ''], ['Huang', 'Rongjie', ''], ['Yang', 'Dongchao', ''], ['Ye', 'Zhenhui', ''], ['Zhang', 'Chen', ''], ['Liu', 'Jinglin', ''], ['Yin', 'Xiang', ''], ['Ma', 'Zejun', ''], ['Zhao', 'Zhou', '']]",0,0,2023-05-29,1,10,4,0,0,0,83d4b22d803ae856cf6b308482bd504fa151d39e,258968091.0,https://www.semanticscholar.org/paper/83d4b22d803ae856cf6b308482bd504fa151d39e,arXiv.org,2023.0,51.0,4.0,0.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '3068086', 'name': 'Jia-Bin Huang'}, {'authorId': '1500435161', 'name': 'Yi Ren'}, {'authorId': '2048021099', 'name': 'Rongjie Huang'}, {'authorId': '1752879605', 'name': 'Dongchao Yang'}, {'authorId': '1704406169', 'name': 'Zhenhui Ye'}, {'authorId': '2111573042', 'name': 'Chen Zhang'}, {'authorId': '48211720', 'name': 'Jinglin Liu'}, {'authorId': '145158503', 'name': 'Xiang Yin'}, {'authorId': '2919563', 'name': 'Zejun Ma'}, {'authorId': '2156163667', 'name': 'Zhou Zhao'}]","['Peking University', 'Zhejiang University']",['China'],2023-05
2305.18498,Di Huang,"Di Huang, Ziyuan Nan, Xing Hu, Pengwei Jin, Shaohui Peng, Yuanbo Wen,
  Rui Zhang, Zidong Du, Qi Guo, Yewen Pu, Yunji Chen",ANPL: Compiling Natural Programs with Interactive Decomposition,,,,,cs.PL cs.AI cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The advents of Large Language Models (LLMs) have shown promise in augmenting programming using natural interactions. However, while LLMs are proficient in compiling common usage patterns into a programming language, e.g., Python, it remains a challenge how to edit and debug an LLM-generated program. We introduce ANPL, a programming system that allows users to decompose user-specific tasks. In an ANPL program, a user can directly manipulate sketch, which specifies the data flow of the generated program. The user annotates the modules, or hole with natural language descriptions offloading the expensive task of generating functionalities to the LLM. Given an ANPL program, the ANPL compiler generates a cohesive Python program that implements the functionalities in hole, while respecting the dataflows specified in sketch. We deploy ANPL on the Abstraction and Reasoning Corpus (ARC), a set of unique tasks that are challenging for state-of-the-art AI systems, showing it outperforms baseline programming systems that (a) without the ability to decompose tasks interactively and (b) without the guarantee that the modules can be correctly composed together. We obtain a dataset consisting of 300/400 ARC tasks that were successfully decomposed and grounded in Python, providing valuable insights into how humans decompose programmatic tasks. See the dataset at https://iprc-dip.github.io/DARC. ","[{'version': 'v1', 'created': 'Mon, 29 May 2023 14:19:40 GMT'}]",2023-05-31,"[['Huang', 'Di', ''], ['Nan', 'Ziyuan', ''], ['Hu', 'Xing', ''], ['Jin', 'Pengwei', ''], ['Peng', 'Shaohui', ''], ['Wen', 'Yuanbo', ''], ['Zhang', 'Rui', ''], ['Du', 'Zidong', ''], ['Guo', 'Qi', ''], ['Pu', 'Yewen', ''], ['Chen', 'Yunji', '']]",0,0,2023-05-29,1,11,4,0,0,0,1a28e9c62eeb76a1a77dc152197027c15310927b,258967389.0,https://www.semanticscholar.org/paper/1a28e9c62eeb76a1a77dc152197027c15310927b,arXiv.org,2023.0,67.0,3.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2110180755', 'name': 'Di Huang'}, {'authorId': '2150708879', 'name': 'Ziyuan Nan'}, {'authorId': '2109635961', 'name': 'Xingui Hu'}, {'authorId': '2143807697', 'name': 'Pengwei Jin'}, {'authorId': '2072713784', 'name': 'Shaohui Peng'}, {'authorId': '1742399798', 'name': 'Yuanbo Wen'}, {'authorId': '2118404461', 'name': 'Rui Zhang'}, {'authorId': '1678776', 'name': 'Zidong Du'}, {'authorId': '145461472', 'name': 'Qi Guo'}, {'authorId': '2155555', 'name': 'Yewen Pu'}, {'authorId': '7377735', 'name': 'Yunji Chen'}]","['Autodesk Research', 'University of Chinese Academy of Sciences', 'Chinese Academy of Sciences', 'Institute of Computing Technology']",['China'],2023-05
2305.18507,Yi Hu,"Yi Hu, Haotong Yang, Zhouchen Lin, Muhan Zhang",Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have scaled up to unlock a wide range of complex reasoning tasks with the aid of various prompting methods. However, current prompting methods generate natural language intermediate steps to help reasoning, which can cause imperfect task reduction and confusion. To mitigate such limitations, we explore code prompting, a neural symbolic prompting method with both zero-shot and few-shot versions which triggers code as intermediate steps. We conduct experiments on 7 widely-used benchmarks involving symbolic reasoning and arithmetic reasoning. Code prompting generally outperforms chain-of-thought (CoT) prompting. To further understand the performance and limitations of code prompting, we perform extensive ablation studies and error analyses, and identify several exclusive advantages of using symbolic promptings compared to natural language. We also consider the ensemble of code prompting and CoT prompting to combine the strengths of both. Finally, we show through experiments how code annotations and their locations affect code prompting. ","[{'version': 'v1', 'created': 'Mon, 29 May 2023 15:14:09 GMT'}]",2023-05-31,"[['Hu', 'Yi', ''], ['Yang', 'Haotong', ''], ['Lin', 'Zhouchen', ''], ['Zhang', 'Muhan', '']]",0,0,2023-05-29,1,4,2,0,0,0,0875651b68e6602d45ae08bee67cf63c02faa512,258967319.0,https://www.semanticscholar.org/paper/0875651b68e6602d45ae08bee67cf63c02faa512,arXiv.org,2023.0,42.0,6.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '46972251', 'name': 'Y. Hu'}, {'authorId': '2185454298', 'name': 'Haotong Yang'}, {'authorId': '33383055', 'name': 'Zhouchen Lin'}, {'authorId': '1390814008', 'name': 'Muhan Zhang'}]",['Peking University'],['China'],2023-05
2305.18703,Chen Ling,"Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng,
  Junxiang Wang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, Tianjiao
  Zhao, Amit Panalkar, Wei Cheng, Haoyu Wang, Yanchi Liu, Zhengzhang Chen,
  Haifeng Chen, Chris White, Quanquan Gu, Jian Pei, and Liang Zhao",Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). Domain specification techniques are key to make large language models disruptive in many applications. Specifically, to solve these hurdles, there has been a notable increase in research and practices conducted in recent years on the domain specialization of LLMs. This emerging field of study, with its substantial potential for impact, necessitates a comprehensive and systematic review to better summarize and guide ongoing work in this area. In this article, we present a comprehensive survey on domain specification techniques for large language models, an emerging direction critical for large language model applications. First, we propose a systematic taxonomy that categorizes the LLM domain-specialization techniques based on the accessibility to LLMs and summarizes the framework for all the subcategories as well as their relations and differences to each other. Second, we present an extensive taxonomy of critical application domains that can benefit dramatically from specialized LLMs, discussing their practical significance and open challenges. Last, we offer our insights into the current research status and future trends in this area. ","[{'version': 'v1', 'created': 'Tue, 30 May 2023 03:00:30 GMT'}, {'version': 'v2', 'created': 'Wed, 31 May 2023 00:43:01 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Jul 2023 15:06:21 GMT'}, {'version': 'v4', 'created': 'Tue, 11 Jul 2023 18:34:08 GMT'}, {'version': 'v5', 'created': 'Sat, 26 Aug 2023 02:42:49 GMT'}]",2023-08-29,"[['Ling', 'Chen', ''], ['Zhao', 'Xujiang', ''], ['Lu', 'Jiaying', ''], ['Deng', 'Chengyuan', ''], ['Zheng', 'Can', ''], ['Wang', 'Junxiang', ''], ['Chowdhury', 'Tanmoy', ''], ['Li', 'Yun', ''], ['Cui', 'Hejie', ''], ['Zhang', 'Xuchao', ''], ['Zhao', 'Tianjiao', ''], ['Panalkar', 'Amit', ''], ['Cheng', 'Wei', ''], ['Wang', 'Haoyu', ''], ['Liu', 'Yanchi', ''], ['Chen', 'Zhengzhang', ''], ['Chen', 'Haifeng', ''], ['White', 'Chris', ''], ['Gu', 'Quanquan', ''], ['Pei', 'Jian', ''], ['Zhao', 'Liang', '']]",0,0,2023-05-30,5,21,2,0,0,0,6847b9658f287f430098199cd81bf26308da13f9,259502302.0,https://www.semanticscholar.org/paper/6847b9658f287f430098199cd81bf26308da13f9,,2023.0,331.0,13.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2059988575', 'name': 'Chen Ling'}, {'authorId': '50879401', 'name': 'Xujiang Zhao'}, {'authorId': '2117727751', 'name': 'Jiaying Lu'}, {'authorId': '151483422', 'name': 'Chengyuan Deng'}, {'authorId': '2182238045', 'name': 'Can Zheng'}, {'authorId': '2120473483', 'name': 'Junxiang Wang'}, {'authorId': '2123930262', 'name': 'Tanmoy Chowdhury'}, {'authorId': '2110425042', 'name': 'Yun-Qing Li'}, {'authorId': '2112821580', 'name': 'Hejie Cui'}, {'authorId': '2048981220', 'name': 'Xuchao Zhang'}, {'authorId': '2211987764', 'name': 'Tian-yu Zhao'}, {'authorId': '2218486790', 'name': 'Amit Panalkar'}, {'authorId': '145859270', 'name': 'Wei Cheng'}, {'authorId': '34269118', 'name': 'Haoyu Wang'}, {'authorId': '3215702', 'name': 'Yanchi Liu'}, {'authorId': '1766853', 'name': 'Zhengzhang Chen'}, {'authorId': '2204622281', 'name': 'Haifeng Chen'}, {'authorId': '2218495127', 'name': 'Chris White'}, {'authorId': '144966687', 'name': 'Quanquan Gu'}, {'authorId': '2188744953', 'name': 'Jian Pei'}, {'authorId': '1390553618', 'name': 'Carl Yang'}, {'authorId': '2151579859', 'name': 'Liang Zhao'}]","['Rutgers University', 'Jiaying University', 'Emory University', 'NEC Labs America, USA', 'Liang Zhao,', 'NEC Labs America, Princeton, NJ, USA', 'CHENGYUAN DENG * , NEC Labs America, USA', 'University of South Alabama', 'George Mason University', 'HAIFENG CHEN, CHRIS WHITE, NEC Labs America, USA', 'NEC Labs America, Princeton, NJ, USA; Haifeng Chen, Chris White,', 'NEC Labs America, Princeton, NJ, USA;', 'Duke University', 'Microsoft', 'Xujiang Zhao,', 'and University of Pittsburgh, USA', 'TIANJIAO ZHAO, AMIT PANALKAR, Blackrock, Inc., USA', 'University of California, Los Angeles', 'CAN ZHENG * , NEC Labs America, USA', 'BlackRock', 'and Rutgers University, New Brunswick, NJ, USA;', 'NEC Labs America, Princeton, NJ, USA and University of Pittsburgh, Pittsburgh, PA, USA; Junxiang Wang,']","['China', 'United States']",2023-05
2305.18752,Lin Song,"Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, Ying
  Shan",GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction,,,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper aims to efficiently enable Large Language Models (LLMs) to use multimodal tools. Advanced proprietary LLMs, such as ChatGPT and GPT-4, have shown great potential for tool usage through sophisticated prompt engineering. Nevertheless, these models typically rely on prohibitive computational costs and publicly inaccessible data. To address these challenges, we propose the GPT4Tools based on self-instruct to enable open-source LLMs, such as LLaMA and OPT, to use tools. It generates an instruction-following dataset by prompting an advanced teacher with various multi-modal contexts. By using the Low-Rank Adaptation (LoRA) optimization, our approach facilitates the open-source LLMs to solve a range of visual problems, including visual comprehension and image generation. Moreover, we provide a benchmark to evaluate the ability of LLMs to use tools, which is performed in both zero-shot and fine-tuning ways. Extensive experiments demonstrate the effectiveness of our method on various language models, which not only significantly improves the accuracy of invoking seen tools, but also enables the zero-shot capacity for unseen tools. The code and demo are available at https://github.com/StevenGrove/GPT4Tools. ","[{'version': 'v1', 'created': 'Tue, 30 May 2023 05:27:21 GMT'}]",2023-05-31,"[['Yang', 'Rui', ''], ['Song', 'Lin', ''], ['Li', 'Yanwei', ''], ['Zhao', 'Sijie', ''], ['Ge', 'Yixiao', ''], ['Li', 'Xiu', ''], ['Shan', 'Ying', '']]",1,1,2023-05-30,1,7,2,4,2,2,b458fc5261595f44b36325e5eaea1f874d65138f,258967184.0,https://www.semanticscholar.org/paper/b458fc5261595f44b36325e5eaea1f874d65138f,arXiv.org,2023.0,63.0,30.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': None, 'name': 'Rui Yang'}, {'authorId': '2150597337', 'name': 'Lin Song'}, {'authorId': '2154551803', 'name': 'Yanwei Li'}, {'authorId': '2124493792', 'name': 'Sijie Zhao'}, {'authorId': '152988335', 'name': 'Yixiao Ge'}, {'authorId': '2127382771', 'name': 'Xiu Li'}, {'authorId': '1387190008', 'name': 'Ying Shan'}]","['Tencent', 'Tsinghua University', 'Chinese University of Hong Kong']",['China'],2023-05
2305.18898,Bei Liu,"Chuhao Jin, Wenhui Tan, Jiange Yang, Bei Liu, Ruihua Song, Limin Wang,
  Jianlong Fu",AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation,,,,,cs.RO cs.AI,http://creativecommons.org/licenses/by/4.0/,"  We propose a novel framework for learning high-level cognitive capabilities in robot manipulation tasks, such as making a smiley face using building blocks. These tasks often involve complex multi-step reasoning, presenting significant challenges due to the limited paired data connecting human instructions (e.g., making a smiley face) and robot actions (e.g., end-effector movement). Existing approaches relieve this challenge by adopting an open-loop paradigm decomposing high-level instructions into simple sub-task plans, and executing them step-by-step using low-level control models. However, these approaches are short of instant observations in multi-step reasoning, leading to sub-optimal results. To address this issue, we propose to automatically collect a cognitive robot dataset by Large Language Models (LLMs). The resulting dataset AlphaBlock consists of 35 comprehensive high-level tasks of multi-step text plans and paired observation sequences. To enable efficient data acquisition, we employ elaborated multi-round prompt designs that effectively reduce the burden of extensive human involvement. We further propose a closed-loop multi-modal embodied planning model that autoregressively generates plans by taking image observations as input. To facilitate effective learning, we leverage MiniGPT-4 with a frozen visual encoder and LLM, and finetune additional vision adapter and Q-former to enable fine-grained spatial perception for manipulation tasks. We conduct experiments to verify the superiority over existing open and closed-loop methods, and achieve a significant increase in success rate by 21.4% and 14.5% over ChatGPT and GPT-4 based robot tasks. Real-world demos are shown in https://www.youtube.com/watch?v=ayAzID1_qQk . ","[{'version': 'v1', 'created': 'Tue, 30 May 2023 09:54:20 GMT'}]",2023-05-31,"[['Jin', 'Chuhao', ''], ['Tan', 'Wenhui', ''], ['Yang', 'Jiange', ''], ['Liu', 'Bei', ''], ['Song', 'Ruihua', ''], ['Wang', 'Limin', ''], ['Fu', 'Jianlong', '']]",1,1,2023-05-30,1,7,2,2,0,2,80be1426825288ff876acb8cc0babcc6629fa644,258967880.0,https://www.semanticscholar.org/paper/80be1426825288ff876acb8cc0babcc6629fa644,arXiv.org,2023.0,44.0,6.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2168787686', 'name': 'Chuhao Jin'}, {'authorId': '2218557953', 'name': 'Wenhui Tan'}, {'authorId': '4010904', 'name': 'Jiange Yang'}, {'authorId': '2127734772', 'name': 'Bei Liu'}, {'authorId': '35119829', 'name': 'Ruihua Song'}, {'authorId': '2161327433', 'name': 'Limin Wang'}, {'authorId': '3247966', 'name': 'Jianlong Fu'}]","['Nanjing University', 'Microsoft', 'Renmin University of China']","['China', 'India']",2023-05
2305.19118,Tian Liang,"Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang,
  Yujiu Yang, Zhaopeng Tu, Shuming Shi",Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate,Work in progress,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of ""tit for tat"" and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of ""tit for tat"" state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents. Codes: https://github.com/Skytliang/Multi-Agents-Debate ","[{'version': 'v1', 'created': 'Tue, 30 May 2023 15:25:45 GMT'}]",2023-05-31,"[['Liang', 'Tian', ''], ['He', 'Zhiwei', ''], ['Jiao', 'Wenxiang', ''], ['Wang', 'Xing', ''], ['Wang', 'Yan', ''], ['Wang', 'Rui', ''], ['Yang', 'Yujiu', ''], ['Tu', 'Zhaopeng', ''], ['Shi', 'Shuming', '']]",1,1,2023-05-30,1,9,1,1,0,1,385c74957858e7d6856d48e72b5a902b4c1aa28c,258967540.0,https://www.semanticscholar.org/paper/385c74957858e7d6856d48e72b5a902b4c1aa28c,arXiv.org,2023.0,32.0,42.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '31395252', 'name': 'Tian Liang'}, {'authorId': '2610876', 'name': 'Zhiwei He'}, {'authorId': '12386833', 'name': 'Wenxiang Jiao'}, {'authorId': '48631170', 'name': 'Xing Wang'}, {'authorId': '2152547279', 'name': 'Yan Wang'}, {'authorId': '2151039787', 'name': 'Rui Wang'}, {'authorId': '2108585311', 'name': 'Yujiu Yang'}, {'authorId': '2909321', 'name': 'Zhaopeng Tu'}, {'authorId': '2072684668', 'name': 'Shuming Shi'}]","['Shanghai Jiao Tong University', 'Tsinghua University', 'Tencent']",['China'],2023-05
2305.19213,Xiao Liu,"Xiao Liu, Da Yin, Chen Zhang, Yansong Feng, Dongyan Zhao",The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code,"Findings of ACL 2023. Code and data are available at
  https://github.com/xxxiaol/magic-if",,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  Causal reasoning, the ability to identify cause-and-effect relationship, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Given the fact that programming code may express causal relations more often and explicitly with conditional statements like ``if``, we want to explore whether Code-LLMs acquire better causal reasoning abilities. Our experiments show that compared to text-only LLMs, Code-LLMs with code prompts are significantly better in causal reasoning. We further intervene on the prompts from different aspects, and discover that the programming structure is crucial in code prompt design, while Code-LLMs are robust towards format perturbations. ","[{'version': 'v1', 'created': 'Tue, 30 May 2023 17:02:58 GMT'}]",2023-05-31,"[['Liu', 'Xiao', ''], ['Yin', 'Da', ''], ['Zhang', 'Chen', ''], ['Feng', 'Yansong', ''], ['Zhao', 'Dongyan', '']]",0,0,2023-05-30,1,5,1,0,0,0,498d1406fc4cddb05cd46477793f2e726a6fe238,258968140.0,https://www.semanticscholar.org/paper/498d1406fc4cddb05cd46477793f2e726a6fe238,Annual Meeting of the Association for Computational Linguistics,2023.0,52.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '49543720', 'name': 'Xiao Liu'}, {'authorId': '144508458', 'name': 'Da Yin'}, {'authorId': '2111574159', 'name': 'Chen Zhang'}, {'authorId': '2115387922', 'name': 'Yansong Feng'}, {'authorId': '144060462', 'name': 'Dongyan Zhao'}]","['Peking University', 'University of California, Los Angeles', 'State Key Laboratory of Media Convergence Production Technology and Systems', 'Beijing Academy of Artificial Intelligence']","['China', 'United States']",2023-05
2305.19308,Hongxin Li,"Hongxin Li, Jingran Su, Yuntao Chen, Qing Li, Zhaoxiang Zhang",SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models,,,,,cs.SE cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Computer end users have spent billions of hours completing daily tasks like tabular data processing and project timeline scheduling. Most of these tasks are repetitive and error-prone, yet most end users lack the skill of automating away these burdensome works. With the advent of large language models (LLMs), directing software with natural language user requests become a reachable goal. In this work, we propose a SheetCopilot agent which takes natural language task and control spreadsheet to fulfill the requirements. We propose a set of atomic actions as an abstraction of spreadsheet software functionalities. We further design a state machine-based task planning framework for LLMs to robustly interact with spreadsheets. We curate a representative dataset containing 221 spreadsheet control tasks and establish a fully automated evaluation pipeline for rigorously benchmarking the ability of LLMs in software control tasks. Our SheetCopilot correctly completes 44.3\% of tasks for a single generation, outperforming the strong code generation baseline by a wide margin. Our project page:https://sheetcopilot-demo.github.io/. ","[{'version': 'v1', 'created': 'Tue, 30 May 2023 17:59:30 GMT'}]",2023-06-01,"[['Li', 'Hongxin', ''], ['Su', 'Jingran', ''], ['Chen', 'Yuntao', ''], ['Li', 'Qing', ''], ['Zhang', 'Zhaoxiang', '']]",0,0,2023-05-30,1,5,3,0,0,0,e45036dddb5f27d3e87d2f14a2d9e6a402e7b5b7,258987743.0,https://www.semanticscholar.org/paper/e45036dddb5f27d3e87d2f14a2d9e6a402e7b5b7,arXiv.org,2023.0,92.0,3.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2117889035', 'name': 'Hongxin Li'}, {'authorId': '2116965668', 'name': 'Jingran Su'}, {'authorId': '2798406', 'name': 'Yuntao Chen'}, {'authorId': '2218689404', 'name': 'Qing Li'}, {'authorId': '2206162255', 'name': 'Zhaoxiang Zhang'}]","['University of Chinese Academy of Sciences', 'Centre for Artificial Intelligence and Robotics', 'Chinese Academy of Sciences', 'Shanghai Artificial Intelligence Laboratory', 'Hong Kong Polytechnic University']","['China', 'India', 'Hong Kong']",2023-05
2305.19466,Amirhossein Kazemnejad,"Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy,
  Payel Das, Siva Reddy",The Impact of Positional Encoding on Length Generalization in Transformers,,,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Length generalization, the ability to generalize from small training context sizes to larger ones, is a critical challenge in the development of Transformer-based language models. Positional encoding (PE) has been identified as a major factor influencing length generalization, but the exact impact of different PE schemes on extrapolation in downstream tasks remains unclear. In this paper, we conduct a systematic empirical study comparing the length generalization performance of decoder-only Transformers with five different position encoding approaches including Absolute Position Embedding (APE), T5's Relative PE, ALiBi, and Rotary, in addition to Transformers without positional encoding (NoPE). Our evaluation encompasses a battery of reasoning and mathematical tasks. Our findings reveal that the most commonly used positional encoding methods, such as ALiBi, Rotary, and APE, are not well suited for length generalization in downstream tasks. More importantly, NoPE outperforms other explicit positional encoding methods while requiring no additional computation. We theoretically demonstrate that NoPE can represent both absolute and relative PEs, but when trained with SGD, it mostly resembles T5's relative PE attention patterns. Finally, we find that scratchpad is not always helpful to solve length generalization and its format highly impacts the model's performance. Overall, our work suggests that explicit position embeddings are not essential for decoder-only Transformers to generalize well to longer sequences. ","[{'version': 'v1', 'created': 'Wed, 31 May 2023 00:29:55 GMT'}]",2023-06-01,"[['Kazemnejad', 'Amirhossein', ''], ['Padhi', 'Inkit', ''], ['Ramamurthy', 'Karthikeyan Natesan', ''], ['Das', 'Payel', ''], ['Reddy', 'Siva', '']]",0,0,2023-05-31,1,5,3,1,1,0,6f6e2e0311589a9af045f6acd00b7dee6d19fce4,258987259.0,https://www.semanticscholar.org/paper/6f6e2e0311589a9af045f6acd00b7dee6d19fce4,arXiv.org,2023.0,54.0,20.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1754452702', 'name': 'Amirhossein Kazemnejad'}, {'authorId': '8350409', 'name': 'Inkit Padhi'}, {'authorId': '1704263', 'name': 'K. Ramamurthy'}, {'authorId': '1730372', 'name': 'Payel Das'}, {'authorId': '145732771', 'name': 'Siva Reddy'}]","['ServiceNow Research', 'IBM Research - China', 'McGill University', 'Meta']","['Canada', 'United States', 'China']",2023-05
2305.19545,Deng Bowen,Bowen Deng,Catalysis distillation neural network for the few shot open catalyst challenge,"24 pages, 4 figures",,,,physics.chem-ph cs.CE cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The integration of artificial intelligence and science has resulted in substantial progress in computational chemistry methods for the design and discovery of novel catalysts. Nonetheless, the challenges of electrocatalytic reactions and developing a large-scale language model in catalysis persist, and the recent success of ChatGPT's (Chat Generative Pre-trained Transformer) few-shot methods surpassing BERT (Bidirectional Encoder Representation from Transformers) underscores the importance of addressing limited data, expensive computations, time constraints and structure-activity relationship in research. Hence, the development of few-shot techniques for catalysis is critical and essential, regardless of present and future requirements. This paper introduces the Few-Shot Open Catalyst Challenge 2023, a competition aimed at advancing the application of machine learning technology for predicting catalytic reactions on catalytic surfaces, with a specific focus on dual-atom catalysts in hydrogen peroxide electrocatalysis. To address the challenge of limited data in catalysis, we propose a machine learning approach based on MLP-Like and a framework called Catalysis Distillation Graph Neural Network (CDGNN). Our results demonstrate that CDGNN effectively learns embeddings from catalytic structures, enabling the capture of structure-adsorption relationships. This accomplishment has resulted in the utmost advanced and efficient determination of the reaction pathway for hydrogen peroxide, surpassing the current graph neural network approach by 16.1%.. Consequently, CDGNN presents a promising approach for few-shot learning in catalysis. ","[{'version': 'v1', 'created': 'Wed, 31 May 2023 04:23:56 GMT'}]",2023-06-01,"[['Deng', 'Bowen', '']]",1,1,2023-05-31,1,1,3,1,0,1,c7515920c5fd7427962bb2aa9fbbcdce7b4899cf,258987292.0,https://www.semanticscholar.org/paper/c7515920c5fd7427962bb2aa9fbbcdce7b4899cf,arXiv.org,2023.0,21.0,0.0,0.0,True,"['Computer Science', 'Physics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Physics', 'source': 'external'}, {'category': 'Chemistry', 'source': 's2-fos-model'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2151004887', 'name': 'B. Deng'}]",['Guangxi University'],['China'],2023-05
2305.19599,Zutao Jiang,"Guian Fang, Zutao Jiang, Jianhua Han, Guansong Lu, Hang Xu, Xiaodan
  Liang",Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic Rewards,,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advances in text-to-image diffusion models have achieved remarkable success in generating high-quality, realistic images from given text prompts. However, previous methods fail to perform accurate modality alignment between text concepts and generated images due to the lack of fine-level semantic guidance that successfully diagnoses the modality discrepancy. In this paper, we propose FineRewards to improve the alignment between text and images in text-to-image diffusion models by introducing two new fine-grained semantic rewards: the caption reward and the Semantic Segment Anything (SAM) reward. From the global semantic view, the caption reward generates a corresponding detailed caption that depicts all important contents in the synthetic image via a BLIP-2 model and then calculates the reward score by measuring the similarity between the generated caption and the given prompt. From the local semantic view, the SAM reward segments the generated images into local parts with category labels, and scores the segmented parts by measuring the likelihood of each category appearing in the prompted scene via a large language model, i.e., Vicuna-7B. Additionally, we adopt an assemble reward-ranked learning strategy to enable the integration of multiple reward functions to jointly guide the model training. Adapting results of text-to-image models on the MS-COCO benchmark show that the proposed semantic reward outperforms other baseline reward functions with a considerable margin on both visual quality and semantic similarity with the input prompt. Moreover, by adopting the assemble reward-ranked learning strategy, we further demonstrate that model performance is further improved when adapting under the unifying of the proposed semantic reward with the current image rewards. ","[{'version': 'v1', 'created': 'Wed, 31 May 2023 06:59:21 GMT'}, {'version': 'v2', 'created': 'Thu, 1 Jun 2023 10:07:46 GMT'}]",2023-06-02,"[['Fang', 'Guian', ''], ['Jiang', 'Zutao', ''], ['Han', 'Jianhua', ''], ['Lu', 'Guansong', ''], ['Xu', 'Hang', ''], ['Liang', 'Xiaodan', '']]",0,0,2023-05-31,2,6,2,1,1,0,d92a9b40ff621b2cc46a1c0266919b643f0f2d28,258987978.0,https://www.semanticscholar.org/paper/d92a9b40ff621b2cc46a1c0266919b643f0f2d28,arXiv.org,2023.0,45.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2218534972', 'name': 'Guian Fang'}, {'authorId': '10705544', 'name': 'Zutao Jiang'}, {'authorId': '47180442', 'name': 'Jianhua Han'}, {'authorId': '2193386588', 'name': 'Guangsong Lu'}, {'authorId': '2143534132', 'name': 'Hang Xu'}, {'authorId': '2153397698', 'name': 'Xiaodan Liang'}]","['Huawei Technologies (China)', 'Inception Institute of Artificial Intelligence', 'Sun Yat-sen University', 'Mohamed bin Zayed University of Artificial Intelligence']","['China', 'United Arab Emirates']",2023-05
2305.19835,Li Bei,"Bei Li, Rui Wang, Junliang Guo, Kaitao Song, Xu Tan, Hany Hassan, Arul
  Menezes, Tong Xiao, Jiang Bian and JingBo Zhu",Deliberate then Generate: Enhanced Prompting Framework for Text Generation,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have shown remarkable success across a wide range of natural language generation tasks, where proper prompt designs make great impacts. While existing prompting methods are normally restricted to providing correct information, in this paper, we encourage the model to deliberate by proposing a novel Deliberate then Generate (DTG) prompting framework, which consists of error detection instructions and candidates that may contain errors. DTG is a simple yet effective technique that can be applied to various text generation tasks with minimal modifications. We conduct extensive experiments on 20+ datasets across 7 text generation tasks, including summarization, translation, dialogue, and more. We show that DTG consistently outperforms existing prompting methods and achieves state-of-the-art performance on multiple text generation tasks. We also provide in-depth analyses to reveal the underlying mechanisms of DTG, which may inspire future research on prompting for LLMs. ","[{'version': 'v1', 'created': 'Wed, 31 May 2023 13:23:04 GMT'}]",2023-06-01,"[['Li', 'Bei', ''], ['Wang', 'Rui', ''], ['Guo', 'Junliang', ''], ['Song', 'Kaitao', ''], ['Tan', 'Xu', ''], ['Hassan', 'Hany', ''], ['Menezes', 'Arul', ''], ['Xiao', 'Tong', ''], ['Bian', 'Jiang', ''], ['Zhu', 'JingBo', '']]",0,0,2023-05-31,1,10,2,0,0,0,c85c90ef9e9a71efe031c3f7d6e34561f91168fe,258987341.0,https://www.semanticscholar.org/paper/c85c90ef9e9a71efe031c3f7d6e34561f91168fe,arXiv.org,2023.0,48.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '49730090', 'name': 'Bei Li'}, {'authorId': '2151038979', 'name': 'Rui Wang'}, {'authorId': '13838086', 'name': 'Junliang Guo'}, {'authorId': '50982078', 'name': 'Kaitao Song'}, {'authorId': '2112782687', 'name': 'Xuejiao Tan'}, {'authorId': '143925074', 'name': 'Hany Hassan'}, {'authorId': '145280401', 'name': 'Arul Menezes'}, {'authorId': '1391183811', 'name': 'Tong Xiao'}, {'authorId': '2192822005', 'name': 'Jiang Bian'}, {'authorId': '1728004', 'name': 'Jingbo Zhu'}]","['Northeastern University', 'NiuTrans Research', 'Microsoft']","['China', 'United States']",2023-05
2305.19926,Jen-Tse Huang,"Jen-tse Huang, Wenxuan Wang, Man Ho Lam, Eric John Li, Wenxiang Jiao,
  Michael R. Lyu","ChatGPT an ENFJ, Bard an ISTJ: Empirical Study on Personalities of Large Language Models","Added robustness analysis against fine-tuning (results of
  text-davinci-003); Added results of ChatGLM; Added limitations",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have made remarkable advancements in the field of artificial intelligence, significantly reshaping the human-computer interaction. We not only focus on the performance of LLMs, but also explore their features from a psychological perspective, acknowledging the importance of understanding their behavioral characteristics. Our study examines the behavioral patterns displayed by LLMs by employing trait theory, a psychological framework. We first focus on evaluating the consistency of personality types exhibited by ChatGPT. Furthermore, experiments include cross-lingual effects on seven additional languages, and the investigation of six other LLMs. Moreover, the study investigates whether ChatGPT can exhibit personality changes in response to instructions or contextual cues. The findings show that ChatGPT consistently maintains its ENFJ personality regardless of instructions or contexts. By shedding light on the personalization of LLMs, we anticipate that our study will serve as a catalyst for further research in this field. ","[{'version': 'v1', 'created': 'Wed, 31 May 2023 15:03:28 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Jun 2023 13:45:30 GMT'}]",2023-06-08,"[['Huang', 'Jen-tse', ''], ['Wang', 'Wenxuan', ''], ['Lam', 'Man Ho', ''], ['Li', 'Eric John', ''], ['Jiao', 'Wenxiang', ''], ['Lyu', 'Michael R.', '']]",1,1,2023-05-31,2,6,1,1,0,1,999380f3e1ad1ec97fc3193413610add4d12d905,258987445.0,https://www.semanticscholar.org/paper/999380f3e1ad1ec97fc3193413610add4d12d905,arXiv.org,2023.0,38.0,9.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Psychology', 'source': 's2-fos-model'}]","[{'authorId': '2161306685', 'name': 'Jen-tse Huang'}, {'authorId': '2144328160', 'name': 'Wenxuan Wang'}, {'authorId': '2218791687', 'name': 'Man Ho Lam'}, {'authorId': '2210505780', 'name': 'Eric Li'}, {'authorId': '12386833', 'name': 'Wenxiang Jiao'}, {'authorId': '2146840128', 'name': 'Michael R. Lyu'}]","['Tencent', 'Chinese University of Hong Kong']",['China'],2023-05
2306.00014,Zhuocheng Gong,"Zhuocheng Gong, Jiahao Liu, Qifan Wang, Yang Yang, Jingang Wang, Wei
  Wu, Yunsen Xian, Dongyan Zhao, Rui Yan",PreQuant: A Task-agnostic Quantization Approach for Pre-trained Language Models,Findings of ACL2023,,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  While transformer-based pre-trained language models (PLMs) have dominated a number of NLP applications, these models are heavy to deploy and expensive to use. Therefore, effectively compressing large-scale PLMs becomes an increasingly important problem. Quantization, which represents high-precision tensors with low-bit fix-point format, is a viable solution. However, most existing quantization methods are task-specific, requiring customized training and quantization with a large number of trainable parameters on each individual task. Inspired by the observation that the over-parameterization nature of PLMs makes it possible to freeze most of the parameters during the fine-tuning stage, in this work, we propose a novel ``quantize before fine-tuning'' framework, PreQuant, that differs from both quantization-aware training and post-training quantization. PreQuant is compatible with various quantization strategies, with outlier-aware parameter-efficient fine-tuning incorporated to correct the induced quantization error. We demonstrate the effectiveness of PreQuant on the GLUE benchmark using BERT, RoBERTa, and T5. We also provide an empirical investigation into the workflow of PreQuant, which sheds light on its efficacy. ","[{'version': 'v1', 'created': 'Tue, 30 May 2023 08:41:33 GMT'}]",2023-06-02,"[['Gong', 'Zhuocheng', ''], ['Liu', 'Jiahao', ''], ['Wang', 'Qifan', ''], ['Yang', 'Yang', ''], ['Wang', 'Jingang', ''], ['Wu', 'Wei', ''], ['Xian', 'Yunsen', ''], ['Zhao', 'Dongyan', ''], ['Yan', 'Rui', '']]",0,0,2023-05-30,1,9,2,1,1,0,0967134fb5c778d73003dabc9ccaa5841f09372b,258999233.0,https://www.semanticscholar.org/paper/0967134fb5c778d73003dabc9ccaa5841f09372b,Annual Meeting of the Association for Computational Linguistics,2023.0,43.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2165228008', 'name': 'Zhuocheng Gong'}, {'authorId': '2108421184', 'name': 'Jiahao Liu'}, {'authorId': '2145778781', 'name': 'Qifan Wang'}, {'authorId': '2152915671', 'name': 'Yang Yang'}, {'authorId': '2109593338', 'name': 'Jingang Wang'}, {'authorId': '2118256028', 'name': 'Wei Wu'}, {'authorId': '2069503881', 'name': 'Yunsen Xian'}, {'authorId': '144060462', 'name': 'Dongyan Zhao'}, {'authorId': '144539156', 'name': 'Rui Yan'}]","['Ministry of Education', 'Beijing Academy of Artificial Intelligence', 'Renmin University of China', 'National Key Laboratory of General Artificial Intelligence', 'Peking University', 'Meituan', 'Meta']","['China', 'United States', 'Thailand']",2023-05
2306.00024,Chandan Singh,"Zelalem Gero, Chandan Singh, Hao Cheng, Tristan Naumann, Michel
  Galley, Jianfeng Gao, Hoifung Poon",Self-Verification Improves Few-Shot Clinical Information Extraction,,IMLH 2023,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Extracting patient information from unstructured text is a critical task in health decision-support and clinical research. Large language models (LLMs) have shown the potential to accelerate clinical curation via few-shot in-context learning, in contrast to supervised learning which requires much more costly human annotations. However, despite drastic advances in modern LLMs such as GPT-4, they still struggle with issues regarding accuracy and interpretability, especially in mission-critical domains such as health. Here, we explore a general mitigation framework using self-verification, which leverages the LLM to provide provenance for its own extraction and check its own outputs. This is made possible by the asymmetry between verification and generation, where the latter is often much easier than the former. Experimental results show that our method consistently improves accuracy for various LLMs in standard clinical information extraction tasks. Additionally, self-verification yields interpretations in the form of a short text span corresponding to each output, which makes it very efficient for human experts to audit the results, paving the way towards trustworthy extraction of clinical information in resource-constrained scenarios. To facilitate future research in this direction, we release our code and prompts. ","[{'version': 'v1', 'created': 'Tue, 30 May 2023 22:05:11 GMT'}]",2023-06-21,"[['Gero', 'Zelalem', ''], ['Singh', 'Chandan', ''], ['Cheng', 'Hao', ''], ['Naumann', 'Tristan', ''], ['Galley', 'Michel', ''], ['Gao', 'Jianfeng', ''], ['Poon', 'Hoifung', '']]",0,1,2023-05-30,1,7,2,1,0,1,34e1a8a75bf6f35084ac6d714a136f39d02c649e,258999642.0,https://www.semanticscholar.org/paper/34e1a8a75bf6f35084ac6d714a136f39d02c649e,arXiv.org,2023.0,43.0,6.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1395101702', 'name': 'Zelalem Gero'}, {'authorId': '145229121', 'name': 'Chandan Singh'}, {'authorId': '47413820', 'name': 'Hao Cheng'}, {'authorId': '40466858', 'name': 'Tristan Naumann'}, {'authorId': '1947267', 'name': 'Michel Galley'}, {'authorId': '48441311', 'name': 'Jianfeng Gao'}, {'authorId': '1759772', 'name': 'Hoifung Poon'}]",['Microsoft'],['China'],2023-05
2306.00409,Shubin Huang,"Shubin Huang, Qiong Wu, Yiyi Zhou, Weijie Chen, Rongsheng Zhang,
  Xiaoshuai Sun, Rongrong Ji",Adapting Pre-trained Language Models to Vision-Language Tasks via Dynamic Visual Prompting,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained language models (PLMs) have played an increasing role in multimedia research. In terms of vision-language (VL) tasks, they often serve as a language encoder and still require an additional fusion network for VL reasoning, resulting in excessive memory overhead. In this paper, we focus on exploring PLMs as a stand-alone model for VL reasoning tasks. Inspired by the recently popular prompt tuning, we first prove that the processed visual features can be also projected onto the semantic space of PLMs and act as prompt tokens to bridge the gap between single- and multi-modal learning. However, this solution exhibits obvious redundancy in visual information and model inference, and the placement of prompt tokens also greatly affects the final performance. Based on these observations, we further propose a novel transfer learning approach for PLMs, termed Dynamic Visual Prompting (DVP). Concretely, DVP first deploys a cross-attention module to obtain text-related and compact visual prompt tokens, thereby greatly reducing the input length of PLMs. To obtain the optimal placement, we also equip DVP with a reinforcement-learning based search algorithm, which can automatically merge DVP with PLMs for different VL tasks via a very short search process. In addition, we also experiment DVP with the recently popular adapter approach to keep the most parameters of PLMs intact when adapting to VL tasks, helping PLMs achieve a quick shift between single- and multi-modal tasks. We apply DVP to two representative PLMs, namely BERT and T5, and conduct extensive experiments on a set of VL reasoning benchmarks including VQA2.0, GQA and SNLIVE. The experimental results not only show the advantage of DVP on efficiency and performance, but also confirm its superiority in adapting pre-trained language models to VL tasks. ","[{'version': 'v1', 'created': 'Thu, 1 Jun 2023 07:19:28 GMT'}, {'version': 'v2', 'created': 'Tue, 22 Aug 2023 07:45:09 GMT'}]",2023-08-23,"[['Huang', 'Shubin', ''], ['Wu', 'Qiong', ''], ['Zhou', 'Yiyi', ''], ['Chen', 'Weijie', ''], ['Zhang', 'Rongsheng', ''], ['Sun', 'Xiaoshuai', ''], ['Ji', 'Rongrong', '']]",0,0,2023-06-01,2,7,1,1,1,0,43ec80eeb6f22431ae741796996b25ca3b6bf3e2,258999114.0,https://www.semanticscholar.org/paper/43ec80eeb6f22431ae741796996b25ca3b6bf3e2,arXiv.org,2023.0,79.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2162837466', 'name': 'Shubin Huang'}, {'authorId': '2112260110', 'name': 'Qiong Wu'}, {'authorId': '2110191063', 'name': 'Yiyi Zhou'}, {'authorId': '2109956572', 'name': 'Weijie Chen'}, {'authorId': '48263731', 'name': 'Rongsheng Zhang'}, {'authorId': '1759841', 'name': 'Xiaoshuai Sun'}, {'authorId': '1572139630', 'name': 'Rongrong Ji'}]","['Xiamen University', 'NetEase']",['China'],2023-06
2306.00526,Wenjin Wang,"Wenjin Wang, Yunhao Li, Yixin Ou, Yin Zhang",Layout and Task Aware Instruction Prompt for Zero-shot Document Image Question Answering,"Add the LATIN-Tuning for Alapca. Code is available at
  https://github.com/WenjinW/LATIN-Prompt",,,,cs.CL cs.AI cs.CV,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Layout-aware pre-trained models has achieved significant progress on document image question answering. They introduce extra learnable modules into existing language models to capture layout information within document images from text bounding box coordinates obtained by OCR tools. However, extra modules necessitate pre-training on extensive document images. This prevents these methods from directly utilizing off-the-shelf instruction-tuning language foundation models, which have recently shown promising potential in zero-shot learning. Instead, in this paper, we find that instruction-tuning language models like Claude and ChatGPT can understand layout by spaces and line breaks. Based on this observation, we propose the LAyout and Task aware Instruction Prompt (LATIN-Prompt), which consists of layout-aware document content and task-aware instruction. Specifically, the former uses appropriate spaces and line breaks to recover the layout information among text segments obtained by OCR tools, and the latter ensures that generated answers adhere to formatting requirements. Moreover, we propose the LAyout and Task aware Instruction Tuning (LATIN-Tuning) to improve the performance of small instruction-tuning models like Alpaca. Experimental results show that LATIN-Prompt enables zero-shot performance of Claude and ChatGPT to be comparable to the fine-tuning performance of SOTAs on document image question answering, and LATIN-Tuning enhances the zero-shot performance of Alpaca significantly. For example, LATIN-Prompt improves the performance of Claude and ChatGPT on DocVQA by 263% and 20% respectively. LATIN-Tuning improves the performance of Alpaca on DocVQA by 87.7%. Quantitative and qualitative analyses demonstrate the effectiveness of LATIN-Prompt and LATIN-Tuning. We provide the code in supplementary and will release it to facilitate future research. ","[{'version': 'v1', 'created': 'Thu, 1 Jun 2023 10:28:12 GMT'}, {'version': 'v2', 'created': 'Fri, 30 Jun 2023 12:03:58 GMT'}, {'version': 'v3', 'created': 'Wed, 6 Sep 2023 03:30:14 GMT'}, {'version': 'v4', 'created': 'Thu, 7 Sep 2023 08:40:16 GMT'}]",2023-09-08,"[['Wang', 'Wenjin', ''], ['Li', 'Yunhao', ''], ['Ou', 'Yixin', ''], ['Zhang', 'Yin', '']]",1,1,2023-06-01,4,4,3,3,0,3,1e25118f99e03ffecf79412b46dda8a2966752c8,258999997.0,https://www.semanticscholar.org/paper/1e25118f99e03ffecf79412b46dda8a2966752c8,arXiv.org,2023.0,83.0,1.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2117833477', 'name': 'Wenjin Wang'}, {'authorId': '2135329729', 'name': 'Yunhao Li'}, {'authorId': '2196928874', 'name': 'Yixin Ou'}, {'authorId': '46867455', 'name': 'Yin Zhang'}]","['Zhejiang University', 'https://github.com/WenjinW/LATIN-Prompt']",['China'],2023-06
2306.00652,Han Cui,"Han Cui, Shangzhan Li, Yu Zhang and Qi Shi",Explanation Graph Generation via Generative Pre-training over Synthetic Graphs,Accepted by ACL23-Findings,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  The generation of explanation graphs is a significant task that aims to produce explanation graphs in response to user input, revealing the internal reasoning process. This task is challenging due to the significant discrepancy between unstructured user queries and structured explanation graphs. Current research commonly fine-tunes a text-based pre-trained language model on a small downstream dataset that is annotated with labeled graphs. However, due to the limited scale of available datasets, this approach may prove to be insufficient in bridging the gap between natural language text and structured graphs. In this paper, to alleviate the above limitations, we propose a novel pre-trained framework EG3P(for Explanation Graph Generation via Generative Pre-training over synthetic graphs) for the explanation graph generation task. Specifically, we first propose a text-to-graph generative task to pre-train the model with the goal of bridging the text-graph gap. Additionally, we propose an automatic corpus synthesis strategy for synthesizing a large scale of high-quality corpus, reducing the reliance on costly manual annotation methods. Experimental results on ExplaGraphs show the effectiveness of EG3P that our model surpasses all baseline systems with remarkable margins. Besides, further analysis demonstrates that EG3P is able to generate better explanation graphs on actual reasoning tasks such as CommonsenseQA and OpenbookQA. ","[{'version': 'v1', 'created': 'Thu, 1 Jun 2023 13:20:22 GMT'}]",2023-06-02,"[['Cui', 'Han', ''], ['Li', 'Shangzhan', ''], ['Zhang', 'Yu', ''], ['Shi', 'Qi', '']]",0,1,2023-06-01,1,4,2,0,0,0,a5ad326e6f726d2b87ca1f55e78c9de3bca00b39,258999803.0,https://www.semanticscholar.org/paper/a5ad326e6f726d2b87ca1f55e78c9de3bca00b39,Annual Meeting of the Association for Computational Linguistics,2023.0,49.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '3129614', 'name': 'H. Cui'}, {'authorId': '2109154767', 'name': 'Sha Li'}, {'authorId': '49889909', 'name': 'Yu Zhang'}, {'authorId': '117637793', 'name': 'Qi Shi'}]",['Harbin Institute of Technology'],['China'],2023-06
2306.00693,Ning Ding,"Ning Ding, Yehui Tang, Zhongqian Fu, Chao Xu, Kai Han, Yunhe Wang",GPT4Image: Can Large Pre-trained Models Help Vision Models on Perception Tasks?,"GitHub:
  https://github.com/huawei-noah/Efficient-Computing/tree/master/GPT4Image/",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The recent upsurge in pre-trained large models (e.g. GPT-4) has swept across the entire deep learning community. Such powerful large language models (LLMs) demonstrate advanced generative ability and multimodal understanding capability, which quickly achieve new state-of-the-art performances on a variety of benchmarks. The pre-trained LLM usually plays the role as a universal AI model that can conduct various tasks, including context reasoning, article analysis and image content comprehension. However, considering the prohibitively high memory and computational cost for implementing such a large model, the conventional models (such as CNN and ViT), are still essential for many visual perception tasks. In this paper, we propose to enhance the representation ability of ordinary vision models for perception tasks (e.g. image classification) by taking advantage of large pre-trained models. We present a new learning paradigm in which the knowledge extracted from large pre-trained models are utilized to help models like CNN and ViT learn enhanced representations and achieve better performance. Firstly, we curate a high quality description set by prompting a multimodal LLM to generate descriptive text for all training images. Furthermore, we feed these detailed descriptions into a pre-trained encoder to extract text embeddings with rich semantic information that encodes the content of images. During training, text embeddings will serve as extra supervising signals and be aligned with image representations learned by vision models. The alignment process helps vision models learn better and achieve higher accuracy with the assistance of pre-trained LLMs. We conduct extensive experiments to verify that the proposed algorithm consistently improves the performance for various vision models with heterogeneous architectures. ","[{'version': 'v1', 'created': 'Thu, 1 Jun 2023 14:02:45 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Jun 2023 13:59:25 GMT'}]",2023-06-08,"[['Ding', 'Ning', ''], ['Tang', 'Yehui', ''], ['Fu', 'Zhongqian', ''], ['Xu', 'Chao', ''], ['Han', 'Kai', ''], ['Wang', 'Yunhe', '']]",0,1,2023-06-01,2,6,1,1,0,1,31a68755ca6899e6c360ec8568704ae74f223a25,258999962.0,https://www.semanticscholar.org/paper/31a68755ca6899e6c360ec8568704ae74f223a25,arXiv.org,2023.0,52.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2066768061', 'name': 'N. Ding'}, {'authorId': '103603255', 'name': 'Yehui Tang'}, {'authorId': '2068056090', 'name': 'Zhongqian Fu'}, {'authorId': '30136198', 'name': 'Chaoting Xu'}, {'authorId': '3826388', 'name': 'Kai Han'}, {'authorId': '2108702980', 'name': 'Yunhe Wang'}]","['Peking University', 'Huawei Technologies (China)']",['China'],2023-06
2306.00978,Haotian Tang,"Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Chuang
  Gan, Song Han",AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration,Code available at: https://github.com/mit-han-lab/llm-awq,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have shown excellent performance on various tasks, but the astronomical model size raises the hardware barrier for serving (memory size) and slows down token generation (memory bandwidth). In this paper, we propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only quantization. Our method is based on the observation that weights are not equally important: protecting only 1% of salient weights can greatly reduce quantization error. We then propose to search for the optimal per-channel scaling that protects the salient weights by observing the activation, not weights. AWQ does not rely on any backpropagation or reconstruction, so it can well preserve LLMs' generalization ability on different domains and modalities, without overfitting to the calibration set. AWQ outperforms existing work on various language modeling and domain-specific benchmarks. Thanks to better generalization, it achieves excellent quantization performance for instruction-tuned LMs and, for the first time, multi-modal LMs. Alongside AWQ, we implement an efficient and flexible inference framework tailored for LLMs on the edge, offering more than 3x speedup over the Huggingface FP16 implementation on both desktop and mobile GPUs. It also democratizes the deployment of the 70B Llama-2 model on mobile GPU (NVIDIA Jetson Orin 64GB). ","[{'version': 'v1', 'created': 'Thu, 1 Jun 2023 17:59:10 GMT'}, {'version': 'v2', 'created': 'Tue, 3 Oct 2023 18:20:01 GMT'}]",2023-10-05,"[['Lin', 'Ji', ''], ['Tang', 'Jiaming', ''], ['Tang', 'Haotian', ''], ['Yang', 'Shang', ''], ['Dang', 'Xingyu', ''], ['Gan', 'Chuang', ''], ['Han', 'Song', '']]",0,0,2023-06-01,2,7,1,1,1,0,2c5ab7d87e3342d2dba7d1d113ca1b16c545e344,258999941.0,https://www.semanticscholar.org/paper/2c5ab7d87e3342d2dba7d1d113ca1b16c545e344,arXiv.org,2023.0,59.0,43.0,10.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '46698300', 'name': 'Ji Lin'}, {'authorId': '2214687479', 'name': 'Jiaming Tang'}, {'authorId': '150127950', 'name': 'Haotian Tang'}, {'authorId': '2202210853', 'name': 'Shang Yang'}, {'authorId': '2219266839', 'name': 'Xingyu Dang'}, {'authorId': '2115659426', 'name': 'Song Han'}]",['Tsinghua University'],['China'],2023-06
2306.01242,Zhizheng Zhang,"Zhizheng Zhang, Xiaoyi Zhang, Wenxuan Xie, Yan Lu",Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators,,,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The recent success of Large Language Models (LLMs) signifies an impressive stride towards artificial general intelligence. They have shown a promising prospect in automatically completing tasks upon user instructions, functioning as brain-like coordinators. The associated risks will be revealed as we delegate an increasing number of tasks to machines for automated completion. A big question emerges: how can we make machines behave responsibly when helping humans automate tasks as personal copilots? In this paper, we explore this question in depth from the perspectives of feasibility, completeness and security. In specific, we present Responsible Task Automation (ResponsibleTA) as a fundamental framework to facilitate responsible collaboration between LLM-based coordinators and executors for task automation with three empowered capabilities: 1) predicting the feasibility of the commands for executors; 2) verifying the completeness of executors; 3) enhancing the security (e.g., the protection of users' privacy). We further propose and compare two paradigms for implementing the first two capabilities. One is to leverage the generic knowledge of LLMs themselves via prompt engineering while the other is to adopt domain-specific learnable models. Moreover, we introduce a local memory mechanism for achieving the third capability. We evaluate our proposed ResponsibleTA on UI task automation and hope it could bring more attentions to ensuring LLMs more responsible in diverse scenarios. The research project homepage is at https://task-automation-research.github.io/responsible_task_automation. ","[{'version': 'v1', 'created': 'Fri, 2 Jun 2023 02:42:58 GMT'}]",2023-06-05,"[['Zhang', 'Zhizheng', ''], ['Zhang', 'Xiaoyi', ''], ['Xie', 'Wenxuan', ''], ['Lu', 'Yan', '']]",0,0,2023-06-02,1,4,2,0,0,0,615962d8969c8e0ffe43319689dce6c50cbf1f29,259063857.0,https://www.semanticscholar.org/paper/615962d8969c8e0ffe43319689dce6c50cbf1f29,arXiv.org,2023.0,46.0,2.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1486397342', 'name': 'Zhizheng Zhang'}, {'authorId': '46446933', 'name': 'Xiaoyi Zhang'}, {'authorId': '2675648', 'name': 'Wenxuan Xie'}, {'authorId': '2198169719', 'name': 'Yan Lu'}]",['Microsoft'],['China'],2023-06
2306.01388,Lei Chen,"Sihem Amer-Yahia, Angela Bonifati, Lei Chen, Guoliang Li, Kyuseok
  Shim, Jianliang Xu, Xiaochun Yang",From Large Language Models to Databases and Back: A discussion on research and education,"7 pages, 2 figures, the Panel at the 28th International Conference on
  Database Systems for Advanced Applications (DASFAA 2023)",,,,cs.DB,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This discussion was conducted at a recent panel at the 28th International Conference on Database Systems for Advanced Applications (DASFAA 2023), held April 17-20, 2023 in Tianjin, China. The title of the panel was ""What does LLM (ChatGPT) Bring to Data Science Research and Education? Pros and Cons"". It was moderated by Lei Chen and Xiaochun Yang. The discussion raised several questions on how large language models (LLMs) and database research and education can help each other and the potential risks of LLMs. ","[{'version': 'v1', 'created': 'Fri, 2 Jun 2023 09:18:34 GMT'}, {'version': 'v2', 'created': 'Sat, 8 Jul 2023 02:28:49 GMT'}]",2023-07-11,"[['Amer-Yahia', 'Sihem', ''], ['Bonifati', 'Angela', ''], ['Chen', 'Lei', ''], ['Li', 'Guoliang', ''], ['Shim', 'Kyuseok', ''], ['Xu', 'Jianliang', ''], ['Yang', 'Xiaochun', '']]",1,1,2023-06-02,2,7,1,1,0,1,3d8bbb4869ab69f2735d08f8f339aa4d554d4295,259063913.0,https://www.semanticscholar.org/paper/3d8bbb4869ab69f2735d08f8f339aa4d554d4295,SIGMOD record,2023.0,40.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Education', 'source': 's2-fos-model'}]","[{'authorId': '1403657578', 'name': 'S. Amer-Yahia'}, {'authorId': '1699192', 'name': 'A. Bonifati'}, {'authorId': '2146071014', 'name': 'Lei Chen'}, {'authorId': '2217441359', 'name': 'Guoliang Li'}, {'authorId': '144232630', 'name': 'Kyuseok Shim'}, {'authorId': '2157263249', 'name': 'Jianliang Xu'}, {'authorId': '2204051805', 'name': 'Xiaochun Yang'}]","[""Laboratoire d'Informatique en Images et Systmes d'Information"", 'Hong Kong University of Science and Technology', 'Seoul National University', 'Tsinghua University', 'Universit Grenoble Alpes', 'Hong Kong Baptist University']","['South Korea', 'China', 'France']",2023-06
2306.01499,Zhuo Wang,"Zhuo Wang, Rongzhen Li, Bowen Dong, Jie Wang, Xiuxing Li, Ning Liu,
  Chenhui Mao, Wei Zhang, Liling Dong, Jing Gao, Jianyong Wang","Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? Maybe, but not today","16 pages, 6 figures",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent investigations show that large language models (LLMs), specifically GPT-4, not only have remarkable capabilities in common Natural Language Processing (NLP) tasks but also exhibit human-level performance on various professional and academic benchmarks. However, whether GPT-4 can be directly used in practical applications and replace traditional artificial intelligence (AI) tools in specialized domains requires further experimental validation. In this paper, we explore the potential of LLMs such as GPT-4 to outperform traditional AI tools in dementia diagnosis. Comprehensive comparisons between GPT-4 and traditional AI tools are conducted to examine their diagnostic accuracy in a clinical setting. Experimental results on two real clinical datasets show that, although LLMs like GPT-4 demonstrate potential for future advancements in dementia diagnosis, they currently do not surpass the performance of traditional AI tools. The interpretability and faithfulness of GPT-4 are also evaluated by comparison with real doctors. We discuss the limitations of GPT-4 in its current state and propose future research directions to enhance GPT-4 in dementia diagnosis. ","[{'version': 'v1', 'created': 'Fri, 2 Jun 2023 12:47:45 GMT'}]",2023-06-05,"[['Wang', 'Zhuo', ''], ['Li', 'Rongzhen', ''], ['Dong', 'Bowen', ''], ['Wang', 'Jie', ''], ['Li', 'Xiuxing', ''], ['Liu', 'Ning', ''], ['Mao', 'Chenhui', ''], ['Zhang', 'Wei', ''], ['Dong', 'Liling', ''], ['Gao', 'Jing', ''], ['Wang', 'Jianyong', '']]",0,1,2023-06-02,1,11,2,1,0,1,c7dbd6c80ae941daf1de81439b0d1f992da130ab,259064252.0,https://www.semanticscholar.org/paper/c7dbd6c80ae941daf1de81439b0d1f992da130ab,arXiv.org,2023.0,39.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2218836424', 'name': 'Zhuo Wang'}, {'authorId': '2209392929', 'name': 'R. Li'}, {'authorId': '50660465', 'name': 'Bowen Dong'}, {'authorId': '31737970', 'name': 'Jie Wang'}, {'authorId': '2116521868', 'name': 'Xiuxing Li'}, {'authorId': '2152354910', 'name': 'Ning Liu'}, {'authorId': '39245374', 'name': 'C. Mao'}, {'authorId': '2155468861', 'name': 'Wei Zhang'}, {'authorId': '5851496', 'name': 'L. Dong'}, {'authorId': '2115556978', 'name': 'Jing Gao'}, {'authorId': '2115642141', 'name': 'Jianyong Wang'}]","['University of Chinese Academy of Sciences', 'First Affiliated Hospital of Nanchang University', 'Chinese Academy of Medical Sciences & Peking Union Medical College', 'Shandong University', 'Tsinghua University', 'Institute of Computing Technology', 'East China Normal University']",['China'],2023-06
2306.01784,Lian Wen,"Zizhuo Zhang, Lian Wen, Shaoyang Zhang, David Chen, Yanfei Jiang",Evaluating GPT's Programming Capability through CodeWars' Katas,9 pages,,,,cs.AI cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the burgeoning field of artificial intelligence (AI), understanding the capabilities and limitations of programming-oriented models is crucial. This paper presents a novel evaluation of the programming proficiency of Generative Pretrained Transformer (GPT) models, specifically GPT-3.5 and GPT-4, against coding problems of varying difficulty levels drawn from Codewars. The experiments reveal a distinct boundary at the 3kyu level, beyond which these GPT models struggle to provide solutions. These findings led to the proposal of a measure for coding problem complexity that incorporates both problem difficulty and the time required for solution. The research emphasizes the need for validation and creative thinking capabilities in AI models to better emulate human problem-solving techniques. Future work aims to refine this proposed complexity measure, enhance AI models with these suggested capabilities, and develop an objective measure for programming problem difficulty. The results of this research offer invaluable insights for improving AI programming capabilities and advancing the frontier of AI problem-solving abilities. ","[{'version': 'v1', 'created': 'Wed, 31 May 2023 10:36:16 GMT'}]",2023-06-06,"[['Zhang', 'Zizhuo', ''], ['Wen', 'Lian', ''], ['Zhang', 'Shaoyang', ''], ['Chen', 'David', ''], ['Jiang', 'Yanfei', '']]",0,1,2023-05-31,1,5,2,2,0,2,1317f78f31ed124b490422a5a1ac71ec849510e2,259075881.0,https://www.semanticscholar.org/paper/1317f78f31ed124b490422a5a1ac71ec849510e2,arXiv.org,2023.0,33.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Business', 'source': 's2-fos-model'}]","[{'authorId': '2155575981', 'name': 'Zizhuo Zhang'}, {'authorId': '2152129867', 'name': 'Lian Wen'}, {'authorId': '2218949590', 'name': 'Shaoyang Zhang'}, {'authorId': '2218927171', 'name': 'David Chen'}, {'authorId': '2219041251', 'name': 'Yanfei Jiang'}]","['Griffith University', ""Chang'an University"", ""Xi'an Rail Transit Group Company Limited, Xi'an, China,""]","['China', 'Australia']",2023-05
2306.02096,Prince Gideon Kubendran Amos,P G Kubendran Amos,Befriending ChatGPT and other superchatbots: An AI-integrated take-home assessment preserving integrity,,,,,physics.ed-ph,http://creativecommons.org/licenses/by/4.0/,"  With the launch of ChatGPT, serious concerns have reasonably been raised of its ill-effect on the integrity of remote take-home exams. By way of mitigating the concern, in this study, a rather straightforward Artificial-Intelligence (AI)-integrated take-home assessment technique is proposed, and the outcome of its practice is discussed. Despite involving AI, in the form of ChatGPT, the assessment adheres to the convention of posing questions invoking critical thinking and problem solving skills. However, AI is characteristically integrated in this assessment by instructing the learners to employ ChatGPT as one of the primary sources. The learners are directed to report the use of ChatGPT by including both the prompts and its responses, before expressing their thoughts on AI-generated answers and their own concluding statement. These three characteristic components of the present techniques -- the handling of ChatGPT through the prompts, comments on the AI-responses and the concluding thoughts -- are evaluated to gauge the learning.   The proposed assessment was assigned as a take-home group activity for a batch of seventy eight students, divided into thirteen groups. Despite addressing the same questions, there was no significant overlap in the answers. Moreover, a wide range of approaches were adopted by the groups in handling ChatGPT, which in-turn rendered different responses, ultimately drawing distinct answers. Besides preventing the undesired use of ChatGPT by explicitly integrating it, the proposed assessment seemingly helped the learners question the accuracy of its responses. This self-realised skepticism can be expected to curtail blatant malpractices involving ChatGPT in the long run. ","[{'version': 'v1', 'created': 'Sat, 3 Jun 2023 12:12:46 GMT'}]",2023-06-06,"[['Amos', 'P G Kubendran', '']]",1,1,2023-06-03,1,1,1,1,0,1,1e4f8aa797e0842d035690a0c04f1bea8a10ce56,259075165.0,https://www.semanticscholar.org/paper/1e4f8aa797e0842d035690a0c04f1bea8a10ce56,,2023.0,30.0,0.0,0.0,False,['Physics'],"[{'category': 'Physics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Education', 'source': 's2-fos-model'}]","[{'authorId': '2219560075', 'name': 'P. G. K. Amos'}]","['Karlsruhe Institute of Technology', 'Amos Enterprise (China)', 'National Institute of Technology Tiruchirappalli']","['Germany', 'India', 'China']",2023-06
2306.02245,Dingyuan Zhang,"Dingyuan Zhang, Dingkang Liang, Hongcheng Yang, Zhikang Zou, Xiaoqing
  Ye, Zhe Liu, Xiang Bai",SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model,"Technical Report. The code is released at
  https://github.com/DYZhang09/SAM3D",,,,cs.CV eess.IV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the development of large language models, many remarkable linguistic systems like ChatGPT have thrived and achieved astonishing success on many tasks, showing the incredible power of foundation models. In the spirit of unleashing the capability of foundation models on vision tasks, the Segment Anything Model (SAM), a vision foundation model for image segmentation, has been proposed recently and presents strong zero-shot ability on many downstream 2D tasks. However, whether SAM can be adapted to 3D vision tasks has yet to be explored, especially 3D object detection. With this inspiration, we explore adapting the zero-shot ability of SAM to 3D object detection in this paper. We propose a SAM-powered BEV processing pipeline to detect objects and get promising results on the large-scale Waymo open dataset. As an early attempt, our method takes a step toward 3D object detection with vision foundation models and presents the opportunity to unleash their power on 3D vision tasks. The code is released at https://github.com/DYZhang09/SAM3D. ","[{'version': 'v1', 'created': 'Sun, 4 Jun 2023 03:09:21 GMT'}]",2023-06-12,"[['Zhang', 'Dingyuan', ''], ['Liang', 'Dingkang', ''], ['Yang', 'Hongcheng', ''], ['Zou', 'Zhikang', ''], ['Ye', 'Xiaoqing', ''], ['Liu', 'Zhe', ''], ['Bai', 'Xiang', '']]",1,1,2023-06-04,1,7,2,1,0,1,1ed36ffa0555efa22542a0af26d04fb809dceb33,259075893.0,https://www.semanticscholar.org/paper/1ed36ffa0555efa22542a0af26d04fb809dceb33,arXiv.org,2023.0,27.0,4.0,0.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2141082048', 'name': 'Dingyuan Zhang'}, {'authorId': '94882716', 'name': 'Dingkang Liang'}, {'authorId': '2118570465', 'name': 'Hongcheng Yang'}, {'authorId': '13380914', 'name': 'Zhikang Zou'}, {'authorId': '1845887158', 'name': 'Xiaoqing Ye'}, {'authorId': '47781621', 'name': 'Zhe Liu'}, {'authorId': '2117845424', 'name': 'Xiang Bai'}]","['Huazhong University of Science and Technology', 'Baidu']",['China'],2023-06
2306.02408,Beichen Zhang,"Beichen Zhang, Kun Zhou, Xilin Wei, Wayne Xin Zhao, Jing Sha, Shijin
  Wang, Ji-Rong Wen",Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning,"17 pages, working in progress",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Chain-of-thought prompting~(CoT) and tool augmentation have been validated in recent work as effective practices for improving large language models~(LLMs) to perform step-by-step reasoning on complex math-related tasks. However, most existing math reasoning datasets may be not able to fully evaluate and analyze the ability of LLMs in manipulating tools and performing reasoning, as they may only require very few invocations of tools or miss annotations for evaluating intermediate reasoning steps. To address the issue, we construct \textbf{CARP}, a new Chinese dataset consisting of 4,886 computation-intensive algebra problems with formulated annotations on intermediate steps. In CARP, we test four LLMs with CoT prompting, and find that they are all prone to make mistakes at the early steps of the solution, leading to wrong answers. Based on this finding, we propose a new approach that can deliberate the reasoning steps with tool interfaces, namely \textbf{DELI}. In DELI, we first initialize a step-by-step solution based on retrieved exemplars, then iterate two deliberation procedures that check and refine the intermediate steps of the generated solution, from the perspectives of tool manipulation and natural language reasoning, until obtaining converged solutions or reaching the maximum turn. Experimental results on CARP and six other datasets show that the proposed DELI mostly outperforms competitive baselines, and can further boost the performance of existing CoT methods. Our data and code are available in \url{https://github.com/RUCAIBox/CARP}. ","[{'version': 'v1', 'created': 'Sun, 4 Jun 2023 17:02:59 GMT'}]",2023-06-06,"[['Zhang', 'Beichen', ''], ['Zhou', 'Kun', ''], ['Wei', 'Xilin', ''], ['Zhao', 'Wayne Xin', ''], ['Sha', 'Jing', ''], ['Wang', 'Shijin', ''], ['Wen', 'Ji-Rong', '']]",0,0,2023-06-04,1,7,1,0,0,0,587f22e4e04d77ba0750deea69192fbfb73d7435,259075484.0,https://www.semanticscholar.org/paper/587f22e4e04d77ba0750deea69192fbfb73d7435,arXiv.org,2023.0,36.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Mathematics', 'source': 's2-fos-model'}]","[{'authorId': '2107926615', 'name': 'Beichen Zhang'}, {'authorId': '1423651904', 'name': 'Kun Zhou'}, {'authorId': '2422218', 'name': 'Xilin Wei'}, {'authorId': '2542603', 'name': 'Wayne Xin Zhao'}, {'authorId': '2165225571', 'name': 'Jing Sha'}, {'authorId': '2108620507', 'name': 'Shijin Wang'}, {'authorId': '153693432', 'name': 'Ji-rong Wen'}]","['Beijing Key Laboratory of Big Data Management and Analysis Methods.', 'Sichuan University', 'Renmin University of China', 'iFLYTEK AI Research (Central China']",['China'],2023-06
2306.02552,Xu Chen,"Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu
  Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng
  Dou, Jun Wang, Ji-Rong Wen",When Large Language Model based Agent Meets User Behavior Analysis: A Novel User Simulation Paradigm,"26 pages, 9 figures",,,,cs.IR cs.AI,http://creativecommons.org/licenses/by/4.0/,"  User behavior analysis is crucial in human-centered AI applications. In this field, the collection of sufficient and high-quality user behavior data has always been a fundamental yet challenging problem. An intuitive idea to address this problem is automatically simulating the user behaviors. However, due to the subjective and complex nature of human cognitive processes, reliably simulating the user behavior is difficult. Recently, large language models (LLM) have obtained remarkable successes, showing great potential to achieve human-like intelligence. We argue that these models present significant opportunities for reliable user simulation, and have the potential to revolutionize traditional study paradigms in user behavior analysis. In this paper, we take recommender system as an example to explore the potential of using LLM for user simulation. Specifically, we regard each user as an LLM-based autonomous agent, and let different agents freely communicate, behave and evolve in a virtual simulator called RecAgent. For comprehensively simulation, we not only consider the behaviors within the recommender system (\emph{e.g.}, item browsing and clicking), but also accounts for external influential factors, such as, friend chatting and social advertisement. Our simulator contains at most 1000 agents, and each agent is composed of a profiling module, a memory module and an action module, enabling it to behave consistently, reasonably and reliably. In addition, to more flexibly operate our simulator, we also design two global functions including real-human playing and system intervention. To evaluate the effectiveness of our simulator, we conduct extensive experiments from both agent and system perspectives. In order to advance this direction, we have released our project at {https://github.com/RUC-GSAI/YuLan-Rec}. ","[{'version': 'v1', 'created': 'Mon, 5 Jun 2023 02:58:35 GMT'}, {'version': 'v2', 'created': 'Mon, 18 Sep 2023 14:36:55 GMT'}]",2023-09-19,"[['Wang', 'Lei', ''], ['Zhang', 'Jingsen', ''], ['Yang', 'Hao', ''], ['Chen', 'Zhiyuan', ''], ['Tang', 'Jiakai', ''], ['Zhang', 'Zeyu', ''], ['Chen', 'Xu', ''], ['Lin', 'Yankai', ''], ['Song', 'Ruihua', ''], ['Zhao', 'Wayne Xin', ''], ['Xu', 'Jun', ''], ['Dou', 'Zhicheng', ''], ['Wang', 'Jun', ''], ['Wen', 'Ji-Rong', '']]",0,0,2023-06-05,2,14,2,0,0,0,773d8eb9715847f279cc55386080208d1e84cdc5,259075591.0,https://www.semanticscholar.org/paper/773d8eb9715847f279cc55386080208d1e84cdc5,,2023.0,59.0,6.0,1.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2152509786', 'name': 'Lei Wang'}, {'authorId': '2144163813', 'name': 'Jingsen Zhang'}, {'authorId': '2257352534', 'name': 'Hao Yang'}, {'authorId': '2241452075', 'name': 'Zhiyuan Chen'}, {'authorId': '144010962', 'name': 'Jiakai Tang'}, {'authorId': '2223760889', 'name': 'Zeyu Zhang'}, {'authorId': '2144230136', 'name': 'Xu Chen'}, {'authorId': '2149202150', 'name': 'Yankai Lin'}, {'authorId': '35119829', 'name': 'Ruihua Song'}, {'authorId': '2542603', 'name': 'Wayne Xin Zhao'}, {'authorId': '2241424564', 'name': 'Jun Xu'}, {'authorId': '1897235', 'name': 'Zhicheng Dou'}, {'authorId': '2241311153', 'name': 'Jun Wang'}, {'authorId': '153693432', 'name': 'Ji-rong Wen'}]","['University College London', 'Renmin University of China']","['China', 'United Kingdom']",2023-06
2306.02796,Liner Yang,"Ruining Chong, Luming Lu, Liner Yang, Jinran Nie, Shuhan Zhou, Yaoxin
  Li, Erhong Yang",MCTS: A Multi-Reference Chinese Text Simplification Dataset,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Text simplification aims to make the text easier to understand by applying rewriting transformations. There has been very little research on Chinese text simplification for a long time. The lack of generic evaluation data is an essential reason for this phenomenon. In this paper, we introduce MCTS, a multi-reference Chinese text simplification dataset. We describe the annotation process of the dataset and provide a detailed analysis of it. Furthermore, we evaluate the performance of some unsupervised methods and advanced large language models. We hope to build a basic understanding of Chinese text simplification through the foundational work and provide references for future research. We release our data at https://github.com/blcuicall/mcts. ","[{'version': 'v1', 'created': 'Mon, 5 Jun 2023 11:46:36 GMT'}]",2023-06-06,"[['Chong', 'Ruining', ''], ['Lu', 'Luming', ''], ['Yang', 'Liner', ''], ['Nie', 'Jinran', ''], ['Zhou', 'Shuhan', ''], ['Li', 'Yaoxin', ''], ['Yang', 'Erhong', '']]",0,0,2023-06-05,1,7,1,0,0,0,786b3fa82eaf03f416d471515a5bbc7bf1180be6,259076271.0,https://www.semanticscholar.org/paper/786b3fa82eaf03f416d471515a5bbc7bf1180be6,arXiv.org,2023.0,35.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '2162791358', 'name': 'Ruining Chong'}, {'authorId': '2218875779', 'name': 'Luming Lu'}, {'authorId': '2250995', 'name': 'Liner Yang'}, {'authorId': '2147399904', 'name': 'Jinran Nie'}, {'authorId': '2149165466', 'name': 'Shuhan Zhou'}, {'authorId': '2203371003', 'name': 'Yaoxin Li'}, {'authorId': '1851775', 'name': 'Erhong Yang'}]",['Beijing Language and Culture University'],['China'],2023-06
2306.02858,Hang Zhang,"Hang Zhang, Xin Li, Lidong Bing",Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding,"Technical Report; Code, Pretrained Model, and Dataset:
  https://github.com/DAMO-NLP-SG/Video-LLaMA",,,,cs.CL cs.CV cs.SD eess.AS,http://creativecommons.org/licenses/by/4.0/,"  We present Video-LLaMA, a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual & audio encoders and the frozen LLMs. Unlike previous vision-LLMs that focus on static image comprehensions such as MiniGPT-4 and LLaVA, Video-LLaMA mainly tackles two challenges in video understanding: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble the pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities as the pre-trained audio encoder, and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual & audio encoders with LLM's embedding space, we train Video-LLaMA on massive video/image-caption pairs as well as visual-instruction-tuning datasets of moderate amount but higher quality. We found Video-LLaMA showcases the ability to perceive and comprehend video content, generating meaningful responses that are grounded in the visual and auditory information presented in the videos. This highlights the potential of Video-LLaMA as a promising prototype for audio-visual AI assistants. ","[{'version': 'v1', 'created': 'Mon, 5 Jun 2023 13:17:27 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Jun 2023 12:28:37 GMT'}, {'version': 'v3', 'created': 'Mon, 12 Jun 2023 02:28:57 GMT'}]",2023-06-13,"[['Zhang', 'Hang', ''], ['Li', 'Xin', ''], ['Bing', 'Lidong', '']]",0,1,2023-06-05,3,3,4,1,1,0,5d321194696f1f75cf9da045e6022b2f20ba5b9c,259075356.0,https://www.semanticscholar.org/paper/5d321194696f1f75cf9da045e6022b2f20ba5b9c,arXiv.org,2023.0,42.0,65.0,8.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2119077859', 'name': 'Hang Zhang'}, {'authorId': '40613621', 'name': 'Xin Li'}, {'authorId': '1996394', 'name': 'Lidong Bing'}]","['Alibaba', 'Hupan Lab, 310023, Hangzhou, China']",['China'],2023-06
2306.02907,Shuyang Jiang,"Shuyang Jiang, Yuhao Wang, Yu Wang",SelfEvolve: A Code Evolution Framework via Large Language Models,,,,,cs.CL cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have already revolutionized code generation, after being pretrained on publicly available code data. However, while various methods have been proposed to augment LLMs with retrieved knowledge and enhance the quality of code generation, the performance of these retrieval-based methods is limited by the strength of the retrievers used. In addition, while LLMs show great emergent ability, they still struggle to produce the correct code in one turn. To address these challenges, we propose a novel two-step pipeline, called \autoknow, that leverages LLMs as both knowledge providers and self-reflective programmers. Unlike retrieval-based methods, \autoknow~obtains the knowledge from input prompts and generates intermediate code based on the generated knowledge. After that, \autoknow~asks LLM to act as an expert programmer to perform debugging for the generated code. This is achieved by receiving the error message from the interpreter, without requiring special test cases for correctness verification. We evaluate \autoknow~on three code generation datasets, including DS-1000 for data science code, HumanEval for software engineering code, and TransCoder for C++-to-Python translation. Our empirical experiments show that \autoknow~outperforms strong baselines by a significant margin on all datasets. We also conduct exhaustive analytical experiments to validate the effectiveness of the two stages of \autoknow, and find that both are superior to other prompting-based methods. Further scalability analysis demonstrates that \autoknow~can be adapted to other more advanced models, such as GPT-4, and bring consistent efficacy improvement. ","[{'version': 'v1', 'created': 'Mon, 5 Jun 2023 14:12:46 GMT'}]",2023-06-06,"[['Jiang', 'Shuyang', ''], ['Wang', 'Yuhao', ''], ['Wang', 'Yu', '']]",0,1,2023-06-05,1,3,2,1,0,1,eb36681fc4c5dfce4f3e05540fc92b007de278ca,259076266.0,https://www.semanticscholar.org/paper/eb36681fc4c5dfce4f3e05540fc92b007de278ca,arXiv.org,2023.0,54.0,8.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2119327053', 'name': 'Shuyang Jiang'}, {'authorId': '2198466349', 'name': 'Yuhao Wang'}, {'authorId': '2153609392', 'name': 'Yu Wang'}]","['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University']",['China'],2023-06
2306.03422,Wenfeng Yan,"Wenfeng Yan, Shaoxiang Chen, Zuxuan Wu, Yu-Gang Jiang",Prompting Large Language Models to Reformulate Queries for Moment Localization,"4 pages, 2 figures",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The task of moment localization is to localize a temporal moment in an untrimmed video for a given natural language query. Since untrimmed video contains highly redundant contents, the quality of the query is crucial for accurately localizing moments, i.e., the query should provide precise information about the target moment so that the localization model can understand what to look for in the videos. However, the natural language queries in current datasets may not be easy to understand for existing models. For example, the Ego4D dataset uses question sentences as the query to describe relatively complex moments. While being natural and straightforward for humans, understanding such question sentences are challenging for mainstream moment localization models like 2D-TAN. Inspired by the recent success of large language models, especially their ability of understanding and generating complex natural language contents, in this extended abstract, we make early attempts at reformulating the moment queries into a set of instructions using large language models and making them more friendly to the localization models. ","[{'version': 'v1', 'created': 'Tue, 6 Jun 2023 05:48:09 GMT'}]",2023-06-07,"[['Yan', 'Wenfeng', ''], ['Chen', 'Shaoxiang', ''], ['Wu', 'Zuxuan', ''], ['Jiang', 'Yu-Gang', '']]",0,0,2023-06-06,1,4,1,0,0,0,d8bf2fe6005113d7144e00c7dd823d0fc58a2ad4,259089311.0,https://www.semanticscholar.org/paper/d8bf2fe6005113d7144e00c7dd823d0fc58a2ad4,arXiv.org,2023.0,15.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2219009993', 'name': 'Wenfeng Yan'}, {'authorId': '2118434440', 'name': 'Shaoxiang Chen'}, {'authorId': '3099139', 'name': 'Zuxuan Wu'}, {'authorId': '1717861', 'name': 'Yu-Gang Jiang'}]",['Fudan University'],['China'],2023-06
2306.03799,Dong Yang,"Fobo Shi, Peijun Qing, Dong Yang, Nan Wang, Youbo Lei, Haonan Lu,
  Xiaodong Lin",Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models,Natural language processing (NLP),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Prompt engineering is an essential technique for enhancing the abilities of large language models (LLMs) by providing explicit and specific instructions. It enables LLMs to excel in various tasks, such as arithmetic reasoning, question answering, summarization, relation extraction, machine translation, and sentiment analysis. Researchers have been actively exploring different prompt engineering strategies, such as Chain of Thought (CoT), Zero-CoT, and In-context learning. However, an unresolved problem arises from the fact that current approaches lack a solid theoretical foundation for determining optimal prompts. To address this issue in prompt engineering, we propose a new and effective approach called Prompt Space. Our methodology utilizes text embeddings to obtain basis vectors by matrix decomposition, and then constructs a space for representing all prompts. Prompt Space significantly outperforms state-of-the-art prompt paradigms on ten public reasoning benchmarks. Notably, without the help of the CoT method and the prompt ""Let's think step by step"", Prompt Space shows superior performance over the few-shot method. Overall, our approach provides a robust and fundamental theoretical framework for selecting simple and effective prompts. This advancement marks a significant step towards improving prompt engineering for a wide variety of applications in LLMs. ","[{'version': 'v1', 'created': 'Tue, 6 Jun 2023 15:43:16 GMT'}]",2023-06-07,"[['Shi', 'Fobo', ''], ['Qing', 'Peijun', ''], ['Yang', 'Dong', ''], ['Wang', 'Nan', ''], ['Lei', 'Youbo', ''], ['Lu', 'Haonan', ''], ['Lin', 'Xiaodong', '']]",0,0,2023-06-06,1,7,1,0,0,0,2d338cdd12091814dec11155d3f6f848d7bab4d8,259088958.0,https://www.semanticscholar.org/paper/2d338cdd12091814dec11155d3f6f848d7bab4d8,arXiv.org,2023.0,42.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2212609873', 'name': 'Fobo Shi'}, {'authorId': '2188984699', 'name': 'Peijun Qing'}, {'authorId': '144041880', 'name': 'D. Yang'}, {'authorId': '144457723', 'name': 'Nan Wang'}, {'authorId': '2181902670', 'name': 'Youbo Lei'}, {'authorId': '2130373', 'name': 'H. Lu'}, {'authorId': '2117690698', 'name': 'Xiaodong Lin'}]","[""Xi'an Jiaotong University"", 'OPPO', 'Dartmouth College', 'Central China Normal University', 'Rugster University']","['China', 'United States']",2023-06
2306.03823,Sukhpal Singh Gill,"Sukhpal Singh Gill, Minxian Xu, Panos Patros, Huaming Wu, Rupinder
  Kaur, Kamalpreet Kaur, Stephanie Fuller, Manmeet Singh, Priyansh Arora, Ajith
  Kumar Parlikad, Vlado Stankovski, Ajith Abraham, Soumya K. Ghosh, Hanan
  Lutfiyya, Salil S. Kanhere, Rami Bahsoon, Omer Rana, Schahram Dustdar, Rizos
  Sakellariou, Steve Uhlig, Rajkumar Buyya",Transformative Effects of ChatGPT on Modern Education: Emerging Era of AI Chatbots,Preprint submitted to IoTCPS Elsevier (2023),"Internet of Things and Cyber-Physical Systems (Elsevier), Volume
  4, 2024, Pages 19-23",10.1016/j.iotcps.2023.06.002,,cs.CY cs.AI cs.CL,http://creativecommons.org/publicdomain/zero/1.0/,"  ChatGPT, an AI-based chatbot, was released to provide coherent and useful replies based on analysis of large volumes of data. In this article, leading scientists, researchers and engineers discuss the transformative effects of ChatGPT on modern education. This research seeks to improve our knowledge of ChatGPT capabilities and its use in the education sector, identifying potential concerns and challenges. Our preliminary evaluation concludes that ChatGPT performed differently in each subject area including finance, coding and maths. While ChatGPT has the ability to help educators by creating instructional content, offering suggestions and acting as an online educator to learners by answering questions and promoting group work, there are clear drawbacks in its use, such as the possibility of producing inaccurate or false data and circumventing duplicate content (plagiarism) detectors where originality is essential. The often reported hallucinations within Generative AI in general, and also relevant for ChatGPT, can render its use of limited benefit where accuracy is essential. What ChatGPT lacks is a stochastic measure to help provide sincere and sensitive communication with its users. Academic regulations and evaluation practices used in educational institutions need to be updated, should ChatGPT be used as a tool in education. To address the transformative effects of ChatGPT on the learning environment, educating teachers and students alike about its capabilities and limitations will be crucial. ","[{'version': 'v1', 'created': 'Thu, 25 May 2023 17:35:57 GMT'}]",2023-07-18,"[['Gill', 'Sukhpal Singh', ''], ['Xu', 'Minxian', ''], ['Patros', 'Panos', ''], ['Wu', 'Huaming', ''], ['Kaur', 'Rupinder', ''], ['Kaur', 'Kamalpreet', ''], ['Fuller', 'Stephanie', ''], ['Singh', 'Manmeet', ''], ['Arora', 'Priyansh', ''], ['Parlikad', 'Ajith Kumar', ''], ['Stankovski', 'Vlado', ''], ['Abraham', 'Ajith', ''], ['Ghosh', 'Soumya K.', ''], ['Lutfiyya', 'Hanan', ''], ['Kanhere', 'Salil S.', ''], ['Bahsoon', 'Rami', ''], ['Rana', 'Omer', ''], ['Dustdar', 'Schahram', ''], ['Sakellariou', 'Rizos', ''], ['Uhlig', 'Steve', ''], ['Buyya', 'Rajkumar', '']]",1,1,2023-05-25,1,21,3,1,0,1,95f0884c30bdc83551fe3de67a6bb473cc38f893,259088562.0,https://www.semanticscholar.org/paper/95f0884c30bdc83551fe3de67a6bb473cc38f893,Internet of Things and Cyber-Physical Systems,2023.0,39.0,14.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Education', 'source': 's2-fos-model'}]","[{'authorId': '31043248', 'name': 'S. S. Gill'}, {'authorId': '2889350', 'name': 'Minxian Xu'}, {'authorId': '151473641', 'name': 'Panos Patros'}, {'authorId': '47987518', 'name': 'Huaming Wu'}, {'authorId': '2130749816', 'name': 'Rupinder Kaur'}, {'authorId': '1751000', 'name': 'K. Kaur'}, {'authorId': '2014841369', 'name': 'Stephanie Fuller'}, {'authorId': '2110677193', 'name': 'Manmeet Singh'}, {'authorId': '2219452948', 'name': 'Priyansh Arora'}, {'authorId': '52050652', 'name': 'A. Parlikad'}, {'authorId': '3034012', 'name': 'V. Stankovski'}, {'authorId': '2064038663', 'name': 'Ajith Abraham'}, {'authorId': '2155615327', 'name': 'Soumya K. Ghosh'}, {'authorId': '1769104', 'name': 'H. Lutfiyya'}, {'authorId': '1733096', 'name': 'S. Kanhere'}, {'authorId': '1804288', 'name': 'R. Bahsoon'}, {'authorId': '2099659875', 'name': 'O. Rana'}, {'authorId': '1691109', 'name': 'S. Dustdar'}, {'authorId': '1741531', 'name': 'R. Sakellariou'}, {'authorId': '2157668837', 'name': 'Steve Uhlig'}, {'authorId': '1709598', 'name': 'R. Buyya'}]","['Queen Mary University of London', 'TU Wien', 'Western University', 'Raygun Performance Monitoring, Wellington, New Zealand', 'University of Melbourne', 'The University of Texas at Austin', 'University of Ljubljana', 'Shenzhen Institutes of Advanced Technology', 'Microsoft', 'University of Cambridge', 'University of Manchester', 'Machine Intelligence Research Labs', 'UNSW Sydney', 'University of Birmingham', ""King's College Hospital"", 'Cymax Group Technologies, British Columbia, Canada', 'Cardiff University', 'Indian Institute of Tropical Meteorology', 'Indian Institute of Technology Kharagpur', 'Tianjin University']","['Canada', 'Slovenia', 'New Zealand', 'United States', 'India', 'United Kingdom', 'China', 'Austria', 'Australia']",2023-05
2306.03856,Pinzhen Chen,"Pinzhen Chen, Zhicheng Guo, Barry Haddow, Kenneth Heafield",Iterative Translation Refinement with Large Language Models,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models have shown surprising performances in understanding instructions and performing natural language tasks. In this paper, we propose iterative translation refinement to leverage the power of large language models for more natural translation and post-editing. We show that by simply involving a large language model in an iterative process, the output quality improves beyond mere translation. Extensive test scenarios with GPT-3.5 reveal that although iterations reduce string-based metric scores, neural metrics indicate comparable if not improved translation quality. Further, human evaluations demonstrate that our method effectively reduces translationese compared to initial GPT translations and even human references, especially for into-English directions. Ablation studies underscore the importance of anchoring the refinement process to the source input and a reasonable initial translation. ","[{'version': 'v1', 'created': 'Tue, 6 Jun 2023 16:51:03 GMT'}]",2023-06-07,"[['Chen', 'Pinzhen', ''], ['Guo', 'Zhicheng', ''], ['Haddow', 'Barry', ''], ['Heafield', 'Kenneth', '']]",0,1,2023-06-06,1,4,2,1,0,1,bf4810017b54e50354cccffd8966121c7166cb17,259088848.0,https://www.semanticscholar.org/paper/bf4810017b54e50354cccffd8966121c7166cb17,arXiv.org,2023.0,60.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '143616669', 'name': 'Pinzhen Chen'}, {'authorId': '1557391036', 'name': 'Zhicheng Guo'}, {'authorId': '2259100', 'name': 'B. Haddow'}, {'authorId': '1702066', 'name': 'Kenneth Heafield'}]","['University of Edinburgh', 'Tsinghua University']","['China', 'United Kingdom']",2023-06
2306.03901,Chenxu Hu,"Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, Hang Zhao",ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory,,,,,cs.AI cs.CL cs.DB cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) with memory are computationally universal. However, mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning. Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the SQL databases. We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning. The project website is available at https://chatdatabase.github.io/ . ","[{'version': 'v1', 'created': 'Tue, 6 Jun 2023 17:58:24 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Jun 2023 17:22:22 GMT'}]",2023-06-08,"[['Hu', 'Chenxu', ''], ['Fu', 'Jie', ''], ['Du', 'Chenzhuang', ''], ['Luo', 'Simian', ''], ['Zhao', 'Junbo', ''], ['Zhao', 'Hang', '']]",0,0,2023-06-06,2,6,4,0,0,0,50f44ef10335d59cec145b15effae20ff22c1fdb,259088875.0,https://www.semanticscholar.org/paper/50f44ef10335d59cec145b15effae20ff22c1fdb,arXiv.org,2023.0,38.0,20.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2118956725', 'name': 'Chenxu Hu'}, {'authorId': '2215497308', 'name': 'Jie Fu'}, {'authorId': '2107625784', 'name': 'Chenzhuang Du'}, {'authorId': '2159647250', 'name': 'Simian Luo'}, {'authorId': '7818229', 'name': 'J. Zhao'}, {'authorId': '2146231364', 'name': 'Hang Zhao'}]","['Tsinghua University', 'Beijing Academy of Artificial Intelligence', 'Zhejiang University']",['China'],2023-06
2306.04188,Shiping Yang,Shiping Yang and Renliang Sun and Xiaojun Wan,A New Dataset and Empirical Study for Sentence Simplification in Chinese,Accepted by ACL2023 main conference,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Sentence Simplification is a valuable technique that can benefit language learners and children a lot. However, current research focuses more on English sentence simplification. The development of Chinese sentence simplification is relatively slow due to the lack of data. To alleviate this limitation, this paper introduces CSS, a new dataset for assessing sentence simplification in Chinese. We collect manual simplifications from human annotators and perform data analysis to show the difference between English and Chinese sentence simplifications. Furthermore, we test several unsupervised and zero/few-shot learning methods on CSS and analyze the automatic evaluation and human evaluation results. In the end, we explore whether Large Language Models can serve as high-quality Chinese sentence simplification systems by evaluating them on CSS. ","[{'version': 'v1', 'created': 'Wed, 7 Jun 2023 06:47:34 GMT'}]",2023-06-08,"[['Yang', 'Shiping', ''], ['Sun', 'Renliang', ''], ['Wan', 'Xiaojun', '']]",0,0,2023-06-07,1,3,1,0,0,0,45c32573ca63698a704132f99bcc5680726f22fd,259096054.0,https://www.semanticscholar.org/paper/45c32573ca63698a704132f99bcc5680726f22fd,Annual Meeting of the Association for Computational Linguistics,2023.0,54.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2189419845', 'name': 'Shiping Yang'}, {'authorId': '2068172988', 'name': 'Renliang Sun'}, {'authorId': '9714242', 'name': 'Xiao-Yi Wan'}]",['Peking University'],['China'],2023-06
2306.04362,Qinghao Ye,"Haiyang Xu, Qinghao Ye, Xuan Wu, Ming Yan, Yuan Miao, Jiabo Ye, Guohai
  Xu, Anwen Hu, Yaya Shi, Guangwei Xu, Chenliang Li, Qi Qian, Maofei Que, Ji
  Zhang, Xiao Zeng, Fei Huang",Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks,Working in progress,,,,cs.CV cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  To promote the development of Vision-Language Pre-training (VLP) and multimodal Large Language Model (LLM) in the Chinese community, we firstly release the largest public Chinese high-quality video-language dataset named Youku-mPLUG, which is collected from Youku, a well-known Chinese video-sharing website, with strict criteria of safety, diversity, and quality. Youku-mPLUG contains 10 million Chinese video-text pairs filtered from 400 million raw videos across a wide range of 45 diverse categories for large-scale pre-training. In addition, to facilitate a comprehensive evaluation of video-language models, we carefully build the largest human-annotated Chinese benchmarks covering three popular video-language tasks of cross-modal retrieval, video captioning, and video category classification. Youku-mPLUG can enable researchers to conduct more in-depth multimodal research and develop better applications in the future. Furthermore, we release popular video-language pre-training models, ALPRO and mPLUG-2, and our proposed modularized decoder-only model mPLUG-video pre-trained on Youku-mPLUG. Experiments show that models pre-trained on Youku-mPLUG gain up to 23.1% improvement in video category classification. Besides, mPLUG-video achieves a new state-of-the-art result on these benchmarks with 80.5% top-1 accuracy in video category classification and 68.9 CIDEr score in video captioning, respectively. Finally, we scale up mPLUG-video based on the frozen Bloomz with only 1.7% trainable parameters as Chinese multimodal LLM, and demonstrate impressive instruction and video understanding ability. The zero-shot instruction understanding experiment indicates that pretraining with Youku-mPLUG can enhance the ability to comprehend overall and detailed visual semantics, recognize scene text, and leverage open-domain knowledge. ","[{'version': 'v1', 'created': 'Wed, 7 Jun 2023 11:52:36 GMT'}]",2023-06-08,"[['Xu', 'Haiyang', ''], ['Ye', 'Qinghao', ''], ['Wu', 'Xuan', ''], ['Yan', 'Ming', ''], ['Miao', 'Yuan', ''], ['Ye', 'Jiabo', ''], ['Xu', 'Guohai', ''], ['Hu', 'Anwen', ''], ['Shi', 'Yaya', ''], ['Xu', 'Guangwei', ''], ['Li', 'Chenliang', ''], ['Qian', 'Qi', ''], ['Que', 'Maofei', ''], ['Zhang', 'Ji', ''], ['Zeng', 'Xiao', ''], ['Huang', 'Fei', '']]",0,0,2023-06-07,1,16,2,1,1,0,d7a4b09a0e2c2d7b118144cf09895c640896da7b,259095579.0,https://www.semanticscholar.org/paper/d7a4b09a0e2c2d7b118144cf09895c640896da7b,arXiv.org,2023.0,44.0,1.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '153194420', 'name': 'Haiyang Xu'}, {'authorId': '2199011713', 'name': 'Qinghao Ye'}, {'authorId': '2145345189', 'name': 'Xuan-Wei Wu'}, {'authorId': '2114009661', 'name': 'Mingshi Yan'}, {'authorId': '2219274454', 'name': 'Yuan Miao'}, {'authorId': '2153258288', 'name': 'Jiabo Ye'}, {'authorId': '2115723816', 'name': 'Guohai Xu'}, {'authorId': '120897486', 'name': 'Anwen Hu'}, {'authorId': '37198550', 'name': 'Yaya Shi'}, {'authorId': '2149131512', 'name': 'Guangwei Xu'}, {'authorId': '2829009', 'name': 'Chenliang Li'}, {'authorId': '2177336921', 'name': 'Qingfang Qian'}, {'authorId': '1949860790', 'name': 'Maofei Que'}, {'authorId': '2116921824', 'name': 'Ji Zhang'}, {'authorId': '2111552792', 'name': 'Xiaoyan Zeng'}, {'authorId': '2194508991', 'name': 'Feiyan Huang'}]",['Alibaba'],['China'],2023-06
2306.04371,Jiahuan Zhang,"Suyuan Zhao, Jiahuan Zhang, Zaiqing Nie",Large-Scale Cell Representation Learning via Divide-and-Conquer Contrastive Learning,,,,,cs.CE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Single-cell RNA sequencing (scRNA-seq) data is a potent tool for comprehending the ""language of life"" and can provide insights into various downstream biomedical tasks. Large-scale language models (LLMs) are starting to be used for cell representation learning. However, current LLM-based cell representation learning methods depend solely on the BERT architecture, causing an anisotropic embedding space that leads to inefficient semantic representation. Contrastive learning alleviates this problem by distributing the embeddings uniformly. As a larger batch size in contrastive learning results in better representation, the practical application of contrastive learning in cell representation learning is hampered by the high dimensionality of scRNA-seq data and the large parameter volume of LLMs. To address the batch size limitation, we propose a novel divide-and-conquer contrastive learning approach to decouple the batch size from the GPU memory size for cell representation learning. Based on our divide-and-conquer contrastive learning approach, we introduce Single-Cell Language Model CellLM, a large-scale cell representation learning model to handle high-dimensional scRNA-seq data with tens of thousands of genes. CellLM has over 50 million parameters trained with 2 million scRNA-seq data and makes the first attempt to learn cell language models from both normal cells and cancer cells. CellLM achieves new state-of-the-art (SOTA) results in all evaluated downstream tasks: including a 71.8 F_1-score for cell type annotation (a 3.0% absolute improvement over scBERT), an average F_1-score of 88.9 for single-cell drug sensitivity prediction in a few-shot scenario (an 8.3% absolute improvement), and a 93.4 Pearson's correlation for single-omics cell line drug sensitivity prediction (a 6.2% absolute improvement). ","[{'version': 'v1', 'created': 'Wed, 7 Jun 2023 12:08:26 GMT'}]",2023-06-08,"[['Zhao', 'Suyuan', ''], ['Zhang', 'Jiahuan', ''], ['Nie', 'Zaiqing', '']]",0,0,2023-06-07,1,3,1,0,0,0,2cea5bbb6a4e45667df475e22023422c0f2c7008,259096072.0,https://www.semanticscholar.org/paper/2cea5bbb6a4e45667df475e22023422c0f2c7008,arXiv.org,2023.0,56.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Biology', 'source': 's2-fos-model'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2219662847', 'name': 'Suyuan Zhao'}, {'authorId': '2124839951', 'name': 'Jiahuan Zhang'}, {'authorId': '38301933', 'name': 'Zaiqing Nie'}]",['Tsinghua University'],['China'],2023-06
2306.04508,Gong Cheng,"Zixian Huang, Jiaying Zhou, Gengyang Xiao, Gong Cheng",Enhancing In-Context Learning with Answer Feedback for Multi-Span Question Answering,"12 pages, submitted to NLPCC 2023",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Whereas the recent emergence of large language models (LLMs) like ChatGPT has exhibited impressive general performance, it still has a large gap with fully-supervised models on specific tasks such as multi-span question answering. Previous researches found that in-context learning is an effective approach to exploiting LLM, by using a few task-related labeled data as demonstration examples to construct a few-shot prompt for answering new questions. A popular implementation is to concatenate a few questions and their correct answers through simple templates, informing LLM of the desired output. In this paper, we propose a novel way of employing labeled data such that it also informs LLM of some undesired output, by extending demonstration examples with feedback about answers predicted by an off-the-shelf model, e.g., correct, incorrect, or incomplete. Experiments on three multi-span question answering datasets as well as a keyphrase extraction dataset show that our new prompting strategy consistently improves LLM's in-context learning performance. ","[{'version': 'v1', 'created': 'Wed, 7 Jun 2023 15:20:24 GMT'}]",2023-06-08,"[['Huang', 'Zixian', ''], ['Zhou', 'Jiaying', ''], ['Xiao', 'Gengyang', ''], ['Cheng', 'Gong', '']]",1,1,2023-06-07,1,4,2,1,0,1,c1647923704251875f4160e91b59afbbdc58483e,259095877.0,https://www.semanticscholar.org/paper/c1647923704251875f4160e91b59afbbdc58483e,Natural Language Processing and Chinese Computing,2023.0,35.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1504347669', 'name': 'Zixian Huang'}, {'authorId': '2107920017', 'name': 'Jiaying Zhou'}, {'authorId': '2219273035', 'name': 'Gengyang Xiao'}, {'authorId': '2164040809', 'name': 'Gong Cheng'}]",['Nanjing University'],['China'],2023-06
2306.04528,Jindong Wang,"Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong
  Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, Xing Xie",PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts,"Technical report; updated with new experiments and related work; 27
  pages; code is at: https://github.com/microsoft/promptbench",,,,cs.CL cs.CR cs.LG,http://creativecommons.org/licenses/by/4.0/,"  The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptBench, a robustness benchmark designed to measure LLMs' resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. These prompts are then employed in diverse tasks, such as sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4,032 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets, with 567,084 test samples in total. Our findings demonstrate that contemporary LLMs are vulnerable to adversarial prompts. Furthermore, we present comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users. We make our code, prompts, and methodologies to generate adversarial prompts publicly accessible, thereby enabling and encouraging collaborative exploration in this pivotal field: https://github.com/microsoft/promptbench. ","[{'version': 'v1', 'created': 'Wed, 7 Jun 2023 15:37:00 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Jun 2023 12:44:10 GMT'}, {'version': 'v3', 'created': 'Thu, 24 Aug 2023 07:09:25 GMT'}]",2023-08-25,"[['Zhu', 'Kaijie', ''], ['Wang', 'Jindong', ''], ['Zhou', 'Jiaheng', ''], ['Wang', 'Zichen', ''], ['Chen', 'Hao', ''], ['Wang', 'Yidong', ''], ['Yang', 'Linyi', ''], ['Ye', 'Wei', ''], ['Gong', 'Neil Zhenqiang', ''], ['Zhang', 'Yue', ''], ['Xie', 'Xing', '']]",0,0,2023-06-07,3,11,3,0,0,0,77d6d7482d1a32ad147c39993758b6c63816f5c0,259095572.0,https://www.semanticscholar.org/paper/77d6d7482d1a32ad147c39993758b6c63816f5c0,arXiv.org,2023.0,123.0,46.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2219270546', 'name': 'Kaijie Zhu'}, {'authorId': '1519290245', 'name': 'Jindong Wang'}, {'authorId': '2188574368', 'name': 'Jiaheng Zhou'}, {'authorId': '47196709', 'name': 'Zichen Wang'}, {'authorId': '2051536212', 'name': 'Hao Chen'}, {'authorId': '2108024273', 'name': 'Yidong Wang'}, {'authorId': '2145500840', 'name': 'Linyi Yang'}, {'authorId': '2147205193', 'name': 'Weirong Ye'}, {'authorId': '144516687', 'name': 'N. Gong'}, {'authorId': '2211964951', 'name': 'Yue Zhang'}, {'authorId': '1576441343', 'name': 'Xingxu Xie'}]","['Duke University', 'Chinese Academy of Sciences', 'Microsoft']","['China', 'United States']",2023-06
2306.04556,Arjun Guha,"Hannah McLean Babe, Sydney Nguyen, Yangtian Zi, Arjun Guha, Molly Q
  Feldman, Carolyn Jane Anderson",StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code,,,,,cs.LG cs.HC cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Code LLMs are being rapidly deployed and there is evidence that they can make professional programmers more productive. Current benchmarks for code generation measure whether models generate correct programs given an expert prompt. In this paper, we present a new benchmark containing multiple prompts per problem, written by a specific population of non-expert prompters: beginning programmers. StudentEval contains 1,749 prompts for 48 problems, written by 80 students who have only completed one semester of Python programming. Our students wrote these prompts while working interactively with a Code LLM, and we observed very mixed success rates. We use StudentEval to evaluate 5 Code LLMs and find that StudentEval is a better discriminator of model performance than existing benchmarks. We analyze the prompts and find significant variation in students' prompting techniques. We also find that nondeterministic LLM sampling could mislead students into thinking that their prompts are more (or less) effective than they actually are, which has implications for how to teach with Code LLMs. ","[{'version': 'v1', 'created': 'Wed, 7 Jun 2023 16:03:55 GMT'}]",2023-06-08,"[['Babe', 'Hannah McLean', ''], ['Nguyen', 'Sydney', ''], ['Zi', 'Yangtian', ''], ['Guha', 'Arjun', ''], ['Feldman', 'Molly Q', ''], ['Anderson', 'Carolyn Jane', '']]",0,0,2023-06-07,1,6,3,0,0,0,a4929de687f3c6937dabbf733258af635781d3c4,259095478.0,https://www.semanticscholar.org/paper/a4929de687f3c6937dabbf733258af635781d3c4,arXiv.org,2023.0,27.0,6.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2219267303', 'name': 'Hannah McLean Babe'}, {'authorId': '2181287609', 'name': 'S. Nguyen'}, {'authorId': '2181729017', 'name': 'Yangtian Zi'}, {'authorId': '2100712', 'name': 'Arjun Guha'}, {'authorId': '47637572', 'name': 'Molly Q. Feldman'}, {'authorId': '144901955', 'name': 'Carolyn Jane Anderson'}]","['Wellesley College', 'Oberlin College', 'Northeastern University']","['China', 'United States']",2023-06
2306.04618,Yangyi Chen,"Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Fangyuan Zou,
  Xingyi Cheng, Heng Ji, Zhiyuan Liu, Maosong Sun","Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations",Code is available at \url{https://github.com/lifan-yuan/OOD_NLP},,,,cs.CL cs.CR cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  This paper reexamines the research on out-of-distribution (OOD) robustness in the field of NLP. We find that the distribution shift settings in previous studies commonly lack adequate challenges, hindering the accurate evaluation of OOD robustness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we conduct a series of experiments on pre-trained language models for analysis and evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the relationship between in-distribution (ID) and OOD performance. We identify three typical types that unveil the inner learning mechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and find that, despite exhibiting some effectiveness in specific cases, they do not offer significant improvement compared to vanilla fine-tuning. Further, we evaluate 5 LLMs with various adaptation paradigms and find that when sufficient ID data is available, fine-tuning domain-specific models outperform LLMs on ID examples significantly. However, in the case of OOD instances, prioritizing LLMs with in-context learning yields better results. We identify that both fine-tuned small models and LLMs face challenges in effectively addressing downstream tasks. The code is public at \url{https://github.com/lifan-yuan/OOD_NLP}. ","[{'version': 'v1', 'created': 'Wed, 7 Jun 2023 17:47:03 GMT'}]",2023-06-08,"[['Yuan', 'Lifan', ''], ['Chen', 'Yangyi', ''], ['Cui', 'Ganqu', ''], ['Gao', 'Hongcheng', ''], ['Zou', 'Fangyuan', ''], ['Cheng', 'Xingyi', ''], ['Ji', 'Heng', ''], ['Liu', 'Zhiyuan', ''], ['Sun', 'Maosong', '']]",0,0,2023-06-07,1,9,3,0,0,0,1a55d16c14587edda62dc9c9ff09e0b531dd169c,259096157.0,https://www.semanticscholar.org/paper/1a55d16c14587edda62dc9c9ff09e0b531dd169c,arXiv.org,2023.0,114.0,6.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2152195191', 'name': 'Lifan Yuan'}, {'authorId': '123331686', 'name': 'Yangyi Chen'}, {'authorId': '52297757', 'name': 'Ganqu Cui'}, {'authorId': '2162081759', 'name': 'Hongcheng Gao'}, {'authorId': '2073382741', 'name': 'Fangyuan Zou'}, {'authorId': '26382255', 'name': 'Xingyi Cheng'}, {'authorId': '2072975663', 'name': 'Heng Ji'}, {'authorId': '2141313179', 'name': 'Zhiyuan Liu'}, {'authorId': '1753344', 'name': 'Maosong Sun'}]","['University of Illinois Urbana-Champaign', 'Tsinghua University', 'University of Chinese Academy of Sciences']","['China', 'United States']",2023-06
2306.04811,Che Liu,"Yinda Chen, Che Liu, Wei Huang, Sibo Cheng, Rossella Arcucci, Zhiwei
  Xiong",Generative Text-Guided 3D Vision-Language Pretraining for Unified Medical Image Segmentation,,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Vision-Language Pretraining (VLP) has demonstrated remarkable capabilities in learning visual representations from textual descriptions of images without annotations. Yet, effective VLP demands large-scale image-text pairs, a resource that suffers scarcity in the medical domain. Moreover, conventional VLP is limited to 2D images while medical images encompass diverse modalities, often in 3D, making the learning process more challenging. To address these challenges, we present Generative Text-Guided 3D Vision-Language Pretraining for Unified Medical Image Segmentation (GTGM), a framework that extends of VLP to 3D medical images without relying on paired textual descriptions. Specifically, GTGM utilizes large language models (LLM) to generate medical-style text from 3D medical images. This synthetic text is then used to supervise 3D visual representation learning. Furthermore, a negative-free contrastive learning objective strategy is introduced to cultivate consistent visual representations between augmented 3D medical image patches, which effectively mitigates the biases associated with strict positive-negative sample pairings. We evaluate GTGM on three imaging modalities - Computed Tomography (CT), Magnetic Resonance Imaging (MRI), and electron microscopy (EM) over 13 datasets. GTGM's superior performance across various medical image segmentation tasks underscores its effectiveness and versatility, by enabling VLP extension into 3D medical imagery while bypassing the need for paired text. ","[{'version': 'v1', 'created': 'Wed, 7 Jun 2023 22:20:51 GMT'}]",2023-06-09,"[['Chen', 'Yinda', ''], ['Liu', 'Che', ''], ['Huang', 'Wei', ''], ['Cheng', 'Sibo', ''], ['Arcucci', 'Rossella', ''], ['Xiong', 'Zhiwei', '']]",0,0,2023-06-07,1,6,2,0,0,0,9b11f5e8b40b109cb774e29e5cf5a5baa8beeed8,259108631.0,https://www.semanticscholar.org/paper/9b11f5e8b40b109cb774e29e5cf5a5baa8beeed8,arXiv.org,2023.0,52.0,8.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2183975210', 'name': 'Yin Chen'}, {'authorId': '2116898056', 'name': 'Che Liu'}, {'authorId': '2000111554', 'name': 'Wei Huang'}, {'authorId': '9537080', 'name': 'Sibo Cheng'}, {'authorId': '2058854', 'name': 'Rossella Arcucci'}, {'authorId': '2352456', 'name': 'Zhiwei Xiong'}]","['Imperial College London', 'University of Science and Technology of China', 'Inception Institute of Artificial Intelligence']","['China', 'United Kingdom', 'United Arab Emirates']",2023-06
2306.05032,Jinyang Liu,"Jinyang Liu, Junjie Huang, Yintong Huo, Zhihan Jiang, Jiazhen Gu,
  Zhuangbin Chen, Cong Feng, Minzhi Yan and Michael R. Lyu",Log-based Anomaly Detection based on EVT Theory with feedback,,,,,cs.SE cs.LG,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  System logs play a critical role in maintaining the reliability of software systems. Fruitful studies have explored automatic log-based anomaly detection and achieved notable accuracy on benchmark datasets. However, when applied to large-scale cloud systems, these solutions face limitations due to high resource consumption and lack of adaptability to evolving logs. In this paper, we present an accurate, lightweight, and adaptive log-based anomaly detection framework, referred to as SeaLog. Our method introduces a Trie-based Detection Agent (TDA) that employs a lightweight, dynamically-growing trie structure for real-time anomaly detection. To enhance TDA's accuracy in response to evolving log data, we enable it to receive feedback from experts. Interestingly, our findings suggest that contemporary large language models, such as ChatGPT, can provide feedback with a level of consistency comparable to human experts, which can potentially reduce manual verification efforts. We extensively evaluate SeaLog on two public datasets and an industrial dataset. The results show that SeaLog outperforms all baseline methods in terms of effectiveness, runs 2X to 10X faster and only consumes 5% to 41% of the memory resource. ","[{'version': 'v1', 'created': 'Thu, 8 Jun 2023 08:34:58 GMT'}, {'version': 'v2', 'created': 'Sat, 30 Sep 2023 04:09:55 GMT'}]",2023-10-03,"[['Liu', 'Jinyang', ''], ['Huang', 'Junjie', ''], ['Huo', 'Yintong', ''], ['Jiang', 'Zhihan', ''], ['Gu', 'Jiazhen', ''], ['Chen', 'Zhuangbin', ''], ['Feng', 'Cong', ''], ['Yan', 'Minzhi', ''], ['Lyu', 'Michael R.', '']]",1,1,2023-06-08,2,9,2,1,0,1,4bc275b87cf16c5603a988b2de851b9fa3839aa4,259108762.0,https://www.semanticscholar.org/paper/4bc275b87cf16c5603a988b2de851b9fa3839aa4,,2023.0,41.0,1.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108510525', 'name': 'Jinyang Liu'}, {'authorId': '145505727', 'name': 'Junjie Huang'}, {'authorId': '2036553451', 'name': 'Yintong Huo'}, {'authorId': '47653892', 'name': 'Zhihan Jiang'}, {'authorId': '2216587231', 'name': 'Jia-Yuan Gu'}, {'authorId': '9220842', 'name': 'Zhuangbin Chen'}, {'authorId': '2219751380', 'name': 'Cong Feng'}, {'authorId': '2955814', 'name': 'Minzhi Yan'}, {'authorId': '2217490665', 'name': 'Michael R. Lyu'}]","['Sun Yat-sen University', 'Chinese University of Hong Kong', 'Huawei Technologies (China)']",['China'],2023-06
2306.05033,Nicholas Keeley,Nicholas Gerard Keeley,"Cash, Credibility, and Conversion: The Influence of Synthetic Media on Investment Behavior",34 pages,,,,cs.CY,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Prior to November of 2022, the topic of synthetic media was largely buried within academic journals, constrained to conversations about national security, and often fundamentally misunderstood. The release of ChatGPT, however, has accelerated discourse on the societal impacts of synthetic media. This study first highlights several gaps within existing literature on synthetic media, structuring the impact potential and limitations of synthetic media threats within a theoretical framework. Second, it identifies financial information environments as prime candidates for future disruption via synthetic text modalities, proposing an experimental survey for measuring the influential power of synthetic financial text on global investment communities. Rather than merely assessing the ability of survey participants to distinguish genuine from synthetic text, the experiment contained within this study measures synthetic media influence by observing its ability to manipulate belief via a series of behavioral variables. The results indicate that synthetic text can significantly shift investor sentiment away from what it might otherwise have been under truthful information conditions. Furthermore, synthetic financial text demonstrated a unique ability to ""convert"" investors, inspiring extreme changes in outlook about a company compared to genuine financial texts. This trend should inspire concern within the global financial community, particularly given the historical vulnerability of equity markets to investor sentiment shocks. ","[{'version': 'v1', 'created': 'Thu, 8 Jun 2023 08:35:50 GMT'}]",2023-06-09,"[['Keeley', 'Nicholas Gerard', '']]",1,1,2023-06-08,1,1,1,1,0,1,5cf36b039491324ae72b4892c16f5f4a528cd8cb,259108962.0,https://www.semanticscholar.org/paper/5cf36b039491324ae72b4892c16f5f4a528cd8cb,arXiv.org,2023.0,127.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Business', 'source': 's2-fos-model'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Economics', 'source': 's2-fos-model'}]","[{'authorId': '80100659', 'name': 'N. Keeley'}]",['Tsinghua University'],['China'],2023-06
2306.05064,Cheng Deng,"Cheng Deng, Tianhang Zhang, Zhongmou He, Yi Xu, Qiyuan Chen, Yuanyuan
  Shi, Luoyi Fu, Weinan Zhang, Xinbing Wang, Chenghu Zhou, Zhouhan Lin, Junxian
  He",K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have achieved great success in general domains of natural language processing. In this paper, we bring LLMs to the realm of geoscience with the objective of advancing research and applications in this field. To this end, we present the first-ever LLM in geoscience, K2, alongside a suite of resources developed to further promote LLM research within geoscience. For instance, we have curated the first geoscience instruction tuning dataset, GeoSignal, which aims to align LLM responses to geoscience-related user queries. Additionally, we have established the first geoscience benchmark, GeoBench, to evaluate LLMs in the context of geoscience. In this work, we experiment with a complete recipe to adapt a pre-trained general-domain LLM to the geoscience domain. Specifically, we further train the LLaMA-7B model on 5.5B tokens of geoscience text corpus, including over 1 million pieces of geoscience literature, and utilize GeoSignal's supervised data to fine-tune the model. Moreover, we share a protocol that can efficiently gather domain-specific data and construct domain-supervised data, even in situations where manpower is scarce. Meanwhile, we equip K2 with the abilities of using tools to be a naive geoscience aide. Experiments conducted on the GeoBench demonstrate the effectiveness of our approach and datasets on geoscience knowledge understanding and utilization.We open-source all the training data and K2 model checkpoints at https://github.com/davendw49/k2. ","[{'version': 'v1', 'created': 'Thu, 8 Jun 2023 09:29:05 GMT'}, {'version': 'v2', 'created': 'Wed, 13 Sep 2023 19:33:18 GMT'}]",2023-09-15,"[['Deng', 'Cheng', ''], ['Zhang', 'Tianhang', ''], ['He', 'Zhongmou', ''], ['Xu', 'Yi', ''], ['Chen', 'Qiyuan', ''], ['Shi', 'Yuanyuan', ''], ['Fu', 'Luoyi', ''], ['Zhang', 'Weinan', ''], ['Wang', 'Xinbing', ''], ['Zhou', 'Chenghu', ''], ['Lin', 'Zhouhan', ''], ['He', 'Junxian', '']]",0,0,2023-06-08,2,12,2,1,1,0,32f541216112de78037d8e0f95ddc152eb6f05fa,259108887.0,https://www.semanticscholar.org/paper/32f541216112de78037d8e0f95ddc152eb6f05fa,,2023.0,55.0,1.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Geology', 'source': 's2-fos-model'}]","[{'authorId': '2057947708', 'name': 'Cheng Deng'}, {'authorId': '2041677111', 'name': 'Tianhang Zhang'}, {'authorId': '2219746164', 'name': 'Zhongmou He'}, {'authorId': '2157430536', 'name': 'Qiyuan Chen'}, {'authorId': '1965884588', 'name': 'Yuanyuan Shi'}, {'authorId': '2144764948', 'name': 'Le Zhou'}, {'authorId': '1922573', 'name': 'Luoyi Fu'}, {'authorId': '2108309435', 'name': 'Weinan Zhang'}, {'authorId': '2107937507', 'name': 'Xinbing Wang'}, {'authorId': '2111168706', 'name': 'Cheng Zhou'}, {'authorId': '3146592', 'name': 'Zhouhan Lin'}, {'authorId': '6215698', 'name': 'Junxian He'}]","['Shanghai Jiao Tong University', 'University of Waterloo', 'Chinese Academy of Sciences']","['China', 'Canada']",2023-06
2306.05087,Yidong Wang,"Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao
  Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang,
  Yue Zhang",PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential. However, establishing such a benchmark is not a trivial task due to the challenges associated with evaluation accuracy and privacy protection. In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM's focus extends beyond just the objective correctness of responses, which is the main focus of traditional evaluation datasets. It addresses vital subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. To ensure the reliability of PandaLM, we collect a diverse human-annotated test dataset, where all contexts are generated by humans and labels are aligned with human preferences. Our results indicate that PandaLM-7B achieves 93.75% of GPT-3.5's evaluation ability and 88.28% of GPT-4's in terms of F1-score on our test dataset. PandaLM enables the evaluation of LLM to be fairer but with less cost, evidenced by significant improvements achieved by models tuned through PandaLM compared to their counterparts trained with default Alpaca's hyperparameters. In addition, PandaLM does not depend on API-based evaluations, thus avoiding potential data leakage. All resources of PandaLM are released at https://github.com/WeOpenML/PandaLM. ","[{'version': 'v1', 'created': 'Thu, 8 Jun 2023 10:41:56 GMT'}]",2023-06-09,"[['Wang', 'Yidong', ''], ['Yu', 'Zhuohao', ''], ['Zeng', 'Zhengran', ''], ['Yang', 'Linyi', ''], ['Wang', 'Cunxiang', ''], ['Chen', 'Hao', ''], ['Jiang', 'Chaoya', ''], ['Xie', 'Rui', ''], ['Wang', 'Jindong', ''], ['Xie', 'Xing', ''], ['Ye', 'Wei', ''], ['Zhang', 'Shikun', ''], ['Zhang', 'Yue', '']]",0,1,2023-06-08,1,13,2,3,0,3,ccd94602e3acecf999d0c9ba62b1a8bc02e9f696,259108266.0,https://www.semanticscholar.org/paper/ccd94602e3acecf999d0c9ba62b1a8bc02e9f696,arXiv.org,2023.0,59.0,28.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108024273', 'name': 'Yidong Wang'}, {'authorId': '2164113313', 'name': 'Zhuohao Yu'}, {'authorId': '1557363420', 'name': 'Zhengran Zeng'}, {'authorId': '2145500840', 'name': 'Linyi Yang'}, {'authorId': '35504092', 'name': 'Cunxiang Wang'}, {'authorId': '2051536212', 'name': 'Hao Chen'}, {'authorId': '1657285750', 'name': 'Chaoya Jiang'}, {'authorId': '2143721734', 'name': 'Rui Xie'}, {'authorId': '1519290245', 'name': 'Jindong Wang'}, {'authorId': '1576441343', 'name': 'Xingxu Xie'}, {'authorId': '145235149', 'name': 'Wei Ye'}, {'authorId': '2145403564', 'name': 'Shi-Bo Zhang'}, {'authorId': '2211964951', 'name': 'Yue Zhang'}]","['Peking University', 'Westlake University', 'Microsoft']",['China'],2023-06
2306.05179,Wenxuan Zhang,"Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Yew Ken Chia,
  Lidong Bing","M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models",,,,,cs.CL cs.CV,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Despite the existence of various benchmarks for evaluating natural language processing models, we argue that human exams are a more suitable means of evaluating general intelligence for large language models (LLMs), as they inherently demand a much wider range of abilities such as language understanding, domain knowledge, and problem-solving skills. To this end, we introduce M3Exam, a novel benchmark sourced from real and official human exam questions for evaluating LLMs in a multilingual, multimodal, and multilevel context. M3Exam exhibits three unique characteristics: (1) multilingualism, encompassing questions from multiple countries that require strong multilingual proficiency and cultural knowledge; (2) multimodality, accounting for the multimodal nature of many exam questions to test the model's multimodal understanding capability; and (3) multilevel structure, featuring exams from three critical educational periods to comprehensively assess a model's proficiency at different levels. In total, M3Exam contains 12,317 questions in 9 diverse languages with three educational levels, where about 23\% of the questions require processing images for successful solving. We assess the performance of top-performing LLMs on M3Exam and find that current models, including GPT-4, still struggle with multilingual text, particularly in low-resource and non-Latin script languages. Multimodal LLMs also perform poorly with complex multimodal questions. We believe that M3Exam can be a valuable resource for comprehensively evaluating LLMs by examining their multilingual and multimodal abilities and tracking their development. Data and evaluation code is available at \url{https://github.com/DAMO-NLP-SG/M3Exam}. ","[{'version': 'v1', 'created': 'Thu, 8 Jun 2023 13:21:29 GMT'}]",2023-06-09,"[['Zhang', 'Wenxuan', ''], ['Aljunied', 'Sharifah Mahani', ''], ['Gao', 'Chang', ''], ['Chia', 'Yew Ken', ''], ['Bing', 'Lidong', '']]",0,1,2023-06-08,1,5,2,1,0,1,89689059d0cdcb52d7fbb6007ab953db22936a90,259108959.0,https://www.semanticscholar.org/paper/89689059d0cdcb52d7fbb6007ab953db22936a90,arXiv.org,2023.0,45.0,11.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '150341144', 'name': 'Wenxuan Zhang'}, {'authorId': '30537701', 'name': 'Sharifah Mahani Aljunied'}, {'authorId': '2090142517', 'name': 'Chang Gao'}, {'authorId': '2066312627', 'name': 'Yew Ken Chia'}, {'authorId': '1996394', 'name': 'Lidong Bing'}]","['Alibaba', 'Singapore University of Technology and Design', 'Chinese University of Hong Kong']","['China', 'Singapore']",2023-06
2306.05212,Jiongnan Liu,"Jiongnan Liu, Jiajie Jin, Zihan Wang, Jiehan Cheng, Zhicheng Dou,
  Ji-Rong Wen",RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit,Technical Report for RETA-LLM,,,,cs.IR,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Although Large Language Models (LLMs) have demonstrated extraordinary capabilities in many domains, they still have a tendency to hallucinate and generate fictitious responses to user requests. This problem can be alleviated by augmenting LLMs with information retrieval (IR) systems (also known as retrieval-augmented LLMs). Applying this strategy, LLMs can generate more factual texts in response to user input according to the relevant content retrieved by IR systems from external corpora as references. In addition, by incorporating external knowledge, retrieval-augmented LLMs can answer in-domain questions that cannot be answered by solely relying on the world knowledge stored in parameters. To support research in this area and facilitate the development of retrieval-augmented LLM systems, we develop RETA-LLM, a {RET}reival-{A}ugmented LLM toolkit. In RETA-LLM, we create a complete pipeline to help researchers and users build their customized in-domain LLM-based systems. Compared with previous retrieval-augmented LLM systems, RETA-LLM provides more plug-and-play modules to support better interaction between IR systems and LLMs, including {request rewriting, document retrieval, passage extraction, answer generation, and fact checking} modules. Our toolkit is publicly available at https://github.com/RUC-GSAI/YuLan-IR/tree/main/RETA-LLM. ","[{'version': 'v1', 'created': 'Thu, 8 Jun 2023 14:10:54 GMT'}]",2023-06-09,"[['Liu', 'Jiongnan', ''], ['Jin', 'Jiajie', ''], ['Wang', 'Zihan', ''], ['Cheng', 'Jiehan', ''], ['Dou', 'Zhicheng', ''], ['Wen', 'Ji-Rong', '']]",0,0,2023-06-08,1,6,1,0,0,0,cc78babfacce48e715dac56886d7dd9746cfcab0,259108339.0,https://www.semanticscholar.org/paper/cc78babfacce48e715dac56886d7dd9746cfcab0,arXiv.org,2023.0,25.0,10.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1830383266', 'name': 'Jiongnan Liu'}, {'authorId': '4376097', 'name': 'Jiajie Jin'}, {'authorId': '2243360876', 'name': 'Zihan Wang'}, {'authorId': '2219726925', 'name': 'Jiehan Cheng'}, {'authorId': '1897235', 'name': 'Zhicheng Dou'}, {'authorId': '153693432', 'name': 'Ji-rong Wen'}]","['University of Science and Technology of China', 'Renmin University of China']",['China'],2023-06
2306.05301,Qiaoyu Tang,"Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi
  Cao, Le Sun",ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Enabling large language models to utilize real-world tools effectively is crucial for achieving embodied intelligence. Existing approaches to tool learning have either primarily relied on extremely large language models, such as GPT-4, to attain generalized tool-use abilities in a zero-shot manner, or utilized supervised learning to train limited scopes of tools on compact models. However, it remains uncertain whether smaller language models can achieve generalized tool-use abilities without tool-specific training. To address this question, this paper introduces ToolAlpaca, a novel framework designed to automatically generate a diverse tool-use corpus and learn generalized tool-use abilities on compact language models with minimal human intervention. Specifically, ToolAlpaca first automatically creates a highly diversified tool-use corpus by building a multi-agent simulation environment. The corpus contains 3938 tool-use instances from more than 400 real-world tool APIs spanning 50 distinct categories. Subsequently, the constructed corpus is employed to fine-tune compact language models, resulting in two models, namely ToolAlpaca-7B and ToolAlpaca-13B, respectively. Finally, we evaluate the ability of these models to utilize previously unseen tools without specific training. Experimental results demonstrate that ToolAlpaca achieves effective generalized tool-use capabilities comparable to those of extremely large language models like GPT-3.5, demonstrating that learning generalized tool-use ability is feasible for compact language models. ","[{'version': 'v1', 'created': 'Thu, 8 Jun 2023 15:46:32 GMT'}, {'version': 'v2', 'created': 'Thu, 7 Sep 2023 12:20:45 GMT'}]",2023-09-08,"[['Tang', 'Qiaoyu', ''], ['Deng', 'Ziliang', ''], ['Lin', 'Hongyu', ''], ['Han', 'Xianpei', ''], ['Liang', 'Qiao', ''], ['Cao', 'Boxi', ''], ['Sun', 'Le', '']]",0,1,2023-06-08,2,7,1,2,0,2,455866ca838f356b53a7e3e5b344834f9e93dbbc,259108190.0,https://www.semanticscholar.org/paper/455866ca838f356b53a7e3e5b344834f9e93dbbc,arXiv.org,2023.0,31.0,14.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2217344770', 'name': 'Qiaoyu Tang'}, {'authorId': '34461180', 'name': 'Ziliang Deng'}, {'authorId': '2116455765', 'name': 'Hongyu Lin'}, {'authorId': '2118233348', 'name': 'Xianpei Han'}, {'authorId': '2219702574', 'name': 'Qiao Liang'}, {'authorId': '2110832778', 'name': 'Le Sun'}]","['University of Chinese Academy of Sciences', 'Chinese Academy of Sciences', 'Chinese Information Processing Laboratory']",['China'],2023-06
2306.05443,Qianqian Xie,"Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng,
  Alejandro Lopez-Lira, Jimin Huang","PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance","12 pages, 1 figures",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although large language models (LLMs) has shown great performance on natural language processing (NLP) in the financial domain, there are no publicly available financial tailtored LLMs, instruction tuning datasets, and evaluation benchmarks, which is critical for continually pushing forward the open-source development of financial artificial intelligence (AI). This paper introduces PIXIU, a comprehensive framework including the first financial LLM based on fine-tuning LLaMA with instruction data, the first instruction data with 136K data samples to support the fine-tuning, and an evaluation benchmark with 5 tasks and 9 datasets. We first construct the large-scale multi-task instruction data considering a variety of financial tasks, financial document types, and financial data modalities. We then propose a financial LLM called FinMA by fine-tuning LLaMA with the constructed dataset to be able to follow instructions for various financial tasks. To support the evaluation of financial LLMs, we propose a standardized benchmark that covers a set of critical financial tasks, including five financial NLP tasks and one financial prediction task. With this benchmark, we conduct a detailed analysis of FinMA and several existing LLMs, uncovering their strengths and weaknesses in handling critical financial tasks. The model, datasets, benchmark, and experimental results are open-sourced to facilitate future research in financial AI. ","[{'version': 'v1', 'created': 'Thu, 8 Jun 2023 14:20:29 GMT'}]",2023-06-12,"[['Xie', 'Qianqian', ''], ['Han', 'Weiguang', ''], ['Zhang', 'Xiao', ''], ['Lai', 'Yanzhao', ''], ['Peng', 'Min', ''], ['Lopez-Lira', 'Alejandro', ''], ['Huang', 'Jimin', '']]",0,0,2023-06-08,1,7,2,1,1,0,109929be7890ef982fb3b6be0d78609cfab1ea13,259129602.0,https://www.semanticscholar.org/paper/109929be7890ef982fb3b6be0d78609cfab1ea13,arXiv.org,2023.0,38.0,19.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '145229872', 'name': 'Qianqian Xie'}, {'authorId': '104843747', 'name': 'Weiguang Han'}, {'authorId': '2177329782', 'name': 'Xiao Zhang'}, {'authorId': '2202405535', 'name': 'Yanzhao Lai'}, {'authorId': '47278503', 'name': 'Min Peng'}, {'authorId': '1411351217', 'name': 'Alejandro Lopez-Lira'}, {'authorId': '2555230', 'name': 'Jimin Huang'}]","['University of Florida', 'Wuhan University', 'Sun Yat-sen University', 'ChanceFocus AMC. Shanghai, China', 'Southwest Jiaotong University']","['China', 'United States']",2023-06
2306.05499,Yi Liu,"Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang
  Liu, Haoyu Wang, Yan Zheng and Yang Liu",Prompt Injection attack against LLM-integrated Applications,,,,,cs.CR cs.AI cs.CL cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs), renowned for their superior proficiency in language comprehension and generation, stimulate a vibrant ecosystem of applications around them. However, their extensive assimilation into various services introduces significant security risks. This study deconstructs the complexities and implications of prompt injection attacks on actual LLM-integrated applications. Initially, we conduct an exploratory analysis on ten commercial applications, highlighting the constraints of current attack strategies in practice. Prompted by these limitations, we subsequently formulate HouYi, a novel black-box prompt injection attack technique, which draws inspiration from traditional web injection attacks. HouYi is compartmentalized into three crucial elements: a seamlessly-incorporated pre-constructed prompt, an injection prompt inducing context partition, and a malicious payload designed to fulfill the attack objectives. Leveraging HouYi, we unveil previously unknown and severe attack outcomes, such as unrestricted arbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi on 36 actual LLM-integrated applications and discern 31 applications susceptible to prompt injection. 10 vendors have validated our discoveries, including Notion, which has the potential to impact millions of users. Our investigation illuminates both the possible risks of prompt injection attacks and the possible tactics for mitigation. ","[{'version': 'v1', 'created': 'Thu, 8 Jun 2023 18:43:11 GMT'}]",2023-06-12,"[['Liu', 'Yi', ''], ['Deng', 'Gelei', ''], ['Li', 'Yuekang', ''], ['Wang', 'Kailong', ''], ['Zhang', 'Tianwei', ''], ['Liu', 'Yepang', ''], ['Wang', 'Haoyu', ''], ['Zheng', 'Yan', ''], ['Liu', 'Yang', '']]",0,0,2023-06-08,1,9,4,0,0,0,db4cf9f6a653d5c15973e836c800ea47743251ae,259129807.0,https://www.semanticscholar.org/paper/db4cf9f6a653d5c15973e836c800ea47743251ae,arXiv.org,2023.0,55.0,29.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2153627626', 'name': 'Yi Liu'}, {'authorId': '73776889', 'name': 'Gelei Deng'}, {'authorId': '22799258', 'name': 'Yuekang Li'}, {'authorId': '3088630', 'name': 'Kailong Wang'}, {'authorId': '2146331573', 'name': 'Tianwei Zhang'}, {'authorId': '39584070', 'name': 'Yepang Liu'}, {'authorId': '51225422', 'name': 'Haoyu Wang'}, {'authorId': '2124949853', 'name': 'Yanhong Zheng'}, {'authorId': '2152798056', 'name': 'Yang Liu'}]","['UNSW Sydney', 'Southern University of Science and Technology', 'Nanyang Technological University', 'Tianjin University', 'Huazhong University of Science and Technology']","['China', 'Singapore', 'Australia']",2023-06
2306.05642,Tong Zhang PhD,"Bang Yang, Asif Raza, Yuexian Zou, Tong Zhang",Customizing General-Purpose Foundation Models for Medical Report Generation,"14 pages, 3 figures",,,,cs.CV cs.AI cs.CL cs.IR,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Medical caption prediction which can be regarded as a task of medical report generation (MRG), requires the automatic generation of coherent and accurate captions for the given medical images. However, the scarcity of labelled medical image-report pairs presents great challenges in the development of deep and large-scale neural networks capable of harnessing the potential artificial general intelligence power like large language models (LLMs). In this work, we propose customizing off-the-shelf general-purpose large-scale pre-trained models, i.e., foundation models (FMs), in computer vision and natural language processing with a specific focus on medical report generation. Specifically, following BLIP-2, a state-of-the-art vision-language pre-training approach, we introduce our encoder-decoder-based MRG model. This model utilizes a lightweight query Transformer to connect two FMs: the giant vision Transformer EVA-ViT-g and a bilingual LLM trained to align with human intentions (referred to as ChatGLM-6B). Furthermore, we conduct ablative experiments on the trainable components of the model to identify the crucial factors for effective transfer learning. Our findings demonstrate that unfreezing EVA-ViT-g to learn medical image representations, followed by parameter-efficient training of ChatGLM-6B to capture the writing styles of medical reports, is essential for achieving optimal results. Our best attempt (PCLmed Team) achieved the 4th and the 2nd, respectively, out of 13 participating teams, based on the BERTScore and ROUGE-1 metrics, in the ImageCLEFmedical Caption 2023 Caption Prediction Task competition. ","[{'version': 'v1', 'created': 'Fri, 9 Jun 2023 03:02:36 GMT'}]",2023-06-12,"[['Yang', 'Bang', ''], ['Raza', 'Asif', ''], ['Zou', 'Yuexian', ''], ['Zhang', 'Tong', '']]",0,0,2023-06-09,1,4,4,1,1,0,2933acb28b7369c7ea5b8728f6d8cb55e1beef98,259129379.0,https://www.semanticscholar.org/paper/2933acb28b7369c7ea5b8728f6d8cb55e1beef98,arXiv.org,2023.0,68.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2115355581', 'name': 'Bang Yang'}, {'authorId': '2057543075', 'name': 'Asif Raza'}, {'authorId': '26981150', 'name': 'Yuexian Zou'}, {'authorId': '2146324900', 'name': 'Tong Zhang'}]","['Peking University', 'Peng Cheng Laboratory']",['China'],2023-06
2306.05783,Zhouhong Gu,"Zhouhong Gu, Xiaoxuan Zhu, Haoning Ye, Lin Zhang, Jianchen Wang,
  Sihang Jiang, Zhuozhi Xiong, Zihan Li, Qianyu He, Rui Xu, Wenhao Huang, Zili
  Wang, Shusen Wang, Weiguo Zheng, Hongwei Feng, Yanghua Xiao",Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation,Under review of NeurIPS 2023,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  New Natural Langauge Process~(NLP) benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present Xiezhi, the most comprehensive evaluation suite designed to assess holistic domain knowledge. Xiezhi comprises multiple-choice questions across 516 diverse disciplines ranging from 13 different subjects with 249,587 questions and accompanied by Xiezhi-Specialty and Xiezhi-Interdiscipline, both with 15k questions. We conduct evaluation of the 47 cutting-edge LLMs on Xiezhi. Results indicate that LLMs exceed average performance of humans in science, engineering, agronomy, medicine, and art, but fall short in economics, jurisprudence, pedagogy, literature, history, and management. We anticipate Xiezhi will help analyze important strengths and shortcomings of LLMs, and the benchmark is released in~\url{https://github.com/MikeGu721/XiezhiBenchmark}. ","[{'version': 'v1', 'created': 'Fri, 9 Jun 2023 09:52:05 GMT'}, {'version': 'v2', 'created': 'Thu, 15 Jun 2023 06:51:40 GMT'}]",2023-06-16,"[['Gu', 'Zhouhong', ''], ['Zhu', 'Xiaoxuan', ''], ['Ye', 'Haoning', ''], ['Zhang', 'Lin', ''], ['Wang', 'Jianchen', ''], ['Jiang', 'Sihang', ''], ['Xiong', 'Zhuozhi', ''], ['Li', 'Zihan', ''], ['He', 'Qianyu', ''], ['Xu', 'Rui', ''], ['Huang', 'Wenhao', ''], ['Wang', 'Zili', ''], ['Wang', 'Shusen', ''], ['Zheng', 'Weiguo', ''], ['Feng', 'Hongwei', ''], ['Xiao', 'Yanghua', '']]",0,0,2023-06-09,2,16,1,0,0,0,eda08c6f5919f39979acf0b3bc52e903063b5ba4,259129613.0,https://www.semanticscholar.org/paper/eda08c6f5919f39979acf0b3bc52e903063b5ba4,arXiv.org,2023.0,61.0,10.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '2160631240', 'name': 'Zhouhong Gu'}, {'authorId': '2215265340', 'name': 'Xiaoxuan Zhu'}, {'authorId': '2215246086', 'name': 'Haoning Ye'}, {'authorId': '2143838757', 'name': 'Lin Zhang'}, {'authorId': '2219864610', 'name': 'Jianchen Wang'}, {'authorId': '1999030240', 'name': 'Sihang Jiang'}, {'authorId': '2215212876', 'name': 'Zhuozhi Xiong'}, {'authorId': '2118274188', 'name': 'Zihan Li'}, {'authorId': '2152880833', 'name': 'Qi He'}, {'authorId': '2115803996', 'name': 'Rui Xu'}, {'authorId': '2158103505', 'name': 'Wenhao Huang'}, {'authorId': '1807168', 'name': 'Weiguo Zheng'}, {'authorId': '27155736', 'name': 'Hongwei Feng'}, {'authorId': '2116642640', 'name': 'Yanghua Xiao'}]","['Joint Research Center', 'Fudan University']","['China', 'Spain']",2023-06
2306.05817,Jianghao Lin,"Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li,
  Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming Tang, Weinan Zhang",How Can Recommender Systems Benefit from Large Language Models: A Survey,15 pages; 3 figures; summarization table in appendix,,,,cs.IR cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recommender systems (RS) play important roles to match users' information needs for Internet applications. In natural language processing (NLP) domains, large language model (LLM) has shown astonishing emergent abilities (e.g., instruction following, reasoning), thus giving rise to the promising research direction of adapting LLM to RS for performance enhancements and user experience improvements. In this paper, we conduct a comprehensive survey on this research direction from an application-oriented view. We first summarize existing research works from two orthogonal perspectives: where and how to adapt LLM to RS. For the ""WHERE"" question, we discuss the roles that LLM could play in different stages of the recommendation pipeline, i.e., feature engineering, feature encoder, scoring/ranking function, and pipeline controller. For the ""HOW"" question, we investigate the training and inference strategies, resulting in two fine-grained taxonomy criteria, i.e., whether to tune LLMs or not, and whether to involve conventional recommendation model (CRM) for inference. Detailed analysis and general development trajectories are provided for both questions, respectively. Then, we highlight key challenges in adapting LLM to RS from three aspects, i.e., efficiency, effectiveness, and ethics. Finally, we summarize the survey and discuss the future prospects. We also actively maintain a GitHub repository for papers and other related resources in this rising direction: https://github.com/CHIANGEL/Awesome-LLM-for-RecSys. ","[{'version': 'v1', 'created': 'Fri, 9 Jun 2023 11:31:50 GMT'}, {'version': 'v2', 'created': 'Mon, 12 Jun 2023 04:11:14 GMT'}, {'version': 'v3', 'created': 'Tue, 27 Jun 2023 06:03:57 GMT'}, {'version': 'v4', 'created': 'Wed, 28 Jun 2023 01:59:11 GMT'}]",2023-06-29,"[['Lin', 'Jianghao', ''], ['Dai', 'Xinyi', ''], ['Xi', 'Yunjia', ''], ['Liu', 'Weiwen', ''], ['Chen', 'Bo', ''], ['Li', 'Xiangyang', ''], ['Zhu', 'Chenxu', ''], ['Guo', 'Huifeng', ''], ['Yu', 'Yong', ''], ['Tang', 'Ruiming', ''], ['Zhang', 'Weinan', '']]",0,0,2023-06-09,4,11,2,0,0,0,bac54736112098616f0e1c90435888ef3e119d32,259129651.0,https://www.semanticscholar.org/paper/bac54736112098616f0e1c90435888ef3e119d32,arXiv.org,2023.0,87.0,25.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2144908858', 'name': 'Jianghao Lin'}, {'authorId': '2105646417', 'name': 'Xinyi Dai'}, {'authorId': '2056826850', 'name': 'Yunjia Xi'}, {'authorId': '2130051800', 'name': 'Weiwen Liu'}, {'authorId': '92633145', 'name': 'Bo Chen'}, {'authorId': '2181637944', 'name': 'Xiangyang Li'}, {'authorId': '2115802321', 'name': 'Chenxu Zhu'}, {'authorId': '3339005', 'name': 'Huifeng Guo'}, {'authorId': '2156098229', 'name': 'Yong Yu'}, {'authorId': '2824766', 'name': 'Ruiming Tang'}, {'authorId': '2108309275', 'name': 'Weinan Zhang'}]","['Shanghai Jiao Tong University', 'Huawei Technologies (China)']",['China'],2023-06
2306.06211,Chaoning Zhang,"Chaoning Zhang, Fachrina Dewi Puspitasari, Sheng Zheng, Chenghao Li,
  Yu Qiao, Taegoo Kang, Xinru Shan, Chenshuang Zhang, Caiyan Qin, Francois
  Rameau, Lik-Hang Lee, Sung-Ho Bae, Choong Seon Hong",A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering,"First survey on Segment Anything Model (SAM), work under progress",,,,cs.CV,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Segment anything model (SAM) developed by Meta AI Research has recently attracted significant attention. Trained on a large segmentation dataset of over 1 billion masks, SAM is capable of segmenting any object on a certain image. In the original SAM work, the authors turned to zero-short transfer tasks (like edge detection) for evaluating the performance of SAM. Recently, numerous works have attempted to investigate the performance of SAM in various scenarios to recognize and segment objects. Moreover, numerous projects have emerged to show the versatility of SAM as a foundation model by combining it with other models, like Grounding DINO, Stable Diffusion, ChatGPT, etc. With the relevant papers and projects increasing exponentially, it is challenging for the readers to catch up with the development of SAM. To this end, this work conducts the first yet comprehensive survey on SAM. This is an ongoing project and we intend to update the manuscript on a regular basis. Therefore, readers are welcome to contact us if they complete new works related to SAM so that we can include them in our next version. ","[{'version': 'v1', 'created': 'Fri, 12 May 2023 07:21:59 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Jun 2023 01:12:29 GMT'}, {'version': 'v3', 'created': 'Mon, 3 Jul 2023 08:35:14 GMT'}]",2023-07-04,"[['Zhang', 'Chaoning', ''], ['Puspitasari', 'Fachrina Dewi', ''], ['Zheng', 'Sheng', ''], ['Li', 'Chenghao', ''], ['Qiao', 'Yu', ''], ['Kang', 'Taegoo', ''], ['Shan', 'Xinru', ''], ['Zhang', 'Chenshuang', ''], ['Qin', 'Caiyan', ''], ['Rameau', 'Francois', ''], ['Lee', 'Lik-Hang', ''], ['Bae', 'Sung-Ho', ''], ['Hong', 'Choong Seon', '']]",1,1,2023-05-12,3,13,1,1,0,1,42219b26a503d03bf70e9953edc3af94c255cb2a,259138732.0,https://www.semanticscholar.org/paper/42219b26a503d03bf70e9953edc3af94c255cb2a,arXiv.org,2023.0,150.0,19.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '31044159', 'name': 'Chaoning Zhang'}, {'authorId': '2211960167', 'name': 'Sheng Zheng'}, {'authorId': '2144124301', 'name': 'Chenghao Li'}, {'authorId': '2209185953', 'name': 'Yu Qiao'}, {'authorId': '2213065322', 'name': 'Taegoo Kang'}, {'authorId': '2036753387', 'name': 'Xinru Shan'}, {'authorId': '48934876', 'name': 'Chenshuang Zhang'}, {'authorId': '2219915744', 'name': 'Caiyan Qin'}, {'authorId': '2771818', 'name': 'Franois Rameau'}, {'authorId': '40547898', 'name': 'S. Bae'}, {'authorId': '2159807650', 'name': 'Choong-Seon Hong'}]","['Beijing Institute of Technology', 'Harbin Institute of Technology', 'Microsoft', 'Hong Kong Polytechnic University', 'Kyung Hee University']","['China', 'United States', 'South Korea', 'Hong Kong']",2023-05
2306.06427,Qiushi Sun,"Jianing Wang, Qiushi Sun, Nuo Chen, Xiang Li, Ming Gao",Boosting Language Models Reasoning with Chain-of-Knowledge Prompting,Work in progress,,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks. ","[{'version': 'v1', 'created': 'Sat, 10 Jun 2023 12:42:36 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Jul 2023 08:47:14 GMT'}]",2023-07-21,"[['Wang', 'Jianing', ''], ['Sun', 'Qiushi', ''], ['Chen', 'Nuo', ''], ['Li', 'Xiang', ''], ['Gao', 'Ming', '']]",0,0,2023-06-10,2,5,1,0,0,0,9efa81ec4954b0859c47dad8f42edfaf8bced69b,259138909.0,https://www.semanticscholar.org/paper/9efa81ec4954b0859c47dad8f42edfaf8bced69b,arXiv.org,2023.0,106.0,11.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '46584777', 'name': 'J. Wang'}, {'authorId': '2112455065', 'name': 'Qiushi Sun'}, {'authorId': '119895609', 'name': 'Nuo Chen'}, {'authorId': '1737850', 'name': 'Xiang Lorraine Li'}, {'authorId': '2147415336', 'name': 'Ming Gao'}]","['National University of Singapore', 'East China Normal University']","['China', 'Singapore']",2023-06
2306.06615,Jiatong Li,"Jiatong Li, Yunqing Liu, Wenqi Fan, Xiao-Yong Wei, Hui Liu, Jiliang
  Tang, and Qing Li",Empowering Molecule Discovery for Molecule-Caption Translation with Large Language Models: A ChatGPT Perspective,"Our implementation is available at:
  https://github.com/phenixace/MolReGPT",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Molecule discovery plays a crucial role in various scientific fields, advancing the design of tailored materials and drugs. Traditional methods for molecule discovery follow a trial-and-error process, which are both time-consuming and costly, while computational approaches such as artificial intelligence (AI) have emerged as revolutionary tools to expedite various tasks, like molecule-caption translation. Despite the importance of molecule-caption translation for molecule discovery, most of the existing methods heavily rely on domain experts, require excessive computational cost, and suffer from poor performance. On the other hand, Large Language Models (LLMs), like ChatGPT, have shown remarkable performance in various cross-modal tasks due to their great powerful capabilities in natural language understanding, generalization, and reasoning, which provides unprecedented opportunities to advance molecule discovery. To address the above limitations, in this work, we propose a novel LLMs-based framework (\textbf{MolReGPT}) for molecule-caption translation, where a retrieval-based prompt paradigm is introduced to empower molecule discovery with LLMs like ChatGPT without fine-tuning. More specifically, MolReGPT leverages the principle of molecular similarity to retrieve similar molecules and their text descriptions from a local database to ground the generation of LLMs through in-context few-shot molecule learning. We evaluate the effectiveness of MolReGPT via molecule-caption translation, which includes molecule understanding and text-based molecule generation. Experimental results show that MolReGPT outperforms fine-tuned models like MolT5-base without any additional training. To the best of our knowledge, MolReGPT is the first work to leverage LLMs in molecule-caption translation for advancing molecule discovery. ","[{'version': 'v1', 'created': 'Sun, 11 Jun 2023 08:16:25 GMT'}]",2023-06-13,"[['Li', 'Jiatong', ''], ['Liu', 'Yunqing', ''], ['Fan', 'Wenqi', ''], ['Wei', 'Xiao-Yong', ''], ['Liu', 'Hui', ''], ['Tang', 'Jiliang', ''], ['Li', 'Qing', '']]",1,1,2023-06-11,1,7,2,1,0,1,073e4f0c3a66b7557abd053301b5104cdc582636,259137456.0,https://www.semanticscholar.org/paper/073e4f0c3a66b7557abd053301b5104cdc582636,arXiv.org,2023.0,65.0,15.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2109018826', 'name': 'Jiatong Li'}, {'authorId': '2208630682', 'name': 'Yunqing Liu'}, {'authorId': '41031455', 'name': 'Wenqi Fan'}, {'authorId': '2115493866', 'name': 'Xiao Wei'}, {'authorId': '2146672392', 'name': 'Hui Liu'}, {'authorId': '1736632', 'name': 'Jiliang Tang'}, {'authorId': '2117897052', 'name': 'Qing Li'}]","['Michigan State University', 'Sichuan University', 'Hong Kong Polytechnic University']","['China', 'United States', 'Hong Kong']",2023-06
2306.06624,Yifan Song,"Yifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song,
  Hailiang Huang, Cheng Li, Ke Wang, Rong Yao, Ye Tian, Sujian Li",RestGPT: Connecting Large Language Models with Real-World RESTful APIs,Add RestBench to evaluate RestGPT,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Tool-augmented large language models (LLMs) have achieved remarkable progress in tackling a broad range of tasks. However, existing methods are mainly restricted to specifically designed tools and fail to fulfill complex instructions, having great limitations when confronted with real-world scenarios. In this paper, we explore a more realistic scenario by connecting LLMs with RESTful APIs, which adhere to the widely adopted REST software architectural style for web service development. To address the practical challenges of tackling complex instructions, we propose RestGPT, which exploits the power of LLMs and conducts a coarse-to-fine online planning mechanism to enhance the abilities of task decomposition and API selection. RestGPT also contains an API executor tailored for calling RESTful APIs, which can meticulously formulate parameters and parse API responses. To fully evaluate the performance of RestGPT, we propose RestBench, a high-quality benchmark which consists of two real-world scenarios and human-annotated instructions with gold solution paths. Experiments show that RestGPT is able to achieve impressive results in complex tasks and has strong robustness, which paves a new way towards AGI. RestGPT and RestBench is publicly available at https://restgpt.github.io/. ","[{'version': 'v1', 'created': 'Sun, 11 Jun 2023 08:53:12 GMT'}, {'version': 'v2', 'created': 'Sun, 27 Aug 2023 02:55:36 GMT'}]",2023-08-29,"[['Song', 'Yifan', ''], ['Xiong', 'Weimin', ''], ['Zhu', 'Dawei', ''], ['Wu', 'Wenhao', ''], ['Qian', 'Han', ''], ['Song', 'Mingbo', ''], ['Huang', 'Hailiang', ''], ['Li', 'Cheng', ''], ['Wang', 'Ke', ''], ['Yao', 'Rong', ''], ['Tian', 'Ye', ''], ['Li', 'Sujian', '']]",0,1,2023-06-11,2,12,1,0,0,0,adae495ceae5399f581aa7755142786369abac17,261241602.0,https://www.semanticscholar.org/paper/adae495ceae5399f581aa7755142786369abac17,,2023.0,25.0,3.0,1.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2183730942', 'name': 'Yifan Song'}, {'authorId': '2211953037', 'name': 'Weimin Xiong'}, {'authorId': '2235570177', 'name': 'Dawei Zhu'}, {'authorId': '2139644141', 'name': 'Wenhao Wu'}, {'authorId': '2234324238', 'name': 'Han Qian'}, {'authorId': '2219216093', 'name': 'Mingbo Song'}, {'authorId': '2234851134', 'name': 'Hailiang Huang'}, {'authorId': '2155795167', 'name': 'Chengzu Li'}, {'authorId': '2235070752', 'name': 'Ke Wang'}, {'authorId': '2234325474', 'name': 'Rong Yao'}, {'authorId': '2219970494', 'name': 'Ye Tian'}, {'authorId': '48831399', 'name': 'Sujian Li'}]","['Peking University', 'Huawei Technologies (China)']",['China'],2023-06
2306.06687,Zhenfei Yin,"Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai
  Li, Lu Sheng, Lei Bai, Xiaoshui Huang, Zhiyong Wang, Jing Shao, Wanli Ouyang","LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark","37 pages, 33 figures. Code available at
  https://github.com/OpenLAMM/LAMM ; Project page: https://openlamm.github.io/",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models have become a potential pathway toward achieving artificial general intelligence. Recent works on multi-modal large language models have demonstrated their effectiveness in handling visual modalities. In this work, we extend the research of MLLMs to point clouds and present the LAMM-Dataset and LAMM-Benchmark for 2D image and 3D point cloud understanding. We also establish an extensible framework to facilitate the extension of MLLMs to additional modalities. Our main contribution is three-fold: 1) We present the LAMM-Dataset and LAMM-Benchmark, which cover almost all high-level vision tasks for 2D and 3D vision. Extensive experiments validate the effectiveness of our dataset and benchmark. 2) We demonstrate the detailed methods of constructing instruction-tuning datasets and benchmarks for MLLMs, which will enable future research on MLLMs to scale up and extend to other domains, tasks, and modalities faster. 3) We provide a primary but potential MLLM training framework optimized for modalities' extension. We also provide baseline models, comprehensive experimental observations, and analysis to accelerate future research. Codes and datasets are now available at https://github.com/OpenLAMM/LAMM. ","[{'version': 'v1', 'created': 'Sun, 11 Jun 2023 14:01:17 GMT'}, {'version': 'v2', 'created': 'Sun, 18 Jun 2023 13:15:47 GMT'}]",2023-06-21,"[['Yin', 'Zhenfei', ''], ['Wang', 'Jiong', ''], ['Cao', 'Jianjian', ''], ['Shi', 'Zhelun', ''], ['Liu', 'Dingning', ''], ['Li', 'Mukai', ''], ['Sheng', 'Lu', ''], ['Bai', 'Lei', ''], ['Huang', 'Xiaoshui', ''], ['Wang', 'Zhiyong', ''], ['Shao', 'Jing', ''], ['Ouyang', 'Wanli', '']]",0,0,2023-06-11,2,12,1,0,0,0,fd755dc7b5b206c17fd953db04e1c888d45b6e4e,259138958.0,https://www.semanticscholar.org/paper/fd755dc7b5b206c17fd953db04e1c888d45b6e4e,arXiv.org,2023.0,99.0,21.0,6.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '13050405', 'name': 'Zhen-fei Yin'}, {'authorId': '2110170885', 'name': 'Jiong Wang'}, {'authorId': '2115872322', 'name': 'Jianjian Cao'}, {'authorId': '2144146362', 'name': 'Zhelun Shi'}, {'authorId': '2115296810', 'name': 'Dingning Liu'}, {'authorId': '2027599235', 'name': 'Mukai Li'}, {'authorId': '37145669', 'name': 'Lu Sheng'}, {'authorId': '50010487', 'name': 'Lei Bai'}, {'authorId': '2116084167', 'name': 'Xiaoshui Huang'}, {'authorId': '2184760304', 'name': 'Zhiyong Wang'}, {'authorId': '3001348', 'name': 'Wanli Ouyang'}, {'authorId': '1388486428', 'name': 'Jing Shao'}]","['University of Sydney', 'Shanghai Artificial Intelligence Laboratory', 'Beihang University', 'Dalian University of Technology', 'Fudan University']","['China', 'Australia']",2023-06
2306.07154,Jiale Xu,"Jiale Xu, Xintao Wang, Yan-Pei Cao, Weihao Cheng, Ying Shan, Shenghua
  Gao",InstructP2P: Learning to Edit 3D Point Clouds with Text Instructions,,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Enhancing AI systems to perform tasks following human instructions can significantly boost productivity. In this paper, we present InstructP2P, an end-to-end framework for 3D shape editing on point clouds, guided by high-level textual instructions. InstructP2P extends the capabilities of existing methods by synergizing the strengths of a text-conditioned point cloud diffusion model, Point-E, and powerful language models, enabling color and geometry editing using language instructions. To train InstructP2P, we introduce a new shape editing dataset, constructed by integrating a shape segmentation dataset, off-the-shelf shape programs, and diverse edit instructions generated by a large language model, ChatGPT. Our proposed method allows for editing both color and geometry of specific regions in a single forward pass, while leaving other regions unaffected. In our experiments, InstructP2P shows generalization capabilities, adapting to novel shape categories and instructions, despite being trained on a limited amount of data. ","[{'version': 'v1', 'created': 'Mon, 12 Jun 2023 14:42:23 GMT'}]",2023-06-13,"[['Xu', 'Jiale', ''], ['Wang', 'Xintao', ''], ['Cao', 'Yan-Pei', ''], ['Cheng', 'Weihao', ''], ['Shan', 'Ying', ''], ['Gao', 'Shenghua', '']]",1,1,2023-06-12,1,6,1,1,0,1,d0b04635d7c9602a1e8fd0b2df3a61bd68954f0b,259137588.0,https://www.semanticscholar.org/paper/d0b04635d7c9602a1e8fd0b2df3a61bd68954f0b,arXiv.org,2023.0,70.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2125239096', 'name': 'Jiale Xu'}, {'authorId': '47119707', 'name': 'Xintao Wang'}, {'authorId': '2125469538', 'name': 'Yannan Cao'}, {'authorId': '10405156', 'name': 'Weihao Cheng'}, {'authorId': '2187307826', 'name': 'Ying Shan'}, {'authorId': '1702868', 'name': 'Shenghua Gao'}]","['Shanghai Engineering Research Center of Intelligent Vision and Imaging', 'Tencent', 'Shanghai Engineering Research Center of Energy Efficient and Custom AI IC', 'ShanghaiTech University']",['China'],2023-06
2306.07207,Ziwang Zhao,"Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng
  Lu, Tao Wang, Zhongyu Wei",Valley: Video Assistant with Large Language model Enhanced abilitY,,,,,cs.CV cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, several multi-modal models have been developed for joint image and language understanding, which have demonstrated impressive chat abilities by utilizing advanced large language models (LLMs). The process of developing such models is straightforward yet effective. It involves pre-training an adaptation module to align the semantics of the vision encoder and language model, followed by fine-tuning on the instruction-following data. However, despite the success of this pipeline in image and language understanding, its effectiveness in joint video and language understanding has not been widely explored. In this paper, we aim to develop a novel multi-modal foundation model capable of perceiving video, image, and language within a general framework. To achieve this goal, we introduce Valley: Video Assistant with Large Language model Enhanced ability. Specifically, our proposed Valley model is designed with a simple projection module that bridges video, image, and language modalities, and is further unified with a multi-lingual LLM. We also collect multi-source vision-text pairs and adopt a spatio-temporal pooling strategy to obtain a unified vision encoding of video and image input for pre-training. Furthermore, we generate multi-task instruction-following video data, including multi-shot captions, long video descriptions, action recognition, causal relationship inference, etc. To obtain the instruction-following data, we design diverse rounds of task-oriented conversations between humans and videos, facilitated by ChatGPT. Qualitative examples demonstrate that our proposed model has the potential to function as a highly effective multilingual video assistant that can make complex video understanding scenarios easy. Code, data, and models will be available at https://github.com/RupertLuo/Valley. ","[{'version': 'v1', 'created': 'Mon, 12 Jun 2023 16:11:10 GMT'}]",2023-06-13,"[['Luo', 'Ruipu', ''], ['Zhao', 'Ziwang', ''], ['Yang', 'Min', ''], ['Dong', 'Junwei', ''], ['Qiu', 'Minghui', ''], ['Lu', 'Pengcheng', ''], ['Wang', 'Tao', ''], ['Wei', 'Zhongyu', '']]",1,1,2023-06-12,1,8,3,1,0,1,4c4d176c6e28f48041f215d563f6ee8633534cff,259138706.0,https://www.semanticscholar.org/paper/4c4d176c6e28f48041f215d563f6ee8633534cff,arXiv.org,2023.0,39.0,14.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2136663149', 'name': 'Ruipu Luo'}, {'authorId': '2189730154', 'name': 'Ziwang Zhao'}, {'authorId': '2144399770', 'name': 'Min Yang'}, {'authorId': '2187557166', 'name': 'Junwei Dong'}, {'authorId': '2143386869', 'name': 'Ming-Hui Qiu'}, {'authorId': '2069299919', 'name': 'Pengcheng Lu'}, {'authorId': '2155450386', 'name': 'Tao Wang'}, {'authorId': '2118602528', 'name': 'Zhongyu Wei'}]","['Beijing University of Posts and Telecommunications', 'Beijing Institute of Technology', 'ByteDance', 'Fudan University', 'Chongqing University']",['China'],2023-06
2306.07209,Wenqi Zhang,"Wenqi Zhang, Yongliang Shen, Weiming Lu, Yueting Zhuang",Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow,,,,,cs.CL cs.AI cs.CE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Various industries such as finance, meteorology, and energy generate vast amounts of heterogeneous data every day. There is a natural demand for humans to manage, process, and display data efficiently. However, it necessitates labor-intensive efforts and a high level of expertise for these data-related tasks. Considering that large language models (LLMs) have showcased promising capabilities in semantic understanding and reasoning, we advocate that the deployment of LLMs could autonomously manage and process massive amounts of data while displaying and interacting in a human-friendly manner. Based on this belief, we propose Data-Copilot, an LLM-based system that connects numerous data sources on one end and caters to diverse human demands on the other end. Acting like an experienced expert, Data-Copilot autonomously transforms raw data into visualization results that best match the user's intent. Specifically, Data-Copilot autonomously designs versatile interfaces (tools) for data management, processing, prediction, and visualization. In real-time response, it automatically deploys a concise workflow by invoking corresponding interfaces step by step for the user's request. The interface design and deployment processes are fully controlled by Data-Copilot itself, without human assistance. Besides, we create a Data-Copilot demo that links abundant data from different domains (stock, fund, company, economics, and live news) and accurately respond to diverse requests, serving as a reliable AI assistant. ","[{'version': 'v1', 'created': 'Mon, 12 Jun 2023 16:12:56 GMT'}]",2023-06-13,"[['Zhang', 'Wenqi', ''], ['Shen', 'Yongliang', ''], ['Lu', 'Weiming', ''], ['Zhuang', 'Yueting', '']]",0,0,2023-06-12,1,4,3,0,0,0,473eb062612a17c965eaa62136322f0dec6b1f8e,259137864.0,https://www.semanticscholar.org/paper/473eb062612a17c965eaa62136322f0dec6b1f8e,arXiv.org,2023.0,37.0,9.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2135282890', 'name': 'Wenqi Zhang'}, {'authorId': '1471660296', 'name': 'Yongliang Shen'}, {'authorId': '1776903', 'name': 'Weiming Lu'}, {'authorId': '2056432541', 'name': 'Y. Zhuang'}]",['Zhejiang University'],['China'],2023-06
2306.07257,Huiguo He,"Junchen Zhu, Huan Yang, Huiguo He, Wenjing Wang, Zixi Tuo, Wen-Huang
  Cheng, Lianli Gao, Jingkuan Song, Jianlong Fu",MovieFactory: Automatic Movie Creation from Text using Large Generative Models for Language and Images,,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we present MovieFactory, a powerful framework to generate cinematic-picture (3072$\times$1280), film-style (multi-scene), and multi-modality (sounding) movies on the demand of natural languages. As the first fully automated movie generation model to the best of our knowledge, our approach empowers users to create captivating movies with smooth transitions using simple text inputs, surpassing existing methods that produce soundless videos limited to a single scene of modest quality. To facilitate this distinctive functionality, we leverage ChatGPT to expand user-provided text into detailed sequential scripts for movie generation. Then we bring scripts to life visually and acoustically through vision generation and audio retrieval. To generate videos, we extend the capabilities of a pretrained text-to-image diffusion model through a two-stage process. Firstly, we employ spatial finetuning to bridge the gap between the pretrained image model and the new video dataset. Subsequently, we introduce temporal learning to capture object motion. In terms of audio, we leverage sophisticated retrieval models to select and align audio elements that correspond to the plot and visual content of the movie. Extensive experiments demonstrate that our MovieFactory produces movies with realistic visuals, diverse scenes, and seamlessly fitting audio, offering users a novel and immersive experience. Generated samples can be found in YouTube or Bilibili (1080P). ","[{'version': 'v1', 'created': 'Mon, 12 Jun 2023 17:31:23 GMT'}]",2023-06-13,"[['Zhu', 'Junchen', ''], ['Yang', 'Huan', ''], ['He', 'Huiguo', ''], ['Wang', 'Wenjing', ''], ['Tuo', 'Zixi', ''], ['Cheng', 'Wen-Huang', ''], ['Gao', 'Lianli', ''], ['Song', 'Jingkuan', ''], ['Fu', 'Jianlong', '']]",1,1,2023-06-12,1,9,1,1,0,1,05aea4a4646cb971bb253ced42e8935034a60b57,259138745.0,https://www.semanticscholar.org/paper/05aea4a4646cb971bb253ced42e8935034a60b57,ACM Multimedia,2023.0,46.0,4.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2146280506', 'name': 'Junchen Zhu'}, {'authorId': '46402216', 'name': 'Huan Yang'}, {'authorId': '1643688444', 'name': 'Huiguo He'}, {'authorId': '2124932147', 'name': 'Wenjing Wang'}, {'authorId': '2211976206', 'name': 'Zixi Tuo'}, {'authorId': '2116794819', 'name': 'Wen-Huang Cheng'}, {'authorId': '2671321', 'name': 'Lianli Gao'}, {'authorId': '2346105', 'name': 'Jingkuan Song'}, {'authorId': '3247966', 'name': 'Jianlong Fu'}]","['University of Electronic Science and Technology of China', 'Microsoft', 'National Taiwan University']","['China', 'India', 'Taiwan']",2023-06
2306.07401,Jiaxi Cheng,"Zecong Wang, Jiaxi Cheng, Chen Cui, and Chenhao Yu",Implementing BERT and fine-tuned RobertA to detect AI generated news by ChatGPT,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The abundance of information on social media has increased the necessity of accurate real-time rumour detection. Manual techniques of identifying and verifying fake news generated by AI tools are impracticable and time-consuming given the enormous volume of information generated every day. This has sparked an increase in interest in creating automated systems to find fake news on the Internet. The studies in this research demonstrate that the BERT and RobertA models with fine-tuning had the best success in detecting AI generated news. With a score of 98%, tweaked RobertA in particular showed excellent precision. In conclusion, this study has shown that neural networks can be used to identify bogus news AI generation news created by ChatGPT. The RobertA and BERT models' excellent performance indicates that these models can play a critical role in the fight against misinformation. ","[{'version': 'v1', 'created': 'Fri, 9 Jun 2023 17:53:19 GMT'}]",2023-06-14,"[['Wang', 'Zecong', ''], ['Cheng', 'Jiaxi', ''], ['Cui', 'Chen', ''], ['Yu', 'Chenhao', '']]",1,1,2023-06-09,1,4,2,1,0,1,da420c73bd3cee3d8a97a8ddacc4efcf2d7f5c9d,259145150.0,https://www.semanticscholar.org/paper/da420c73bd3cee3d8a97a8ddacc4efcf2d7f5c9d,arXiv.org,2023.0,39.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2220037070', 'name': 'Zecong Wang'}, {'authorId': '15178033', 'name': 'Jiaxi Cheng'}, {'authorId': '2219977210', 'name': 'Chen Cui'}, {'authorId': '2116578119', 'name': 'Chenhao Yu'}]","['UNSW Sydney', 'Inner Mongolia Normal University', 'Zhejiang University']","['China', 'Australia', 'Mongolia']",2023-06
2306.07557,Arif Ali Khan,"Muhammad Azeem Akbar, Arif Ali Khan, Peng Liang",Ethical Aspects of ChatGPT in Software Engineering Research,"Preprint accepted for publication in IEEE Transactions on Artificial
  Intelligence, 2023",,,,cs.SE,http://creativecommons.org/licenses/by/4.0/,"  ChatGPT can improve Software Engineering (SE) research practices by offering efficient, accessible information analysis and synthesis based on natural language interactions. However, ChatGPT could bring ethical challenges, encompassing plagiarism, privacy, data security, and the risk of generating biased or potentially detrimental data. This research aims to fill the given gap by elaborating on the key elements: motivators, demotivators, and ethical principles of using ChatGPT in SE research. To achieve this objective, we conducted a literature survey, identified the mentioned elements, and presented their relationships by developing a taxonomy. Further, the identified literature-based elements (motivators, demotivators, and ethical principles) were empirically evaluated by conducting a comprehensive questionnaire-based survey involving SE researchers. Additionally, we employed Interpretive Structure Modeling (ISM) approach to analyze the relationships between the ethical principles of using ChatGPT in SE research and develop a level based decision model. We further conducted a Cross-Impact Matrix Multiplication Applied to Classification (MICMAC) analysis to create a cluster-based decision model. These models aim to help SE researchers devise effective strategies for ethically integrating ChatGPT into SE research by following the identified principles through adopting the motivators and addressing the demotivators. The findings of this study will establish a benchmark for incorporating ChatGPT services in SE research with an emphasis on ethical considerations. ","[{'version': 'v1', 'created': 'Tue, 13 Jun 2023 06:13:21 GMT'}, {'version': 'v2', 'created': 'Sun, 13 Aug 2023 06:36:28 GMT'}]",2023-08-15,"[['Akbar', 'Muhammad Azeem', ''], ['Khan', 'Arif Ali', ''], ['Liang', 'Peng', '']]",1,1,2023-06-13,2,3,1,1,0,1,0a5f818915de233b1ce8262a47e78290c84af866,259144786.0,https://www.semanticscholar.org/paper/0a5f818915de233b1ce8262a47e78290c84af866,IEEE Transactions on Artificial Intelligence,2023.0,69.0,5.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Business', 'source': 's2-fos-model'}]","[{'authorId': '32584448', 'name': 'M. Akbar'}, {'authorId': '1484746239', 'name': 'A. Khan'}, {'authorId': '2174084595', 'name': 'Peng Liang'}]","['Wuhan University', 'Lappeenranta-Lahti University of Technology', 'University of Oulu']","['China', 'Finland']",2023-06
2306.07906,Xiao Liu,"Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng
  Zhang, Yuxiao Dong, Jie Tang",WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences,Accepted to KDD 2023,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  We present WebGLM, a web-enhanced question-answering system based on the General Language Model (GLM). Its goal is to augment a pre-trained large language model (LLM) with web search and retrieval capabilities while being efficient for real-world deployments. To achieve this, we develop WebGLM with strategies for the LLM-augmented retriever, bootstrapped generator, and human preference-aware scorer. Specifically, we identify and address the limitations of WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency, and cost-effectiveness advantages. In addition, we propose systematic criteria for evaluating web-enhanced QA systems. We conduct multi-dimensional human evaluation and quantitative ablation studies, which suggest the outperformance of the proposed WebGLM designs over existing systems. WebGLM with the 10-billion-parameter GLM (10B) is shown to perform better than the similar-sized WebGPT (13B) and even comparably to WebGPT (175B) in human evaluation. The code, demo, and data are at \url{https://github.com/THUDM/WebGLM}. ","[{'version': 'v1', 'created': 'Tue, 13 Jun 2023 16:57:53 GMT'}]",2023-06-14,"[['Liu', 'Xiao', ''], ['Lai', 'Hanyu', ''], ['Yu', 'Hao', ''], ['Xu', 'Yifan', ''], ['Zeng', 'Aohan', ''], ['Du', 'Zhengxiao', ''], ['Zhang', 'Peng', ''], ['Dong', 'Yuxiao', ''], ['Tang', 'Jie', '']]",0,1,2023-06-13,1,9,2,2,1,1,07bfba1087176d862c953a55df389e3d5b3d38ac,259144903.0,https://www.semanticscholar.org/paper/07bfba1087176d862c953a55df389e3d5b3d38ac,Knowledge Discovery and Data Mining,2023.0,43.0,15.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2111312892', 'name': 'Xiao Liu'}, {'authorId': '2051311700', 'name': 'Hanyu Lai'}, {'authorId': '2110750027', 'name': 'Hao Yu'}, {'authorId': '2125063007', 'name': 'Yifan Xu'}, {'authorId': '2051712753', 'name': 'Aohan Zeng'}, {'authorId': '66395694', 'name': 'Zhengxiao Du'}, {'authorId': '47243067', 'name': 'P. Zhang'}, {'authorId': '2047998', 'name': 'Yuxiao Dong'}, {'authorId': '2148911956', 'name': 'Jie Tang'}]","['Jie Tang ', 'Zhipu.AI Beijing, China', 'Beihang University', 'Tsinghua University', 'Peng Zhang', ""KDD '23, August 6-10, 2023, Long Beach, CA, USA.""]","['China', 'United States']",2023-06
2306.07932,Zefan Cai,"Zefan Cai, Baobao Chang, Wenjuan Han",Human-in-the-Loop through Chain-of-Thought,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While the emergence of powerful language models along with Chain-of-thought prompting has made automation more and more omnipresent, it sometimes demonstrates its weakness in long-term or multi-step logical reasoning. For example, users don't always get desirable answers for complex mathematical problems without human involvement. Against this background, we present the Manual Correction System (MCS) -- a human-in-the-loop system enhanced by Chain-of-Thought prompting, which explores how manual correction of sub-logics in rationales can improve LLM's reasoning performance. Moving one step forward, considering a system with human-in-the-loop involves more than having humans improve performance but also controlling the cost. Therefore, we post a Cost-utility Analysis Model for Human-in-the-Loop systems (CAMLOP) based on classical economics theory to analyze, quantify and balance the utility and the corresponding cost. We conduct experiments of MCS and CAMLOP with twelve datasets. A significant advantage w.r.t cost and utility proves its superiority over strong baselines. ","[{'version': 'v1', 'created': 'Sat, 10 Jun 2023 04:31:57 GMT'}, {'version': 'v2', 'created': 'Fri, 23 Jun 2023 05:56:51 GMT'}]",2023-06-26,"[['Cai', 'Zefan', ''], ['Chang', 'Baobao', ''], ['Han', 'Wenjuan', '']]",0,0,2023-06-10,2,3,2,0,0,0,4713dc19179cdd9083e47067fa9504751f8759c6,259144807.0,https://www.semanticscholar.org/paper/4713dc19179cdd9083e47067fa9504751f8759c6,arXiv.org,2023.0,35.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2117632647', 'name': 'Zefan Cai'}, {'authorId': '7267809', 'name': 'Baobao Chang'}, {'authorId': '144836032', 'name': 'Wenjuan Han'}]","['Peking University', 'Beijing Jiaotong University']",['China'],2023-06
2306.07968,Gyungin Shin,"Gyungin Shin, Weidi Xie, Samuel Albanie",arXiVeri: Automatic table verification with GPT,Tech report,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Without accurate transcription of numerical data in scientific documents, a scientist cannot draw accurate conclusions. Unfortunately, the process of copying numerical data from one paper to another is prone to human error. In this paper, we propose to meet this challenge through the novel task of automatic table verification (AutoTV), in which the objective is to verify the accuracy of numerical data in tables by cross-referencing cited sources. To support this task, we propose a new benchmark, arXiVeri, which comprises tabular data drawn from open-access academic papers on arXiv. We introduce metrics to evaluate the performance of a table verifier in two key areas: (i) table matching, which aims to identify the source table in a cited document that corresponds to a target table, and (ii) cell matching, which aims to locate shared cells between a target and source table and identify their row and column indices accurately. By leveraging the flexible capabilities of modern large language models (LLMs), we propose simple baselines for table verification. Our findings highlight the complexity of this task, even for state-of-the-art LLMs like OpenAI's GPT-4. The code and benchmark will be made publicly available. ","[{'version': 'v1', 'created': 'Tue, 13 Jun 2023 17:59:57 GMT'}]",2023-06-14,"[['Shin', 'Gyungin', ''], ['Xie', 'Weidi', ''], ['Albanie', 'Samuel', '']]",0,1,2023-06-13,1,3,2,1,0,1,9255caa2757db1728190f22dabcb729b52ad0648,259145157.0,https://www.semanticscholar.org/paper/9255caa2757db1728190f22dabcb729b52ad0648,arXiv.org,2023.0,36.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '104387308', 'name': 'Gyungin Shin'}, {'authorId': '10096695', 'name': 'Weidi Xie'}, {'authorId': '7641268', 'name': 'Samuel Albanie'}]","['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'University of Oxford', 'University of Cambridge', 'University of Edinburgh']","['China', 'United Kingdom']",2023-06
2306.08018,Ningyu Zhang,"Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo
  Chen, Xiaohui Fan, Huajun Chen",Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models,Project homepage: https://github.com/zjunlp/Mol-Instructions,,,,q-bio.QM cs.AI cs.CE cs.CL cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a comprehensive instruction dataset designed for the biomolecular domain. Mol-Instructions encompasses three key components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions. Each component aims to improve the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on LLMs, we demonstrate the effectiveness of Mol-Instructions in enhancing large models' performance in the intricate realm of biomolecular studies, thus fostering progress in the biomolecular research community. Mol-Instructions is publicly available for ongoing research and will undergo regular updates to enhance its applicability. ","[{'version': 'v1', 'created': 'Tue, 13 Jun 2023 14:35:34 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Aug 2023 17:13:05 GMT'}, {'version': 'v3', 'created': 'Mon, 2 Oct 2023 15:27:20 GMT'}]",2023-10-03,"[['Fang', 'Yin', ''], ['Liang', 'Xiaozhuan', ''], ['Zhang', 'Ningyu', ''], ['Liu', 'Kangwei', ''], ['Huang', 'Rui', ''], ['Chen', 'Zhuo', ''], ['Fan', 'Xiaohui', ''], ['Chen', 'Huajun', '']]",0,0,2023-06-13,3,8,6,0,0,0,46ae37aac816ed375a8a6134c003dedd63bb82d4,259164901.0,https://www.semanticscholar.org/paper/46ae37aac816ed375a8a6134c003dedd63bb82d4,arXiv.org,2023.0,73.0,6.0,0.0,True,"['Biology', 'Computer Science']","[{'category': 'Biology', 'source': 'external'}, {'category': 'Computer Science', 'source': 'external'}, {'category': 'Biology', 'source': 's2-fos-model'}]","[{'authorId': '2112787103', 'name': 'Yin Fang'}, {'authorId': '2153398295', 'name': 'Xiaozhuan Liang'}, {'authorId': '2153010067', 'name': 'Ningyu Zhang'}, {'authorId': '1664778155', 'name': 'Kangwei Liu'}, {'authorId': '2200711091', 'name': 'Rui Huang'}, {'authorId': '2111498496', 'name': 'Zhuo Chen'}, {'authorId': '2152774386', 'name': 'Xiaohui Fan'}, {'authorId': '2144200945', 'name': 'Huajun Chen'}]",['Zhejiang University'],['China'],2023-06
2306.08223,Zhigang Kan,"Zhigang Kan, Linbo Qiao, Hao Yu, Liwen Peng, Yifu Gao, Dongsheng Li",Protecting User Privacy in Remote Conversational Systems: A Privacy-Preserving framework based on text sanitization,"9 pages, 2 figures",,,,cs.CR cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) are gaining increasing attention due to their exceptional performance across numerous tasks. As a result, the general public utilize them as an influential tool for boosting their productivity while natural language processing researchers endeavor to employ them in solving existing or new research problems. Unfortunately, individuals can only access such powerful AIs through APIs, which ultimately leads to the transmission of raw data to the models' providers and increases the possibility of privacy data leakage. Current privacy-preserving methods for cloud-deployed language models aim to protect privacy information in the pre-training dataset or during the model training phase. However, they do not meet the specific challenges presented by the remote access approach of new large-scale language models.   This paper introduces a novel task, ""User Privacy Protection for Dialogue Models,"" which aims to safeguard sensitive user information from any possible disclosure while conversing with chatbots. We also present an evaluation scheme for this task, which covers evaluation metrics for privacy protection, data availability, and resistance to simulation attacks. Moreover, we propose the first framework for this task, namely privacy protection through text sanitization. Before sending the input to remote large models, it filters out the sensitive information, using several rounds of text sanitization based on privacy types that users define. Upon receiving responses from the larger model, our framework automatically restores privacy to ensure that the conversation goes smoothly, without intervention from the privacy filter. Experiments based on real-world datasets demonstrate the efficacy of our privacy-preserving approach against eavesdropping from potential attackers. ","[{'version': 'v1', 'created': 'Wed, 14 Jun 2023 03:28:51 GMT'}]",2023-06-16,"[['Kan', 'Zhigang', ''], ['Qiao', 'Linbo', ''], ['Yu', 'Hao', ''], ['Peng', 'Liwen', ''], ['Gao', 'Yifu', ''], ['Li', 'Dongsheng', '']]",0,0,2023-06-14,1,6,2,0,0,0,fa7bbdd62c230ba4708661a880ba8b34aad7f491,259164458.0,https://www.semanticscholar.org/paper/fa7bbdd62c230ba4708661a880ba8b34aad7f491,arXiv.org,2023.0,77.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '150356963', 'name': 'Zhigang Kan'}, {'authorId': '2570205', 'name': 'Linbo Qiao'}, {'authorId': '2110750027', 'name': 'Hao Yu'}, {'authorId': '1382547201', 'name': 'Liwen Peng'}, {'authorId': '2118545089', 'name': 'Yifu Gao'}, {'authorId': '2129495284', 'name': 'Dongsheng Li'}]","['Technical University of Munich', 'National University of Defense Technology']","['China', 'Germany']",2023-06
2306.08543,Yuxian Gu,"Yuxian Gu, Li Dong, Furu Wei, Minlie Huang",Knowledge Distillation of Large Language Models,"20 pages, 12 figures",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge from white-box generative LLMs is still under-explored, which becomes more and more important with the prosperity of LLMs. In this work, we propose MiniLLM that distills smaller language models from generative larger language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. Extensive experiments in the instruction-following setting show that the MiniLLM models generate more precise responses with the higher overall quality, lower exposure bias, better calibration, and higher long-text generation performance. Our method is also scalable for different model families with 120M to 13B parameters. We will release our code and model checkpoints at https://aka.ms/MiniLLM. ","[{'version': 'v1', 'created': 'Wed, 14 Jun 2023 14:44:03 GMT'}]",2023-06-16,"[['Gu', 'Yuxian', ''], ['Dong', 'Li', ''], ['Wei', 'Furu', ''], ['Huang', 'Minlie', '']]",1,1,2023-06-14,1,4,2,1,0,1,f5359f596e0306599b4aa4157e6fe03567b35c01,259164722.0,https://www.semanticscholar.org/paper/f5359f596e0306599b4aa4157e6fe03567b35c01,arXiv.org,2023.0,71.0,13.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2116405624', 'name': 'Yuxian Gu'}, {'authorId': '145307652', 'name': 'Li Dong'}, {'authorId': '49807919', 'name': 'Furu Wei'}, {'authorId': '1730108', 'name': 'Minlie Huang'}]","['Tsinghua University', 'Microsoft']","['China', 'India']",2023-06
2306.08568,Can Xu,"Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu,
  Chongyang Tao, Jing Ma, Qingwei Lin, Daxin Jiang",WizardCoder: Empowering Code Large Language Models with Evol-Instruct,"Large Language model, Code Generation, Code LLMs",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https://github.com/nlpxucan/WizardLM ","[{'version': 'v1', 'created': 'Wed, 14 Jun 2023 15:18:48 GMT'}]",2023-06-16,"[['Luo', 'Ziyang', ''], ['Xu', 'Can', ''], ['Zhao', 'Pu', ''], ['Sun', 'Qingfeng', ''], ['Geng', 'Xiubo', ''], ['Hu', 'Wenxiang', ''], ['Tao', 'Chongyang', ''], ['Ma', 'Jing', ''], ['Lin', 'Qingwei', ''], ['Jiang', 'Daxin', '']]",0,0,2023-06-14,1,10,2,2,0,2,454c8fef2957aa2fb13eb2c7a454393a2ee83805,259164815.0,https://www.semanticscholar.org/paper/454c8fef2957aa2fb13eb2c7a454393a2ee83805,arXiv.org,2023.0,37.0,85.0,16.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '23523733', 'name': 'Ziyang Luo'}, {'authorId': '46747953', 'name': 'Can Xu'}, {'authorId': '2007757792', 'name': 'Pu Zhao'}, {'authorId': '2112549330', 'name': 'Qingfeng Sun'}, {'authorId': '2442662', 'name': 'Xiubo Geng'}, {'authorId': '50105419', 'name': 'Wenxiang Hu'}, {'authorId': '8801869', 'name': 'Chongyang Tao'}, {'authorId': '2157405974', 'name': 'Jing Ma'}, {'authorId': '2793487', 'name': 'Qingwei Lin'}, {'authorId': '2086994543', 'name': 'Daxin Jiang'}]",['Hong Kong Baptist University'],['China'],2023-06
2306.08666,Zhengliang Liu,"Zhengliang Liu, Aoxiao Zhong, Yiwei Li, Longtao Yang, Chao Ju, Zihao
  Wu, Chong Ma, Peng Shu, Cheng Chen, Sekeun Kim, Haixing Dai, Lin Zhao,
  Dajiang Zhu, Jun Liu, Wei Liu, Dinggang Shen, Xiang Li, Quanzheng Li,
  Tianming Liu",Radiology-GPT: A Large Language Model for Radiology,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  We introduce Radiology-GPT, a large language model for radiology. Using an instruction tuning approach on an extensive dataset of radiology domain knowledge, Radiology-GPT demonstrates superior performance compared to general language models such as StableLM, Dolly and LLaMA. It exhibits significant versatility in radiological diagnosis, research, and communication. This work serves as a catalyst for future developments in clinical NLP. The successful implementation of Radiology-GPT is indicative of the potential of localizing generative large language models, specifically tailored for distinctive medical specialties, while ensuring adherence to privacy standards such as HIPAA. The prospect of developing individualized, large-scale language models that cater to specific needs of various hospitals presents a promising direction. The fusion of conversational competence and domain-specific knowledge in these models is set to foster future development in healthcare AI. A demo of Radiology-GPT is available at https://huggingface.co/spaces/allen-eric/radiology-gpt. ","[{'version': 'v1', 'created': 'Wed, 14 Jun 2023 17:57:24 GMT'}]",2023-06-16,"[['Liu', 'Zhengliang', ''], ['Zhong', 'Aoxiao', ''], ['Li', 'Yiwei', ''], ['Yang', 'Longtao', ''], ['Ju', 'Chao', ''], ['Wu', 'Zihao', ''], ['Ma', 'Chong', ''], ['Shu', 'Peng', ''], ['Chen', 'Cheng', ''], ['Kim', 'Sekeun', ''], ['Dai', 'Haixing', ''], ['Zhao', 'Lin', ''], ['Zhu', 'Dajiang', ''], ['Liu', 'Jun', ''], ['Liu', 'Wei', ''], ['Shen', 'Dinggang', ''], ['Li', 'Xiang', ''], ['Li', 'Quanzheng', ''], ['Liu', 'Tianming', '']]",0,1,2023-06-14,1,19,2,1,1,0,a4f16dda8d25bdc0f93d2deb3b0876d278de9284,259164788.0,https://www.semanticscholar.org/paper/a4f16dda8d25bdc0f93d2deb3b0876d278de9284,arXiv.org,2023.0,51.0,23.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2145977326', 'name': 'Zheng Liu'}, {'authorId': '40153228', 'name': 'Aoxiao Zhong'}, {'authorId': '2111161355', 'name': 'Yiwei Li'}, {'authorId': '6553816', 'name': 'Longtao Yang'}, {'authorId': '2220096793', 'name': 'Chao Ju'}, {'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '2132543537', 'name': 'Chong Ma'}, {'authorId': '2220096705', 'name': 'Peng Shu'}, {'authorId': '1390805683', 'name': 'Cheng Chen'}, {'authorId': '40991527', 'name': 'Sekeun Kim'}, {'authorId': '29944950', 'name': 'Haixing Dai'}, {'authorId': '2111641126', 'name': 'Lin Zhao'}, {'authorId': '2181182', 'name': 'Dajiang Zhu'}, {'authorId': '2157176152', 'name': 'Jun Liu'}, {'authorId': '46641573', 'name': 'W. Liu'}, {'authorId': '2150038187', 'name': 'Dinggang Shen'}, {'authorId': '113075935', 'name': 'Xiang Li'}, {'authorId': '1762919', 'name': 'Quanzheng Li'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}]","['Northwestern Polytechnical University', 'Second Xiangya Hospital of Central South University', 'Mayo Clinic', 'Harvard University', 'Shanghai United Imaging Intelligence Co., Ltd.', 'ShanghaiTech University', 'Shanghai Clinical Research Center', 'Massachusetts General Hospital', 'University of Georgia', 'The University of Texas at Arlington']","['China', 'United States']",2023-06
2306.08891,Zihui Gu,"Zihui Gu, Ju Fan, Nan Tang, Songyue Zhang, Yuxin Zhang, Zui Chen, Lei
  Cao, Guoliang Li, Sam Madden, Xiaoyong Du",Interleaving Pre-Trained Language Models and Large Language Models for Zero-Shot NL2SQL Generation,Working in progress,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Zero-shot NL2SQL is crucial in achieving natural language to SQL that is adaptive to new environments (e.g., new databases, new linguistic phenomena or SQL structures) with zero annotated NL2SQL samples from such environments. Existing approaches either fine-tune pre-trained language models (PLMs) based on annotated data or use prompts to guide fixed large language models (LLMs) such as ChatGPT. PLMs can perform well in schema alignment but struggle to achieve complex reasoning, while LLMs is superior in complex reasoning tasks but cannot achieve precise schema alignment. In this paper, we propose a ZeroNL2SQL framework that combines the complementary advantages of PLMs and LLMs for supporting zero-shot NL2SQL. ZeroNL2SQL first uses PLMs to generate an SQL sketch via schema alignment, then uses LLMs to fill the missing information via complex reasoning. Moreover, in order to better align the generated SQL queries with values in the given database instances, we design a predicate calibration method to guide the LLM in completing the SQL sketches based on the database instances and select the optimal SQL query via an execution-based strategy. Comprehensive experiments show that ZeroNL2SQL can achieve the best zero-shot NL2SQL performance on real-world benchmarks. Specifically, ZeroNL2SQL outperforms the state-of-the-art PLM-based methods by 3.2% to 13% and exceeds LLM-based methods by 10% to 20% on execution accuracy. ","[{'version': 'v1', 'created': 'Thu, 15 Jun 2023 06:50:51 GMT'}]",2023-06-16,"[['Gu', 'Zihui', ''], ['Fan', 'Ju', ''], ['Tang', 'Nan', ''], ['Zhang', 'Songyue', ''], ['Zhang', 'Yuxin', ''], ['Chen', 'Zui', ''], ['Cao', 'Lei', ''], ['Li', 'Guoliang', ''], ['Madden', 'Sam', ''], ['Du', 'Xiaoyong', '']]",1,1,2023-06-15,1,10,1,1,0,1,71e996ff55b972946b9fe0f88394c19425f5a3ab,259165667.0,https://www.semanticscholar.org/paper/71e996ff55b972946b9fe0f88394c19425f5a3ab,arXiv.org,2023.0,52.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2082344591', 'name': 'Zihui Gu'}, {'authorId': '1704755170', 'name': 'Ju Fan'}, {'authorId': '2213989628', 'name': 'Nan Tang'}, {'authorId': '151226330', 'name': 'Songyue Zhang'}, {'authorId': '2220137935', 'name': 'Yuxin Zhang'}, {'authorId': '48354042', 'name': 'Zui Chen'}, {'authorId': '50260667', 'name': 'Lei Cao'}, {'authorId': '2108491555', 'name': 'Guoliang Li'}, {'authorId': '2053630301', 'name': 'Sam Madden'}, {'authorId': '2152944669', 'name': 'Xiaoyong Du'}]","['University of Arizona', 'Tsinghua University', 'Qatar Computing Research Institute', 'Massachusetts Institute of Technology', 'Renmin University of China']","['China', 'United States', 'Qatar']",2023-06
2306.08952,Qingyu Tan,"Qingyu Tan, Hwee Tou Ng, Lidong Bing",Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models,ACL 2023,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Reasoning about time is of fundamental importance. Many facts are time-dependent. For example, athletes change teams from time to time, and different government officials are elected periodically. Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types. In this paper, we introduce a comprehensive probing dataset \tempreason to evaluate the temporal reasoning capability of large language models. Our dataset includes questions of three temporal reasoning levels. In addition, we also propose a novel learning framework to improve the temporal reasoning capability of large language models, based on temporal span extraction and time-sensitive reinforcement learning. We conducted experiments in closed book QA, open book QA, and reasoning QA settings and demonstrated the effectiveness of our approach. Our code and data are released on https://github.com/DAMO-NLP-SG/TempReason. ","[{'version': 'v1', 'created': 'Thu, 15 Jun 2023 08:44:41 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Jun 2023 05:39:25 GMT'}]",2023-06-28,"[['Tan', 'Qingyu', ''], ['Ng', 'Hwee Tou', ''], ['Bing', 'Lidong', '']]",0,0,2023-06-15,2,3,2,0,0,0,11daaaedd317ae23c7de7df506572d9155017ae3,259165281.0,https://www.semanticscholar.org/paper/11daaaedd317ae23c7de7df506572d9155017ae3,Annual Meeting of the Association for Computational Linguistics,2023.0,54.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '118358816', 'name': 'Qingyu Tan'}, {'authorId': '34789794', 'name': 'H. Ng'}, {'authorId': '1996394', 'name': 'Lidong Bing'}]","['Alibaba', 'National University of Singapore']","['China', 'Singapore']",2023-06
2306.09093,Longyue Wang,"Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu,
  Zefeng Du, Shuming Shi, Zhaopeng Tu","Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration","Longyue Wang is the corresponding author. Our project page is at
  https://github.com/lyuchenyang/Macaw-LLM",,,,cs.CL cs.AI cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Although instruction-tuned large language models (LLMs) have exhibited remarkable capabilities across various NLP tasks, their effectiveness on other data modalities beyond text has not been fully studied. In this work, we propose Macaw-LLM, a novel multi-modal LLM that seamlessly integrates visual, audio, and textual information. Macaw-LLM consists of three main components: a modality module for encoding multi-modal data, a cognitive module for harnessing pretrained LLMs, and an alignment module for harmonizing diverse representations. Our novel alignment module seamlessly bridges multi-modal features to textual features, simplifying the adaptation process from the modality modules to the cognitive module. In addition, we construct a large-scale multi-modal instruction dataset in terms of multi-turn dialogue, including 69K image instances and 50K video instances. We have made our data, code and model publicly available, which we hope can pave the way for future research in multi-modal LLMs and expand the capabilities of LLMs to handle diverse data modalities and address complex real-world scenarios. ","[{'version': 'v1', 'created': 'Thu, 15 Jun 2023 12:45:25 GMT'}]",2023-06-16,"[['Lyu', 'Chenyang', ''], ['Wu', 'Minghao', ''], ['Wang', 'Longyue', ''], ['Huang', 'Xinting', ''], ['Liu', 'Bingshuai', ''], ['Du', 'Zefeng', ''], ['Shi', 'Shuming', ''], ['Tu', 'Zhaopeng', '']]",0,0,2023-06-15,1,8,3,0,0,0,0983883619a0ca597d055d0e58da2f514052913d,259165461.0,https://www.semanticscholar.org/paper/0983883619a0ca597d055d0e58da2f514052913d,arXiv.org,2023.0,54.0,22.0,6.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2082426870', 'name': 'Chenyang Lyu'}, {'authorId': '2145209409', 'name': 'Minghao Wu'}, {'authorId': '1800190', 'name': 'Longyue Wang'}, {'authorId': '14799547', 'name': 'Xinting Huang'}, {'authorId': '47655790', 'name': 'Bingshuai Liu'}, {'authorId': '2112455515', 'name': 'Zefeng Du'}, {'authorId': '2072684668', 'name': 'Shuming Shi'}, {'authorId': '2909321', 'name': 'Zhaopeng Tu'}]","['Monash University', 'Dublin City University', 'Tencent']","['China', 'Ireland', 'Australia']",2023-06
2306.09212,Haonan Li,"Haonan Li and Yixuan Zhang and Fajri Koto and Yifei Yang and Hai Zhao
  and Yeyun Gong and Nan Duan and Timothy Baldwin",CMMLU: Measuring massive multitask language understanding in Chinese,,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  As the capabilities of large language models (LLMs) continue to advance, evaluating their performance becomes increasingly crucial and challenging. This paper aims to bridge this gap by introducing CMMLU, a comprehensive Chinese benchmark that covers various subjects, including natural science, social sciences, engineering, and humanities. We conduct a thorough evaluation of 18 advanced multilingual- and Chinese-oriented LLMs, assessing their performance across different subjects and settings. The results reveal that most existing LLMs struggle to achieve an average accuracy of 50%, even when provided with in-context examples and chain-of-thought prompts, whereas the random baseline stands at 25%. This highlights significant room for improvement in LLMs. Additionally, we conduct extensive experiments to identify factors impacting the models' performance and propose directions for enhancing LLMs. CMMLU fills the gap in evaluating the knowledge and reasoning capabilities of large language models within the Chinese context. ","[{'version': 'v1', 'created': 'Thu, 15 Jun 2023 15:49:51 GMT'}]",2023-06-16,"[['Li', 'Haonan', ''], ['Zhang', 'Yixuan', ''], ['Koto', 'Fajri', ''], ['Yang', 'Yifei', ''], ['Zhao', 'Hai', ''], ['Gong', 'Yeyun', ''], ['Duan', 'Nan', ''], ['Baldwin', 'Timothy', '']]",0,0,2023-06-15,1,8,1,0,0,0,2ed07f3574fde65772b625153e853d716c2a6e14,259164635.0,https://www.semanticscholar.org/paper/2ed07f3574fde65772b625153e853d716c2a6e14,arXiv.org,2023.0,43.0,27.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '49404498', 'name': 'Haonan Li'}, {'authorId': '2211919724', 'name': 'Yixuan Zhang'}, {'authorId': '2789148', 'name': 'Fajri Koto'}, {'authorId': '2108989265', 'name': 'Yifei Yang'}, {'authorId': '2146232510', 'name': 'Hai Zhao'}, {'authorId': '2171182', 'name': 'Yeyun Gong'}, {'authorId': '46429989', 'name': 'Nan Duan'}, {'authorId': '123917295', 'name': 'Tim Baldwin'}]","['University of Melbourne', 'Shanghai Jiao Tong University', 'Microsoft', 'Mohamed bin Zayed University of Artificial Intelligence', 'Laureate Institute for Brain Research']","['China', 'United States', 'United Arab Emirates', 'Australia']",2023-06
2306.09265,Wenqi Shao,"Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei,
  Fanqing Meng, Siyuan Huang, Yu Qiao, Ping Luo",LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models,"28 pages, 10 figures, a comprehensive evaluation of large
  vision-language models",,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Vision-Language Models (LVLMs) have recently played a dominant role in multimodal vision-language learning. Despite the great success, it lacks a holistic evaluation of their efficacy. This paper presents a comprehensive evaluation of publicly available large multimodal models by building a LVLM evaluation Hub (LVLM-eHub). Our LVLM-eHub consists of $8$ representative LVLMs such as InstructBLIP and MiniGPT-4, which are thoroughly evaluated by a quantitative capability evaluation and an online arena platform. The former evaluates $6$ categories of multimodal capabilities of LVLMs such as visual question answering and embodied artificial intelligence on $47$ standard text-related visual benchmarks, while the latter provides the user-level evaluation of LVLMs in an open-world question-answering scenario. The study reveals several innovative findings. First, instruction-tuned LVLM with massive in-domain data such as InstructBLIP heavily overfits many existing tasks, generalizing poorly in the open-world scenario. Second, instruction-tuned LVLM with moderate instruction-following data may result in object hallucination issues (i.e., generate objects that are inconsistent with target images in the descriptions). It either makes the current evaluation metric such as CIDEr for image captioning ineffective or generates wrong answers. Third, employing a multi-turn reasoning evaluation framework can mitigate the issue of object hallucination, shedding light on developing an effective pipeline for LVLM evaluation. The findings provide a foundational framework for the conception and assessment of innovative strategies aimed at enhancing zero-shot multimodal techniques. Our LVLM-eHub will be available at https://github.com/OpenGVLab/Multi-Modality-Arena ","[{'version': 'v1', 'created': 'Thu, 15 Jun 2023 16:39:24 GMT'}]",2023-06-16,"[['Xu', 'Peng', ''], ['Shao', 'Wenqi', ''], ['Zhang', 'Kaipeng', ''], ['Gao', 'Peng', ''], ['Liu', 'Shuo', ''], ['Lei', 'Meng', ''], ['Meng', 'Fanqing', ''], ['Huang', 'Siyuan', ''], ['Qiao', 'Yu', ''], ['Luo', 'Ping', '']]",0,1,2023-06-15,1,10,2,0,0,0,a8d02ff6d075c3cc48f0b97801cc52765c8f9ac9,259165040.0,https://www.semanticscholar.org/paper/a8d02ff6d075c3cc48f0b97801cc52765c8f9ac9,arXiv.org,2023.0,84.0,31.0,8.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '104382958', 'name': 'Peng Xu'}, {'authorId': '1485702259', 'name': 'Wenqi Shao'}, {'authorId': '3393556', 'name': 'Kaipeng Zhang'}, {'authorId': '144740494', 'name': 'Peng Gao'}, {'authorId': '50152261', 'name': 'Shuo Liu'}, {'authorId': '2220083562', 'name': 'Meng Lei'}, {'authorId': '2029351458', 'name': 'Fanqing Meng'}, {'authorId': '1713084', 'name': 'Siyuan Huang'}, {'authorId': '2059129841', 'name': 'Y. Qiao'}, {'authorId': '2143481782', 'name': 'Ping Luo'}]","['Peking University', 'Shanghai Artificial Intelligence Laboratory', 'OpenGVLab, Shanghai AI Laboratory', 'University of Hong Kong']","['China', 'Hong Kong']",2023-06
2306.09296,Zijun Yao,"Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li, Xin
  Lv, Hao Peng, Zijun Yao, Xiaohan Zhang, Hanming Li, Chunyang Li, Zheyuan
  Zhang, Yushi Bai, Yantao Liu, Amy Xin, Nianyi Lin, Kaifeng Yun, Linlu Gong,
  Jianhui Chen, Zhili Wu, Yunjia Qi, Weikai Li, Yong Guan, Kaisheng Zeng, Ji
  Qi, Hailong Jin, Jinxin Liu, Yu Gu, Yuan Yao, Ning Ding, Lei Hou, Zhiyuan
  Liu, Bin Xu, Jie Tang, Juanzi Li",KoLA: Carefully Benchmarking World Knowledge of Large Language Models,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The unprecedented performance of large language models (LLMs) necessitates improvements in evaluations. Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of world knowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark (KoLA), in which we carefully design three crucial factors: (1) For ability modeling, we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering $19$ tasks. (2) For data, to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge. (3) For evaluation criteria, we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models and a unique self-contrast metric for automatically evaluating knowledge hallucination. We evaluate $21$ open-source and commercial LLMs and obtain some intriguing findings. The KoLA dataset and open-participation leaderboard are publicly released at https://kola.xlore.cn and will be continuously updated to provide references for developing LLMs and knowledge-related systems. ","[{'version': 'v1', 'created': 'Thu, 15 Jun 2023 17:20:46 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Jul 2023 17:25:10 GMT'}]",2023-07-10,"[['Yu', 'Jifan', ''], ['Wang', 'Xiaozhi', ''], ['Tu', 'Shangqing', ''], ['Cao', 'Shulin', ''], ['Zhang-Li', 'Daniel', ''], ['Lv', 'Xin', ''], ['Peng', 'Hao', ''], ['Yao', 'Zijun', ''], ['Zhang', 'Xiaohan', ''], ['Li', 'Hanming', ''], ['Li', 'Chunyang', ''], ['Zhang', 'Zheyuan', ''], ['Bai', 'Yushi', ''], ['Liu', 'Yantao', ''], ['Xin', 'Amy', ''], ['Lin', 'Nianyi', ''], ['Yun', 'Kaifeng', ''], ['Gong', 'Linlu', ''], ['Chen', 'Jianhui', ''], ['Wu', 'Zhili', ''], ['Qi', 'Yunjia', ''], ['Li', 'Weikai', ''], ['Guan', 'Yong', ''], ['Zeng', 'Kaisheng', ''], ['Qi', 'Ji', ''], ['Jin', 'Hailong', ''], ['Liu', 'Jinxin', ''], ['Gu', 'Yu', ''], ['Yao', 'Yuan', ''], ['Ding', 'Ning', ''], ['Hou', 'Lei', ''], ['Liu', 'Zhiyuan', ''], ['Xu', 'Bin', ''], ['Tang', 'Jie', ''], ['Li', 'Juanzi', '']]",0,0,2023-06-15,2,35,1,0,0,0,3e826e52754d0876611c8cf2fa7a781a701c39e6,259165244.0,https://www.semanticscholar.org/paper/3e826e52754d0876611c8cf2fa7a781a701c39e6,arXiv.org,2023.0,70.0,18.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2116034394', 'name': 'Jifan Yu'}, {'authorId': '48631777', 'name': 'Xiaozhi Wang'}, {'authorId': '2116520118', 'name': 'Shangqing Tu'}, {'authorId': '1712738522', 'name': 'S. Cao'}, {'authorId': '2165225735', 'name': 'Daniel Zhang-li'}, {'authorId': '48574888', 'name': 'Xin Lv'}, {'authorId': '47837854', 'name': 'Hao Peng'}, {'authorId': '1423719712', 'name': 'Zijun Yao'}, {'authorId': '2181337399', 'name': 'Xiaohan Zhang'}, {'authorId': '2188777141', 'name': 'Hanming Li'}, {'authorId': '2118001750', 'name': 'Chun-yan Li'}, {'authorId': '2133195231', 'name': 'Zheyu Zhang'}, {'authorId': '2141377570', 'name': 'Yushi Bai'}, {'authorId': '2211723524', 'name': 'Yan-Tie Liu'}, {'authorId': '2220101512', 'name': 'Amy Xin'}, {'authorId': '2210119346', 'name': 'Nianyi Lin'}, {'authorId': '2220099357', 'name': 'Kaifeng Yun'}, {'authorId': '2220092688', 'name': 'Linlu Gong'}, {'authorId': '2220188202', 'name': 'Jianhui Chen'}, {'authorId': '2220655963', 'name': 'Zhili Wu'}, {'authorId': '121817444', 'name': 'Y. Qi'}, {'authorId': '2143447165', 'name': 'Weikai Li'}, {'authorId': '2069570219', 'name': 'Yong Guan'}, {'authorId': '10673612', 'name': 'Kaisheng Zeng'}, {'authorId': '2091076497', 'name': 'Ji Qi'}, {'authorId': '2109790016', 'name': 'Hailong Jin'}, {'authorId': '2187062668', 'name': 'Jinxi Liu'}, {'authorId': '2116405624', 'name': 'Yuxian Gu'}, {'authorId': '2022231256', 'name': 'Yu Gu'}, {'authorId': '1390925224', 'name': 'Yuan Yao'}, {'authorId': '46649145', 'name': 'Ning Ding'}, {'authorId': '2055765060', 'name': 'Lei Hou'}, {'authorId': '1390625267', 'name': 'Zhiyuan Liu'}, {'authorId': '2113744169', 'name': 'Bin Xu'}, {'authorId': '2148911975', 'name': 'Jie Tang'}, {'authorId': '2133353675', 'name': 'Juanzi Li'}]",['Tsinghua University'],['China'],2023-06
2306.09308,Myles Foley,"Myles Foley, Ambrish Rawat, Taesung Lee, Yufang Hou, Gabriele Picco,
  Giulio Zizzo",Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models,,,,,cs.CL cs.AI cs.CR,http://creativecommons.org/licenses/by/4.0/,"  The wide applicability and adaptability of generative large language models (LLMs) has enabled their rapid adoption. While the pre-trained models can perform many tasks, such models are often fine-tuned to improve their performance on various downstream applications. However, this leads to issues over violation of model licenses, model theft, and copyright infringement. Moreover, recent advances show that generative technology is capable of producing harmful content which exacerbates the problems of accountability within model supply chains. Thus, we need a method to investigate how a model was trained or a piece of text was generated and what their pre-trained base model was. In this paper we take the first step to address this open problem by tracing back the origin of a given fine-tuned LLM to its corresponding pre-trained base model. We consider different knowledge levels and attribution strategies, and find that we can correctly trace back 8 out of the 10 fine tuned models with our best method. ","[{'version': 'v1', 'created': 'Thu, 15 Jun 2023 17:42:48 GMT'}]",2023-06-16,"[['Foley', 'Myles', ''], ['Rawat', 'Ambrish', ''], ['Lee', 'Taesung', ''], ['Hou', 'Yufang', ''], ['Picco', 'Gabriele', ''], ['Zizzo', 'Giulio', '']]",0,0,2023-06-15,1,6,3,0,0,0,2150ef9e3d2aebfba3264044b0fc0b875c142e17,259164398.0,https://www.semanticscholar.org/paper/2150ef9e3d2aebfba3264044b0fc0b875c142e17,Annual Meeting of the Association for Computational Linguistics,2023.0,55.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2166091148', 'name': 'Myles Foley'}, {'authorId': '22261698', 'name': 'Ambrish Rawat'}, {'authorId': '2801206', 'name': 'Taesung Lee'}, {'authorId': '39517968', 'name': 'Yufang Hou'}, {'authorId': '46310814', 'name': 'Gabriele Picco'}, {'authorId': '152109289', 'name': 'Giulio Zizzo'}]","['Imperial College London', 'IBM Research - China']","['China', 'United Kingdom']",2023-06
2306.09597,Yi Zhu,"Yi Zhu, Han Wang, Ye Wang, Yun Li, Yunhao Yuan, Jipeng Qiang",Clickbait Detection via Large Language Models,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Clickbait, which aims to induce users with some surprising and even thrilling headlines for increasing click-through rates, permeates almost all online content publishers, such as news portals and social media. Recently, Large Language Models (LLMs) have emerged as a powerful instrument and achieved tremendous success in a serious of NLP downstream tasks. However, it is not yet known whether LLMs can be served as a high-quality clickbait detection system. In this paper, we analyze the performance of LLMs in the few-shot scenarios on a number of English and Chinese benchmark datasets. Experimental results show that LLMs cannot achieve the best results compared to the state-of-the-art deep and fine-tuning PLMs methods. Different from the human intuition, the experiments demonstrated that LLMs cannot make satisfied clickbait detection just by the headlines. ","[{'version': 'v1', 'created': 'Fri, 16 Jun 2023 02:49:20 GMT'}]",2023-06-19,"[['Zhu', 'Yi', ''], ['Wang', 'Han', ''], ['Wang', 'Ye', ''], ['Li', 'Yun', ''], ['Yuan', 'Yunhao', ''], ['Qiang', 'Jipeng', '']]",0,0,2023-06-16,1,6,2,0,0,0,f38dff23567c815d8f738052eea3d3f3eb1c0ed6,259187872.0,https://www.semanticscholar.org/paper/f38dff23567c815d8f738052eea3d3f3eb1c0ed6,arXiv.org,2023.0,30.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2143400684', 'name': 'Yi Zhu'}, {'authorId': '40465036', 'name': 'H. Wang'}, {'authorId': '2220239793', 'name': 'Ye Wang'}, {'authorId': '2117014265', 'name': 'Yun Li'}, {'authorId': '143870880', 'name': 'Yunhao Yuan'}, {'authorId': '39845702', 'name': 'Jipeng Qiang'}]",['Yangzhou University'],['China'],2023-06
2306.09649,Jackie (Junrui) Yang,"Jackie (Junrui) Yang, Karina Li, Daniel Wan Rosli, Shuning Zhang,
  Yuhan Zhang, Monica S. Lam, James A. Landay",ReactGenie: An Object-Oriented State Abstraction for Complex Multimodal Interactions Using Large Language Models,,,,,cs.HC cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal interactions have been shown to be more flexible, efficient, and adaptable for diverse users and tasks than traditional graphical interfaces. However, existing multimodal development frameworks either do not handle the complexity and compositionality of multimodal commands well or require developers to write a substantial amount of code to support these multimodal interactions. In this paper, we present ReactGenie, a programming framework that uses a shared object-oriented state abstraction to support building complex multimodal mobile applications. Having different modalities share the same state abstraction allows developers using ReactGenie to seamlessly integrate and compose these modalities to deliver multimodal interaction.   ReactGenie is a natural extension to the existing workflow of building a graphical app, like the workflow with React-Redux. Developers only have to add a few annotations and examples to indicate how natural language is mapped to the user-accessible functions in the program. ReactGenie automatically handles the complex problem of understanding natural language by generating a parser that leverages large language models.   We evaluated the ReactGenie framework by using it to build three demo apps. We evaluated the accuracy of the language parser using elicited commands from crowd workers and evaluated the usability of the generated multimodal app with 16 participants. Our results show that ReactGenie can be used to build versatile multimodal applications with highly accurate language parsers, and the multimodal app can lower users' cognitive load and task completion time. ","[{'version': 'v1', 'created': 'Fri, 16 Jun 2023 06:53:26 GMT'}]",2023-06-19,"[['Jackie', '', '', 'Junrui'], ['Yang', '', ''], ['Li', 'Karina', ''], ['Rosli', 'Daniel Wan', ''], ['Zhang', 'Shuning', ''], ['Zhang', 'Yuhan', ''], ['Lam', 'Monica S.', ''], ['Landay', 'James A.', '']]",0,0,2023-06-16,1,8,2,0,0,0,372c2123d813685527387aa71865fcd2f2464ea2,259187980.0,https://www.semanticscholar.org/paper/372c2123d813685527387aa71865fcd2f2464ea2,arXiv.org,2023.0,54.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '3886207', 'name': 'Jackie Yang'}, {'authorId': '2220791894', 'name': 'Karina Li'}, {'authorId': '2220218129', 'name': 'Daniel Wan Rosli'}, {'authorId': '2167035973', 'name': 'Shuning Zhang'}, {'authorId': '2220237818', 'name': 'Yuhan Zhang'}, {'authorId': '39682108', 'name': 'M. Lam'}, {'authorId': '9522307', 'name': 'J. Landay'}]","['Stanford University', 'Tsinghua University']","['China', 'United States']",2023-06
2306.09776,Ze Gao Mr,"Yuqian Sun, Xingyu Li, Ze Gao",Inspire creativity with ORIBA: Transform Artists' Original Characters into Chatbots through Large Language Model,"5 pages, 2 figures, 1 table",,,,cs.MM cs.AI cs.HC,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  This research delves into the intersection of illustration art and artificial intelligence (AI), focusing on how illustrators engage with AI agents that embody their original characters (OCs). We introduce 'ORIBA', a customizable AI chatbot that enables illustrators to converse with their OCs. This approach allows artists to not only receive responses from their OCs but also to observe their inner monologues and behavior. Despite the existing tension between artists and AI, our study explores innovative collaboration methods that are inspiring to illustrators. By examining the impact of AI on the creative process and the boundaries of authorship, we aim to enhance human-AI interactions in creative fields, with potential applications extending beyond illustration to interactive storytelling and more. ","[{'version': 'v1', 'created': 'Fri, 16 Jun 2023 11:25:44 GMT'}]",2023-06-19,"[['Sun', 'Yuqian', ''], ['Li', 'Xingyu', ''], ['Gao', 'Ze', '']]",0,0,2023-06-16,1,3,3,0,0,0,69412cba7097994617a8e008ea71cb91eb8d6dee,259188107.0,https://www.semanticscholar.org/paper/69412cba7097994617a8e008ea71cb91eb8d6dee,UbiComp/ISWC Adjunct,2023.0,27.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Art', 'source': 's2-fos-model'}]","[{'authorId': '1666629650', 'name': 'Yuqian Sun'}, {'authorId': '2155446933', 'name': 'Xingyu Li'}, {'authorId': '2256489428', 'name': 'Jun Peng'}, {'authorId': '2181257140', 'name': 'Ze Gao'}]","['Royal College of Art', 'Georgia Institute of Technology', 'Hong Kong University of Science and Technology']","['China', 'United States', 'United Kingdom']",2023-06
2306.09782,Kai Lv,"Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, Xipeng Qiu",Full Parameter Fine-tuning for Large Language Models with Limited Resources,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but demand massive GPU resources for training. Lowering the threshold for LLMs training would encourage greater participation from researchers, benefiting both academia and society. While existing approaches have focused on parameter-efficient fine-tuning, which tunes or adds a small number of parameters, few have addressed the challenge of tuning the full parameters of LLMs with limited resources. In this work, we propose a new optimizer, LOw-Memory Optimization (LOMO), which fuses the gradient computation and the parameter update in one step to reduce memory usage. By integrating LOMO with existing memory saving techniques, we reduce memory usage to 10.8% compared to the standard approach (DeepSpeed solution). Consequently, our approach enables the full parameter fine-tuning of a 65B model on a single machine with 8 RTX 3090, each with 24GB memory. ","[{'version': 'v1', 'created': 'Fri, 16 Jun 2023 11:37:15 GMT'}]",2023-06-19,"[['Lv', 'Kai', ''], ['Yang', 'Yuqing', ''], ['Liu', 'Tengxiao', ''], ['Gao', 'Qinghui', ''], ['Guo', 'Qipeng', ''], ['Qiu', 'Xipeng', '']]",0,0,2023-06-16,1,6,1,0,0,0,f4bdec0cf595720bc8ee5df2196324bac8f52ab4,259187846.0,https://www.semanticscholar.org/paper/f4bdec0cf595720bc8ee5df2196324bac8f52ab4,arXiv.org,2023.0,31.0,12.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2055634356', 'name': 'Kai Lv'}, {'authorId': '2145435513', 'name': 'Yuqing Yang'}, {'authorId': '2136108329', 'name': 'Tengxiao Liu'}, {'authorId': '2188485375', 'name': 'Qi-jie Gao'}, {'authorId': '153683057', 'name': 'Qipeng Guo'}, {'authorId': '2188058565', 'name': 'Xipeng Qiu'}]",['Fudan University'],['China'],2023-06
2306.09968,Guoxing Yang,"Guangyu Wang, Guoxing Yang, Zongxin Du, Longjun Fan, Xiaohu Li",ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data and Comprehensive Evaluation,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large language models have exhibited exceptional performance on various Natural Language Processing (NLP) tasks, leveraging techniques such as the pre-training, and instruction fine-tuning. Despite these advances, their effectiveness in medical applications is limited, due to challenges such as factual inaccuracies, reasoning abilities, and lack grounding in real-world experience. In this study, we present ClinicalGPT, a language model explicitly designed and optimized for clinical scenarios. By incorporating extensive and diverse real-world data, such as medical records, domain-specific knowledge, and multi-round dialogue consultations in the training process, ClinicalGPT is better prepared to handle multiple clinical task. Furthermore, we introduce a comprehensive evaluation framework that includes medical knowledge question-answering, medical exams, patient consultations, and diagnostic analysis of medical records. Our results demonstrate that ClinicalGPT significantly outperforms other models in these tasks, highlighting the effectiveness of our approach in adapting large language models to the critical domain of healthcare. ","[{'version': 'v1', 'created': 'Fri, 16 Jun 2023 16:56:32 GMT'}]",2023-06-19,"[['Wang', 'Guangyu', ''], ['Yang', 'Guoxing', ''], ['Du', 'Zongxin', ''], ['Fan', 'Longjun', ''], ['Li', 'Xiaohu', '']]",0,1,2023-06-16,1,5,1,0,0,0,ebc502a4d173f6550a8cd6384cb06f2c43c7c1a3,259187929.0,https://www.semanticscholar.org/paper/ebc502a4d173f6550a8cd6384cb06f2c43c7c1a3,arXiv.org,2023.0,26.0,11.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2220640752', 'name': 'Guangyu Wang'}, {'authorId': '2109946595', 'name': 'Guoxing Yang'}, {'authorId': '2199268643', 'name': 'Zongxin Du'}, {'authorId': '2220631186', 'name': 'Longjun Fan'}, {'authorId': '2220239349', 'name': 'Xiaohu Li'}]",['Beijing University of Posts and Telecommunications'],['China'],2023-06
2306.10241,Jiaan Wang,"Jiaan Wang, Jianfeng Qu, Yunlong Liang, Zhixu Li, An Liu, Guanfeng
  Liu, Xin Zheng",Snowman: A Million-scale Chinese Commonsense Knowledge Graph Distilled from Foundation Model,tech report,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Constructing commonsense knowledge graphs (CKGs) has attracted wide research attention due to its significant importance in cognitive intelligence. Nevertheless, existing CKGs are typically oriented to English, limiting the research in non-English languages. Meanwhile, the emergence of foundation models like ChatGPT and GPT-4 has shown promising intelligence with the help of reinforcement learning from human feedback. Under the background, in this paper, we utilize foundation models to construct a Chinese CKG, named Snowman. Specifically, we distill different types of commonsense head items from ChatGPT, and continue to use it to collect tail items with respect to the head items and pre-defined relations. Based on the preliminary analysis, we find the negative commonsense knowledge distilled by ChatGPT achieves lower human acceptance compared to other knowledge. Therefore, we design a simple yet effective self-instruct filtering strategy to filter out invalid negative commonsense. Overall, the constructed Snowman covers more than ten million Chinese commonsense triples, making it the largest Chinese CKG. Moreover, human studies show the acceptance of Snowman achieves 90.6\%, indicating the high-quality triples distilled by the cutting-edge foundation model. We also conduct experiments on commonsense knowledge models to show the usability and effectiveness of our Snowman. ","[{'version': 'v1', 'created': 'Sat, 17 Jun 2023 02:51:33 GMT'}]",2023-06-21,"[['Wang', 'Jiaan', ''], ['Qu', 'Jianfeng', ''], ['Liang', 'Yunlong', ''], ['Li', 'Zhixu', ''], ['Liu', 'An', ''], ['Liu', 'Guanfeng', ''], ['Zheng', 'Xin', '']]",1,1,2023-06-17,1,7,2,2,0,2,72a064a656f278358251efecf4106c802afa0e92,259202815.0,https://www.semanticscholar.org/paper/72a064a656f278358251efecf4106c802afa0e92,arXiv.org,2023.0,47.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2118328782', 'name': 'Jiaan Wang'}, {'authorId': '2069479032', 'name': 'Jianfeng Qu'}, {'authorId': '3389712', 'name': 'Yunlong Liang'}, {'authorId': '115419489', 'name': 'Zhixu Li'}, {'authorId': '144806511', 'name': 'An Liu'}, {'authorId': '8540458', 'name': 'Guanfeng Liu'}, {'authorId': '1430762233', 'name': 'Xin Zheng'}]",['Beijing Jiaotong University'],['China'],2023-06
2306.10322,Xiwen Liang,"Xiwen Liang, Liang Ma, Shanshan Guo, Jianhua Han, Hang Xu, Shikui Ma,
  Xiaodan Liang",MO-VLN: A Multi-Task Benchmark for Open-set Zero-Shot Vision-and-Language Navigation,23 pages,,,,cs.CV cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Given a natural language, a general robot has to comprehend the instruction and find the target object or location based on visual observations even in unexplored environments. Most agents rely on massive diverse training data to achieve better generalization, which requires expensive labor. These agents often focus on common objects and fewer tasks, thus are not intelligent enough to handle different types of instructions. To facilitate research in open-set vision-and-language navigation, we propose a benchmark named MO-VLN, aiming at testing the effectiveness and generalization of the agent in the multi-task setting. First, we develop a 3D simulator rendered by realistic scenarios using Unreal Engine 5, containing more realistic lights and details. The simulator contains three scenes, i.e., cafe, restaurant, and nursing house, of high value in the industry. Besides, our simulator involves multiple uncommon objects, such as takeaway cup and medical adhesive tape, which are more complicated compared with existing environments. Inspired by the recent success of large language models (e.g., ChatGPT, Vicuna), we construct diverse high-quality data of instruction type without human annotation. Our benchmark MO-VLN provides four tasks: 1) goal-conditioned navigation given a specific object category (e.g., ""fork""); 2) goal-conditioned navigation given simple instructions (e.g., ""Search for and move towards a tennis ball""); 3) step-by-step instruction following; 4) finding abstract object based on high-level instruction (e.g., ""I am thirsty""). ","[{'version': 'v1', 'created': 'Sat, 17 Jun 2023 11:44:04 GMT'}, {'version': 'v2', 'created': 'Tue, 26 Sep 2023 05:18:49 GMT'}]",2023-09-27,"[['Liang', 'Xiwen', ''], ['Ma', 'Liang', ''], ['Guo', 'Shanshan', ''], ['Han', 'Jianhua', ''], ['Xu', 'Hang', ''], ['Ma', 'Shikui', ''], ['Liang', 'Xiaodan', '']]",1,1,2023-06-17,2,7,3,2,1,1,17a52cc9d01b3189bf1c0370a431517180e03f34,259202797.0,https://www.semanticscholar.org/paper/17a52cc9d01b3189bf1c0370a431517180e03f34,arXiv.org,2023.0,97.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '51291599', 'name': 'Xiwen Liang'}, {'authorId': '145499462', 'name': 'Liang Ma'}, {'authorId': '2119111826', 'name': 'Shanshan Guo'}, {'authorId': '47180442', 'name': 'Jianhua Han'}, {'authorId': '2143534132', 'name': 'Hang Xu'}, {'authorId': '2220617506', 'name': 'Shikui Ma'}, {'authorId': '2153397698', 'name': 'Xiaodan Liang'}]","['Northeastern University', 'Sun Yat-sen University', 'Oliver Crispin Robotics (United Kingdom)', 'Huawei Technologies (China)']","['China', 'United Kingdom']",2023-06
2306.10512,Yan Zhuang,"Yan Zhuang, Qi Liu, Yuting Ning, Weizhe Huang, Rui Lv, Zhenya Huang,
  Guanhao Zhao, Zheng Zhang, Qingyang Mao, Shijin Wang, Enhong Chen",Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs), like ChatGPT, have shown some human-like cognitive abilities. For comparing these abilities of different models, several benchmarks (i.e. sets of standard test questions) from different fields (e.g., Literature, Biology and Psychology) are often adopted and the test results under traditional metrics such as accuracy, recall and F1, are reported. However, such way for evaluating LLMs can be inefficient and inaccurate from the cognitive science perspective. Inspired by Computerized Adaptive Testing (CAT) used in psychometrics, we propose an adaptive testing framework for LLM evaluation. Rather than using a standard test set and simply reporting accuracy, this approach dynamically adjusts the characteristics of the test questions, such as difficulty, based on the model's performance. This allows for a more accurate estimation of the model's abilities, using fewer questions. More importantly, it allows LLMs to be compared with humans easily, which is essential for NLP models that aim for human-level ability. Our diagnostic reports have found that ChatGPT often behaves like a ``careless student'', prone to slip and occasionally guessing the questions. We conduct a fine-grained diagnosis and rank the latest 6 instruction-tuned LLMs from three aspects of Subject Knowledge, Mathematical Reasoning, and Programming, where GPT4 can outperform other models significantly and reach the cognitive ability of middle-level students. Different tests for different models using efficient adaptive testing -- we believe this has the potential to become a new norm in evaluating large language models. ","[{'version': 'v1', 'created': 'Sun, 18 Jun 2023 09:54:33 GMT'}]",2023-06-21,"[['Zhuang', 'Yan', ''], ['Liu', 'Qi', ''], ['Ning', 'Yuting', ''], ['Huang', 'Weizhe', ''], ['Lv', 'Rui', ''], ['Huang', 'Zhenya', ''], ['Zhao', 'Guanhao', ''], ['Zhang', 'Zheng', ''], ['Mao', 'Qingyang', ''], ['Wang', 'Shijin', ''], ['Chen', 'Enhong', '']]",1,1,2023-06-18,1,11,1,2,0,2,ecf3556506a68d230a4de169465220379891e0ab,259204023.0,https://www.semanticscholar.org/paper/ecf3556506a68d230a4de169465220379891e0ab,arXiv.org,2023.0,51.0,9.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2075333635', 'name': 'Yan Zhuang'}, {'authorId': '50384171', 'name': 'Qi Liu'}, {'authorId': '2106771355', 'name': 'Yuting Ning'}, {'authorId': '145909469', 'name': 'Wei Huang'}, {'authorId': '2106277178', 'name': 'Rui Lv'}, {'authorId': '3374015', 'name': 'Zhenya Huang'}, {'authorId': '2220892203', 'name': 'Guanhao Zhao'}, {'authorId': '1852415', 'name': 'Zheng Zhang'}, {'authorId': '2196832819', 'name': 'Qingyang Mao'}, {'authorId': '2144180493', 'name': 'Shijin Wang'}, {'authorId': '2173129111', 'name': 'Enhong Chen'}]","['University of Science and Technology of China', 'State Key Laboratory of Cognitive Intelligence']",['China'],2023-06
2306.10933,Yunjia Xi,"Yunjia Xi, Weiwen Liu, Jianghao Lin, Jieming Zhu, Bo Chen, Ruiming
  Tang, Weinan Zhang, Rui Zhang, Yong Yu",Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models,,,,,cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recommender systems play a vital role in various online services. However, the insulated nature of training and deploying separately within a specific domain limits their access to open-world knowledge. Recently, the emergence of large language models (LLMs) has shown promise in bridging this gap by encoding extensive world knowledge and demonstrating reasoning capability. Nevertheless, previous attempts to directly use LLMs as recommenders have not achieved satisfactory results. In this work, we propose an Open-World Knowledge Augmented Recommendation Framework with Large Language Models, dubbed KAR, to acquire two types of external knowledge from LLMs -- the reasoning knowledge on user preferences and the factual knowledge on items. We introduce factorization prompting to elicit accurate reasoning on user preferences. The generated reasoning and factual knowledge are effectively transformed and condensed into augmented vectors by a hybrid-expert adaptor in order to be compatible with the recommendation task. The obtained vectors can then be directly used to enhance the performance of any recommendation model. We also ensure efficient inference by preprocessing and prestoring the knowledge from the LLM. Extensive experiments show that KAR significantly outperforms the state-of-the-art baselines and is compatible with a wide range of recommendation algorithms. ","[{'version': 'v1', 'created': 'Mon, 19 Jun 2023 13:44:48 GMT'}, {'version': 'v2', 'created': 'Sun, 25 Jun 2023 07:05:30 GMT'}]",2023-06-27,"[['Xi', 'Yunjia', ''], ['Liu', 'Weiwen', ''], ['Lin', 'Jianghao', ''], ['Zhu', 'Jieming', ''], ['Chen', 'Bo', ''], ['Tang', 'Ruiming', ''], ['Zhang', 'Weinan', ''], ['Zhang', 'Rui', ''], ['Yu', 'Yong', '']]",0,0,2023-06-19,2,9,1,0,0,0,c5481668f78ab0c8ef2de9230f2fc1ce27eea6e4,259202547.0,https://www.semanticscholar.org/paper/c5481668f78ab0c8ef2de9230f2fc1ce27eea6e4,arXiv.org,2023.0,63.0,12.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2056826850', 'name': 'Yunjia Xi'}, {'authorId': '2130051800', 'name': 'Weiwen Liu'}, {'authorId': '2144908858', 'name': 'Jianghao Lin'}, {'authorId': '2108997533', 'name': 'Jieming Zhu'}, {'authorId': '92633145', 'name': 'Bo Chen'}, {'authorId': '2824766', 'name': 'Ruiming Tang'}, {'authorId': '2108309275', 'name': 'Weinan Zhang'}, {'authorId': '144142354', 'name': 'Rui Zhang'}, {'authorId': '2156098229', 'name': 'Yong Yu'}]","['Shanghai Jiao Tong University', 'Shenzhen, China Yong Yu', 'Huawei Technologies (China)', 'Cicatelli Associates']","['China', 'United States']",2023-06
2306.10968,Shaolei Zhang,"Shaolei Zhang, Qingkai Fang, Zhuocheng Zhang, Zhengrui Ma, Yan Zhou,
  Langlin Huang, Mengyu Bu, Shangtong Gui, Yunji Chen, Xilin Chen, Yang Feng",BayLing: Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation for Large Language Models,Try BayLing's online demo at http://nlp.ict.ac.cn/bayling/demo,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Large language models (LLMs) have demonstrated remarkable prowess in language understanding and generation. Advancing from foundation LLMs to instructionfollowing LLMs, instruction tuning plays a vital role in aligning LLMs to human preferences. However, the existing LLMs are usually focused on English, leading to inferior performance in non-English languages. In order to improve the performance for non-English languages, it is necessary to collect language-specific training data for foundation LLMs and construct language-specific instructions for instruction tuning, both of which are heavy loads. To minimize human workload, we propose to transfer the capabilities of language generation and instruction following from English to other languages through an interactive translation task. We have developed BayLing, an instruction-following LLM by utilizing LLaMA as the foundation LLM and automatically constructing interactive translation instructions for instructing tuning. Extensive assessments demonstrate that BayLing achieves comparable performance to GPT-3.5-turbo, despite utilizing a considerably smaller parameter size of only 13 billion. Experimental results on translation tasks show that BayLing achieves 95% of single-turn translation capability compared to GPT-4 with automatic evaluation and 96% of interactive translation capability compared to GPT-3.5-turbo with human evaluation. To estimate the performance on general tasks, we created a multi-turn instruction test set called BayLing-80. The experimental results on BayLing-80 indicate that BayLing achieves 89% of performance compared to GPT-3.5-turbo. BayLing also demonstrates outstanding performance on knowledge assessment of Chinese GaoKao and English SAT, second only to GPT-3.5-turbo among a multitude of instruction-following LLMs. Demo, homepage, code and models of BayLing are available. ","[{'version': 'v1', 'created': 'Mon, 19 Jun 2023 14:30:52 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Jun 2023 11:31:50 GMT'}]",2023-06-22,"[['Zhang', 'Shaolei', ''], ['Fang', 'Qingkai', ''], ['Zhang', 'Zhuocheng', ''], ['Ma', 'Zhengrui', ''], ['Zhou', 'Yan', ''], ['Huang', 'Langlin', ''], ['Bu', 'Mengyu', ''], ['Gui', 'Shangtong', ''], ['Chen', 'Yunji', ''], ['Chen', 'Xilin', ''], ['Feng', 'Yang', '']]",0,1,2023-06-19,2,11,2,3,1,2,4656ec3210a3d28f9f30a8ec8a202aae7ed3bf1f,259203913.0,https://www.semanticscholar.org/paper/4656ec3210a3d28f9f30a8ec8a202aae7ed3bf1f,arXiv.org,2023.0,29.0,15.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Education', 'source': 's2-fos-model'}]","[{'authorId': '2480521', 'name': 'Shaolei Zhang'}, {'authorId': '2159548678', 'name': 'Qingkai Fang'}, {'authorId': '2220326406', 'name': 'Zhuocheng Zhang'}, {'authorId': '2125040671', 'name': 'Zhengrui Ma'}, {'authorId': '2150920535', 'name': 'Yan Zhou'}, {'authorId': '2220326166', 'name': 'Langlin Huang'}, {'authorId': '2220288217', 'name': 'Mengyu Bu'}, {'authorId': '2211433994', 'name': 'Shangtong Gui'}, {'authorId': '2220685592', 'name': 'Yunji Chen'}, {'authorId': '2108945209', 'name': 'Xilin Chen'}, {'authorId': '144599698', 'name': 'Yang Feng'}]",['Institute of Computing Technology'],['China'],2023-06
2306.11027,Kun Zhou,"Wayne Xin Zhao, Kun Zhou, Beichen Zhang, Zheng Gong, Zhipeng Chen,
  Yuanhang Zhou, Ji-Rong Wen, Jing Sha, Shijin Wang, Cong Liu, Guoping Hu",JiuZhang 2.0: A Unified Chinese Pre-trained Language Model for Multi-task Mathematical Problem Solving,"Accepted by KDD 2023 ADS track, the 2.0 version of JiuZhang
  (arxiv:2206.06315v1)",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although pre-trained language models~(PLMs) have recently advanced the research progress in mathematical reasoning, they are not specially designed as a capable multi-task solver, suffering from high cost for multi-task deployment (\eg a model copy for a task) and inferior performance on complex mathematical problems in practical applications. To address these issues, in this paper, we propose \textbf{JiuZhang~2.0}, a unified Chinese PLM specially for multi-task mathematical problem solving. Our idea is to maintain a moderate-sized model and employ the \emph{cross-task knowledge sharing} to improve the model capacity in a multi-task setting. Specially, we construct a Mixture-of-Experts~(MoE) architecture for modeling mathematical text, so as to capture the common mathematical knowledge across tasks. For optimizing the MoE architecture, we design \emph{multi-task continual pre-training} and \emph{multi-task fine-tuning} strategies for multi-task adaptation. These training strategies can effectively decompose the knowledge from the task data and establish the cross-task sharing via expert networks. In order to further improve the general capacity of solving different complex tasks, we leverage large language models~(LLMs) as complementary models to iteratively refine the generated solution by our PLM, via in-context learning. Extensive experiments have demonstrated the effectiveness of our model. ","[{'version': 'v1', 'created': 'Mon, 19 Jun 2023 15:45:36 GMT'}]",2023-06-21,"[['Zhao', 'Wayne Xin', ''], ['Zhou', 'Kun', ''], ['Zhang', 'Beichen', ''], ['Gong', 'Zheng', ''], ['Chen', 'Zhipeng', ''], ['Zhou', 'Yuanhang', ''], ['Wen', 'Ji-Rong', ''], ['Sha', 'Jing', ''], ['Wang', 'Shijin', ''], ['Liu', 'Cong', ''], ['Hu', 'Guoping', '']]",0,0,2023-06-19,1,11,2,0,0,0,fae57797d357bfa3b39b220336d1a2e8deba5318,259203134.0,https://www.semanticscholar.org/paper/fae57797d357bfa3b39b220336d1a2e8deba5318,Knowledge Discovery and Data Mining,2023.0,59.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2542603', 'name': 'Wayne Xin Zhao'}, {'authorId': '1423651904', 'name': 'Kun Zhou'}, {'authorId': '2107926615', 'name': 'Beichen Zhang'}, {'authorId': '2164092564', 'name': 'Zheng Gong'}, {'authorId': '2111335050', 'name': 'Zhipeng Chen'}, {'authorId': '2116568362', 'name': 'Yuanhang Zhou'}, {'authorId': '153693432', 'name': 'Ji-rong Wen'}, {'authorId': '2165225571', 'name': 'Jing Sha'}, {'authorId': '2108620507', 'name': 'Shijin Wang'}, {'authorId': '2155398718', 'name': 'Cong Liu'}, {'authorId': '2090465180', 'name': 'Guoping Hu'}]","['iFLYTEK Research State Key Laboratory of Cognitive Intelligence Hefei, China iFLYTEK AI Research (Central China) Wuhan, China', 'Shijin Wang, Cong Liu, Guoping Hu. 2023. JiuZhang', ""KDD '23, August 6-10, 2023, Long Beach, CA, USA"", 'Renmin University of China']","['China', 'United States']",2023-06
2306.11232,Haiping Huang,Haiping Huang,Eight challenges in developing theory of intelligence,"19 pages, 103 references, no figures",,,,q-bio.NC cond-mat.stat-mech cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  A good theory of mathematical beauty is more practical than any current observation, as new predictions of physical reality can be verified self-consistently. This belief applies to the current status of understanding deep neural networks including large language models and even the biological intelligence. Toy models provide a metaphor of physical reality, allowing mathematically formulating that reality (i.e., the so-called theory), which can be updated as more conjectures are justified or refuted. One does not need to pack all details into a model, but rather, more abstract models are constructed, as complex systems like brains or deep networks have many sloppy dimensions but much less stiff dimensions that strongly impact macroscopic observables. This kind of bottom-up mechanistic modeling is still promising in the modern era of understanding the natural or artificial intelligence. Here, we shed light on eight challenges in developing theory of intelligence following this theoretical paradigm. ","[{'version': 'v1', 'created': 'Tue, 20 Jun 2023 01:45:42 GMT'}]",2023-06-21,"[['Huang', 'Haiping', '']]",0,0,2023-06-20,1,1,4,0,0,0,f60137ff115c4741abb40c909ab94dfee92becd9,259202577.0,https://www.semanticscholar.org/paper/f60137ff115c4741abb40c909ab94dfee92becd9,arXiv.org,2023.0,111.0,1.0,0.0,True,"['Computer Science', 'Biology', 'Physics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Biology', 'source': 'external'}, {'category': 'Physics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2146284428', 'name': 'Haiping Huang'}]",['Sun Yat-sen University'],['China'],2023-06
2306.11335,Yuhang Wen,"Pengzhen Ren, Kaidong Zhang, Hetao Zheng, Zixuan Li, Yuhang Wen,
  Fengda Zhu, Mas Ma, Xiaodan Liang",RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with Progressive Reasoning Tasks,,,,,cs.RO cs.AI cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, the advent of pre-trained large-scale language models (LLMs) like ChatGPT and GPT-4 have significantly advanced the machine's natural language understanding capabilities. This breakthrough has allowed us to seamlessly integrate these open-source LLMs into a unified robot simulator environment to help robots accurately understand and execute human natural language instructions. To this end, in this work, we introduce a realistic robotic manipulation simulator and build a Robotic Manipulation with Progressive Reasoning Tasks (RM-PRT) benchmark on this basis. Specifically, the RM-PRT benchmark builds a new high-fidelity digital twin scene based on Unreal Engine 5, which includes 782 categories, 2023 objects, and 15K natural language instructions generated by ChatGPT for a detailed evaluation of robot manipulation. We propose a general pipeline for the RM-PRT benchmark that takes as input multimodal prompts containing natural language instructions and automatically outputs actions containing the movement and position transitions. We set four natural language understanding tasks with progressive reasoning levels and evaluate the robot's ability to understand natural language instructions in two modes of adsorption and grasping. In addition, we also conduct a comprehensive analysis and comparison of the differences and advantages of 10 different LLMs in instruction understanding and generation quality. We hope the new simulator and benchmark will facilitate future research on language-guided robotic manipulation. Project website: https://necolizer.github.io/RM-PRT/ . ","[{'version': 'v1', 'created': 'Tue, 20 Jun 2023 07:06:04 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Jun 2023 06:56:47 GMT'}]",2023-06-22,"[['Ren', 'Pengzhen', ''], ['Zhang', 'Kaidong', ''], ['Zheng', 'Hetao', ''], ['Li', 'Zixuan', ''], ['Wen', 'Yuhang', ''], ['Zhu', 'Fengda', ''], ['Ma', 'Mas', ''], ['Liang', 'Xiaodan', '']]",1,1,2023-06-20,2,8,4,2,0,2,9dbb39eccbcd31b8f6b4ff0a2c96f61a7c34e54b,259202791.0,https://www.semanticscholar.org/paper/9dbb39eccbcd31b8f6b4ff0a2c96f61a7c34e54b,arXiv.org,2023.0,63.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '51056374', 'name': 'Pengzhen Ren'}, {'authorId': '47969138', 'name': 'Kaiwen Zhang'}, {'authorId': '2220886900', 'name': 'Hetao Zheng'}, {'authorId': '46947005', 'name': 'Zixuan Li'}, {'authorId': '2153698189', 'name': 'Yuhang Wen'}, {'authorId': '94228656', 'name': 'Fengda Zhu'}, {'authorId': '2220606052', 'name': 'Mas Ma'}, {'authorId': '2153397698', 'name': 'Xiaodan Liang'}]","['Monash University', 'Sun Yat-sen University', 'Mohamed bin Zayed University of Artificial Intelligence']","['China', 'United Arab Emirates', 'Australia']",2023-06
2306.11372,Xuan Phi Nguyen,"Xuan-Phi Nguyen and Sharifah Mahani Aljunied and Shafiq Joty and
  Lidong Bing",Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts,Pre-print,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Large language models (LLMs) are known to effectively perform tasks by simply observing few exemplars. However, in low-resource languages, obtaining such hand-picked exemplars can still be challenging, where unsupervised techniques may be necessary. Moreover, competent generative capabilities of LLMs are observed only in high-resource languages, while their performances among under-represented languages fall behind due to pre-training data imbalance. To elicit LLMs' ability onto low-resource languages without any supervised data, we propose to assemble synthetic exemplars from a diverse set of high-resource languages to prompt the LLMs to translate from any language into English. These prompts are then used to create intra-lingual exemplars to perform tasks in the target languages. Our unsupervised prompting method performs on par with supervised few-shot learning in LLMs of different sizes for translations between English and 13 Indic and 21 African low-resource languages. We also show that fine-tuning a 7B model on data generated from our method helps it perform competitively with a 175B model. In non-English translation tasks, our method even outperforms supervised prompting by up to 3 chrF++ in many low-resource languages. When evaluated on zero-shot multilingual summarization, our method surpasses other English-pivoting baselines by up to 4 ROUGE-L and is also favored by GPT-4. ","[{'version': 'v1', 'created': 'Tue, 20 Jun 2023 08:27:47 GMT'}]",2023-06-21,"[['Nguyen', 'Xuan-Phi', ''], ['Aljunied', 'Sharifah Mahani', ''], ['Joty', 'Shafiq', ''], ['Bing', 'Lidong', '']]",0,1,2023-06-20,1,4,2,1,0,1,e0867e9f3a715851a90d17423f7f3b33a2a66bb1,259203268.0,https://www.semanticscholar.org/paper/e0867e9f3a715851a90d17423f7f3b33a2a66bb1,arXiv.org,2023.0,0.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1399659909', 'name': 'Xuan-Phi Nguyen'}, {'authorId': '30537701', 'name': 'Sharifah Mahani Aljunied'}, {'authorId': '2708940', 'name': 'Shafiq R. Joty'}, {'authorId': '1996394', 'name': 'Lidong Bing'}]","['Alibaba', 'Nanyang Technological University']","['China', 'Singapore']",2023-06
2306.11489,Linyao Yang,Linyao Yang and Hongyang Chen and Zhao Li and Xiao Ding and Xindong Wu,ChatGPT is not Enough: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, ChatGPT, a representative large language model (LLM), has gained considerable attention due to its powerful emergent abilities. Some researchers suggest that LLMs could potentially replace structured knowledge bases like knowledge graphs (KGs) and function as parameterized knowledge bases. However, while LLMs are proficient at learning probabilistic language patterns based on large corpus and engaging in conversations with humans, they, like previous smaller pre-trained language models (PLMs), still have difficulty in recalling facts while generating knowledge-grounded contents. To overcome these limitations, researchers have proposed enhancing data-driven PLMs with knowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus improving their performance to generate texts requiring factual knowledge and providing more informed responses to user queries. This paper reviews the studies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced pre-trained language models (KGPLMs) as well as their applications. Inspired by existing studies on KGPLM, this paper proposes to enhance LLMs with KGs by developing knowledge graph-enhanced large language models (KGLLMs). KGLLM provides a solution to enhance LLMs' factual reasoning ability, opening up new avenues for LLM research. ","[{'version': 'v1', 'created': 'Tue, 20 Jun 2023 12:21:06 GMT'}]",2023-06-21,"[['Yang', 'Linyao', ''], ['Chen', 'Hongyang', ''], ['Li', 'Zhao', ''], ['Ding', 'Xiao', ''], ['Wu', 'Xindong', '']]",1,1,2023-06-20,1,5,2,1,0,1,d89d8c93f62e66f8f22f0fbbddc688fb4017a15d,259203671.0,https://www.semanticscholar.org/paper/d89d8c93f62e66f8f22f0fbbddc688fb4017a15d,arXiv.org,2023.0,155.0,18.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '40577530', 'name': 'Lin F. Yang'}, {'authorId': '2144212252', 'name': 'Hongyang Chen'}, {'authorId': '2193503627', 'name': 'Zhao Li'}, {'authorId': '2117434160', 'name': 'Xiao Ding'}, {'authorId': '1748808', 'name': 'Xindong Wu'}]","['Zhejiang Lab', 'Harbin Institute of Technology']",['China'],2023-06
2306.11507,Yue Huang,Yue Huang and Qihui Zhang and Philip S. Y and Lichao Sun,TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models,We are currently expanding this work and welcome collaborators!,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) such as ChatGPT, have gained significant attention due to their impressive natural language processing capabilities. It is crucial to prioritize human-centered principles when utilizing these models. Safeguarding the ethical and moral compliance of LLMs is of utmost importance. However, individual ethical issues have not been well studied on the latest LLMs. Therefore, this study aims to address these gaps by introducing a new benchmark -- TrustGPT. TrustGPT provides a comprehensive evaluation of LLMs in three crucial areas: toxicity, bias, and value-alignment. Initially, TrustGPT examines toxicity in language models by employing toxic prompt templates derived from social norms. It then quantifies the extent of bias in models by measuring quantifiable toxicity values across different groups. Lastly, TrustGPT assesses the value of conversation generation models from both active value-alignment and passive value-alignment tasks. Through the implementation of TrustGPT, this research aims to enhance our understanding of the performance of conversation generation models and promote the development of language models that are more ethical and socially responsible. ","[{'version': 'v1', 'created': 'Tue, 20 Jun 2023 12:53:39 GMT'}]",2023-06-21,"[['Huang', 'Yue', ''], ['Zhang', 'Qihui', ''], ['Y', 'Philip S.', ''], ['Sun', 'Lichao', '']]",1,1,2023-06-20,1,4,2,1,0,1,9d81ec931b85d6c6cf3453126670cd7a30a689e7,259202452.0,https://www.semanticscholar.org/paper/9d81ec931b85d6c6cf3453126670cd7a30a689e7,arXiv.org,2023.0,61.0,8.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108715615', 'name': 'Yue Huang'}, {'authorId': '46324457', 'name': 'Qihui Zhang'}, {'authorId': '2191036692', 'name': 'Philip S. Yu'}, {'authorId': '46732871', 'name': 'Lichao Sun'}]","['Lehigh University', 'University of Illinois at Chicago', 'Sichuan University']","['China', 'United States']",2023-06
2306.11585,Minghua He,"Minghua He, Nanfei Gu, Yuntao Shi, Qionghui Zhang, Yaying Chen",FAIR: A Causal Framework for Accurately Inferring Judgments Reversals,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Artificial intelligence researchers have made significant advances in legal intelligence in recent years. However, the existing studies have not focused on the important value embedded in judgments reversals, which limits the improvement of the efficiency of legal intelligence. In this paper, we propose a causal Framework for Accurately Inferring case Reversals (FAIR), which models the problem of judgments reversals based on real Chinese judgments. We mine the causes of judgments reversals by causal inference methods and inject the obtained causal relationships into the neural network as a priori knowledge. And then, our framework is validated on a challenging dataset as a legal judgment prediction task. The experimental results show that our framework can tap the most critical factors in judgments reversal, and the obtained causal relationships can effectively improve the neural network's performance. In addition, we discuss the generalization ability of large language models for legal intelligence tasks using ChatGPT as an example. Our experiment has found that the generalization ability of large language models still has defects, and mining causal relationships can effectively improve the accuracy and explain ability of model predictions. ","[{'version': 'v1', 'created': 'Tue, 20 Jun 2023 15:02:25 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Jul 2023 14:31:10 GMT'}]",2023-07-24,"[['He', 'Minghua', ''], ['Gu', 'Nanfei', ''], ['Shi', 'Yuntao', ''], ['Zhang', 'Qionghui', ''], ['Chen', 'Yaying', '']]",1,1,2023-06-20,2,5,1,1,0,1,e73692f030adc112c10cc365cf584ebdf8fc7899,259203304.0,https://www.semanticscholar.org/paper/e73692f030adc112c10cc365cf584ebdf8fc7899,arXiv.org,2023.0,24.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2163015691', 'name': 'Minghua He'}, {'authorId': '2220302508', 'name': 'Nanfei Gu'}, {'authorId': '2155598287', 'name': 'Yuntao Shi'}, {'authorId': '2220327158', 'name': 'Qionghui Zhang'}, {'authorId': '2220322546', 'name': 'Yaying Chen'}]","['University of Southern California', 'Jilin University']","['China', 'United States']",2023-06
2306.11698,Boxin Wang,"Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang,
  Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T.
  Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng,
  Sanmi Koyejo, Dawn Song, Bo Li",DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,,,,,cs.CL cs.AI cs.CR,http://creativecommons.org/licenses/by-sa/4.0/,"  Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in capabilities, capturing the interest of practitioners and the public alike. Yet, while the literature on the trustworthiness of GPT models remains limited, practitioners have proposed employing capable GPT models for sensitive applications to healthcare and finance - where mistakes can be costly. To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives - including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history. We also find that although GPT-4 is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more vulnerable given jailbreaking system or user prompts, potentially due to the reason that GPT-4 follows the (misleading) instructions more precisely. Our work illustrates a comprehensive trustworthiness evaluation of GPT models and sheds light on the trustworthiness gaps. Our benchmark is publicly available at https://decodingtrust.github.io/. ","[{'version': 'v1', 'created': 'Tue, 20 Jun 2023 17:24:23 GMT'}]",2023-06-21,"[['Wang', 'Boxin', ''], ['Chen', 'Weixin', ''], ['Pei', 'Hengzhi', ''], ['Xie', 'Chulin', ''], ['Kang', 'Mintong', ''], ['Zhang', 'Chenhui', ''], ['Xu', 'Chejian', ''], ['Xiong', 'Zidi', ''], ['Dutta', 'Ritik', ''], ['Schaeffer', 'Rylan', ''], ['Truong', 'Sang T.', ''], ['Arora', 'Simran', ''], ['Mazeika', 'Mantas', ''], ['Hendrycks', 'Dan', ''], ['Lin', 'Zinan', ''], ['Cheng', 'Yu', ''], ['Koyejo', 'Sanmi', ''], ['Song', 'Dawn', ''], ['Li', 'Bo', '']]",0,1,2023-06-20,1,19,3,2,0,2,e5d5d32b098671cf71dcb4ca2039524154183db8,259202782.0,https://www.semanticscholar.org/paper/e5d5d32b098671cf71dcb4ca2039524154183db8,arXiv.org,2023.0,0.0,28.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '51454501', 'name': 'Boxin Wang'}, {'authorId': '2108947078', 'name': 'Weixin Chen'}, {'authorId': '146922081', 'name': 'Hengzhi Pei'}, {'authorId': '150961077', 'name': 'Chulin Xie'}, {'authorId': '2153110066', 'name': 'Mintong Kang'}, {'authorId': '2146063748', 'name': 'Chenhui Zhang'}, {'authorId': '2153079868', 'name': 'Chejian Xu'}, {'authorId': '2155965725', 'name': 'Zidi Xiong'}, {'authorId': '151183175', 'name': 'Ritik Dutta'}, {'authorId': '1749176844', 'name': 'Rylan Schaeffer'}, {'authorId': '2127191901', 'name': 'Sang Truong'}, {'authorId': '2220301960', 'name': 'Simran Arora'}, {'authorId': '16787428', 'name': 'Mantas Mazeika'}, {'authorId': '3422872', 'name': 'Dan Hendrycks'}, {'authorId': '2695029', 'name': 'Zi-Han Lin'}, {'authorId': '98742322', 'name': 'Yuk-Kit Cheng'}, {'authorId': '123593472', 'name': 'Sanmi Koyejo'}, {'authorId': '143711382', 'name': 'D. Song'}, {'authorId': '2165245120', 'name': 'Bo Li'}]","['Shanghai FRP Research Institute (China)', 'University of California, Berkeley', 'Stanford University', 'Chinese University of Hong Kong', 'Microsoft', 'Center for AI Safety', 'University of Illinois Urbana-Champaign']","['China', 'United States']",2023-06
2306.11702,Chen Zui,"Zui Chen, Lei Cao, Sam Madden",Lingua Manga: A Generic Large Language Model Centric System for Data Curation,"4 pages, 6 figures, VLDB 2023 Demo paper",,,,cs.DB cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Data curation is a wide-ranging area which contains many critical but time-consuming data processing tasks. However, the diversity of such tasks makes it challenging to develop a general-purpose data curation system. To address this issue, we present Lingua Manga, a user-friendly and versatile system that utilizes pre-trained large language models. Lingua Manga offers automatic optimization for achieving high performance and label efficiency while facilitating flexible and rapid development. Through three example applications with distinct objectives and users of varying levels of technical proficiency, we demonstrate that Lingua Manga can effectively assist both skilled programmers and low-code or even no-code users in addressing data curation challenges. ","[{'version': 'v1', 'created': 'Tue, 20 Jun 2023 17:30:02 GMT'}, {'version': 'v2', 'created': 'Fri, 1 Sep 2023 15:40:40 GMT'}]",2023-09-04,"[['Chen', 'Zui', ''], ['Cao', 'Lei', ''], ['Madden', 'Sam', '']]",0,0,2023-06-20,2,3,2,0,0,0,d0cfff2d0187dbe6829a8b2a785e3bc22595cf72,259203938.0,https://www.semanticscholar.org/paper/d0cfff2d0187dbe6829a8b2a785e3bc22595cf72,Proceedings of the VLDB Endowment,2023.0,20.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '48354042', 'name': 'Zui Chen'}, {'authorId': '50260667', 'name': 'Lei Cao'}, {'authorId': '144478906', 'name': 'S. Madden'}]","['Massachusetts Institute of Technology', 'Tsinghua University', 'U of Arizona/MIT Tucson, Arizona']","['China', 'United States']",2023-06
2306.11731,Huiguo He,"Huiguo He, Tianfu Wang, Huan Yang, Jianlong Fu, Nicholas Jing Yuan,
  Jian Yin, Hongyang Chao, Qi Zhang",Learning Profitable NFT Image Diffusions via Multiple Visual-Policy Guided Reinforcement Learning,Accepted by ACM-MM 2023,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We study the task of generating profitable Non-Fungible Token (NFT) images from user-input texts. Recent advances in diffusion models have shown great potential for image generation. However, existing works can fall short in generating visually-pleasing and highly-profitable NFT images, mainly due to the lack of 1) plentiful and fine-grained visual attribute prompts for an NFT image, and 2) effective optimization metrics for generating high-quality NFT images. To solve these challenges, we propose a Diffusion-based generation framework with Multiple Visual-Policies as rewards (i.e., Diffusion-MVP) for NFT images. The proposed framework consists of a large language model (LLM), a diffusion-based image generator, and a series of visual rewards by design. First, the LLM enhances a basic human input (such as ""panda"") by generating more comprehensive NFT-style prompts that include specific visual attributes, such as ""panda with Ninja style and green background."" Second, the diffusion-based image generator is fine-tuned using a large-scale NFT dataset to capture fine-grained image styles and accessory compositions of popular NFT elements. Third, we further propose to utilize multiple visual-policies as optimization goals, including visual rarity levels, visual aesthetic scores, and CLIP-based text-image relevances. This design ensures that our proposed Diffusion-MVP is capable of minting NFT images with high visual quality and market value. To facilitate this research, we have collected the largest publicly available NFT image dataset to date, consisting of 1.5 million high-quality images with corresponding texts and market values. Extensive experiments including objective evaluations and user studies demonstrate that our framework can generate NFT images showing more visually engaging elements and higher market value, compared with SOTA approaches. ","[{'version': 'v1', 'created': 'Tue, 20 Jun 2023 17:59:46 GMT'}, {'version': 'v2', 'created': 'Thu, 17 Aug 2023 17:57:26 GMT'}]",2023-08-21,"[['He', 'Huiguo', ''], ['Wang', 'Tianfu', ''], ['Yang', 'Huan', ''], ['Fu', 'Jianlong', ''], ['Yuan', 'Nicholas Jing', ''], ['Yin', 'Jian', ''], ['Chao', 'Hongyang', ''], ['Zhang', 'Qi', '']]",0,0,2023-06-20,2,8,1,0,0,0,240fd6b32c48dcca9d847cdf9f9930b3e717709e,259203620.0,https://www.semanticscholar.org/paper/240fd6b32c48dcca9d847cdf9f9930b3e717709e,ACM Multimedia,2023.0,82.0,0.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1643688444', 'name': 'Huiguo He'}, {'authorId': '2155412982', 'name': 'Tianfu Wang'}, {'authorId': '46402216', 'name': 'Huan Yang'}, {'authorId': '3247966', 'name': 'Jianlong Fu'}, {'authorId': '1677643972', 'name': 'N. Yuan'}, {'authorId': '2152938031', 'name': 'Jian Yin'}, {'authorId': '47636228', 'name': 'Hongyang Chao'}, {'authorId': '2145908108', 'name': 'Qi Zhang'}]","['Sun Yat-sen University', 'Microsoft', 'University of Science and Technology of China', 'Sun Yat-Sun University Tianfu Wang', 'Sun Yat-Sun University Qi Zhang']","['China', 'United States']",2023-06
2306.11732,Junting Pan,"Junting Pan, Ziyi Lin, Yuying Ge, Xiatian Zhu, Renrui Zhang, Yi Wang,
  Yu Qiao, Hongsheng Li",Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen Large Language Models,,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Video Question Answering (VideoQA) has been significantly advanced from the scaling of recent Large Language Models (LLMs). The key idea is to convert the visual information into the language feature space so that the capacity of LLMs can be fully exploited. Existing VideoQA methods typically take two paradigms: (1) learning cross-modal alignment, and (2) using an off-the-shelf captioning model to describe the visual data. However, the first design needs costly training on many extra multi-modal data, whilst the second is further limited by limited domain generalization. To address these limitations, a simple yet effective Retrieving-to-Answer (R2A) framework is proposed.Given an input video, R2A first retrieves a set of semantically similar texts from a generic text corpus using a pre-trained multi-modal model (e.g., CLIP). With both the question and the retrieved texts, a LLM (e.g., DeBERTa) can be directly used to yield a desired answer. Without the need for cross-modal fine-tuning, R2A allows for all the key components (e.g., LLM, retrieval model, and text corpus) to plug-and-play. Extensive experiments on several VideoQA benchmarks show that despite with 1.3B parameters and no fine-tuning, our R2A can outperform the 61 times larger Flamingo-80B model even additionally trained on nearly 2.1B multi-modal data. ","[{'version': 'v1', 'created': 'Thu, 15 Jun 2023 20:56:20 GMT'}]",2023-06-21,"[['Pan', 'Junting', ''], ['Lin', 'Ziyi', ''], ['Ge', 'Yuying', ''], ['Zhu', 'Xiatian', ''], ['Zhang', 'Renrui', ''], ['Wang', 'Yi', ''], ['Qiao', 'Yu', ''], ['Li', 'Hongsheng', '']]",0,0,2023-06-15,1,8,1,0,0,0,08706687b02ae65e2303011d0013e96b79007d2c,259203031.0,https://www.semanticscholar.org/paper/08706687b02ae65e2303011d0013e96b79007d2c,arXiv.org,2023.0,68.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '7588865', 'name': 'Junting Pan'}, {'authorId': '2112305433', 'name': 'Ziyi Lin'}, {'authorId': '51123495', 'name': 'Yuying Ge'}, {'authorId': '2116163653', 'name': 'Xiatian Zhu'}, {'authorId': '2115713503', 'name': 'Renrui Zhang'}, {'authorId': '46393411', 'name': 'Yi Wang'}, {'authorId': '2059129841', 'name': 'Y. Qiao'}, {'authorId': '47893312', 'name': 'Hongsheng Li'}]","['Shanghai Artificial Intelligence Laboratory', 'Chinese University of Hong Kong', 'University of Hong Kong', 'University of Surrey']","['China', 'United Kingdom', 'Hong Kong']",2023-06
2306.11976,Zheni Zeng,"Zheni Zeng, Bangchen Yin, Shipeng Wang, Jiarui Liu, Cheng Yang,
  Haishen Yao, Xingzhi Sun, Maosong Sun, Guotong Xie, Zhiyuan Liu",Interactive Molecular Discovery with Natural Language,,,,,cs.CL physics.chem-ph q-bio.BM,http://creativecommons.org/licenses/by/4.0/,"  Natural language is expected to be a key medium for various human-machine interactions in the era of large language models. When it comes to the biochemistry field, a series of tasks around molecules (e.g., property prediction, molecule mining, etc.) are of great significance while having a high technical threshold. Bridging the molecule expressions in natural language and chemical language can not only hugely improve the interpretability and reduce the operation difficulty of these tasks, but also fuse the chemical knowledge scattered in complementary materials for a deeper comprehension of molecules. Based on these benefits, we propose the conversational molecular design, a novel task adopting natural language for describing and editing target molecules. To better accomplish this task, we design ChatMol, a knowledgeable and versatile generative pre-trained model, enhanced by injecting experimental property information, molecular spatial knowledge, and the associations between natural and chemical languages into it. Several typical solutions including large language models (e.g., ChatGPT) are evaluated, proving the challenge of conversational molecular design and the effectiveness of our knowledge enhancement method. Case observations and analysis are conducted to provide directions for further exploration of natural-language interaction in molecular discovery. ","[{'version': 'v1', 'created': 'Wed, 21 Jun 2023 02:05:48 GMT'}]",2023-06-22,"[['Zeng', 'Zheni', ''], ['Yin', 'Bangchen', ''], ['Wang', 'Shipeng', ''], ['Liu', 'Jiarui', ''], ['Yang', 'Cheng', ''], ['Yao', 'Haishen', ''], ['Sun', 'Xingzhi', ''], ['Sun', 'Maosong', ''], ['Xie', 'Guotong', ''], ['Liu', 'Zhiyuan', '']]",1,1,2023-06-21,1,10,3,1,0,1,d85e98bd13a5b4c81577ab3908ff532f445ffa3a,259212118.0,https://www.semanticscholar.org/paper/d85e98bd13a5b4c81577ab3908ff532f445ffa3a,arXiv.org,2023.0,49.0,1.0,0.0,True,"['Computer Science', 'Physics', 'Biology']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Physics', 'source': 'external'}, {'category': 'Biology', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1633538428', 'name': 'Zheni Zeng'}, {'authorId': '2220347713', 'name': 'Bangchen Yin'}, {'authorId': '2108595546', 'name': 'Shipeng Wang'}, {'authorId': '2136138284', 'name': 'Jia-Rou Liu'}, {'authorId': '3443627', 'name': 'Cheng Yang'}, {'authorId': '2162194', 'name': 'Haishen Yao'}, {'authorId': '2109230415', 'name': 'Xingzhi Sun'}, {'authorId': '1753344', 'name': 'Maosong Sun'}, {'authorId': '2118178075', 'name': 'Guotong Xie'}, {'authorId': '2141313179', 'name': 'Zhiyuan Liu'}]","['Beijing University of Posts and Telecommunications', 'Tsinghua University', 'PingAn Technology']",['China'],2023-06
2306.12028,Jieshan Chen,"Yu Cheng, Jieshan Chen, Qing Huang, Zhenchang Xing, Xiwei Xu and
  Qinghua Lu",Prompt Sapper: A LLM-Empowered Production Tool for Building AI Chains,"18 pages, 3 figures",,,,cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The emergence of foundation models, such as large language models (LLMs) GPT-4 and text-to-image models DALL-E, has opened up numerous possibilities across various domains. People can now use natural language (i.e. prompts) to communicate with AI to perform tasks. While people can use foundation models through chatbots (e.g., ChatGPT), chat, regardless of the capabilities of the underlying models, is not a production tool for building reusable AI services. APIs like LangChain allow for LLM-based application development but require substantial programming knowledge, thus posing a barrier. To mitigate this, we propose the concept of AI chain and introduce the best principles and practices that have been accumulated in software engineering for decades into AI chain engineering, to systematise AI chain engineering methodology. We also develop a no-code integrated development environment, Prompt Sapper, which embodies these AI chain engineering principles and patterns naturally in the process of building AI chains, thereby improving the performance and quality of AI chains. With Prompt Sapper, AI chain engineers can compose prompt-based AI services on top of foundation models through chat-based requirement analysis and visual programming. Our user study evaluated and demonstrated the efficiency and correctness of Prompt Sapper. ","[{'version': 'v1', 'created': 'Wed, 21 Jun 2023 05:31:00 GMT'}]",2023-06-22,"[['Cheng', 'Yu', ''], ['Chen', 'Jieshan', ''], ['Huang', 'Qing', ''], ['Xing', 'Zhenchang', ''], ['Xu', 'Xiwei', ''], ['Lu', 'Qinghua', '']]",1,1,2023-06-21,1,6,1,2,0,2,6115bb844813b68ff64188c241cbd1a11a53c88b,259211999.0,https://www.semanticscholar.org/paper/6115bb844813b68ff64188c241cbd1a11a53c88b,arXiv.org,2023.0,54.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Business', 'source': 's2-fos-model'}]","[{'authorId': '2218855657', 'name': 'Yu Cheng'}, {'authorId': '1421235702', 'name': 'Jieshan Chen'}, {'authorId': '2181288908', 'name': 'Qing Huang'}, {'authorId': '3138980', 'name': 'Zhenchang Xing'}, {'authorId': '3087664', 'name': 'Xiwei Xu'}, {'authorId': '2151674574', 'name': 'Qinghua Lu'}]","['Jiangxi Normal University', 'Australian National University']","['China', 'Australia']",2023-06
2306.12174,Zhuo Deng,"Weihao Gao, Zhuo Deng, Zhiyuan Niu, Fuju Rong, Chucheng Chen, Zheng
  Gong, Wenze Zhang, Daimin Xiao, Fang Li, Zhenjie Cao, Zhaoyi Ma, Wenbin Wei,
  Lan Ma",OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant based on Instructions and Dialogue,"OphGLM:The first ophthalmology large language-and-vision assistant
  based on instructions and dialogue",,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Large multimodal language models (LMMs) have achieved significant success in general domains. However, due to the significant differences between medical images and text and general web content, the performance of LMMs in medical scenarios is limited. In ophthalmology, clinical diagnosis relies on multiple modalities of medical images, but unfortunately, multimodal ophthalmic large language models have not been explored to date. In this paper, we study and construct an ophthalmic large multimodal model. Firstly, we use fundus images as an entry point to build a disease assessment and diagnosis pipeline to achieve common ophthalmic disease diagnosis and lesion segmentation. Then, we establish a new ophthalmic multimodal instruction-following and dialogue fine-tuning dataset based on disease-related knowledge data and publicly available real-world medical dialogue. We introduce visual ability into the large language model to complete the ophthalmic large language and vision assistant (OphGLM). Our experimental results demonstrate that the OphGLM model performs exceptionally well, and it has the potential to revolutionize clinical applications in ophthalmology. The dataset, code, and models will be made publicly available at https://github.com/ML-AILab/OphGLM. ","[{'version': 'v1', 'created': 'Wed, 21 Jun 2023 11:09:48 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Jun 2023 01:31:10 GMT'}]",2023-06-23,"[['Gao', 'Weihao', ''], ['Deng', 'Zhuo', ''], ['Niu', 'Zhiyuan', ''], ['Rong', 'Fuju', ''], ['Chen', 'Chucheng', ''], ['Gong', 'Zheng', ''], ['Zhang', 'Wenze', ''], ['Xiao', 'Daimin', ''], ['Li', 'Fang', ''], ['Cao', 'Zhenjie', ''], ['Ma', 'Zhaoyi', ''], ['Wei', 'Wenbin', ''], ['Ma', 'Lan', '']]",0,0,2023-06-21,2,13,1,0,0,0,0f8d12775a4685575f1489796b5dee9e11fbdfb5,259212438.0,https://www.semanticscholar.org/paper/0f8d12775a4685575f1489796b5dee9e11fbdfb5,arXiv.org,2023.0,25.0,4.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Medicine', 'source': 's2-fos-model'}]","[{'authorId': '2153577134', 'name': 'Weihao Gao'}, {'authorId': '2075350702', 'name': 'Zhuo Deng'}, {'authorId': '2220347592', 'name': 'Zhiyuan Niu'}, {'authorId': '2220347621', 'name': 'Fuju Rong'}, {'authorId': '1476827220', 'name': 'Chucheng Chen'}, {'authorId': '2147296261', 'name': 'Zheng Gong'}, {'authorId': '2159605320', 'name': 'Wenze Zhang'}, {'authorId': '2195684927', 'name': 'Daimin Xiao'}, {'authorId': '2180257461', 'name': 'Fangjun Li'}, {'authorId': '2113997618', 'name': 'Zhenjie Cao'}, {'authorId': '2220757882', 'name': 'Zhaoyi Ma'}, {'authorId': '2220693112', 'name': 'Wenbin Wei'}, {'authorId': '2220380082', 'name': 'Lan Ma'}]","['National Health Commission Capacity Building and Continuing Education Center', 'Tsinghua University', 'Beijing Tongren Hospital']",['China'],2023-06
2306.12725,Senbao Shi,"Senbao Shi, Zhenran Xu, Baotian Hu, Min Zhang",Generative Multimodal Entity Linking,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal Entity Linking (MEL) is the task of mapping mentions with multimodal contexts to the referent entities from a knowledge base (e.g. Wikipedia). Existing MEL methods mainly focus on designing complex multimodal interaction mechanisms and require fine-tuning all model parameters, which can be prohibitively costly and difficult to scale in the era of Large Language Models (LLMs). In this work, we propose GEMEL, a simple yet effective Generative Multimodal Entity Linking framework based on LLMs, which directly generates target entity names. We keep the vision and language model frozen and only train a feature mapper to enable cross-modality interactions. To adapt LLMs to the MEL task, we take advantage of the emergent in-context learning capability of LLMs by retrieving multimodal instances as demonstrations. Extensive experiments show that, with only ~0.3% of the model parameters fine-tuned, GEMEL achieves state-of-the-art results on two well-established MEL datasets (7.7% accuracy gains on WikiDiverse and 8.8% accuracy gains on WikiMEL). The performance gain stems from mitigating the popularity bias of LLM predictions and disambiguating less common entities effectively. Further analysis verifies the generality and scalability of GEMEL. Our approach is compatible with any off-the-shelf language model, paving the way towards an efficient and general solution for utilizing LLMs in the MEL task. ","[{'version': 'v1', 'created': 'Thu, 22 Jun 2023 07:57:19 GMT'}, {'version': 'v2', 'created': 'Fri, 18 Aug 2023 05:12:12 GMT'}]",2023-08-21,"[['Shi', 'Senbao', ''], ['Xu', 'Zhenran', ''], ['Hu', 'Baotian', ''], ['Zhang', 'Min', '']]",0,0,2023-06-22,2,4,1,0,0,0,f2f5c0d00e6a4ccaf099c11a9790aa0afefe611f,259224516.0,https://www.semanticscholar.org/paper/f2f5c0d00e6a4ccaf099c11a9790aa0afefe611f,arXiv.org,2023.0,41.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1399890501', 'name': 'Senbao Shi'}, {'authorId': '2149238600', 'name': 'Zhenran Xu'}, {'authorId': '2142726660', 'name': 'Baotian Hu'}, {'authorId': '50495870', 'name': 'M. Zhang'}]",['Harbin Institute of Technology'],['China'],2023-06
2306.13063,Zhiyuan Hu,"Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He,
  Bryan Hooi",Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs,11 Pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The task of empowering large language models (LLMs) to accurately express their confidence, referred to as confidence elicitation, is essential in ensuring reliable and trustworthy decision-making processes. Previous methods, which primarily rely on model logits, have become less suitable for LLMs and even infeasible with the rise of closed-source LLMs (e.g., commercialized LLM APIs). This leads to a growing need to explore the untapped area of \emph{non-logit-based} approaches to estimate the uncertainty of LLMs. Hence, in this study, we investigate approaches for confidence elicitation that do not require model fine-tuning or access to proprietary information. We introduce three categories of methods: verbalize-based, consistency-based, and their hybrid methods for benchmarking, and evaluate their performance across five types of datasets and four widely-used LLMs. Our analysis of these methods uncovers several key insights: 1) LLMs often exhibit a high degree of overconfidence when verbalizing their confidence; 2) Prompting strategies such as CoT, Top-K and Multi-step confidences improve calibration of verbalized confidence; 3) Consistency-based methods outperform the verbalized confidences in most cases, with particularly notable improvements on the arithmetic reasoning task; 4) Hybrid methods consistently deliver the best performance over their baselines, thereby emerging as a promising state-of-the-art approach; 5) Despite these advancements, all investigated methods continue to struggle with challenging tasks, such as those requiring professional knowledge, leaving significant scope for improvement of confidence elicitation. ","[{'version': 'v1', 'created': 'Thu, 22 Jun 2023 17:31:44 GMT'}]",2023-06-23,"[['Xiong', 'Miao', ''], ['Hu', 'Zhiyuan', ''], ['Lu', 'Xinyang', ''], ['Li', 'Yifei', ''], ['Fu', 'Jie', ''], ['He', 'Junxian', ''], ['Hooi', 'Bryan', '']]",0,0,2023-06-22,1,7,1,0,0,0,8f7297454d7f44365b9bcda5ebb9439a43daf5e6,259224389.0,https://www.semanticscholar.org/paper/8f7297454d7f44365b9bcda5ebb9439a43daf5e6,arXiv.org,2023.0,47.0,28.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '38827926', 'name': 'Miao Xiong'}, {'authorId': '48430820', 'name': 'Zhiyuan Hu'}, {'authorId': '2179626088', 'name': 'Xinyang Lu'}, {'authorId': '2157866322', 'name': 'Yifei Li'}, {'authorId': '2215497308', 'name': 'Jie Fu'}, {'authorId': '2109932032', 'name': 'Junxian He'}, {'authorId': '2019961', 'name': 'Bryan Hooi'}]","['cole Polytechnique Fdrale de Lausanne', 'Beijing Academy of Artificial Intelligence', 'Hong Kong University of Science and Technology', 'Miao Xiong', 'National University of Singapore']","['China', 'Singapore', 'Switzerland']",2023-06
2306.13394,Chaoyou Fu,"Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu
  Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Rongrong
  Ji",MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models,"Project page:
  https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the annotations of instruction-answer pairs are all manually designed. The concise instruction design allows us to fairly compare MLLMs, instead of struggling in prompt engineering. Besides, with such an instruction, we can also easily carry out quantitative statistics. A total of 12 advanced MLLMs are comprehensively evaluated on our MME, which not only suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization. ","[{'version': 'v1', 'created': 'Fri, 23 Jun 2023 09:22:36 GMT'}, {'version': 'v2', 'created': 'Sun, 2 Jul 2023 02:56:04 GMT'}]",2023-07-04,"[['Fu', 'Chaoyou', ''], ['Chen', 'Peixian', ''], ['Shen', 'Yunhang', ''], ['Qin', 'Yulei', ''], ['Zhang', 'Mengdan', ''], ['Lin', 'Xu', ''], ['Qiu', 'Zhenyu', ''], ['Lin', 'Wei', ''], ['Yang', 'Jinrui', ''], ['Zheng', 'Xiawu', ''], ['Li', 'Ke', ''], ['Sun', 'Xing', ''], ['Ji', 'Rongrong', '']]",0,0,2023-06-23,2,13,1,0,0,0,697e0add95e880bd42e00bef838181e105f91981,259243928.0,https://www.semanticscholar.org/paper/697e0add95e880bd42e00bef838181e105f91981,arXiv.org,2023.0,43.0,54.0,18.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '50876245', 'name': 'Chaoyou Fu'}, {'authorId': '2158171389', 'name': 'Peixian Chen'}, {'authorId': '1862967', 'name': 'Yunhang Shen'}, {'authorId': '1717081702', 'name': 'Yulei Qin'}, {'authorId': '2516588', 'name': 'Mengdan Zhang'}, {'authorId': '2220508735', 'name': 'Xu Lin'}, {'authorId': '2220506751', 'name': 'Zhenyu Qiu'}, {'authorId': '3153798', 'name': 'Wei Lin'}, {'authorId': '2109732008', 'name': 'Jinrui Yang'}, {'authorId': '51056401', 'name': 'Xiawu Zheng'}, {'authorId': '2149140038', 'name': 'Ke Li'}, {'authorId': '143900241', 'name': 'Xing Sun'}, {'authorId': '1572139630', 'name': 'Rongrong Ji'}]","['Xiamen University', 'Tencent']",['China'],2023-06
2306.13549,Shukang Yin,"Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, Enhong
  Chen",A Survey on Multimodal Large Language Models,"Project
  page:https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models",,,,cs.CV cs.AI cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models. ","[{'version': 'v1', 'created': 'Fri, 23 Jun 2023 15:21:52 GMT'}]",2023-06-26,"[['Yin', 'Shukang', ''], ['Fu', 'Chaoyou', ''], ['Zhao', 'Sirui', ''], ['Li', 'Ke', ''], ['Sun', 'Xing', ''], ['Xu', 'Tong', ''], ['Chen', 'Enhong', '']]",0,0,2023-06-23,1,7,4,0,0,0,ebedc4d7a2356090904baba4104ef0832bc236df,259243718.0,https://www.semanticscholar.org/paper/ebedc4d7a2356090904baba4104ef0832bc236df,arXiv.org,2023.0,108.0,53.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2187312485', 'name': 'Shukang Yin'}, {'authorId': '50876245', 'name': 'Chaoyou Fu'}, {'authorId': '2111005730', 'name': 'Sirui Zhao'}, {'authorId': '2149141063', 'name': 'Ke Li'}, {'authorId': '143900241', 'name': 'Xing Sun'}, {'authorId': '50383766', 'name': 'Tong Xu'}, {'authorId': '2173129111', 'name': 'Enhong Chen'}]","['University of Science and Technology of China', 'Tencent']",['China'],2023-06
2306.14096,Yinyu Lan,"Yinyu Lan, Yanru Wu, Wang Xu, Weiqiang Feng, Youhao Zhang",Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models,"Accepted by (FinLLM 2023)@IJCAI 2023,
  https://finllm.github.io/workshop/#/fcb",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Entity-level fine-grained sentiment analysis in the financial domain is a crucial subtask of sentiment analysis and currently faces numerous challenges. The primary challenge stems from the lack of high-quality and large-scale annotated corpora specifically designed for financial text sentiment analysis, which in turn limits the availability of data necessary for developing effective text processing techniques. Recent advancements in large language models (LLMs) have yielded remarkable performance in natural language processing tasks, primarily centered around language pattern matching. In this paper, we propose a novel and extensive Chinese fine-grained financial sentiment analysis dataset, FinChina SA, for enterprise early warning. We thoroughly evaluate and experiment with well-known existing open-source LLMs using our dataset. We firmly believe that our dataset will serve as a valuable resource to advance the exploration of real-world financial sentiment analysis tasks, which should be the focus of future research. The FinChina SA dataset is publicly available at https://github.com/YerayL/FinChina-SA ","[{'version': 'v1', 'created': 'Sun, 25 Jun 2023 02:24:30 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Jul 2023 05:14:39 GMT'}, {'version': 'v3', 'created': 'Fri, 21 Jul 2023 08:57:38 GMT'}, {'version': 'v4', 'created': 'Mon, 24 Jul 2023 00:58:11 GMT'}, {'version': 'v5', 'created': 'Fri, 15 Sep 2023 08:19:44 GMT'}]",2023-09-18,"[['Lan', 'Yinyu', ''], ['Wu', 'Yanru', ''], ['Xu', 'Wang', ''], ['Feng', 'Weiqiang', ''], ['Zhang', 'Youhao', '']]",0,0,2023-06-25,5,5,2,0,0,0,7991ed80c9309477c1e49f8ad78e97c0704fa74e,259251651.0,https://www.semanticscholar.org/paper/7991ed80c9309477c1e49f8ad78e97c0704fa74e,arXiv.org,2023.0,28.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2105841076', 'name': 'Yinyu Lan'}, {'authorId': '2216550412', 'name': 'Yanru Wu'}, {'authorId': '2220598146', 'name': 'Wang Xu'}, {'authorId': '2220538178', 'name': 'Weiqiang Feng'}, {'authorId': '13584407', 'name': 'Youhao Zhang'}]",['FinChina AI Research'],['China'],2023-06
2306.14122,Yujian Feng,Feng Chen and Yujian Feng,Chain-of-Thought Prompt Distillation for Multimodal Named Entity Recognition and Multimodal Relation Extraction,modification,,,,cs.CL cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Multimodal Named Entity Recognition (MNER) and Multimodal Relation Extraction (MRE) necessitate the fundamental reasoning capacity for intricate linguistic and multimodal comprehension. In this study, we explore distilling the reasoning ability of large language models (LLMs) into a more compact student model by generating a \textit{chain of thought} (CoT) -- a sequence of intermediate reasoning steps. Specifically, we commence by exemplifying the elicitation of such reasoning ability from LLMs through CoT prompts covering multi-grain (noun, sentence, multimodality) and data-augmentation (style, entity, image) dimensions. Subsequently, we present a novel conditional prompt distillation method to assimilate the commonsense reasoning ability from LLMs, thereby enhancing the utility of the student model in addressing text-only inputs without the requisite addition of image and CoT knowledge. Extensive experiments reveal that our approach attains state-of-the-art accuracy and manifests a plethora of advantages concerning interpretability, data efficiency, and cross-domain generalization on MNER and MRE datasets. ","[{'version': 'v1', 'created': 'Sun, 25 Jun 2023 04:33:56 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Jun 2023 04:51:00 GMT'}, {'version': 'v3', 'created': 'Wed, 23 Aug 2023 05:04:58 GMT'}]",2023-08-24,"[['Chen', 'Feng', ''], ['Feng', 'Yujian', '']]",0,0,2023-06-25,3,2,2,0,0,0,b03118cadc0eb92cf4c4d75f2db3f24c8b15c460,261076559.0,https://www.semanticscholar.org/paper/b03118cadc0eb92cf4c4d75f2db3f24c8b15c460,,2023.0,33.0,0.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '144180429', 'name': 'F. Chen'}, {'authorId': '1500382103', 'name': 'Yujian Feng'}]","['Alibaba', 'Nanjing University of Posts and Telecommunications']",['China'],2023-06
2306.14222,Haohan Zhang,"Haohan Zhang, Fengrui Hua, Chengjin Xu, Jian Guo, Hao Kong, Ruiting
  Zuo",Unveiling the Potential of Sentiment: Can Large Language Models Predict Chinese Stock Price Movements?,,,,,cs.CL cs.AI q-fin.ST,http://creativecommons.org/licenses/by/4.0/,"  The rapid advancement of Large Language Models (LLMs) has led to extensive discourse regarding their potential to boost the return of quantitative stock trading strategies. This discourse primarily revolves around harnessing the remarkable comprehension capabilities of LLMs to extract sentiment factors which facilitate informed and high-frequency investment portfolio adjustments. To ensure successful implementations of these LLMs into the analysis of Chinese financial texts and the subsequent trading strategy development within the Chinese stock market, we provide a rigorous and encompassing benchmark as well as a standardized back-testing framework aiming at objectively assessing the efficacy of various types of LLMs in the specialized domain of sentiment factor extraction from Chinese news text data. To illustrate how our benchmark works, we reference three distinctive models: 1) the generative LLM (ChatGPT), 2) the Chinese language-specific pre-trained LLM (Erlangshen-RoBERTa), and 3) the financial domain-specific fine-tuned LLM classifier(Chinese FinBERT). We apply them directly to the task of sentiment factor extraction from large volumes of Chinese news summary texts. We then proceed to building quantitative trading strategies and running back-tests under realistic trading scenarios based on the derived sentiment factors and evaluate their performances with our benchmark. By constructing such a comparative analysis, we invoke the question of what constitutes the most important element for improving a LLM's performance on extracting sentiment factors. And by ensuring that the LLMs are evaluated on the same benchmark, following the same standardized experimental procedures that are designed with sufficient expertise in quantitative trading, we make the first stride toward answering such a question. ","[{'version': 'v1', 'created': 'Sun, 25 Jun 2023 12:08:44 GMT'}]",2023-06-27,"[['Zhang', 'Haohan', ''], ['Hua', 'Fengrui', ''], ['Xu', 'Chengjin', ''], ['Guo', 'Jian', ''], ['Kong', 'Hao', ''], ['Zuo', 'Ruiting', '']]",1,1,2023-06-25,1,6,3,1,0,1,2b47921d12e3d1c6fc4738ad9019029c1bf0fb76,259251760.0,https://www.semanticscholar.org/paper/2b47921d12e3d1c6fc4738ad9019029c1bf0fb76,arXiv.org,2023.0,20.0,3.0,0.0,True,"['Computer Science', 'Economics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Economics', 'source': 'external'}, {'category': 'Business', 'source': 's2-fos-model'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Economics', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '2135736345', 'name': 'Haohan Zhang'}, {'authorId': '2156737826', 'name': 'Fengrui Hua'}, {'authorId': '103750594', 'name': 'Chengjin Xu'}, {'authorId': '2188226506', 'name': 'Jian Guo'}, {'authorId': '2220537570', 'name': 'Hao Kong'}, {'authorId': '49205510', 'name': 'Ruiting Zuo'}]","['Hong Kong University of Science and Technology', 'International Digital Economy Academy (IDEA)']",['China'],2023-06
2306.14238,Xiang Zhang,"Xiang Zhang, Zichun Zhou, Chen Ming, Yi-Yang Sun",GPT-assisted learning of structure-property relationships by graph neural networks: Application to rare-earth doped phosphors,"12 pages, 4 figures",,,,cond-mat.mtrl-sci,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Applications of machine learning techniques in materials science are often based on two key ingredients, a set of empirical descriptors and a database of a particular material property of interest. The advent of graph neural networks, such as the Crystal Graph Convolutional Neural Network (CGCNN), demonstrates the possibility of directly mapping the relationship between material structures and properties without employing empirical descriptors. Another exciting recent advancement is in large language models such as OpenAI's GPT-4, which demonstrates competency at reading comprehension tasks and holds great promise for accelerating the acquisition of databases on material properties. Here, we utilize the combination of GPT-4 and CGCNN to develop rare-earth doped phosphors for solid-state lighting. GPT-4 is applied to data-mine chemical formulas and emission wavelengths of 264 Eu(II)-doped phosphors from 274 papers. A CGCNN model is trained on the acquired dataset, achieving a test $R^2$ of 0.77. The model is then used to screen over 40,000 inorganic materials to make predictions on the emission wavelengths. We also demonstrate the possibility of leveraging transfer learning to fine-tune a bandgap-predicting CGCNN model towards the prediction of phosphor emission wavelengths. The workflow requires minimal human supervision, little domain knowledge about phosphors, and is generalizable to other material properties. ","[{'version': 'v1', 'created': 'Sun, 25 Jun 2023 13:17:44 GMT'}]",2023-06-27,"[['Zhang', 'Xiang', ''], ['Zhou', 'Zichun', ''], ['Ming', 'Chen', ''], ['Sun', 'Yi-Yang', '']]",0,1,2023-06-25,1,4,1,1,0,1,24a58b1917136d696867071c94c013d7ccd06a12,259252016.0,https://www.semanticscholar.org/paper/24a58b1917136d696867071c94c013d7ccd06a12,,2023.0,56.0,0.0,0.0,False,['Physics'],"[{'category': 'Physics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '48505793', 'name': 'Xiang Zhang'}, {'authorId': '2220572227', 'name': 'Zichun Zhou'}, {'authorId': '2059272243', 'name': 'Chen Ming'}, {'authorId': '2212322374', 'name': 'Yi Sun'}]",['Shanghai Institute of Ceramics'],['China'],2023-06
2306.14321,Yilun Zhao,"Yilun Zhao, Chen Zhao, Linyong Nan, Zhenting Qi, Wenlin Zhang, Xiangru
  Tang, Boyu Mi, Dragomir Radev",RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations,Accepted at ACL 2023,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Despite significant progress having been made in question answering on tabular data (Table QA), it's unclear whether, and to what extent existing Table QA models are robust to task-specific perturbations, e.g., replacing key question entities or shuffling table columns. To systematically study the robustness of Table QA models, we propose a benchmark called RobuT, which builds upon existing Table QA datasets (WTQ, WikiSQL-Weak, and SQA) and includes human-annotated adversarial perturbations in terms of table header, table content, and question. Our results indicate that both state-of-the-art Table QA models and large language models (e.g., GPT-3) with few-shot learning falter in these adversarial sets. We propose to address this problem by using large language models to generate adversarial examples to enhance training, which significantly improves the robustness of Table QA models. Our data and code is publicly available at https://github.com/yilunzhao/RobuT. ","[{'version': 'v1', 'created': 'Sun, 25 Jun 2023 19:23:21 GMT'}]",2023-06-27,"[['Zhao', 'Yilun', ''], ['Zhao', 'Chen', ''], ['Nan', 'Linyong', ''], ['Qi', 'Zhenting', ''], ['Zhang', 'Wenlin', ''], ['Tang', 'Xiangru', ''], ['Mi', 'Boyu', ''], ['Radev', 'Dragomir', '']]",0,1,2023-06-25,1,8,2,1,0,1,341d516cc858dc92ba14a788ef40d0559b5a2b26,259252406.0,https://www.semanticscholar.org/paper/341d516cc858dc92ba14a788ef40d0559b5a2b26,Annual Meeting of the Association for Computational Linguistics,2023.0,59.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '46316984', 'name': 'Yilun Zhao'}, {'authorId': '145756130', 'name': 'Chen Zhao'}, {'authorId': '51990260', 'name': 'Linyong Nan'}, {'authorId': '2186056193', 'name': 'Zhenting Qi'}, {'authorId': '2220571367', 'name': 'Wenlin Zhang'}, {'authorId': '47274259', 'name': 'Xiangru Tang'}, {'authorId': '2124195718', 'name': 'Boyu Mi'}, {'authorId': '9215251', 'name': 'Dragomir R. Radev'}]","['New York University', 'Zhejiang University', 'Yale University']","['China', 'United States']",2023-06
2306.14397,Li Ke,"Li Ke, Hong Sheng, Fu Cai, Zhang Yunhe and Liu Ming",Discriminating Human-authored from ChatGPT-Generated Code Via Discernable Feature Analysis,"11 pages, 8 figures, 3 tables",,,,cs.SE cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The ubiquitous adoption of Large Language Generation Models (LLMs) in programming has underscored the importance of differentiating between human-written code and code generated by intelligent models. This paper specifically aims to distinguish code generated by ChatGPT from that authored by humans. Our investigation reveals disparities in programming style, technical level, and readability between these two sources. Consequently, we develop a discriminative feature set for differentiation and evaluate its efficacy through ablation experiments. Additionally, we devise a dataset cleansing technique, which employs temporal and spatial segmentation, to mitigate the dearth of datasets and to secure high-caliber, uncontaminated datasets. To further enrich data resources, we employ ""code transformation,"" ""feature transformation,"" and ""feature customization"" techniques, generating an extensive dataset comprising 10,000 lines of ChatGPT-generated code. The salient contributions of our research include: proposing a discriminative feature set yielding high accuracy in differentiating ChatGPT-generated code from human-authored code in binary classification tasks; devising methods for generating extensive ChatGPT-generated codes; and introducing a dataset cleansing strategy that extracts immaculate, high-grade code datasets from open-source repositories, thus achieving exceptional accuracy in code authorship attribution tasks. ","[{'version': 'v1', 'created': 'Mon, 26 Jun 2023 03:15:06 GMT'}, {'version': 'v2', 'created': 'Tue, 4 Jul 2023 09:23:08 GMT'}]",2023-07-06,"[['Ke', 'Li', ''], ['Sheng', 'Hong', ''], ['Cai', 'Fu', ''], ['Yunhe', 'Zhang', ''], ['Ming', 'Liu', '']]",1,1,2023-06-26,2,5,2,1,0,1,1d57d10fb1e51c0130795cafbb511a7a9c196a3a,259342909.0,https://www.semanticscholar.org/paper/1d57d10fb1e51c0130795cafbb511a7a9c196a3a,2023 IEEE 34th International Symposium on Software Reliability Engineering Workshops (ISSREW),2023.0,35.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2265236507', 'name': 'Ke Li'}, {'authorId': '2221150337', 'name': 'Sheng Hong'}, {'authorId': '2221111279', 'name': 'Cai Fu'}, {'authorId': '2108123875', 'name': 'Yunhe Zhang'}, {'authorId': '2265302946', 'name': 'Ming Liu'}]","['University of Birmingham', 'Yunhe Zhang', 'Huazhong University of Science and Technology']","['China', 'United Kingdom']",2023-06
2306.14795,Xin Chen,"Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, Tao Chen",MotionGPT: Human Motion as a Foreign Language,Project Page: https://github.com/OpenMotionLab/MotionGPT,,,,cs.CV cs.CL cs.GR,http://creativecommons.org/licenses/by/4.0/,"  Though the advancement of pre-trained large language models unfolds, the exploration of building a unified model for language and other multi-modal data, such as motion, remains challenging and untouched so far. Fortunately, human motion displays a semantic coupling akin to human language, often perceived as a form of body language. By fusing language data with large-scale motion models, motion-language pre-training that can enhance the performance of motion-related tasks becomes feasible. Driven by this insight, we propose MotionGPT, a unified, versatile, and user-friendly motion-language model to handle multiple motion-relevant tasks. Specifically, we employ the discrete vector quantization for human motion and transfer 3D motion into motion tokens, similar to the generation process of word tokens. Building upon this ""motion vocabulary"", we perform language modeling on both motion and text in a unified manner, treating human motion as a specific language. Moreover, inspired by prompt learning, we pre-train MotionGPT with a mixture of motion-language data and fine-tune it on prompt-based question-and-answer tasks. Extensive experiments demonstrate that MotionGPT achieves state-of-the-art performances on multiple motion tasks including text-driven motion generation, motion captioning, motion prediction, and motion in-between. ","[{'version': 'v1', 'created': 'Mon, 26 Jun 2023 15:53:02 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Jul 2023 03:39:19 GMT'}]",2023-07-21,"[['Jiang', 'Biao', ''], ['Chen', 'Xin', ''], ['Liu', 'Wen', ''], ['Yu', 'Jingyi', ''], ['Yu', 'Gang', ''], ['Chen', 'Tao', '']]",0,1,2023-06-26,2,6,3,0,0,0,d212fa27f5868f0fd106e1a7bba908fd47da0816,259262201.0,https://www.semanticscholar.org/paper/d212fa27f5868f0fd106e1a7bba908fd47da0816,arXiv.org,2023.0,73.0,14.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2186557954', 'name': 'Biao Jiang'}, {'authorId': '2145230192', 'name': 'Xin Chen'}, {'authorId': '2117935179', 'name': 'Wen Liu'}, {'authorId': '2155403153', 'name': 'Jingyi Yu'}, {'authorId': '2116565951', 'name': 'Gang Yu'}, {'authorId': '144799987', 'name': 'Tao Chen'}]","['Tencent', 'Fudan University', 'ShanghaiTech University']",['China'],2023-06
2306.14824,Li Dong,"Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming
  Ma, Furu Wei",Kosmos-2: Grounding Multimodal Large Language Models to the World,20 pages,,,,cs.CL cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2. ","[{'version': 'v1', 'created': 'Mon, 26 Jun 2023 16:32:47 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Jun 2023 09:11:34 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Jul 2023 05:41:34 GMT'}]",2023-07-14,"[['Peng', 'Zhiliang', ''], ['Wang', 'Wenhui', ''], ['Dong', 'Li', ''], ['Hao', 'Yaru', ''], ['Huang', 'Shaohan', ''], ['Ma', 'Shuming', ''], ['Wei', 'Furu', '']]",0,0,2023-06-26,3,7,2,0,0,0,3b6179c293df29e31d31cea46476f104ab6950f2,259262263.0,https://www.semanticscholar.org/paper/3b6179c293df29e31d31cea46476f104ab6950f2,arXiv.org,2023.0,42.0,73.0,11.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2087004998', 'name': 'Zhiliang Peng'}, {'authorId': '51456429', 'name': 'Wenhui Wang'}, {'authorId': '145307652', 'name': 'Li Dong'}, {'authorId': '34128716', 'name': 'Y. Hao'}, {'authorId': '3110003', 'name': 'Shaohan Huang'}, {'authorId': '2118866998', 'name': 'Shuming Ma'}, {'authorId': '49807919', 'name': 'Furu Wei'}]",['Microsoft'],['China'],2023-06
2306.15195,Zhao Zhang,"Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, Rui Zhao",Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic,,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  In human conversations, individuals can indicate relevant regions within a scene while addressing others. In turn, the other person can then respond by referring to specific regions if necessary. This natural referential ability in dialogue remains absent in current Multimodal Large Language Models (MLLMs). To fill this gap, this paper proposes an MLLM called Shikra, which can handle spatial coordinate inputs and outputs in natural language. Its architecture consists of a vision encoder, an alignment layer, and a LLM. It is designed to be straightforward and simple, without the need for extra vocabularies, position encoder, pre-/post-detection modules, or external plug-in models. All inputs and outputs are in natural language form. Referential dialogue is a superset of various vision-language (VL) tasks. Shikra can naturally handle location-related tasks like REC and PointQA, as well as conventional VL tasks such as Image Captioning and VQA. Experimental results showcase Shikra's promising performance. Furthermore, it enables numerous exciting applications, like providing mentioned objects' coordinates in chains of thoughts and comparing user-pointed regions similarities. Our code, model and dataset are accessed at https://github.com/shikras/shikra. ","[{'version': 'v1', 'created': 'Tue, 27 Jun 2023 04:31:52 GMT'}, {'version': 'v2', 'created': 'Mon, 3 Jul 2023 16:08:00 GMT'}]",2023-07-04,"[['Chen', 'Keqin', ''], ['Zhang', 'Zhao', ''], ['Zeng', 'Weili', ''], ['Zhang', 'Richong', ''], ['Zhu', 'Feng', ''], ['Zhao', 'Rui', '']]",0,0,2023-06-27,2,6,1,0,0,0,def6c12724dec95ec1276a77fd1cf7e200883bdb,259262082.0,https://www.semanticscholar.org/paper/def6c12724dec95ec1276a77fd1cf7e200883bdb,arXiv.org,2023.0,57.0,48.0,16.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '32811782', 'name': 'Ke Chen'}, {'authorId': '2156120640', 'name': 'Zhao Zhang'}, {'authorId': '13886055', 'name': 'Weili Zeng'}, {'authorId': '2109975984', 'name': 'Richong Zhang'}, {'authorId': '2075369514', 'name': 'Feng Zhu'}, {'authorId': '1395873384', 'name': 'Rui Zhao'}]",['Shanghai Jiao Tong University'],['China'],2023-06
2306.15273,Zihang Xu,"Zihang Xu, Ziqing Yang, Yiming Cui, Shijin Wang",IDOL: Indicator-oriented Logic Pre-training for Logical Reasoning,Accepted to the Findings of ACL 2023,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the field of machine reading comprehension (MRC), existing systems have surpassed the average performance of human beings in many tasks like SQuAD. However, there is still a long way to go when it comes to logical reasoning. Although some methods for it have been put forward, they either are designed in a quite complicated way or rely too much on external structures. In this paper, we proposed IDOL (InDicator-Oriented Logic Pre-training), an easy-to-understand but highly effective further pre-training task which logically strengthens the pre-trained models with the help of 6 types of logical indicators and a logically rich dataset LGP (LoGic Pre-training). IDOL achieves state-of-the-art performance on ReClor and LogiQA, the two most representative benchmarks in logical reasoning MRC, and is proven to be capable of generalizing to different pre-trained models and other types of MRC benchmarks like RACE and SQuAD 2.0 while keeping competitive general language understanding ability through testing on tasks in GLUE. Besides, at the beginning of the era of large language models, we take several of them like ChatGPT into comparison and find that IDOL still shows its advantage. ","[{'version': 'v1', 'created': 'Tue, 27 Jun 2023 07:57:42 GMT'}]",2023-06-28,"[['Xu', 'Zihang', ''], ['Yang', 'Ziqing', ''], ['Cui', 'Yiming', ''], ['Wang', 'Shijin', '']]",1,1,2023-06-27,1,4,2,1,0,1,fcd876c8cf67bab257c4bdf9c164b073711c3983,259261999.0,https://www.semanticscholar.org/paper/fcd876c8cf67bab257c4bdf9c164b073711c3983,Annual Meeting of the Association for Computational Linguistics,2023.0,30.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2156719922', 'name': 'Zihang Xu'}, {'authorId': '48599077', 'name': 'Ziqing Yang'}, {'authorId': '3043830', 'name': 'Yiming Cui'}, {'authorId': '2108620507', 'name': 'Shijin Wang'}]","['Harbin Institute of Technology', 'State Key Laboratory of Cognitive Intelligence, iFLYTEK Research, China', 'AI Research (Central China), Wuhan, China']",['China'],2023-06
2306.15401,Zheng Lian,"Zheng Lian, Licai Sun, Mingyu Xu, Haiyang Sun, Ke Xu, Zhuofan Wen,
  Shun Chen, Bin Liu, Jianhua Tao",Explainable Multimodal Emotion Reasoning,,,,,cs.MM cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal emotion recognition is an active research topic in artificial intelligence. Its primary objective is to integrate multi-modalities (such as acoustic, visual, and lexical clues) to identify human emotional states. Current works generally assume accurate emotion labels for benchmark datasets and focus on developing more effective architectures. But due to the inherent subjectivity of emotions, existing datasets often lack high annotation consistency, resulting in potentially inaccurate labels. Consequently, models built on these datasets may struggle to meet the demands of practical applications. To address this issue, it is crucial to enhance the reliability of emotion annotations. In this paper, we propose a novel task called ``\textbf{Explainable Multimodal Emotion Reasoning (EMER)}''. In contrast to previous works that primarily focus on predicting emotions, EMER takes a step further by providing explanations for these predictions. The prediction is considered correct as long as the reasoning process behind the predicted emotion is plausible. This paper presents our initial efforts on EMER, where we introduce a benchmark dataset, establish baseline models, and define evaluation metrics. Meanwhile, we observe the necessity of integrating multi-faceted capabilities to deal with EMER. Therefore, we propose the first multimodal large language model (LLM) in affective computing, called \textbf{AffectGPT}. We aim to tackle the long-standing challenge of label ambiguity and chart a path toward more reliable techniques. Furthermore, EMER offers an opportunity to evaluate the audio-video-text understanding capabilities of recent multimodal LLM. To facilitate further research, we make the code and data available at: https://github.com/zeroQiaoba/AffectGPT. ","[{'version': 'v1', 'created': 'Tue, 27 Jun 2023 11:54:57 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Jun 2023 10:26:05 GMT'}, {'version': 'v3', 'created': 'Thu, 24 Aug 2023 00:27:48 GMT'}]",2023-08-25,"[['Lian', 'Zheng', ''], ['Sun', 'Licai', ''], ['Xu', 'Mingyu', ''], ['Sun', 'Haiyang', ''], ['Xu', 'Ke', ''], ['Wen', 'Zhuofan', ''], ['Chen', 'Shun', ''], ['Liu', 'Bin', ''], ['Tao', 'Jianhua', '']]",0,1,2023-06-27,3,9,2,0,0,0,899316d60fde583dea135b82dc8506024b48bb3b,259261969.0,https://www.semanticscholar.org/paper/899316d60fde583dea135b82dc8506024b48bb3b,arXiv.org,2023.0,19.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '145824699', 'name': 'Zheng Lian'}, {'authorId': '2123571521', 'name': 'Licai Sun'}, {'authorId': '2153556263', 'name': 'Mingyu Xu'}, {'authorId': '2160552498', 'name': 'Haiyang Sun'}, {'authorId': '2117101253', 'name': 'Ke Xu'}, {'authorId': '2220627748', 'name': 'Zhuofan Wen'}, {'authorId': '2220643426', 'name': 'Shun Chen'}, {'authorId': '48265485', 'name': 'B. Liu'}, {'authorId': '2163134242', 'name': 'Jianhua Tao'}]","['University of Chinese Academy of Sciences', 'Tsinghua University', 'Chinese Academy of Sciences']",['China'],2023-06
2306.16092,Jiaxi Cui,"Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen and Li Yuan",ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have shown the potential to revolutionize natural language processing tasks in various domains, sparking great interest in vertical-specific large models. However, unlike proprietary models such as BloombergGPT and FinGPT, which have leveraged their unique data accumulations to make strides in the finance domain, there hasn't not many similar large language models in the Chinese legal domain to facilitate its digital transformation.   In this paper, we propose an open-source legal large language model named ChatLaw. Due to the importance of data quality, we carefully designed a legal domain fine-tuning dataset. Additionally, to overcome the problem of model hallucinations in legal data screening during reference data retrieval, we introduce a method that combines vector database retrieval with keyword retrieval to effectively reduce the inaccuracy of relying solely on vector database retrieval. Furthermore, we propose a self-attention method to enhance the ability of large models to overcome errors present in reference data, further optimizing the issue of model hallucinations at the model level and improving the problem-solving capabilities of large models. We also open-sourced our model and part of the data at https://github.com/PKU-YuanGroup/ChatLaw. ","[{'version': 'v1', 'created': 'Wed, 28 Jun 2023 10:48:34 GMT'}]",2023-06-29,"[['Cui', 'Jiaxi', ''], ['Li', 'Zongjian', ''], ['Yan', 'Yang', ''], ['Chen', 'Bohua', ''], ['Yuan', 'Li', '']]",0,1,2023-06-28,1,5,1,0,0,0,de519a1976d0f5005ffac09f2560b1b61b37603c,259274889.0,https://www.semanticscholar.org/paper/de519a1976d0f5005ffac09f2560b1b61b37603c,arXiv.org,2023.0,12.0,33.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2220852923', 'name': 'Jiaxi Cui'}, {'authorId': '115419547', 'name': 'Zongjia Li'}, {'authorId': '2220811322', 'name': 'Yang Yan'}, {'authorId': '2152692853', 'name': 'Bohua Chen'}, {'authorId': '2087091296', 'name': 'Li Yuan'}]",['Peking University'],['China'],2023-06
2306.16244,Yufei Huang,Yufei Huang and Deyi Xiong,CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Holistically measuring societal biases of large language models is crucial for detecting and reducing ethical risks in highly capable AI models. In this work, we present a Chinese Bias Benchmark dataset that consists of over 100K questions jointly constructed by human experts and generative language models, covering stereotypes and societal biases in 14 social dimensions related to Chinese culture and values. The curation process contains 4 essential steps: bias identification via extensive literature review, ambiguous context generation, AI-assisted disambiguous context generation, snd manual review \& recomposition. The testing instances in the dataset are automatically derived from 3K+ high-quality templates manually authored with stringent quality control. The dataset exhibits wide coverage and high diversity. Extensive experiments demonstrate the effectiveness of the dataset in detecting model bias, with all 10 publicly available Chinese large language models exhibiting strong bias in certain categories. Additionally, we observe from our experiments that fine-tuned models could, to a certain extent, heed instructions and avoid generating outputs that are morally harmful in some types, in the way of ""moral self-correction"". Our dataset and results are publicly available at \href{https://github.com/YFHuangxxxx/CBBQ}{https://github.com/YFHuangxxxx/CBBQ}, offering debiasing research opportunities to a widened community. ","[{'version': 'v1', 'created': 'Wed, 28 Jun 2023 14:14:44 GMT'}]",2023-06-29,"[['Huang', 'Yufei', ''], ['Xiong', 'Deyi', '']]",0,0,2023-06-28,1,2,2,0,0,0,e11111dfda2a1f7aa9ecb8720032739233fb72f4,259274987.0,https://www.semanticscholar.org/paper/e11111dfda2a1f7aa9ecb8720032739233fb72f4,arXiv.org,2023.0,29.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2115640120', 'name': 'Yufei Huang'}, {'authorId': '2694222', 'name': 'Deyi Xiong'}]",['Tianjin University'],['China'],2023-06
2306.16688,Zhiyu Mei,"Zhiyu Mei, Wei Fu, Guangju Wang, Huanchen Zhang, Yi Wu",SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores,"15 pages, 12 figures, 6 tables",,,,cs.DC cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The ever-growing complexity of reinforcement learning (RL) tasks demands a distributed RL system to efficiently generate and process a massive amount of data to train intelligent agents. However, existing open-source libraries suffer from various limitations, which impede their practical use in challenging scenarios where large-scale training is necessary. While industrial systems from OpenAI and DeepMind have achieved successful large-scale RL training, their system architecture and implementation details remain undisclosed to the community. In this paper, we present a novel abstraction on the dataflows of RL training, which unifies practical RL training across diverse applications into a general framework and enables fine-grained optimizations. Following this abstraction, we develop a scalable, efficient, and extensible distributed RL system called ReaLly Scalable RL (SRL). The system architecture of SRL separates major RL computation components and allows massively parallelized training. Moreover, SRL offers user-friendly and extensible interfaces for customized algorithms. Our evaluation shows that SRL outperforms existing academic libraries in both a single machine and a medium-sized cluster. In a large-scale cluster, the novel architecture of SRL leads to up to 3.7x speedup compared to the design choices adopted by the existing libraries. We also conduct a direct benchmark comparison to OpenAI's industrial system, Rapid, in the challenging hide-and-seek environment. SRL reproduces the same solution as reported by OpenAI with up to 5x speedup in wall-clock time. Furthermore, we also examine the performance of SRL in a much harder variant of the hide-and-seek environment and achieve substantial learning speedup by scaling SRL to over 15k CPU cores and 32 A100 GPUs. Notably, SRL is the first in the academic community to perform RL experiments at such a large scale. ","[{'version': 'v1', 'created': 'Thu, 29 Jun 2023 05:16:25 GMT'}, {'version': 'v2', 'created': 'Wed, 5 Jul 2023 08:16:29 GMT'}]",2023-07-06,"[['Mei', 'Zhiyu', ''], ['Fu', 'Wei', ''], ['Wang', 'Guangju', ''], ['Zhang', 'Huanchen', ''], ['Wu', 'Yi', '']]",0,0,2023-06-29,2,5,3,0,0,0,9f176eca4e0cd4d1f52f13879f328cefc11b56b4,259287121.0,https://www.semanticscholar.org/paper/9f176eca4e0cd4d1f52f13879f328cefc11b56b4,arXiv.org,2023.0,43.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2123991452', 'name': 'Zhiyu Mei'}, {'authorId': '144188710', 'name': 'Wei Fu'}, {'authorId': '2152585167', 'name': 'Guang Wang'}, {'authorId': '2043439', 'name': 'Huanchen Zhang'}, {'authorId': '2169946109', 'name': 'Yi Wu'}]","['Tsinghua University', 'Huanchen Zhang', 'Shanghai Qi Zhi Institute Shanghai, China', 'Guangju Wang']",['China'],2023-06
2306.16902,Lyuzhou Chen,"Taiyu Ban, Lyvzhou Chen, Xiangyu Wang, Huanhuan Chen",From Query Tools to Causal Architects: Harnessing Large Language Models for Advanced Causal Discovery from Data,,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) exhibit exceptional abilities for causal analysis between concepts in numerous societally impactful domains, including medicine, science, and law. Recent research on LLM performance in various causal discovery and inference tasks has given rise to a new ladder in the classical three-stage framework of causality. In this paper, we advance the current research of LLM-driven causal discovery by proposing a novel framework that combines knowledge-based LLM causal analysis with data-driven causal structure learning. To make LLM more than a query tool and to leverage its power in discovering natural and new laws of causality, we integrate the valuable LLM expertise on existing causal mechanisms into statistical analysis of objective data to build a novel and practical baseline for causal structure learning.   We introduce a universal set of prompts designed to extract causal graphs from given variables and assess the influence of LLM prior causality on recovering causal structures from data. We demonstrate the significant enhancement of LLM expertise on the quality of recovered causal structures from data, while also identifying critical challenges and issues, along with potential approaches to address them. As a pioneering study, this paper aims to emphasize the new frontier that LLMs are opening for classical causal discovery and inference, and to encourage the widespread adoption of LLM capabilities in data-driven causal analysis. ","[{'version': 'v1', 'created': 'Thu, 29 Jun 2023 12:48:00 GMT'}]",2023-06-30,"[['Ban', 'Taiyu', ''], ['Chen', 'Lyvzhou', ''], ['Wang', 'Xiangyu', ''], ['Chen', 'Huanhuan', '']]",0,0,2023-06-29,1,4,1,0,0,0,26771580661f91f75688de6aef26e98bb5e15725,259287331.0,https://www.semanticscholar.org/paper/26771580661f91f75688de6aef26e98bb5e15725,arXiv.org,2023.0,35.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Law', 'source': 's2-fos-model'}]","[{'authorId': '2137416265', 'name': 'Taiyu Ban'}, {'authorId': '2220860237', 'name': 'Lyvzhou Chen'}, {'authorId': '2144799330', 'name': 'Xiangyu Wang'}, {'authorId': '2145303232', 'name': 'Huanhuan Chen'}]",['University of Science and Technology of China'],['China'],2023-06
2306.17103,Le Zhuo,"Le Zhuo, Ruibin Yuan, Jiahao Pan, Yinghao Ma, Yizhi LI, Ge Zhang, Si
  Liu, Roger Dannenberg, Jie Fu, Chenghua Lin, Emmanouil Benetos, Wenhu Chen,
  Wei Xue, Yike Guo",LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by Whispering to ChatGPT,"9 pages, 2 figures, 5 tables, accepted by ISMIR 2023",,,,cs.CL cs.SD eess.AS,http://creativecommons.org/licenses/by-sa/4.0/,"  We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic lyrics transcription method achieving state-of-the-art performance on various lyrics transcription datasets, even in challenging genres such as rock and metal. Our novel, training-free approach utilizes Whisper, a weakly supervised robust speech recognition model, and GPT-4, today's most performant chat-based large language model. In the proposed method, Whisper functions as the ""ear"" by transcribing the audio, while GPT-4 serves as the ""brain,"" acting as an annotator with a strong performance for contextualized output selection and correction. Our experiments show that LyricWhiz significantly reduces Word Error Rate compared to existing methods in English and can effectively transcribe lyrics across multiple languages. Furthermore, we use LyricWhiz to create the first publicly available, large-scale, multilingual lyrics transcription dataset with a CC-BY-NC-SA copyright license, based on MTG-Jamendo, and offer a human-annotated subset for noise level estimation and evaluation. We anticipate that our proposed method and dataset will advance the development of multilingual lyrics transcription, a challenging and emerging task. ","[{'version': 'v1', 'created': 'Thu, 29 Jun 2023 17:01:51 GMT'}, {'version': 'v2', 'created': 'Fri, 7 Jul 2023 16:32:26 GMT'}]",2023-07-10,"[['Zhuo', 'Le', ''], ['Yuan', 'Ruibin', ''], ['Pan', 'Jiahao', ''], ['Ma', 'Yinghao', ''], ['LI', 'Yizhi', ''], ['Zhang', 'Ge', ''], ['Liu', 'Si', ''], ['Dannenberg', 'Roger', ''], ['Fu', 'Jie', ''], ['Lin', 'Chenghua', ''], ['Benetos', 'Emmanouil', ''], ['Chen', 'Wenhu', ''], ['Xue', 'Wei', ''], ['Guo', 'Yike', '']]",1,1,2023-06-29,2,14,3,2,0,2,19363e9fa47f02744d49f24b8e94c8cdf19bc4af,259287024.0,https://www.semanticscholar.org/paper/19363e9fa47f02744d49f24b8e94c8cdf19bc4af,arXiv.org,2023.0,48.0,1.0,0.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2191619409', 'name': 'Le Zhuo'}, {'authorId': '2032236274', 'name': 'Ruibin Yuan'}, {'authorId': '1944785400', 'name': 'Jiahao Pan'}, {'authorId': '2146277129', 'name': 'Yi Ma'}, {'authorId': '2129449392', 'name': 'Yizhi Li'}, {'authorId': '2143853895', 'name': 'Ge Zhang'}, {'authorId': '2220617810', 'name': 'Si Liu'}, {'authorId': '1697732', 'name': 'R. Dannenberg'}, {'authorId': '49252800', 'name': 'Jie Fu'}, {'authorId': '2268783', 'name': 'Chenghua Lin'}, {'authorId': '2109397', 'name': 'Emmanouil Benetos'}, {'authorId': '2928777', 'name': 'Wenhu Chen'}, {'authorId': '2195745651', 'name': 'Wei Xue'}, {'authorId': '2118270918', 'name': 'Yi-Ting Guo'}]","['University of Sheffield', 'Beijing Academy of Artificial Intelligence', 'University of Waterloo', 'Hong Kong University of Science and Technology', 'Queen Mary University of London', 'Carnegie Mellon University', 'Beihang University']","['Canada', 'China', 'United Kingdom', 'United States']",2023-06
2306.17177,Mohammad Belal,"Mohammad Belal, James She, Simon Wong",Leveraging ChatGPT As Text Annotation Tool For Sentiment Analysis,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Sentiment analysis is a well-known natural language processing task that involves identifying the emotional tone or polarity of a given piece of text. With the growth of social media and other online platforms, sentiment analysis has become increasingly crucial for businesses and organizations seeking to monitor and comprehend customer feedback as well as opinions. Supervised learning algorithms have been popularly employed for this task, but they require human-annotated text to create the classifier. To overcome this challenge, lexicon-based tools have been used. A drawback of lexicon-based algorithms is their reliance on pre-defined sentiment lexicons, which may not capture the full range of sentiments in natural language. ChatGPT is a new product of OpenAI and has emerged as the most popular AI product. It can answer questions on various topics and tasks. This study explores the use of ChatGPT as a tool for data labeling for different sentiment analysis tasks. It is evaluated on two distinct sentiment analysis datasets with varying purposes. The results demonstrate that ChatGPT outperforms other lexicon-based unsupervised methods with significant improvements in overall accuracy. Specifically, compared to the best-performing lexical-based algorithms, ChatGPT achieves a remarkable increase in accuracy of 20% for the tweets dataset and approximately 25% for the Amazon reviews dataset. These findings highlight the exceptional performance of ChatGPT in sentiment analysis tasks, surpassing existing lexicon-based approaches by a significant margin. The evidence suggests it can be used for annotation on different sentiment analysis events and taskss. ","[{'version': 'v1', 'created': 'Sun, 18 Jun 2023 12:20:42 GMT'}]",2023-07-03,"[['Belal', 'Mohammad', ''], ['She', 'James', ''], ['Wong', 'Simon', '']]",1,1,2023-06-18,1,3,1,1,0,1,ae5e38058e9d622666254fa873c35a449a7bb2e1,259309405.0,https://www.semanticscholar.org/paper/ae5e38058e9d622666254fa873c35a449a7bb2e1,arXiv.org,2023.0,11.0,1.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2163002537', 'name': 'Mohammad Belal'}, {'authorId': '2042066180', 'name': 'James She'}, {'authorId': '2111124581', 'name': 'Simon Wong'}]","['Hamad bin Khalifa University', 'Hong Kong University of Science and Technology']","['China', 'Qatar']",2023-06
2306.17492,Feifan Song,"Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li
  and Houfeng Wang",Preference Ranking Optimization for Human Alignment,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) often contain misleading content, emphasizing the need to align them with human values to ensure secur AI systems. Reinforcement learning from human feedback (RLHF) has been employed to achieve this alignment by combining a reward model, typically based on Bradley-Terry paired comparison, with an RL algorithm such as Proximal Policy Optimization (PPO) to optimize LLM responses. However, RLHF exhibits complexity, instability, and sensitivity to hyperparameters. In this paper, we propose Preference Ranking Optimization (PRO) as an alternative to PPO for directly aligning LLMs with the Bradley-Terry comparison. PRO extends the pairwise Bradley-Terry comparison to accommodate preference rankings of any length. By iteratively contrasting the likelihood of generating responses, PRO instructs the LLM to prioritize the best response while progressively ranking the remaining responses. In this manner, PRO effectively transforms human alignment into aligning the probability ranking of $n$ responses generated by LLM with the preference ranking of humans towards these responses. Experiments have shown that PRO outperforms existing alignment algorithms, achieving comparable results to ChatGPT and human responses through automatic-based, reward-based, GPT-4, and human evaluations. Furthermore, we demonstrate that longer, more diverse, and higher-quality preference ranking sequences can consistently enhance the performance of human alignment. ","[{'version': 'v1', 'created': 'Fri, 30 Jun 2023 09:07:37 GMT'}]",2023-07-03,"[['Song', 'Feifan', ''], ['Yu', 'Bowen', ''], ['Li', 'Minghao', ''], ['Yu', 'Haiyang', ''], ['Huang', 'Fei', ''], ['Li', 'Yongbin', ''], ['Wang', 'Houfeng', '']]",1,1,2023-06-30,1,7,2,2,0,2,19db2d61f20a6c439cc79f28ef4c9e4bf26cd20e,259308873.0,https://www.semanticscholar.org/paper/19db2d61f20a6c439cc79f28ef4c9e4bf26cd20e,arXiv.org,2023.0,45.0,32.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '66947198', 'name': 'Feifan Song'}, {'authorId': '48613402', 'name': 'Yu Bowen'}, {'authorId': '123545597', 'name': 'Minghao Li'}, {'authorId': '46493167', 'name': 'Haiyang Yu'}, {'authorId': '143857288', 'name': 'Fei Huang'}, {'authorId': '1527090216', 'name': 'Yongbin Li'}, {'authorId': '1781885', 'name': 'Houfeng Wang'}]","['Peking University', 'Alibaba']",['China'],2023-06
2307.00267,Xiaodong Gu,"Yuetian Mao, Chengcheng Wan, Yuze Jiang, Xiaodong Gu",Self-Supervised Query Reformulation for Code Search,Accepted to be published in ESEC/FSE 2023,,,,cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Automatic query reformulation is a widely utilized technology for enriching user requirements and enhancing the outcomes of code search. It can be conceptualized as a machine translation task, wherein the objective is to rephrase a given query into a more comprehensive alternative. While showing promising results, training such a model typically requires a large parallel corpus of query pairs (i.e., the original query and a reformulated query) that are confidential and unpublished by online code search engines. This restricts its practicality in software development processes. In this paper, we propose SSQR, a self-supervised query reformulation method that does not rely on any parallel query corpus. Inspired by pre-trained models, SSQR treats query reformulation as a masked language modeling task conducted on an extensive unannotated corpus of queries. SSQR extends T5 (a sequence-to-sequence model based on Transformer) with a new pre-training objective named corrupted query completion (CQC), which randomly masks words within a complete query and trains T5 to predict the masked content. Subsequently, for a given query to be reformulated, SSQR identifies potential locations for expansion and leverages the pre-trained T5 model to generate appropriate content to fill these gaps. The selection of expansions is then based on the information gain associated with each candidate. Evaluation results demonstrate that SSQR outperforms unsupervised baselines significantly and achieves competitive performance compared to supervised methods. ","[{'version': 'v1', 'created': 'Sat, 1 Jul 2023 08:17:23 GMT'}]",2023-07-04,"[['Mao', 'Yuetian', ''], ['Wan', 'Chengcheng', ''], ['Jiang', 'Yuze', ''], ['Gu', 'Xiaodong', '']]",0,0,2023-07-01,1,4,1,1,1,0,545753953f9ba7850284d64c1662b24cf2812394,259316899.0,https://www.semanticscholar.org/paper/545753953f9ba7850284d64c1662b24cf2812394,arXiv.org,2023.0,51.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2152119666', 'name': 'Yuetian Mao'}, {'authorId': '38650346', 'name': 'Chengcheng Wan'}, {'authorId': '2162844945', 'name': 'Yuze Jiang'}, {'authorId': '48531491', 'name': 'Xiaodong Gu'}]","['Shanghai Jiao Tong University', 'East China Normal University']",['China'],2023-07
2307.00329,Yanjiang Guo,"Yanjiang Guo, Yen-Jen Wang, Lihan Zha, Zheyuan Jiang, Jianyu Chen",DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment,"25 pages, 14 figures",,,,cs.RO cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) encode a vast amount of semantic knowledge and possess remarkable understanding and reasoning capabilities. Previous work has explored how to ground LLMs in robotic tasks to generate feasible and executable textual plans. However, low-level execution in the physical world may deviate from the high-level textual plan due to environmental perturbations or imperfect controller design. In this paper, we propose \textbf{DoReMi}, a novel language model grounding framework that enables immediate Detection and Recovery from Misalignments between plan and execution. Specifically, we leverage LLMs to play a dual role, aiding not only in high-level planning but also generating constraints that can indicate misalignment during execution. Then vision language models (VLMs) are utilized to detect constraint violations continuously. Our pipeline can monitor the low-level execution and enable timely recovery if certain plan-execution misalignment occurs. Experiments on various complex tasks including robot arms and humanoid robots demonstrate that our method can lead to higher task success rates and shorter task completion times. Videos of DoReMi are available at \url{https://sites.google.com/view/doremi-paper}. ","[{'version': 'v1', 'created': 'Sat, 1 Jul 2023 12:51:02 GMT'}, {'version': 'v2', 'created': 'Thu, 24 Aug 2023 10:25:12 GMT'}, {'version': 'v3', 'created': 'Sat, 30 Sep 2023 13:40:20 GMT'}]",2023-10-03,"[['Guo', 'Yanjiang', ''], ['Wang', 'Yen-Jen', ''], ['Zha', 'Lihan', ''], ['Jiang', 'Zheyuan', ''], ['Chen', 'Jianyu', '']]",0,0,2023-07-01,3,5,2,0,0,0,42b920abd44e76d73708859bfe13034555f1f8cb,259317210.0,https://www.semanticscholar.org/paper/42b920abd44e76d73708859bfe13034555f1f8cb,arXiv.org,2023.0,62.0,6.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2181339548', 'name': 'Yanjiang Guo'}, {'authorId': '2115740911', 'name': 'Yen-Jen Wang'}, {'authorId': '2221011229', 'name': 'Lihan Zha'}, {'authorId': '9432010', 'name': 'Zheyuan Jiang'}, {'authorId': '2121373038', 'name': 'Jianyu Chen'}]",['Tsinghua University'],['China'],2023-07
2307.00769,Xiang Wei,"Xiang Wei, Yufeng Chen, Ning Cheng, Xingyu Cui, Jinan Xu, Wenjuan Han",CollabKG: A Learnable Human-Machine-Cooperative Information Extraction Toolkit for (Event) Knowledge Graph Construction,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In order to construct or extend entity-centric and event-centric knowledge graphs (KG and EKG), the information extraction (IE) annotation toolkit is essential. However, existing IE toolkits have several non-trivial problems, such as not supporting multi-tasks, not supporting automatic updates. In this work, we present CollabKG, a learnable human-machine-cooperative IE toolkit for KG and EKG construction. Specifically, for the multi-task issue, CollabKG unifies different IE subtasks, including named entity recognition (NER), entity-relation triple extraction (RE), and event extraction (EE), and supports both KG and EKG. Then, combining advanced prompting-based IE technology, the human-machine-cooperation mechanism with LLMs as the assistant machine is presented which can provide a lower cost as well as a higher performance. Lastly, owing to the two-way interaction between the human and machine, CollabKG with learning ability allows self-renewal. Besides, CollabKG has several appealing features (e.g., customization, training-free, propagation, etc.) that make the system powerful, easy-to-use, and high-productivity. We holistically compare our toolkit with other existing tools on these features. Human evaluation quantitatively illustrates that CollabKG significantly improves annotation quality, efficiency, and stability simultaneously. ","[{'version': 'v1', 'created': 'Mon, 3 Jul 2023 06:18:13 GMT'}]",2023-07-04,"[['Wei', 'Xiang', ''], ['Chen', 'Yufeng', ''], ['Cheng', 'Ning', ''], ['Cui', 'Xingyu', ''], ['Xu', 'Jinan', ''], ['Han', 'Wenjuan', '']]",0,0,2023-07-03,1,6,1,0,0,0,b31c461b7c5f8c11ffd53ba2c261935479e71a3f,259317091.0,https://www.semanticscholar.org/paper/b31c461b7c5f8c11ffd53ba2c261935479e71a3f,arXiv.org,2023.0,45.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2115494116', 'name': 'Xiang Wei'}, {'authorId': '47559028', 'name': 'Yufeng Chen'}, {'authorId': '2208958032', 'name': 'Ning Cheng'}, {'authorId': '2149528155', 'name': 'Xingyu Cui'}, {'authorId': '2310092', 'name': 'Jinan Xu'}, {'authorId': '144836032', 'name': 'Wenjuan Han'}]",['Beijing Jiaotong University'],['China'],2023-07
2307.01003,Delong Chen,"Delong Chen, Jianfeng Liu, Wenliang Dai, Baoyuan Wang",Visual Instruction Tuning with Polite Flamingo,,,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent research has demonstrated that the multi-task fine-tuning of multi-modal Large Language Models (LLMs) using an assortment of annotated downstream vision-language datasets significantly enhances their performance. Yet, during this process, a side effect, which we termed as the ""multi-modal alignment tax"", surfaces. This side effect negatively impacts the model's ability to format responses appropriately -- for instance, its ""politeness"" -- due to the overly succinct and unformatted nature of raw annotations, resulting in reduced human preference. In this paper, we introduce Polite Flamingo, a multi-modal response rewriter that transforms raw annotations into a more appealing, ""polite"" format. Polite Flamingo is trained to reconstruct high-quality responses from their automatically distorted counterparts and is subsequently applied to a vast array of vision-language datasets for response rewriting. After rigorous filtering, we generate the PF-1M dataset and further validate its value by fine-tuning a multi-modal LLM with it. Combined with novel methodologies including U-shaped multi-stage tuning and multi-turn augmentation, the resulting model, Clever Flamingo, demonstrates its advantages in both multi-modal understanding and response politeness according to automated and human evaluations. ","[{'version': 'v1', 'created': 'Mon, 3 Jul 2023 13:37:00 GMT'}]",2023-07-04,"[['Chen', 'Delong', ''], ['Liu', 'Jianfeng', ''], ['Dai', 'Wenliang', ''], ['Wang', 'Baoyuan', '']]",0,0,2023-07-03,1,4,2,0,0,0,82a9b8984e26fdf234431459bdb445fbcfc3cb76,259316495.0,https://www.semanticscholar.org/paper/82a9b8984e26fdf234431459bdb445fbcfc3cb76,arXiv.org,2023.0,78.0,9.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '94210578', 'name': 'Delong Chen'}, {'authorId': '120809546', 'name': 'Jianfeng Liu'}, {'authorId': '47653392', 'name': 'Wenliang Dai'}, {'authorId': '2450889', 'name': 'Baoyuan Wang'}]",['Hong Kong University of Science and Technology'],['China'],2023-07
2307.01379,Jinhao Duan,"Jinhao Duan, Hao Cheng, Shiqi Wang, Chenan Wang, Alex Zavalny, Renjing
  Xu, Bhavya Kailkhura, Kaidi Xu",Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although Large Language Models (LLMs) have shown great potential in Natural Language Generation, it is still challenging to characterize the uncertainty of model generations, i.e., when users could trust model outputs. Our research is derived from the heuristic facts that tokens are created unequally in reflecting the meaning of generations by auto-regressive LLMs, i.e., some tokens are more relevant (or representative) than others, yet all the tokens are equally valued when estimating uncertainty. It is because of the linguistic redundancy where mostly a few keywords are sufficient to convey the meaning of a long sentence. We name these inequalities as generative inequalities and investigate how they affect uncertainty estimation. Our results reveal that considerable tokens and sentences containing limited semantics are weighted equally or even heavily when estimating uncertainty. To tackle these biases posed by generative inequalities, we propose to jointly Shifting Attention to more Relevant (SAR) components from both the token level and the sentence level while estimating uncertainty. We conduct experiments over popular ""off-the-shelf"" LLMs (e.g., OPT, LLaMA) with model sizes up to 30B and powerful commercial LLMs (e.g., Davinci from OpenAI), across various free-form question-answering tasks. Experimental results and detailed demographic analysis indicate the superior performance of SAR. Code is available at https://github.com/jinhaoduan/shifting-attention-to-relevance. ","[{'version': 'v1', 'created': 'Mon, 3 Jul 2023 22:17:16 GMT'}]",2023-07-06,"[['Duan', 'Jinhao', ''], ['Cheng', 'Hao', ''], ['Wang', 'Shiqi', ''], ['Wang', 'Chenan', ''], ['Zavalny', 'Alex', ''], ['Xu', 'Renjing', ''], ['Kailkhura', 'Bhavya', ''], ['Xu', 'Kaidi', '']]",0,0,2023-07-03,1,8,3,2,2,0,5424e311319c58847b4c690d5c91090e3b6a4ac3,259342406.0,https://www.semanticscholar.org/paper/5424e311319c58847b4c690d5c91090e3b6a4ac3,arXiv.org,2023.0,47.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2004228925', 'name': 'Jinhao Duan'}, {'authorId': '2117983322', 'name': 'Hao Cheng'}, {'authorId': '2108622760', 'name': 'Shiqi Wang'}, {'authorId': '2108811793', 'name': 'Chenan Wang'}, {'authorId': '2162860875', 'name': 'Alex Zavalny'}, {'authorId': '2200295329', 'name': 'Renjing Xu'}, {'authorId': '1749353', 'name': 'B. Kailkhura'}, {'authorId': '46321210', 'name': 'Kaidi Xu'}]","['Drexel University', 'American Welding Society', 'Lawrence Livermore National Laboratory', 'Hong Kong University of Science and Technology']","['China', 'United States']",2023-07
2307.01542,Jian Guan,"Jian Guan, Minlie Huang",Mitigating the Learning Bias towards Repetition by Self-Contrastive Training for Open-Ended Generation,ACL 2023 Short Findings,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the huge progress in myriad generation tasks, pretrained language models (LMs) such as GPT2 still tend to generate repetitive texts with maximization-based decoding algorithms for open-ended generation. We attribute their overestimation of token-level repetition probabilities to the learning bias: LMs capture simple repetitive patterns faster with the MLE loss. We propose self-contrastive training to penalize the output of a premature checkpoint of the same model when it incorrectly predicts repetition, which is shown to mitigate repetition effectively while maintaining fluency on two datasets. Furthermore, we find that LMs use longer-range dependencies to predict repetitive tokens than non-repetitive ones, which may be the cause of sentence-level repetition loops. ","[{'version': 'v1', 'created': 'Tue, 4 Jul 2023 07:53:55 GMT'}]",2023-07-06,"[['Guan', 'Jian', ''], ['Huang', 'Minlie', '']]",0,1,2023-07-04,1,2,1,1,1,0,ccdeb7ec0e1b5c22d57e79e3da1f23f96b895c5f,259341780.0,https://www.semanticscholar.org/paper/ccdeb7ec0e1b5c22d57e79e3da1f23f96b895c5f,Annual Meeting of the Association for Computational Linguistics,2023.0,42.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '145902734', 'name': 'Jian Guan'}, {'authorId': '1730108', 'name': 'Minlie Huang'}]","['State Key Lab of Intelligent Technology and Systems,', 'Tsinghua University', 'Artificial Intelligence Research Institute']","['China', 'Spain']",2023-07
2307.01848,Zhenyu Wu,"Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, Haibin Yan",Embodied Task Planning with Large Language Models,Project Page: https://gary3410.github.io/TaPA,,,,cs.CV cs.AI cs.RO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Equipping embodied agents with commonsense is important for robots to successfully complete complex human instructions in general environments. Recent large language models (LLM) can embed rich semantic knowledge for agents in plan generation of complex tasks, while they lack the information about the realistic world and usually yield infeasible action sequences. In this paper, we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning with physical scene constraint, where the agent generates executable plans according to the existed objects in the scene by aligning LLMs with the visual perception models. Specifically, we first construct a multimodal dataset containing triplets of indoor scenes, instructions and action plans, where we provide the designed prompts and the list of existing objects in the scene for GPT-3.5 to generate a large number of instructions and corresponding planned actions. The generated data is leveraged for grounded plan tuning of pre-trained LLMs. During inference, we discover the objects in the scene by extending open-vocabulary object detectors to multi-view RGB images collected in different achievable locations. Experimental results show that the generated plan from our TaPA framework can achieve higher success rate than LLaVA and GPT-3.5 by a sizable margin, which indicates the practicality of embodied task planning in general and complex environments. ","[{'version': 'v1', 'created': 'Tue, 4 Jul 2023 17:58:25 GMT'}]",2023-07-07,"[['Wu', 'Zhenyu', ''], ['Wang', 'Ziwei', ''], ['Xu', 'Xiuwei', ''], ['Lu', 'Jiwen', ''], ['Yan', 'Haibin', '']]",0,1,2023-07-04,1,5,3,1,0,1,df710c46594c04fb59ef9a93d3b4e1cb387a1b2b,259342896.0,https://www.semanticscholar.org/paper/df710c46594c04fb59ef9a93d3b4e1cb387a1b2b,arXiv.org,2023.0,52.0,9.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2156372999', 'name': 'Zhenyu Wu'}, {'authorId': '2117418088', 'name': 'Ziwei Wang'}, {'authorId': '2158440998', 'name': 'Xiuwei Xu'}, {'authorId': '1697700', 'name': 'Jiwen Lu'}, {'authorId': '2107109176', 'name': 'H. Yan'}]","['Beijing University of Posts and Telecommunications', 'Tsinghua University', 'Beijing Information Science & Technology University', 'Portuguese Environment Agency']","['China', 'Portugal']",2023-07
2307.01981,Jiaxiang Liu,"Jiaxiang Liu, Tianxiang Hu, Yan Zhang, Xiaotang Gai, Yang Feng, Zuozhu
  Liu",A ChatGPT Aided Explainable Framework for Zero-Shot Medical Image Diagnosis,"Workshop on Interpretable ML in Healthcare at International
  Conference on Machine Learning (ICML) 2023",,,,eess.IV cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Zero-shot medical image classification is a critical process in real-world scenarios where we have limited access to all possible diseases or large-scale annotated data. It involves computing similarity scores between a query medical image and possible disease categories to determine the diagnostic result. Recent advances in pretrained vision-language models (VLMs) such as CLIP have shown great performance for zero-shot natural image recognition and exhibit benefits in medical applications. However, an explainable zero-shot medical image recognition framework with promising performance is yet under development. In this paper, we propose a novel CLIP-based zero-shot medical image classification framework supplemented with ChatGPT for explainable diagnosis, mimicking the diagnostic process performed by human experts. The key idea is to query large language models (LLMs) with category names to automatically generate additional cues and knowledge, such as disease symptoms or descriptions other than a single category name, to help provide more accurate and explainable diagnosis in CLIP. We further design specific prompts to enhance the quality of generated texts by ChatGPT that describe visual medical features. Extensive results on one private dataset and four public datasets along with detailed analysis demonstrate the effectiveness and explainability of our training-free zero-shot diagnosis pipeline, corroborating the great potential of VLMs and LLMs for medical applications. ","[{'version': 'v1', 'created': 'Wed, 5 Jul 2023 01:45:19 GMT'}]",2023-07-06,"[['Liu', 'Jiaxiang', ''], ['Hu', 'Tianxiang', ''], ['Zhang', 'Yan', ''], ['Gai', 'Xiaotang', ''], ['Feng', 'Yang', ''], ['Liu', 'Zuozhu', '']]",1,1,2023-07-05,1,6,3,1,0,1,f89b8e79a1b4b9a2febd9b8ab3f7933c89e1c3e0,259342254.0,https://www.semanticscholar.org/paper/f89b8e79a1b4b9a2febd9b8ab3f7933c89e1c3e0,arXiv.org,2023.0,33.0,3.0,0.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2144130913', 'name': 'Jiaxiang Liu'}, {'authorId': '2112911910', 'name': 'Tianxiang Hu'}, {'authorId': '39831806', 'name': 'Yan Zhang'}, {'authorId': '2221127244', 'name': 'Xiaotang Gai'}, {'authorId': '2157371366', 'name': 'Yang Feng'}, {'authorId': '2390951', 'name': 'Zuozhu Liu'}]","['National University of Singapore', 'Workshop on Interpretable ML in Healthcare at International Con-ference on Machine Learning (ICML), Honolulu, Hawaii, USA.', 'Zhejiang University', 'Angelalign Inc., Shanghai, China.', 'University of Illinois Urbana-Champaign']","['China', 'United States', 'Singapore']",2023-07
2307.02047,Yang Luo,"Yang Luo, Xiaozhe Ren, Zangwei Zheng, Zhuo Jiang, Xin Jiang, Yang You",CAME: Confidence-guided Adaptive Memory Efficient Optimization,Accepted by ACL 2023,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Adaptive gradient methods, such as Adam and LAMB, have demonstrated excellent performance in the training of large language models. Nevertheless, the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads. To solve this problem, several memory-efficient optimizers (e.g., Adafactor) have been proposed to obtain a drastic reduction in auxiliary memory usage, but with a performance penalty. In this paper, we first study a confidence-guided strategy to reduce the instability of existing memory efficient optimizers. Based on this strategy, we propose CAME to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods. Extensive experiments demonstrate the training stability and superior performance of CAME across various NLP tasks such as BERT and GPT-2 training. Notably, for BERT pre-training on the large batch size of 32,768, our proposed optimizer attains faster convergence and higher accuracy compared with the Adam optimizer. The implementation of CAME is publicly available. ","[{'version': 'v1', 'created': 'Wed, 5 Jul 2023 06:05:36 GMT'}, {'version': 'v2', 'created': 'Mon, 7 Aug 2023 06:21:31 GMT'}]",2023-08-08,"[['Luo', 'Yang', ''], ['Ren', 'Xiaozhe', ''], ['Zheng', 'Zangwei', ''], ['Jiang', 'Zhuo', ''], ['Jiang', 'Xin', ''], ['You', 'Yang', '']]",0,1,2023-07-05,2,6,1,1,1,0,2c430566b9a12c66ed306a2d1e19d27a29d720aa,259342823.0,https://www.semanticscholar.org/paper/2c430566b9a12c66ed306a2d1e19d27a29d720aa,Annual Meeting of the Association for Computational Linguistics,2023.0,32.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2218050858', 'name': 'Yang Luo'}, {'authorId': '153457264', 'name': 'Xiaozhe Ren'}, {'authorId': '2109654065', 'name': 'Zangwei Zheng'}, {'authorId': '2222322049', 'name': 'Zhuo Jiang'}, {'authorId': '145820291', 'name': 'Xin Jiang'}, {'authorId': '2054451943', 'name': 'Yang You'}]","['National University of Singapore', 'Huawei Technologies (China)']","['China', 'Singapore']",2023-07
2307.02157,Zhi Zheng,"Zhi Zheng, Zhaopeng Qiu, Xiao Hu, Likang Wu, Hengshu Zhu, Hui Xiong",Generative Job Recommendations with Large Language Model,,,,,cs.IR cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The rapid development of online recruitment services has encouraged the utilization of recommender systems to streamline the job seeking process. Predominantly, current job recommendations deploy either collaborative filtering or person-job matching strategies. However, these models tend to operate as ""black-box"" systems and lack the capacity to offer explainable guidance to job seekers. Moreover, conventional matching-based recommendation methods are limited to retrieving and ranking existing jobs in the database, restricting their potential as comprehensive career AI advisors. To this end, here we present GIRL (GeneratIve job Recommendation based on Large language models), a novel approach inspired by recent advancements in the field of Large Language Models (LLMs). We initially employ a Supervised Fine-Tuning (SFT) strategy to instruct the LLM-based generator in crafting suitable Job Descriptions (JDs) based on the Curriculum Vitae (CV) of a job seeker. Moreover, we propose to train a model which can evaluate the matching degree between CVs and JDs as a reward model, and we use Proximal Policy Optimization (PPO)-based Reinforcement Learning (RL) method to further fine-tine the generator. This aligns the generator with recruiter feedback, tailoring the output to better meet employer preferences. In particular, GIRL serves as a job seeker-centric generative model, providing job suggestions without the need of a candidate set. This capability also enhances the performance of existing job recommendation models by supplementing job seeking features with generated content. With extensive experiments on a large-scale real-world dataset, we demonstrate the substantial effectiveness of our approach. We believe that GIRL introduces a paradigm-shifting approach to job recommendation systems, fostering a more personalized and comprehensive job-seeking experience. ","[{'version': 'v1', 'created': 'Wed, 5 Jul 2023 09:58:08 GMT'}]",2023-07-06,"[['Zheng', 'Zhi', ''], ['Qiu', 'Zhaopeng', ''], ['Hu', 'Xiao', ''], ['Wu', 'Likang', ''], ['Zhu', 'Hengshu', ''], ['Xiong', 'Hui', '']]",0,0,2023-07-05,1,6,2,0,0,0,127aa57ffe685d0ea9859ba5311f8d98dee17309,259342592.0,https://www.semanticscholar.org/paper/127aa57ffe685d0ea9859ba5311f8d98dee17309,arXiv.org,2023.0,46.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2115548452', 'name': 'Zhi Zheng'}, {'authorId': '2382872', 'name': 'Zhaopeng Qiu'}, {'authorId': '2221132816', 'name': 'Xiao Hu'}, {'authorId': '12892739', 'name': 'Likang Wu'}, {'authorId': '1968806', 'name': 'Hengshu Zhu'}, {'authorId': '2093122576', 'name': 'Hui Xiong'}]","['Career Science Lab, BOSS Zhipin.', 'University of Science and Technology of China', 'Hong Kong University of Science and Technology']",['China'],2023-07
2307.02419,Yeqi Gao,"Yeqi Gao, Zhao Song, Shenghao Xie",In-Context Learning for Attention Scheme: from Single Softmax Regression to Multiple Softmax Regression via a Tensor Trick,,,,,cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Large language models (LLMs) have brought significant and transformative changes in human society. These models have demonstrated remarkable capabilities in natural language understanding and generation, leading to various advancements and impacts across several domains.   We consider the in-context learning under two formulation for attention related regression in this work. Given matrices $A_1 \in \mathbb{R}^{n \times d}$, and $A_2 \in \mathbb{R}^{n \times d}$ and $B \in \mathbb{R}^{n \times n}$, the purpose is to solve some certain optimization problems: Normalized version $\min_{X} \| D(X)^{-1} \exp(A_1 X A_2^\top) - B \|_F^2$ and Rescaled version $\| \exp(A_1 X A_2^\top) - D(X) \cdot B \|_F^2$. Here $D(X) := \mathrm{diag}( \exp(A_1 X A_2^\top) {\bf 1}_n )$.   Our regression problem shares similarities with previous studies on softmax-related regression. Prior research has extensively investigated regression techniques related to softmax regression: Normalized version $\| \langle \exp(Ax) , {\bf 1}_n \rangle^{-1} \exp(Ax) - b \|_2^2$ and Resscaled version $\| \exp(Ax) - \langle \exp(Ax), {\bf 1}_n \rangle b \|_2^2 $   In contrast to previous approaches, we adopt a vectorization technique to address the regression problem in matrix formulation. This approach expands the dimension from $d$ to $d^2$, resembling the formulation of the regression problem mentioned earlier.   Upon completing the lipschitz analysis of our regression function, we have derived our main result concerning in-context learning. ","[{'version': 'v1', 'created': 'Wed, 5 Jul 2023 16:41:01 GMT'}]",2023-07-06,"[['Gao', 'Yeqi', ''], ['Song', 'Zhao', ''], ['Xie', 'Shenghao', '']]",0,0,2023-07-05,1,3,1,0,0,0,602cea8acd2a25dee3c2970d4f05174cd6f91982,259342611.0,https://www.semanticscholar.org/paper/602cea8acd2a25dee3c2970d4f05174cd6f91982,arXiv.org,2023.0,0.0,9.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2109037343', 'name': 'Yeqi Gao'}, {'authorId': '2214956470', 'name': 'Zhao Song'}, {'authorId': '2213849306', 'name': 'Shenghao Xie'}]","['Chinese University of Hong Kong, Shenzhen']",['China'],2023-07
2307.02485,Hongxin Zhang,"Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua
  B. Tenenbaum, Tianmin Shu, Chuang Gan",Building Cooperative Embodied Agents Modularly with Large Language Models,Project page: https://vis-www.cs.umass.edu/Co-LLM-Agents/,,,,cs.AI cs.CL cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have demonstrated impressive planning abilities in single-agent embodied tasks across various domains. However, their capacity for planning and communication in multi-agent cooperation remains unclear, even though these are crucial skills for intelligent embodied agents. In this paper, we present a novel framework that utilizes LLMs for multi-agent cooperation and tests it in various embodied environments. Our framework enables embodied agents to plan, communicate, and cooperate with other embodied agents or humans to accomplish long-horizon tasks efficiently. We demonstrate that recent LLMs, such as GPT-4, can surpass strong planning-based methods and exhibit emergent effective communication using our framework without requiring fine-tuning or few-shot prompting. We also discover that LLM-based agents that communicate in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for embodied AI and lays the foundation for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/. ","[{'version': 'v1', 'created': 'Wed, 5 Jul 2023 17:59:27 GMT'}]",2023-07-06,"[['Zhang', 'Hongxin', ''], ['Du', 'Weihua', ''], ['Shan', 'Jiaming', ''], ['Zhou', 'Qinhong', ''], ['Du', 'Yilun', ''], ['Tenenbaum', 'Joshua B.', ''], ['Shu', 'Tianmin', ''], ['Gan', 'Chuang', '']]",0,1,2023-07-05,1,8,3,1,0,1,587352c3b95c90de6d37f061c8e117f42be0b575,259342833.0,https://www.semanticscholar.org/paper/587352c3b95c90de6d37f061c8e117f42be0b575,arXiv.org,2023.0,58.0,15.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2118083343', 'name': 'Hongxin Zhang'}, {'authorId': '2214830561', 'name': 'Weihua Du'}, {'authorId': '2193050211', 'name': 'Jiaming Shan'}, {'authorId': '2107604346', 'name': 'Qinhong Zhou'}, {'authorId': '15394275', 'name': 'Yilun Du'}, {'authorId': '1763295', 'name': 'J. Tenenbaum'}, {'authorId': '1844358', 'name': 'Tianmin Shu'}, {'authorId': '2056157586', 'name': 'Chuang Gan'}]","['Shanghai Jiao Tong University', 'Tsinghua University', 'University of Massachusetts Amherst', 'MIT-IBM Watson AI Lab']","['China', 'United States']",2023-07
2307.02499,Anwen Hu,"Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan,
  Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, Qian Qi, Ji Zhang, Fei
  Huang",mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding,"10 pages, 8 figures",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Document understanding refers to automatically extract, analyze and comprehend information from various types of digital documents, such as a web page. Existing Multi-model Large Language Models (MLLMs), including mPLUG-Owl, have demonstrated promising zero-shot capabilities in shallow OCR-free text recognition, indicating their potential for OCR-free document understanding. Nevertheless, without in-domain training, these models tend to ignore fine-grained OCR features, such as sophisticated tables or large blocks of text, which are essential for OCR-free document understanding. In this paper, we propose mPLUG-DocOwl based on mPLUG-Owl for OCR-free document understanding. Specifically, we first construct a instruction tuning dataset featuring a wide range of visual-text understanding tasks. Then, we strengthen the OCR-free document understanding ability by jointly train the model on language-only, general vision-and-language, and document instruction tuning dataset with our unified instruction tuning strategy. We also build an OCR-free document instruction understanding evaluation set LLMDoc to better compare models' capabilities on instruct compliance and document understanding. Experimental results show that our model outperforms existing multi-modal models, demonstrating its strong ability of document understanding. Besides, without specific fine-tuning, mPLUG-DocOwl generalizes well on various downstream tasks. Our code, models, training data and evaluation set are available at https://github.com/X-PLUG/mPLUG-DocOwl. ","[{'version': 'v1', 'created': 'Tue, 4 Jul 2023 11:28:07 GMT'}]",2023-07-07,"[['Ye', 'Jiabo', ''], ['Hu', 'Anwen', ''], ['Xu', 'Haiyang', ''], ['Ye', 'Qinghao', ''], ['Yan', 'Ming', ''], ['Dan', 'Yuhao', ''], ['Zhao', 'Chenlin', ''], ['Xu', 'Guohai', ''], ['Li', 'Chenliang', ''], ['Tian', 'Junfeng', ''], ['Qi', 'Qian', ''], ['Zhang', 'Ji', ''], ['Huang', 'Fei', '']]",0,0,2023-07-04,1,13,2,0,0,0,ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42,259360848.0,https://www.semanticscholar.org/paper/ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42,arXiv.org,2023.0,37.0,7.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2153258288', 'name': 'Jiabo Ye'}, {'authorId': '120897486', 'name': 'Anwen Hu'}, {'authorId': '153194420', 'name': 'Haiyang Xu'}, {'authorId': '2199011713', 'name': 'Qinghao Ye'}, {'authorId': '2114009661', 'name': 'Mingshi Yan'}, {'authorId': '2115610557', 'name': 'Yuhao Dan'}, {'authorId': '2232751254', 'name': 'Chenlin Zhao'}, {'authorId': '2115723816', 'name': 'Guohai Xu'}, {'authorId': '143971529', 'name': 'Chenliang Li'}, {'authorId': '2122989639', 'name': 'Junfeng Tian'}, {'authorId': '50480206', 'name': 'Qiang Qi'}, {'authorId': '2116921824', 'name': 'Ji Zhang'}, {'authorId': '2194508991', 'name': 'Feiyan Huang'}]",['Alibaba'],['China'],2023-07
2307.02503,Man Fai Wong,"Man Fai Wong, Shangxin Guo, Ching Nam Hang, Siu Wai Ho, Chee Wei Tan",Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review,,"Entropy(2023), 25(6), 888",10.3390/e25060888,,cs.SE cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  This paper provides a comprehensive review of the literature concerning the utilization of Natural Language Processing (NLP) techniques, with a particular focus on transformer-based large language models (LLMs) trained using Big Code, within the domain of AI-assisted programming tasks. LLMs, augmented with software naturalness, have played a crucial role in facilitating AI-assisted programming applications, including code generation, code completion, code translation, code refinement, code summarization, defect detection, and clone detection. Notable examples of such applications include the GitHub Copilot powered by OpenAI's Codex and DeepMind AlphaCode. This paper presents an overview of the major LLMs and their applications in downstream tasks related to AI-assisted programming. Furthermore, it explores the challenges and opportunities associated with incorporating NLP techniques with software naturalness in these applications, with a discussion on extending AI-assisted programming capabilities to Apple's Xcode for mobile software development. This paper also presents the challenges of and opportunities for incorporating NLP techniques with software naturalness, empowering developers with advanced coding assistance and streamlining the software development process. ","[{'version': 'v1', 'created': 'Tue, 4 Jul 2023 21:26:51 GMT'}]",2023-07-07,"[['Wong', 'Man Fai', ''], ['Guo', 'Shangxin', ''], ['Hang', 'Ching Nam', ''], ['Ho', 'Siu Wai', ''], ['Tan', 'Chee Wei', '']]",0,0,2023-07-04,1,5,3,2,0,2,8071b53a46b0eb368741afcd90c3f93d95f56fdc,259047693.0,https://www.semanticscholar.org/paper/8071b53a46b0eb368741afcd90c3f93d95f56fdc,Entropy,2023.0,214.0,6.0,0.0,True,"['Computer Science', 'Medicine']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Medicine', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2087077450', 'name': 'M. Wong'}, {'authorId': '2219604241', 'name': 'Shangxin Guo'}, {'authorId': '52205613', 'name': 'C. Hang'}, {'authorId': '2218888501', 'name': 'Siu-Wai Ho'}, {'authorId': '2203380097', 'name': 'C. Tan'}]","['University of Adelaide', 'City University of Hong Kong', 'Nanyang Technological University']","['China', 'Singapore', 'Australia']",2023-07
2307.02514,Xiang Li,"Hongmin Cai, Xiaoke Huang, Zhengliang Liu, Wenxiong Liao, Haixing Dai,
  Zihao Wu, Dajiang Zhu, Hui Ren, Quanzheng Li, Tianming Liu, and Xiang Li",Exploring Multimodal Approaches for Alzheimer's Disease Detection Using Patient Speech Transcript and Audio Data,,,,,eess.AS cs.AI cs.SD,http://creativecommons.org/licenses/by/4.0/,"  Alzheimer's disease (AD) is a common form of dementia that severely impacts patient health. As AD impairs the patient's language understanding and expression ability, the speech of AD patients can serve as an indicator of this disease. This study investigates various methods for detecting AD using patients' speech and transcripts data from the DementiaBank Pitt database. The proposed approach involves pre-trained language models and Graph Neural Network (GNN) that constructs a graph from the speech transcript, and extracts features using GNN for AD detection. Data augmentation techniques, including synonym replacement, GPT-based augmenter, and so on, were used to address the small dataset size. Audio data was also introduced, and WavLM model was used to extract audio features. These features were then fused with text features using various methods. Finally, a contrastive learning approach was attempted by converting speech transcripts back to audio and using it for contrastive learning with the original audio. We conducted intensive experiments and analysis on the above methods. Our findings shed light on the challenges and potential solutions in AD detection using speech and audio data. ","[{'version': 'v1', 'created': 'Wed, 5 Jul 2023 12:40:11 GMT'}]",2023-07-07,"[['Cai', 'Hongmin', ''], ['Huang', 'Xiaoke', ''], ['Liu', 'Zhengliang', ''], ['Liao', 'Wenxiong', ''], ['Dai', 'Haixing', ''], ['Wu', 'Zihao', ''], ['Zhu', 'Dajiang', ''], ['Ren', 'Hui', ''], ['Li', 'Quanzheng', ''], ['Liu', 'Tianming', ''], ['Li', 'Xiang', '']]",0,1,2023-07-05,1,11,3,0,0,0,0590ec99d2b36b8922139078ac1a91fd62eeda61,259360421.0,https://www.semanticscholar.org/paper/0590ec99d2b36b8922139078ac1a91fd62eeda61,arXiv.org,2023.0,38.0,5.0,0.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2209990831', 'name': 'Hongmin Cai'}, {'authorId': None, 'name': 'Xiaoke Huang'}, {'authorId': '2145977326', 'name': 'Zheng Liu'}, {'authorId': '2036578588', 'name': 'Wenxiong Liao'}, {'authorId': '29944950', 'name': 'Haixing Dai'}, {'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '2181182', 'name': 'Dajiang Zhu'}, {'authorId': '1485746845', 'name': 'Hui Ren'}, {'authorId': '1762919', 'name': 'Quanzheng Li'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}, {'authorId': '113075935', 'name': 'Xiang Li'}]","['South China University of Technology', 'University of Georgia', 'Massachusetts General Hospital', 'The University of Texas at Arlington']","['China', 'United States']",2023-07
2307.02599,Shuyang Cai,Shuyang Cai and Wanyun Cui,Evade ChatGPT Detectors via A Single Space,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  ChatGPT brings revolutionary social value but also raises concerns about the misuse of AI-generated content. Consequently, an important question is how to detect whether content is generated by ChatGPT or by human. Existing detectors are built upon the assumption that there are distributional gaps between human-generated and AI-generated content. These gaps are typically identified using statistical information or classifiers. Our research challenges the distributional gap assumption in detectors. We find that detectors do not effectively discriminate the semantic and stylistic gaps between human-generated and AI-generated content. Instead, the ""subtle differences"", such as an extra space, become crucial for detection. Based on this discovery, we propose the SpaceInfi strategy to evade detection. Experiments demonstrate the effectiveness of this strategy across multiple benchmarks and detectors. We also provide a theoretical explanation for why SpaceInfi is successful in evading perplexity-based detection. Our findings offer new insights and challenges for understanding and constructing more applicable ChatGPT detectors. ","[{'version': 'v1', 'created': 'Wed, 5 Jul 2023 18:48:28 GMT'}]",2023-07-07,"[['Cai', 'Shuyang', ''], ['Cui', 'Wanyun', '']]",1,1,2023-07-05,1,2,2,1,0,1,ae39f3c1c8c1bce030d3b32847519dec0405d9aa,259360677.0,https://www.semanticscholar.org/paper/ae39f3c1c8c1bce030d3b32847519dec0405d9aa,arXiv.org,2023.0,38.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '38165865', 'name': 'Shuyang Cai'}, {'authorId': '2813326', 'name': 'Wanyun Cui'}]",['Shanghai University of Finance and Economics'],['China'],2023-07
2307.02779,Yifei Shen,"Yifei Shen, Jiawei Shao, Xinjie Zhang, Zehong Lin, Hao Pan, Dongsheng
  Li, Jun Zhang, Khaled B. Letaief",Large Language Models Empowered Autonomous Edge AI for Connected Intelligence,Magazine paper,,,,cs.IT cs.LG cs.NI eess.SP math.IT,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The evolution of wireless networks gravitates towards connected intelligence, a concept that envisions seamless interconnectivity among humans, objects, and intelligence in a hyper-connected cyber-physical world. Edge AI emerges as a promising solution to achieve connected intelligence by delivering high-quality, low-latency, and privacy-preserving AI services at the network edge. In this article, we introduce an autonomous edge AI system that automatically organizes, adapts, and optimizes itself to meet users' diverse requirements. The system employs a cloud-edge-client hierarchical architecture, where the large language model, i.e., Generative Pretrained Transformer (GPT), resides in the cloud, and other AI models are co-deployed on devices and edge servers. By leveraging the powerful abilities of GPT in language understanding, planning, and code generation, we present a versatile framework that efficiently coordinates edge AI models to cater to users' personal demands while automatically generating code to train new models via edge federated learning. Experimental results demonstrate the system's remarkable ability to accurately comprehend user demands, efficiently execute AI models with minimal cost, and effectively create high-performance AI models through federated learning. ","[{'version': 'v1', 'created': 'Thu, 6 Jul 2023 05:16:55 GMT'}]",2023-07-07,"[['Shen', 'Yifei', ''], ['Shao', 'Jiawei', ''], ['Zhang', 'Xinjie', ''], ['Lin', 'Zehong', ''], ['Pan', 'Hao', ''], ['Li', 'Dongsheng', ''], ['Zhang', 'Jun', ''], ['Letaief', 'Khaled B.', '']]",0,1,2023-07-06,1,8,5,0,0,0,4c55dd37ab4c41565226a3c8194166d7412b0a02,259360434.0,https://www.semanticscholar.org/paper/4c55dd37ab4c41565226a3c8194166d7412b0a02,arXiv.org,2023.0,15.0,7.0,0.0,True,"['Computer Science', 'Engineering', 'Mathematics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Mathematics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2115383310', 'name': 'Yifei Shen'}, {'authorId': '2111877589', 'name': 'Jiawei Shao'}, {'authorId': '2144485201', 'name': 'Xinjie Zhang'}, {'authorId': '2112335060', 'name': 'Zehong Lin'}, {'authorId': '2087703756', 'name': 'Hao Pan'}, {'authorId': '2119081394', 'name': 'Dongsheng Li'}, {'authorId': '2180397299', 'name': 'Jun Zhang'}, {'authorId': '145142172', 'name': 'K. Letaief'}]","['Microsoft', 'Hong Kong University of Science and Technology']",['China'],2023-07
2307.03109,Jindong Wang,"Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu,
  Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi
  Chang, Philip S. Yu, Qiang Yang, Xing Xie",A Survey on Evaluation of Large Language Models,"26 pages; a major update to include more recent works;
  https://llm-eval.github.io/",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey. ","[{'version': 'v1', 'created': 'Thu, 6 Jul 2023 16:28:35 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Jul 2023 12:31:50 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Jul 2023 15:43:03 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Jul 2023 12:33:20 GMT'}, {'version': 'v5', 'created': 'Tue, 18 Jul 2023 08:11:21 GMT'}, {'version': 'v6', 'created': 'Wed, 2 Aug 2023 07:39:17 GMT'}, {'version': 'v7', 'created': 'Mon, 28 Aug 2023 05:50:53 GMT'}]",2023-08-29,"[['Chang', 'Yupeng', ''], ['Wang', 'Xu', ''], ['Wang', 'Jindong', ''], ['Wu', 'Yuan', ''], ['Yang', 'Linyi', ''], ['Zhu', 'Kaijie', ''], ['Chen', 'Hao', ''], ['Yi', 'Xiaoyuan', ''], ['Wang', 'Cunxiang', ''], ['Wang', 'Yidong', ''], ['Ye', 'Wei', ''], ['Zhang', 'Yue', ''], ['Chang', 'Yi', ''], ['Yu', 'Philip S.', ''], ['Yang', 'Qiang', ''], ['Xie', 'Xing', '']]",0,0,2023-07-06,7,16,2,0,0,0,888728745dbb769e29ed475d4f7661eebe1a71cf,259360395.0,https://www.semanticscholar.org/paper/888728745dbb769e29ed475d4f7661eebe1a71cf,arXiv.org,2023.0,280.0,81.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '2140050490', 'name': 'Yu-Chu Chang'}, {'authorId': '2206577797', 'name': 'Xu Wang'}, {'authorId': '1519290245', 'name': 'Jindong Wang'}, {'authorId': '2157349111', 'name': 'Yuanyi Wu'}, {'authorId': '2219270546', 'name': 'Kaijie Zhu'}, {'authorId': '2051536212', 'name': 'Hao Chen'}, {'authorId': '2145500840', 'name': 'Linyi Yang'}, {'authorId': '3393196', 'name': 'Xiaoyuan Yi'}, {'authorId': '35504092', 'name': 'Cunxiang Wang'}, {'authorId': '2108024273', 'name': 'Yidong Wang'}, {'authorId': '2147205193', 'name': 'Weirong Ye'}, {'authorId': '2211964951', 'name': 'Yue Zhang'}, {'authorId': '2131636065', 'name': 'Yi Chang'}, {'authorId': '2191036692', 'name': 'Philip S. Yu'}, {'authorId': '2158406244', 'name': 'Qian Yang'}, {'authorId': '1576441343', 'name': 'Xingxu Xie'}]","['Westlake University', ""Xi'an University of Science and Technology"", 'Hong Kong University of Science and Technology', 'Jilin University', 'Chinese Academy of Sciences', 'Carnegie Mellon University', 'University of Illinois at Chicago', 'Microsoft', 'Peking University']","['China', 'United States']",2023-07
2307.03115,Zijun Yao,"Zijun Yao, Yantao Liu, Xin Lv, Shulin Cao, Jifan Yu, Lei Hou, Juanzi
  Li",KoRC: Knowledge oriented Reading Comprehension Benchmark for Deep Text Understanding,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep text understanding, which requires the connections between a given document and prior knowledge beyond its text, has been highlighted by many benchmarks in recent years. However, these benchmarks have encountered two major limitations. On the one hand, most of them require human annotation of knowledge, which leads to limited knowledge coverage. On the other hand, they usually use choices or spans in the texts as the answers, which results in narrow answer space. To overcome these limitations, we build a new challenging benchmark named KoRc in this paper. Compared with previous benchmarks, KoRC has two advantages, i.e., broad knowledge coverage and flexible answer format. Specifically, we utilize massive knowledge bases to guide annotators or large language models (LLMs) to construct knowledgable questions. Moreover, we use labels in knowledge bases rather than spans or choices as the final answers. We test state-of-the-art models on KoRC and the experimental results show that the strongest baseline only achieves 68.3% and 30.0% F1 measure in the in-distribution and out-of-distribution test set, respectively. These results indicate that deep text understanding is still an unsolved challenge. The benchmark dataset, leaderboard, and baseline methods are released in https://github.com/THU-KEG/KoRC. ","[{'version': 'v1', 'created': 'Thu, 6 Jul 2023 16:35:25 GMT'}]",2023-07-07,"[['Yao', 'Zijun', ''], ['Liu', 'Yantao', ''], ['Lv', 'Xin', ''], ['Cao', 'Shulin', ''], ['Yu', 'Jifan', ''], ['Hou', 'Lei', ''], ['Li', 'Juanzi', '']]",0,0,2023-07-06,1,7,1,0,0,0,42b850269b3619978f65d8d78a3c7e8640b99984,259360839.0,https://www.semanticscholar.org/paper/42b850269b3619978f65d8d78a3c7e8640b99984,Annual Meeting of the Association for Computational Linguistics,2023.0,50.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1423719712', 'name': 'Zijun Yao'}, {'authorId': '2211723524', 'name': 'Yan-Tie Liu'}, {'authorId': '48574888', 'name': 'Xin Lv'}, {'authorId': '1712738522', 'name': 'S. Cao'}, {'authorId': '2116034394', 'name': 'Jifan Yu'}, {'authorId': '2055765060', 'name': 'Lei Hou'}, {'authorId': '2133353675', 'name': 'Juanzi Li'}]","['Tsinghua University', 'Artificial Intelligence Research Institute', 'Department of Computer Science and Technology, 1 BNRist;', 'University of Chinese Academy of Sciences']","['China', 'Spain']",2023-07
2307.03393,Zhikai Chen,"Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei,
  Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, Jiliang Tang",Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs,add code,,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct comprehensive and systematical studies on these two pipelines under various settings. From comprehensive empirical results, we make original observations and find new insights that open new possibilities and suggest promising directions to leverage LLMs for learning on graphs. Our codes and datasets are available at https://github.com/CurryTang/Graph-LLM. ","[{'version': 'v1', 'created': 'Fri, 7 Jul 2023 05:31:31 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Jul 2023 06:06:04 GMT'}, {'version': 'v3', 'created': 'Wed, 2 Aug 2023 07:33:22 GMT'}]",2023-08-03,"[['Chen', 'Zhikai', ''], ['Mao', 'Haitao', ''], ['Li', 'Hang', ''], ['Jin', 'Wei', ''], ['Wen', 'Hongzhi', ''], ['Wei', 'Xiaochi', ''], ['Wang', 'Shuaiqiang', ''], ['Yin', 'Dawei', ''], ['Fan', 'Wenqi', ''], ['Liu', 'Hui', ''], ['Tang', 'Jiliang', '']]",0,0,2023-07-07,3,11,2,0,0,0,105669ec59a58fb2d4dd3021a984af33c227c5ab,259375824.0,https://www.semanticscholar.org/paper/105669ec59a58fb2d4dd3021a984af33c227c5ab,arXiv.org,2023.0,89.0,31.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2109393101', 'name': 'Zhikai Chen'}, {'authorId': '2125202063', 'name': 'Haitao Mao'}, {'authorId': '2145571830', 'name': 'Hang Li'}, {'authorId': '144767914', 'name': 'Wei Jin'}, {'authorId': '30580446', 'name': 'Haifang Wen'}, {'authorId': '7621447', 'name': 'Xiaochi Wei'}, {'authorId': '2386396', 'name': 'Shuaiqiang Wang'}, {'authorId': '2136400100', 'name': 'Dawei Yin'}, {'authorId': '41031455', 'name': 'Wenqi Fan'}, {'authorId': '2146672392', 'name': 'Hui Liu'}, {'authorId': '1736632', 'name': 'Jiliang Tang'}]","['Emory University', 'Michigan State University', 'Hong Kong Polytechnic University', 'Baidu']","['China', 'United States', 'Hong Kong']",2023-07
2307.03601,Shilong Zhang,"Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei
  Zhang, Kai Chen, Ping Luo",GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest,Code has been released at https://github.com/jshilong/GPT4RoI,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Instruction tuning large language model (LLM) on image-text pairs has achieved unprecedented vision-language multimodal abilities. However, their vision-language alignments are only built on image-level, the lack of region-level alignment limits their advancements to fine-grained multimodal understanding. In this paper, we propose instruction tuning on region-of-interest. The key design is to reformulate the bounding box as the format of spatial instruction. The interleaved sequences of visual features extracted by the spatial instruction and the language embedding are input to LLM, and trained on the transformed region-text data in instruction tuning format. Our region-level vision-language model, termed as GPT4RoI, brings brand new conversational and interactive experience beyond image-level understanding. (1) Controllability: Users can interact with our model by both language and spatial instructions to flexibly adjust the detail level of the question. (2) Capacities: Our model supports not only single-region spatial instruction but also multi-region. This unlocks more region-level multimodal capacities such as detailed region caption and complex region reasoning. (3) Composition: Any off-the-shelf object detector can be a spatial instruction provider so as to mine informative object attributes from our model, like color, shape, material, action, relation to other objects, etc. The code, data, and demo can be found at https://github.com/jshilong/GPT4RoI. ","[{'version': 'v1', 'created': 'Fri, 7 Jul 2023 13:43:44 GMT'}]",2023-07-10,"[['Zhang', 'Shilong', ''], ['Sun', 'Peize', ''], ['Chen', 'Shoufa', ''], ['Xiao', 'Min', ''], ['Shao', 'Wenqi', ''], ['Zhang', 'Wenwei', ''], ['Chen', 'Kai', ''], ['Luo', 'Ping', '']]",0,1,2023-07-07,1,8,1,0,0,0,094883e42bb9a41f602c0715c1059bc431e33fb2,259375716.0,https://www.semanticscholar.org/paper/094883e42bb9a41f602c0715c1059bc431e33fb2,arXiv.org,2023.0,98.0,21.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '47179740', 'name': 'Shilong Zhang'}, {'authorId': '2075416446', 'name': 'Pei Sun'}, {'authorId': '2107977968', 'name': 'Shoufa Chen'}, {'authorId': '2099595096', 'name': 'Min Xiao'}, {'authorId': '1485702259', 'name': 'Wenqi Shao'}, {'authorId': '2108037927', 'name': 'Wenwei Zhang'}, {'authorId': '152568027', 'name': 'Kai Chen'}, {'authorId': '2143481782', 'name': 'Ping Luo'}]","['Alibaba', 'Shanghai Artificial Intelligence Laboratory', 'University of Hong Kong']","['China', 'Hong Kong']",2023-07
2307.03637,Nikhil Prakash,"Xander Davies, Max Nadeau, Nikhil Prakash, Tamar Rott Shaham, David
  Bau",Discovering Variable Binding Circuitry with Desiderata,,,,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Recent work has shown that computation in language models may be human-understandable, with successful efforts to localize and intervene on both single-unit features and input-output circuits. Here, we introduce an approach which extends causal mediation experiments to automatically identify model components responsible for performing a specific subtask by solely specifying a set of \textit{desiderata}, or causal attributes of the model components executing that subtask. As a proof of concept, we apply our method to automatically discover shared \textit{variable binding circuitry} in LLaMA-13B, which retrieves variable values for multiple arithmetic tasks. Our method successfully localizes variable binding to only 9 attention heads (of the 1.6k) and one MLP in the final token's residual stream. ","[{'version': 'v1', 'created': 'Fri, 7 Jul 2023 14:51:30 GMT'}]",2023-07-10,"[['Davies', 'Xander', ''], ['Nadeau', 'Max', ''], ['Prakash', 'Nikhil', ''], ['Shaham', 'Tamar Rott', ''], ['Bau', 'David', '']]",0,0,2023-07-07,1,5,1,1,1,0,b5131c07be779d90af946e6f370156b9506b7c0d,259375455.0,https://www.semanticscholar.org/paper/b5131c07be779d90af946e6f370156b9506b7c0d,arXiv.org,2023.0,18.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2212371252', 'name': 'Xander Davies'}, {'authorId': '2131005702', 'name': 'Max Nadeau'}, {'authorId': '2104923802', 'name': 'N. Prakash'}, {'authorId': '3459255', 'name': 'Tamar Rott Shaham'}, {'authorId': '144159726', 'name': 'David Bau'}]","['Northeastern University', 'Max Nadeau', 'Harvard University']","['China', 'United States']",2023-07
2307.03762,Yuxi Ma,"Yuxi Ma, Chi Zhang, Song-Chun Zhu",Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  In this perspective paper, we first comprehensively review existing evaluations of Large Language Models (LLMs) using both standardized tests and ability-oriented benchmarks. We pinpoint several problems with current evaluation methods that tend to overstate the capabilities of LLMs. We then articulate what artificial general intelligence should encompass beyond the capabilities of LLMs. We propose four characteristics of generally intelligent agents: 1) they can perform unlimited tasks; 2) they can generate new tasks within a context; 3) they operate based on a value system that underpins task generation; and 4) they have a world model reflecting reality, which shapes their interaction with the world. Building on this viewpoint, we highlight the missing pieces in artificial general intelligence, that is, the unity of knowing and acting. We argue that active engagement with objects in the real world delivers more robust signals for forming conceptual representations. Additionally, knowledge acquisition isn't solely reliant on passive input but requires repeated trials and errors. We conclude by outlining promising future research directions in the field of artificial general intelligence. ","[{'version': 'v1', 'created': 'Fri, 7 Jul 2023 13:58:16 GMT'}]",2023-07-11,"[['Ma', 'Yuxi', ''], ['Zhang', 'Chi', ''], ['Zhu', 'Song-Chun', '']]",0,0,2023-07-07,1,3,2,0,0,0,50f9f33b284b7363fbd9b9d2da4939b989a1c7cd,259501723.0,https://www.semanticscholar.org/paper/50f9f33b284b7363fbd9b9d2da4939b989a1c7cd,arXiv.org,2023.0,65.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2130200815', 'name': 'Yuxi Ma'}, {'authorId': '2214118920', 'name': 'Chi Zhang'}, {'authorId': '145380991', 'name': 'Song-Chun Zhu'}]","['Peking University', 'Beijing Insitute for General Artificial Intelligence (BIGAI)']",['China'],2023-07
2307.03838,Pin-Yu Chen,Xiaomeng Hu and Pin-Yu Chen and Tsung-Yi Ho,RADAR: Robust AI-Text Detection via Adversarial Learning,Preprint. Project page and demos: https://radar.vizhub.ai,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusation of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a Robust AI-text Detector via Adversarial leaRning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic contents to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets, experimental results show that RADAR significantly outperforms existing AI-text detection methods, especially when paraphrasing is in place. We also identify the strong transferability of RADAR from instruction-tuned LLMs to other LLMs, and evaluate the improved capability of RADAR via GPT-3.5. ","[{'version': 'v1', 'created': 'Fri, 7 Jul 2023 21:13:27 GMT'}]",2023-07-11,"[['Hu', 'Xiaomeng', ''], ['Chen', 'Pin-Yu', ''], ['Ho', 'Tsung-Yi', '']]",1,1,2023-07-07,1,3,3,5,3,2,ef3bdb9e805887a1a14e1e4cdb27145135591305,259501842.0,https://www.semanticscholar.org/paper/ef3bdb9e805887a1a14e1e4cdb27145135591305,arXiv.org,2023.0,36.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2199229406', 'name': 'Xiao-bing Hu'}, {'authorId': '2158177948', 'name': 'Pin-Yu Chen'}, {'authorId': '2103197703', 'name': 'Tsung-Yi Ho'}]","['IBM (United States)', 'Chinese University of Hong Kong']","['China', 'United States']",2023-07
2307.03952,Yu Ji,"Yu Ji, Wen Wu, Hong Zheng, Yi Hu, Xi Chen, Liang He",Is ChatGPT a Good Personality Recognizer? A Preliminary Study,"15 pages, 13 figures, 5 tables",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In recent years, personality has been regarded as a valuable personal factor being incorporated into numerous tasks such as sentiment analysis and product recommendation. This has led to widespread attention to text-based personality recognition task, which aims to identify an individual's personality based on given text. Considering that ChatGPT has recently exhibited remarkable abilities on various natural language processing tasks, we provide a preliminary evaluation of ChatGPT on text-based personality recognition task for generating effective personality data. Concretely, we employ a variety of prompting strategies to explore ChatGPT's ability in recognizing personality from given text, especially the level-oriented prompting strategy we designed for guiding ChatGPT in analyzing given text at a specified level. The experimental results on two representative real-world datasets reveal that ChatGPT with zero-shot chain-of-thought prompting exhibits impressive personality recognition ability and is capable to provide natural language explanations through text-based logical reasoning. Furthermore, by employing the level-oriented prompting strategy to optimize zero-shot chain-of-thought prompting, the performance gap between ChatGPT and corresponding state-of-the-art model has been narrowed even more. However, we observe that ChatGPT shows unfairness towards certain sensitive demographic attributes such as gender and age. Additionally, we discover that eliciting the personality recognition ability of ChatGPT helps improve its performance on personality-related downstream tasks such as sentiment classification and stress prediction. ","[{'version': 'v1', 'created': 'Sat, 8 Jul 2023 11:02:02 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Jul 2023 18:02:42 GMT'}]",2023-07-28,"[['Ji', 'Yu', ''], ['Wu', 'Wen', ''], ['Zheng', 'Hong', ''], ['Hu', 'Yi', ''], ['Chen', 'Xi', ''], ['He', 'Liang', '']]",1,1,2023-07-08,2,6,1,1,0,1,247509f5658374678830582dddfdb6bc4d8af25e,259501586.0,https://www.semanticscholar.org/paper/247509f5658374678830582dddfdb6bc4d8af25e,arXiv.org,2023.0,75.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2161727946', 'name': 'Yuzhe Ji'}, {'authorId': '116354414', 'name': 'Wen Wu'}, {'authorId': '2221771439', 'name': 'Hong Zheng'}, {'authorId': '49994911', 'name': 'Yiqiang Hu'}, {'authorId': '2145309178', 'name': 'Xi Chen'}, {'authorId': '2148951260', 'name': 'Liang He'}]","['East China Normal University', 'Shanghai Changning Mental Health Center']",['China'],2023-07
2307.03972,Fanyi Qu,Fanyi Qu and Yunfang Wu,Evaluating the Capability of Large-scale Language Models on Chinese Grammatical Error Correction Task,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large-scale language models (LLMs) has shown remarkable capability in various of Natural Language Processing (NLP) tasks and attracted lots of attention recently. However, some studies indicated that large language models fail to achieve promising result beyond the state-of-the-art models in English grammatical error correction (GEC) tasks. In this report, we aim to explore the how large language models perform on Chinese grammatical error correction tasks and provide guidance for future work. We conduct experiments with 3 different LLMs of different model scale on 4 Chinese GEC dataset. Our experimental results indicate that the performances of LLMs on automatic evaluation metrics falls short of the previous sota models because of the problem of over-correction. Furthermore, we also discover notable variations in the performance of LLMs when evaluated on different data distributions. Our findings demonstrates that further investigation is required for the application of LLMs on Chinese GEC task. ","[{'version': 'v1', 'created': 'Sat, 8 Jul 2023 13:10:59 GMT'}]",2023-07-11,"[['Qu', 'Fanyi', ''], ['Wu', 'Yunfang', '']]",0,0,2023-07-08,1,2,1,0,0,0,9a2ee8c33deef36c2dbc7af5d2a8a7cc7c87d882,259501459.0,https://www.semanticscholar.org/paper/9a2ee8c33deef36c2dbc7af5d2a8a7cc7c87d882,arXiv.org,2023.0,14.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2127464628', 'name': 'Fanyi Qu'}, {'authorId': '2115377800', 'name': 'Yunfang Wu'}]",['Peking University'],['China'],2023-07
2307.04308,Chao Ye,"Chao Ye, Guoshan Lu, Haobo Wang, Liyao Li, Sai Wu, Gang Chen, Junbo
  Zhao",CT-BERT: Learning Better Tabular Representations Through Cross-Table Pre-training,,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Tabular data -- also known as structured data -- is one of the most common data forms in existence, thanks to the stable development and scaled deployment of database systems in the last few decades. At present however, despite the blast brought by large pre-trained models in other domains such as ChatGPT or SAM, how can we extract common knowledge across tables at a scale that may eventually lead to generalizable representation for tabular data remains a full blank. Indeed, there have been a few works around this topic. Most (if not all) of them are limited in the scope of a single table or fixed form of a schema. In this work, we first identify the crucial research challenges behind tabular data pre-training, particularly towards the cross-table scenario. We position the contribution of this work in two folds: (i)-we collect and curate nearly 2k high-quality tabular datasets, each of which is guaranteed to possess clear semantics, clean labels, and other necessary meta information. (ii)-we propose a novel framework that allows cross-table pre-training dubbed as CT-BERT. Noticeably, in light of pioneering the scaled cross-table training, CT-BERT is fully compatible with both supervised and self-supervised schemes, where the specific instantiation of CT-BERT is very much dependent on the downstream tasks. We further propose and implement a contrastive-learning-based and masked table modeling (MTM) objective into CT-BERT, that is inspired from computer vision and natural language processing communities but sophistically tailored to tables. The extensive empirical results on 15 datasets demonstrate CT-BERT's state-of-the-art performance, where both its supervised and self-supervised setups significantly outperform the prior approaches. ","[{'version': 'v1', 'created': 'Mon, 10 Jul 2023 02:27:38 GMT'}]",2023-07-11,"[['Ye', 'Chao', ''], ['Lu', 'Guoshan', ''], ['Wang', 'Haobo', ''], ['Li', 'Liyao', ''], ['Wu', 'Sai', ''], ['Chen', 'Gang', ''], ['Zhao', 'Junbo', '']]",1,1,2023-07-10,1,7,1,1,0,1,9acd6d8b695187831222d94277f3ffcf1d561d10,259501427.0,https://www.semanticscholar.org/paper/9acd6d8b695187831222d94277f3ffcf1d561d10,arXiv.org,2023.0,66.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2064450893', 'name': 'Chaonan Ye'}, {'authorId': '1491631001', 'name': 'Guoshan Lu'}, {'authorId': '2108909691', 'name': 'Haobo Wang'}, {'authorId': '2221695468', 'name': 'Liyao Li'}, {'authorId': '2149344649', 'name': 'Sai Wu'}, {'authorId': '2163431247', 'name': 'Gang Chen'}, {'authorId': '7818229', 'name': 'J. Zhao'}]",['Zhejiang University'],['China'],2023-07
2307.04408,Jiali Zeng,Jiali Zeng and Fandong Meng and Yongjing Yin and Jie Zhou,TIM: Teaching Large Language Models to Translate with Comparison,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Open-sourced large language models (LLMs) have demonstrated remarkable efficacy in various tasks with instruction tuning. However, these models can sometimes struggle with tasks that require more specialized knowledge such as translation. One possible reason for such deficiency is that instruction tuning aims to generate fluent and coherent text that continues from a given instruction without being constrained by any task-specific requirements. Moreover, it can be more challenging for tuning smaller LLMs with lower-quality training data. To address this issue, we propose a novel framework using examples in comparison to teach LLMs to learn translation. Our approach involves presenting the model with examples of correct and incorrect translations and using a preference loss to guide the model's learning. We evaluate our method on WMT2022 test sets and show that it outperforms existing methods. Our findings offer a new perspective on fine-tuning LLMs for translation tasks and provide a promising solution for generating high-quality translations. Please refer to Github for more details: https://github.com/lemon0830/TIM. ","[{'version': 'v1', 'created': 'Mon, 10 Jul 2023 08:15:40 GMT'}, {'version': 'v2', 'created': 'Thu, 14 Sep 2023 07:58:00 GMT'}]",2023-09-15,"[['Zeng', 'Jiali', ''], ['Meng', 'Fandong', ''], ['Yin', 'Yongjing', ''], ['Zhou', 'Jie', '']]",0,0,2023-07-10,2,4,1,0,0,0,150fa774175cf86e8a6ef3916145c34180998037,259501202.0,https://www.semanticscholar.org/paper/150fa774175cf86e8a6ef3916145c34180998037,arXiv.org,2023.0,34.0,8.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '50409899', 'name': 'Jiali Zeng'}, {'authorId': '33427918', 'name': 'Fandong Meng'}, {'authorId': '79701068', 'name': 'Yongjing Yin'}, {'authorId': '48128428', 'name': 'Jie Zhou'}]","['Westlake University', 'Tencent']",['China'],2023-07
2307.04657,Jiaming Ji,"Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian,
  Chi Zhang, Ruiyang Sun, Yizhou Wang, Yaodong Yang",BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we introduce the BeaverTails dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have compiled safety meta-labels for 30,207 question-answer (QA) pairs and gathered 30,144 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our project page is available at the following URL: https://sites.google.com/view/pku-beavertails. ","[{'version': 'v1', 'created': 'Mon, 10 Jul 2023 15:56:17 GMT'}]",2023-07-11,"[['Ji', 'Jiaming', ''], ['Liu', 'Mickel', ''], ['Dai', 'Juntao', ''], ['Pan', 'Xuehai', ''], ['Zhang', 'Chi', ''], ['Bian', 'Ce', ''], ['Zhang', 'Chi', ''], ['Sun', 'Ruiyang', ''], ['Wang', 'Yizhou', ''], ['Yang', 'Yaodong', '']]",0,0,2023-07-10,1,10,1,0,0,0,92930ed3560ea6c86d53cf52158bc793b089054d,259501579.0,https://www.semanticscholar.org/paper/92930ed3560ea6c86d53cf52158bc793b089054d,arXiv.org,2023.0,70.0,17.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2154630502', 'name': 'Jiaming Ji'}, {'authorId': '2210950163', 'name': 'Mickel Liu'}, {'authorId': '14548852', 'name': 'Juntao Dai'}, {'authorId': '2190800297', 'name': 'Xuehai Pan'}, {'authorId': '2221446410', 'name': 'Chi Zhang'}, {'authorId': '2221566410', 'name': 'Ce Bian'}, {'authorId': '2217316509', 'name': 'Ruiyang Sun'}, {'authorId': '1717863', 'name': 'Yizhou Wang'}, {'authorId': '47796324', 'name': 'Yaodong Yang'}]","['Peking University', 'Artificial Intelligence Research Institute']","['China', 'Spain']",2023-07
2307.04827,Yunlong Tang,"Siting Xu, Yunlong Tang, Feng Zheng",LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad,Accepted by International Computer Music Conference (ICMC) 2023,,,,cs.SD cs.CL cs.MM eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Launchpad is a musical instrument that allows users to create and perform music by pressing illuminated buttons. To assist and inspire the design of the Launchpad light effect, and provide a more accessible approach for beginners to create music visualization with this instrument, we proposed the LaunchpadGPT model to generate music visualization designs on Launchpad automatically. Based on the language model with excellent generation ability, our proposed LaunchpadGPT takes an audio piece of music as input and outputs the lighting effects of Launchpad-playing in the form of a video (Launchpad-playing video). We collect Launchpad-playing videos and process them to obtain music and corresponding video frame of Launchpad-playing as prompt-completion pairs, to train the language model. The experiment result shows the proposed method can create better music visualization than random generation methods and hold the potential for a broader range of music visualization applications. Our code is available at https://github.com/yunlong10/LaunchpadGPT/. ","[{'version': 'v1', 'created': 'Fri, 7 Jul 2023 16:25:59 GMT'}, {'version': 'v2', 'created': 'Sun, 23 Jul 2023 10:20:28 GMT'}]",2023-07-25,"[['Xu', 'Siting', ''], ['Tang', 'Yunlong', ''], ['Zheng', 'Feng', '']]",0,1,2023-07-07,2,3,4,0,0,0,383dc41fbafa9ab8a275013a03646369edb1fee0,259765956.0,https://www.semanticscholar.org/paper/383dc41fbafa9ab8a275013a03646369edb1fee0,arXiv.org,2023.0,7.0,1.0,1.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Art', 'source': 's2-fos-model'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2186266050', 'name': 'Siting Xu'}, {'authorId': '2119309562', 'name': 'Yunlong Tang'}, {'authorId': '2146483847', 'name': 'Feng Zheng'}]",['Southern University of Science and Technology'],['China'],2023-07
2307.04964,Songyang Gao,"Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang,
  Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, Limao Xiong, Lu Chen, Zhiheng Xi,
  Nuo Xu, Wenbin Lai, Minghao Zhu, Cheng Chang, Zhangyue Yin, Rongxiang Weng,
  Wensen Cheng, Haoran Huang, Tianxiang Sun, Hang Yan, Tao Gui, Qi Zhang,
  Xipeng Qiu, Xuanjing Huang",Secrets of RLHF in Large Language Models Part I: PPO,,,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \textbf{reward models} to measure human preferences, \textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first report, we dissect the framework of RLHF, re-evaluate the inner workings of PPO, and explore how the parts comprising PPO algorithms impact policy agent training. We identify policy constraints being the key factor for the effective implementation of the PPO algorithm. Therefore, we explore the PPO-max, an advanced version of PPO algorithm, to efficiently improve the training stability of the policy model. Based on our main results, we perform a comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT. The absence of open-source implementations has posed significant challenges to the investigation of LLMs alignment. Therefore, we are eager to release technical reports, reward models and PPO codes, aiming to make modest contributions to the advancement of LLMs. ","[{'version': 'v1', 'created': 'Tue, 11 Jul 2023 01:55:24 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Jul 2023 08:44:47 GMT'}]",2023-07-19,"[['Zheng', 'Rui', ''], ['Dou', 'Shihan', ''], ['Gao', 'Songyang', ''], ['Hua', 'Yuan', ''], ['Shen', 'Wei', ''], ['Wang', 'Binghai', ''], ['Liu', 'Yan', ''], ['Jin', 'Senjie', ''], ['Liu', 'Qin', ''], ['Zhou', 'Yuhao', ''], ['Xiong', 'Limao', ''], ['Chen', 'Lu', ''], ['Xi', 'Zhiheng', ''], ['Xu', 'Nuo', ''], ['Lai', 'Wenbin', ''], ['Zhu', 'Minghao', ''], ['Chang', 'Cheng', ''], ['Yin', 'Zhangyue', ''], ['Weng', 'Rongxiang', ''], ['Cheng', 'Wensen', ''], ['Huang', 'Haoran', ''], ['Sun', 'Tianxiang', ''], ['Yan', 'Hang', ''], ['Gui', 'Tao', ''], ['Zhang', 'Qi', ''], ['Qiu', 'Xipeng', ''], ['Huang', 'Xuanjing', '']]",1,1,2023-07-11,2,27,3,1,0,1,548278897d46a54958909bb23bcaecf63e24fadf,259766568.0,https://www.semanticscholar.org/paper/548278897d46a54958909bb23bcaecf63e24fadf,arXiv.org,2023.0,45.0,16.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2058585152', 'name': 'Rui Zheng'}, {'authorId': '2042683163', 'name': 'Shihan Dou'}, {'authorId': '2181306462', 'name': 'Songyang Gao'}, {'authorId': '2248291262', 'name': 'Wei Shen'}, {'authorId': '2117227865', 'name': 'Wei-Yuan Shen'}, {'authorId': '2188630983', 'name': 'Bing Wang'}, {'authorId': '2156649786', 'name': 'Yan Liu'}, {'authorId': '2219131195', 'name': 'Senjie Jin'}, {'authorId': '2109185819', 'name': 'Qin Liu'}, {'authorId': '2222630539', 'name': 'Limao Xiong'}, {'authorId': '2115386043', 'name': 'Luyao Chen'}, {'authorId': '2190751523', 'name': 'Zhiheng Xi'}, {'authorId': '2212175381', 'name': 'Yuhao Zhou'}, {'authorId': '2072805812', 'name': 'Nuo Xu'}, {'authorId': '2153857410', 'name': 'Wen-De Lai'}, {'authorId': '40587747', 'name': 'Minghao Zhu'}, {'authorId': '24009282', 'name': 'Rongxiang Weng'}, {'authorId': '2227418', 'name': 'Wen-Chun Cheng'}, {'authorId': '2152341000', 'name': 'Cheng Chang'}, {'authorId': '2155273086', 'name': 'Zhangyue Yin'}, {'authorId': '152738167', 'name': 'Yuan Hua'}, {'authorId': '2039788', 'name': 'Haoran Huang'}, {'authorId': '153345698', 'name': 'Tianxiang Sun'}, {'authorId': '146948229', 'name': 'Hang Yan'}, {'authorId': '2067331064', 'name': 'Tao Gui'}, {'authorId': '47835189', 'name': 'Qi Zhang'}, {'authorId': '1767521', 'name': 'Xipeng Qiu'}, {'authorId': '1790227', 'name': 'Xuanjing Huang'}]",['ByteDance'],['China'],2023-07
2307.05074,Chunxi Guo,"Chunxi Guo, Zhiliang Tian, Jintao Tang, Shasha Li, Zhihua Wen, Kaixuan
  Wang and Ting Wang",Retrieval-augmented GPT-3.5-based Text-to-SQL Framework with Sample-aware Prompting and Dynamic Revision Chain,,,,,cs.IR cs.AI cs.DB,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text-to-SQL aims at generating SQL queries for the given natural language questions and thus helping users to query databases. Prompt learning with large language models (LLMs) has emerged as a recent approach, which designs prompts to lead LLMs to understand the input question and generate the corresponding SQL. However, it faces challenges with strict SQL syntax requirements. Existing work prompts the LLMs with a list of demonstration examples (i.e. question-SQL pairs) to generate SQL, but the fixed prompts can hardly handle the scenario where the semantic gap between the retrieved demonstration and the input question is large. In this paper, we propose a retrieval-augmented prompting method for a LLM-based Text-to-SQL framework, involving sample-aware prompting and a dynamic revision chain. Our approach incorporates sample-aware demonstrations, which include the composition of SQL operators and fine-grained information related to the given question. To retrieve questions sharing similar intents with input questions, we propose two strategies for assisting retrieval. Firstly, we leverage LLMs to simplify the original questions, unifying the syntax and thereby clarifying the users' intentions. To generate executable and accurate SQLs without human intervention, we design a dynamic revision chain which iteratively adapts fine-grained feedback from the previously generated SQL. Experimental results on three Text-to-SQL benchmarks demonstrate the superiority of our method over strong baseline models. ","[{'version': 'v1', 'created': 'Tue, 11 Jul 2023 07:16:22 GMT'}, {'version': 'v2', 'created': 'Mon, 4 Sep 2023 08:10:03 GMT'}]",2023-09-06,"[['Guo', 'Chunxi', ''], ['Tian', 'Zhiliang', ''], ['Tang', 'Jintao', ''], ['Li', 'Shasha', ''], ['Wen', 'Zhihua', ''], ['Wang', 'Kaixuan', ''], ['Wang', 'Ting', '']]",0,1,2023-07-11,2,7,3,1,0,1,191e300e381d4128b749d16fe3d83c8643a3bd1f,259766168.0,https://www.semanticscholar.org/paper/191e300e381d4128b749d16fe3d83c8643a3bd1f,International Conference on Neural Information Processing,2023.0,42.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2215405126', 'name': 'Chunxi Guo'}, {'authorId': '33992653', 'name': 'Zhiliang Tian'}, {'authorId': '1762106', 'name': 'Jintao Tang'}, {'authorId': '2145340498', 'name': 'Shasha Li'}, {'authorId': '2067933778', 'name': 'Zhihua Wen'}, {'authorId': '2222742930', 'name': 'Kaixuan Wang'}, {'authorId': '38972135', 'name': 'Ting Wang'}]",['National University of Defense Technology'],['China'],2023-07
2307.05113,Zhouhong Gu,"Zhouhon Gu, Zihan Li, Lin Zhang, Zhuozhi Xiong, Haoning Ye, Yikai
  Zhang, Wenhao Huang, Xiaoxuan Zhu, Qianyu He, Rui Xu, Sihang Jiang, Shusen
  Wang, Zili Wang, Hongwei Feng, Zhixu Li, Yanghua Xiao",Go Beyond The Obvious: Probing the gap of INFORMAL reasoning ability between Humanity and LLMs by Detective Reasoning Puzzle Benchmark,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Informal reasoning ability is the ability to reason based on common sense, experience, and intuition.Humans use informal reasoning every day to extract the most influential elements for their decision-making from a large amount of life-like information.With the rapid development of language models, the realization of general artificial intelligence has emerged with hope. Given the outstanding informal reasoning ability of humans, how much informal reasoning ability language models have has not been well studied by scholars.In order to explore the gap between humans and language models in informal reasoning ability, this paper constructs a Detective Reasoning Benchmark, which is an assembly of 1,200 questions gathered from accessible online resources, aims at evaluating the model's informal reasoning ability in real-life context.Considering the improvement of the model's informal reasoning ability restricted by the lack of benchmark, we further propose a Self-Question Prompt Framework that mimics human thinking to enhance the model's informal reasoning ability.The goals of self-question are to find key elements, deeply investigate the connections between these elements, encourage the relationship between each element and the problem, and finally, require the model to reasonably answer the problem.The experimental results show that human performance greatly outperforms the SoTA Language Models in Detective Reasoning Benchmark.Besides, Self-Question is proven to be the most effective prompt engineering in improving GPT-4's informal reasoning ability, but it still does not even surpass the lowest score made by human participants.Upon acceptance of the paper, the source code for the benchmark will be made publicly accessible. ","[{'version': 'v1', 'created': 'Tue, 11 Jul 2023 08:45:46 GMT'}, {'version': 'v2', 'created': 'Wed, 9 Aug 2023 12:08:46 GMT'}]",2023-08-10,"[['Gu', 'Zhouhon', ''], ['Li', 'Zihan', ''], ['Zhang', 'Lin', ''], ['Xiong', 'Zhuozhi', ''], ['Ye', 'Haoning', ''], ['Zhang', 'Yikai', ''], ['Huang', 'Wenhao', ''], ['Zhu', 'Xiaoxuan', ''], ['He', 'Qianyu', ''], ['Xu', 'Rui', ''], ['Jiang', 'Sihang', ''], ['Wang', 'Shusen', ''], ['Wang', 'Zili', ''], ['Feng', 'Hongwei', ''], ['Li', 'Zhixu', ''], ['Xiao', 'Yanghua', '']]",0,1,2023-07-11,2,16,1,1,0,1,b96a4bd6c65c32f881401cfe191b16a8d73c91ea,259766557.0,https://www.semanticscholar.org/paper/b96a4bd6c65c32f881401cfe191b16a8d73c91ea,,2023.0,45.0,0.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2160631240', 'name': 'Zhouhong Gu'}, {'authorId': '2118274188', 'name': 'Zihan Li'}, {'authorId': '2143838757', 'name': 'Lin Zhang'}, {'authorId': '2215212876', 'name': 'Zhuozhi Xiong'}, {'authorId': '1999030240', 'name': 'Sihang Jiang'}, {'authorId': '2215265340', 'name': 'Xiaoxuan Zhu'}, {'authorId': '2175973802', 'name': 'Shusen Wang'}, {'authorId': '2117425254', 'name': 'Zili Wang'}, {'authorId': '2219864610', 'name': 'Jianchen Wang'}, {'authorId': '2215246086', 'name': 'Haoning Ye'}, {'authorId': '2158103505', 'name': 'Wenhao Huang'}, {'authorId': '2217263361', 'name': 'Yikai Zhang'}, {'authorId': '27155736', 'name': 'Hongwei Feng'}, {'authorId': '2116642640', 'name': 'Yanghua Xiao'}]","['Joint Research Center', 'Fudan University', 'University of Waterloo']","['China', 'Canada', 'Spain']",2023-07
2307.05354,Liu Chang,"Dongbo Wang, Chang Liu, Zhixiao Zhao, Si Shen, Liu Liu, Bin Li,
  Haotian Hu, Mengcheng Wu, Litao Lin, Xue Zhao, Xiyu Wang",GujiBERT and GujiGPT: Construction of Intelligent Information Processing Foundation Language Models for Ancient Texts,"22pages,0 figure",,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  In the context of the rapid development of large language models, we have meticulously trained and introduced the GujiBERT and GujiGPT language models, which are foundational models specifically designed for intelligent information processing of ancient texts. These models have been trained on an extensive dataset that encompasses both simplified and traditional Chinese characters, allowing them to effectively handle various natural language processing tasks related to ancient books, including but not limited to automatic sentence segmentation, punctuation, word segmentation, part-of-speech tagging, entity recognition, and automatic translation. Notably, these models have exhibited exceptional performance across a range of validation tasks using publicly available datasets. Our research findings highlight the efficacy of employing self-supervised methods to further train the models using classical text corpora, thus enhancing their capability to tackle downstream tasks. Moreover, it is worth emphasizing that the choice of font, the scale of the corpus, and the initial model selection all exert significant influence over the ultimate experimental outcomes. To cater to the diverse text processing preferences of researchers in digital humanities and linguistics, we have developed three distinct categories comprising a total of nine model variations. We believe that by sharing these foundational language models specialized in the domain of ancient texts, we can facilitate the intelligent processing and scholarly exploration of ancient literary works and, consequently, contribute to the global dissemination of China's rich and esteemed traditional culture in this new era. ","[{'version': 'v1', 'created': 'Tue, 11 Jul 2023 15:44:01 GMT'}]",2023-07-12,"[['Wang', 'Dongbo', ''], ['Liu', 'Chang', ''], ['Zhao', 'Zhixiao', ''], ['Shen', 'Si', ''], ['Liu', 'Liu', ''], ['Li', 'Bin', ''], ['Hu', 'Haotian', ''], ['Wu', 'Mengcheng', ''], ['Lin', 'Litao', ''], ['Zhao', 'Xue', ''], ['Wang', 'Xiyu', '']]",0,1,2023-07-11,1,11,1,0,0,0,efe90ad08b1d29266874be1b74027df80fea29bc,259766123.0,https://www.semanticscholar.org/paper/efe90ad08b1d29266874be1b74027df80fea29bc,arXiv.org,2023.0,27.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '47858962', 'name': 'Dongbo Wang'}, {'authorId': '2118483828', 'name': 'Chang Liu'}, {'authorId': '2214963769', 'name': 'Zhixiao Zhao'}, {'authorId': '152909152', 'name': 'Si Shen'}, {'authorId': '144117143', 'name': 'L. Liu'}, {'authorId': '2185909341', 'name': 'Bin Li'}, {'authorId': '151485331', 'name': 'Haotian Hu'}, {'authorId': '2215108670', 'name': 'Mengcheng Wu'}, {'authorId': '113530810', 'name': 'Litao Lin'}, {'authorId': '2167401372', 'name': 'Xue Zhao'}, {'authorId': '2107934862', 'name': 'Xiyu Wang'}]","['Nanjing University of Science and Technology', 'Nanjing University', 'Nanjing Agricultural University', 'Nanjing Normal University']",['China'],2023-07
2307.05493,David Woo,"David James Woo, Kai Guo and Hengky Susanto",Cases of EFL Secondary Students' Prompt Engineering Pathways to Complete a Writing Task with ChatGPT,"41 pages, 6 figures",,10.13140/RG.2.2.31464.85762,,cs.HC cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  ChatGPT is a state-of-the-art (SOTA) chatbot. Although it has potential to support English as a foreign language (EFL) students' writing, to effectively collaborate with it, a student must learn to engineer prompts, that is, the skill of crafting appropriate instructions so that ChatGPT produces desired outputs. However, writing an appropriate prompt for ChatGPT is not straightforward for non-technical users who suffer a trial-and-error process. This paper examines the content of EFL students' ChatGPT prompts when completing a writing task and explores patterns in the quality and quantity of the prompts. The data come from iPad screen recordings of secondary school EFL students who used ChatGPT and other SOTA chatbots for the first time to complete the same writing task. The paper presents a case study of four distinct pathways that illustrate the trial-and-error process and show different combinations of prompt content and quantity. The cases contribute evidence for the need to provide prompt engineering education in the context of the EFL writing classroom, if students are to move beyond an individual trial-and-error process, learning a greater variety of prompt content and more sophisticated prompts to support their writing. ","[{'version': 'v1', 'created': 'Mon, 19 Jun 2023 06:45:04 GMT'}]",2023-07-13,"[['Woo', 'David James', ''], ['Guo', 'Kai', ''], ['Susanto', 'Hengky', '']]",1,1,2023-06-19,1,3,3,1,0,1,344f801663a76aa15e0dd13344261d8648c382a2,259836994.0,https://www.semanticscholar.org/paper/344f801663a76aa15e0dd13344261d8648c382a2,arXiv.org,2023.0,36.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Education', 'source': 's2-fos-model'}]","[{'authorId': '12132198', 'name': 'D. Woo'}, {'authorId': '2061502406', 'name': 'Kai Guo'}, {'authorId': '3353735', 'name': 'Hengky Susanto'}]","['University of Massachusetts Lowell', 'Huawei Technologies (China)', 'Precious Blood Secondary School, 338 San Ha Street, Chai Wan, Hong Kong, China', 'University of Hong Kong']","['China', 'United States', 'Hong Kong']",2023-06
2307.05628,Daoan Zhang,"Daoan Zhang, Weitong Zhang, Yu Zhao, Jianguo Zhang, Bing He, Chenchen
  Qin, Jianhua Yao",DNAGPT: A Generalized Pre-trained Tool for Versatile DNA Sequence Analysis Tasks,,,,,q-bio.GN cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained large language models demonstrate potential in extracting information from DNA sequences, yet adapting to a variety of tasks and data modalities remains a challenge. To address this, we propose DNAGPT, a generalized DNA pre-training model trained on over 200 billion base pairs from all mammals. By enhancing the classic GPT model with a binary classification task (DNA sequence order), a numerical regression task (guanine-cytosine content prediction), and a comprehensive token language, DNAGPT can handle versatile DNA analysis tasks while processing both sequence and numerical data. Our evaluation of genomic signal and region recognition, mRNA abundance regression, and artificial genomes generation tasks demonstrates DNAGPT's superior performance compared to existing models designed for specific downstream tasks, benefiting from pre-training using the newly designed model structure. ","[{'version': 'v1', 'created': 'Tue, 11 Jul 2023 06:30:43 GMT'}, {'version': 'v2', 'created': 'Mon, 7 Aug 2023 07:41:47 GMT'}, {'version': 'v3', 'created': 'Wed, 30 Aug 2023 20:16:55 GMT'}]",2023-09-01,"[['Zhang', 'Daoan', ''], ['Zhang', 'Weitong', ''], ['Zhao', 'Yu', ''], ['Zhang', 'Jianguo', ''], ['He', 'Bing', ''], ['Qin', 'Chenchen', ''], ['Yao', 'Jianhua', '']]",0,1,2023-07-11,3,7,2,0,0,0,5bfce5239842726bcb8b51261630538a0cbe3ca1,259837062.0,https://www.semanticscholar.org/paper/5bfce5239842726bcb8b51261630538a0cbe3ca1,bioRxiv,2023.0,56.0,0.0,0.0,True,"['Biology', 'Computer Science']","[{'category': 'Biology', 'source': 'external'}, {'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Biology', 'source': 's2-fos-model'}]","[{'authorId': '2188765758', 'name': 'Daoan Zhang'}, {'authorId': '2108190244', 'name': 'Weitong Zhang'}, {'authorId': '2186372946', 'name': 'Bing He'}, {'authorId': '97522134', 'name': 'Yu Zhao'}, {'authorId': '2108313681', 'name': 'Jiang Zhang'}, {'authorId': '1919324982', 'name': 'Chenchen Qin'}, {'authorId': '2200180645', 'name': 'Jianhua Yao'}]","['City University of Hong Kong', 'Tencent', 'University of Rochester', 'Southern University of Science and Technology']","['China', 'United States']",2023-07
2307.05722,Likang Wu,"Likang Wu, Zhaopeng Qiu, Zhi Zheng, Hengshu Zhu, and Enhong Chen",Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations,,,,,cs.AI cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have revolutionized natural language processing tasks, demonstrating their exceptional capabilities in various domains. However, their potential for behavior graph understanding in job recommendations remains largely unexplored. This paper focuses on unveiling the capability of large language models in understanding behavior graphs and leveraging this understanding to enhance recommendations in online recruitment, including the promotion of out-of-distribution (OOD) application. We present a novel framework that harnesses the rich contextual information and semantic representations provided by large language models to analyze behavior graphs and uncover underlying patterns and relationships. Specifically, we propose a meta-path prompt constructor that leverages LLM recommender to understand behavior graphs for the first time and design a corresponding path augmentation module to alleviate the prompt bias introduced by path-based sequence input. By leveraging this capability, our framework enables personalized and accurate job recommendations for individual users. We evaluate the effectiveness of our approach on a comprehensive dataset and demonstrate its ability to improve the relevance and quality of recommended quality. This research not only sheds light on the untapped potential of large language models but also provides valuable insights for developing advanced recommendation systems in the recruitment market. The findings contribute to the growing field of natural language processing and offer practical implications for enhancing job search experiences. ","[{'version': 'v1', 'created': 'Mon, 10 Jul 2023 11:29:41 GMT'}]",2023-07-13,"[['Wu', 'Likang', ''], ['Qiu', 'Zhaopeng', ''], ['Zheng', 'Zhi', ''], ['Zhu', 'Hengshu', ''], ['Chen', 'Enhong', '']]",0,0,2023-07-10,1,5,3,0,0,0,09826f769cef899388909d9f4cfaa335429c41a4,259836967.0,https://www.semanticscholar.org/paper/09826f769cef899388909d9f4cfaa335429c41a4,arXiv.org,2023.0,28.0,6.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '12892739', 'name': 'Likang Wu'}, {'authorId': '2382872', 'name': 'Zhaopeng Qiu'}, {'authorId': '2115548452', 'name': 'Zhi Zheng'}, {'authorId': '1968806', 'name': 'Hengshu Zhu'}, {'authorId': '2227868312', 'name': 'Enhong Chen'}]","['University of Science and Technology of China', 'Career Science Lab, BOSS Zhipin']",['China'],2023-07
2307.06281,Haodong Duan,"Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo
  Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, Dahua Lin",MMBench: Is Your Multi-modal Model an All-around Player?,,,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large vision-language models have recently achieved remarkable progress, exhibiting great perception and reasoning abilities concerning visual information. However, how to effectively evaluate these large vision-language models remains a major obstacle, hindering future model development. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but suffer from a lack of fine-grained ability assessment and non-robust evaluation metrics. Recent subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model's abilities by incorporating human labor, but they are not scalable and display significant bias. In response to these challenges, we propose MMBench, a novel multi-modality benchmark. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of two elements. The first element is a meticulously curated dataset that surpasses existing similar benchmarks in terms of the number and variety of evaluation questions and abilities. The second element introduces a novel CircularEval strategy and incorporates the use of ChatGPT. This implementation is designed to convert free-form predictions into pre-defined choices, thereby facilitating a more robust evaluation of the model's predictions. MMBench is a systematically-designed objective benchmark for robustly evaluating the various abilities of vision-language models. We hope MMBench will assist the research community in better evaluating their models and encourage future advancements in this domain. Project page: https://opencompass.org.cn/mmbench. ","[{'version': 'v1', 'created': 'Wed, 12 Jul 2023 16:23:09 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Jul 2023 16:02:57 GMT'}, {'version': 'v3', 'created': 'Sun, 13 Aug 2023 13:12:47 GMT'}]",2023-08-15,"[['Liu', 'Yuan', ''], ['Duan', 'Haodong', ''], ['Zhang', 'Yuanhan', ''], ['Li', 'Bo', ''], ['Zhang', 'Songyang', ''], ['Zhao', 'Wangbo', ''], ['Yuan', 'Yike', ''], ['Wang', 'Jiaqi', ''], ['He', 'Conghui', ''], ['Liu', 'Ziwei', ''], ['Chen', 'Kai', ''], ['Lin', 'Dahua', '']]",1,1,2023-07-12,3,12,2,1,0,1,b37b1dc72b1882858f5120f2cd6883134089a6ed,259837088.0,https://www.semanticscholar.org/paper/b37b1dc72b1882858f5120f2cd6883134089a6ed,arXiv.org,2023.0,47.0,38.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1604959737', 'name': 'Yuanzhan Liu'}, {'authorId': '31463937', 'name': 'Haodong Duan'}, {'authorId': '2145784327', 'name': 'Yuanhan Zhang'}, {'authorId': '2165247100', 'name': 'Bo Li'}, {'authorId': '1734973476', 'name': 'Songyang Zhang'}, {'authorId': '2109435100', 'name': 'Wangbo Zhao'}, {'authorId': '2112499811', 'name': 'Yike Yuan'}, {'authorId': '2110238778', 'name': 'Jiaqi Wang'}, {'authorId': '3486481', 'name': 'Conghui He'}, {'authorId': '2145252993', 'name': 'Ziwei Liu'}, {'authorId': '152568027', 'name': 'Kai Chen'}, {'authorId': '1807606', 'name': 'Dahua Lin'}]","['National University of Singapore', 'Zhejiang University', 'Shanghai Artificial Intelligence Laboratory', 'Chinese University of Hong Kong', 'Nanyang Technological University']","['China', 'Singapore']",2023-07
2307.06435,Humza Naveed,"Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar,
  Muhammad Usman, Naveed Akhtar, Nick Barnes, Ajmal Mian",A Comprehensive Overview of Large Language Models,Work in-progress,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations of the underlying neural networks, context length improvements, model alignment, training datasets, benchmarking, efficiency and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides that overview to the research community. It not only focuses on a systematic treatment of the existing literature on a broad range of LLM related concept, but also pays special attention to providing comprehensive summaries with extensive details about the individual existing models, datasets and major insights. We also pay heed to aligning our overview with the emerging outlook of this research direction by accounting for the other recently materializing reviews of the broader research direction of LLMs. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of this research direction. This review article is intended to not only provide a systematic survey, but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the LLM research direction. ","[{'version': 'v1', 'created': 'Wed, 12 Jul 2023 20:01:52 GMT'}, {'version': 'v2', 'created': 'Fri, 18 Aug 2023 13:53:06 GMT'}, {'version': 'v3', 'created': 'Wed, 13 Sep 2023 12:13:45 GMT'}, {'version': 'v4', 'created': 'Thu, 5 Oct 2023 10:29:02 GMT'}]",2023-10-06,"[['Naveed', 'Humza', ''], ['Khan', 'Asad Ullah', ''], ['Qiu', 'Shi', ''], ['Saqib', 'Muhammad', ''], ['Anwar', 'Saeed', ''], ['Usman', 'Muhammad', ''], ['Akhtar', 'Naveed', ''], ['Barnes', 'Nick', ''], ['Mian', 'Ajmal', '']]",0,0,2023-07-12,4,9,1,0,0,0,ca31b8584b6c022ef15ddfe994fe361e002b7729,259847443.0,https://www.semanticscholar.org/paper/ca31b8584b6c022ef15ddfe994fe361e002b7729,arXiv.org,2023.0,443.0,15.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '32749940', 'name': 'Humza Naveed'}, {'authorId': '2201619569', 'name': 'Asad Ullah Khan'}, {'authorId': '2055117257', 'name': 'Shi Qiu'}, {'authorId': '2153464760', 'name': 'Muhammad Saqib'}, {'authorId': '49053414', 'name': 'Saeed Anwar'}, {'authorId': '2223436301', 'name': 'Muhammad Usman'}, {'authorId': '1712576', 'name': 'N. Barnes'}, {'authorId': '1747500', 'name': 'A. Mian'}]","['King Fahd University of Petroleum and Minerals', 'University of Western Australia', 'University of Melbourne', 'Australian National University', 'Chinese University of Hong Kong', 'Commonwealth Scientific and Industrial Research Organisation', 'University of Engineering and Technology Lahore', 'University of Technology Sydney']","['China', 'Saudi Arabia', 'Pakistan', 'Australia']",2023-07
2307.06569,Yi Cheng,"Yi Cheng, Ziwei Xu, Fen Fang, Dongyun Lin, Hehe Fan, Yongkang Wong,
  Ying Sun, Mohan Kankanhalli",A Study on Differentiable Logic and LLMs for EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2023,Technical report submitted to CVPR 2023 EPIC-Kitchens challenges,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this technical report, we present our findings from a study conducted on the EPIC-KITCHENS-100 Unsupervised Domain Adaptation task for Action Recognition. Our research focuses on the innovative application of a differentiable logic loss in the training to leverage the co-occurrence relations between verb and noun, as well as the pre-trained Large Language Models (LLMs) to generate the logic rules for the adaptation to unseen action labels. Specifically, the model's predictions are treated as the truth assignment of a co-occurrence logic formula to compute the logic loss, which measures the consistency between the predictions and the logic constraints. By using the verb-noun co-occurrence matrix generated from the dataset, we observe a moderate improvement in model performance compared to our baseline framework. To further enhance the model's adaptability to novel action labels, we experiment with rules generated using GPT-3.5, which leads to a slight decrease in performance. These findings shed light on the potential and challenges of incorporating differentiable logic and LLMs for knowledge extraction in unsupervised domain adaptation for action recognition. Our final submission (entitled `NS-LLM') achieved the first place in terms of top-1 action recognition accuracy. ","[{'version': 'v1', 'created': 'Thu, 13 Jul 2023 05:54:05 GMT'}]",2023-07-14,"[['Cheng', 'Yi', ''], ['Xu', 'Ziwei', ''], ['Fang', 'Fen', ''], ['Lin', 'Dongyun', ''], ['Fan', 'Hehe', ''], ['Wong', 'Yongkang', ''], ['Sun', 'Ying', ''], ['Kankanhalli', 'Mohan', '']]",0,1,2023-07-13,1,8,1,1,0,1,8107acd6fa2b608a841876d1eb277ecd79e7f8f4,259847234.0,https://www.semanticscholar.org/paper/8107acd6fa2b608a841876d1eb277ecd79e7f8f4,arXiv.org,2023.0,6.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2146291548', 'name': 'Yi Cheng'}, {'authorId': '1390848082', 'name': 'Ziwei Xu'}, {'authorId': '2998031', 'name': 'Fen Fang'}, {'authorId': '47415204', 'name': 'Dongyun Lin'}, {'authorId': '3446334', 'name': 'Hehe Fan'}, {'authorId': '3026404', 'name': 'Yongkang Wong'}, {'authorId': '1707882', 'name': 'Ying Sun'}, {'authorId': '145977143', 'name': 'Mohan S. Kankanhalli'}]","['National University of Singapore', 'Zhejiang University', 'Institute for Infocomm Research']","['China', 'Singapore']",2023-07
2307.06942,Yi Wang,"Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinyuan
  Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, Yu Qiao",InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation,"Data and Code:
  https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid",,,,cs.CV,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  This paper introduces InternVid, a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation. The InternVid dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words. Our core contribution is to develop a scalable approach to autonomously build a high-quality video-text dataset with large language models (LLM), thereby showcasing its efficacy in learning video-language representation at scale. Specifically, we utilize a multi-scale approach to generate video-related descriptions. Furthermore, we introduce ViCLIP, a video-text representation learning model based on ViT-L. Learned on InternVid via contrastive learning, this model demonstrates leading zero-shot action recognition and competitive video retrieval performance. Beyond basic video understanding tasks like recognition and retrieval, our dataset and model have broad applications. They are particularly beneficial for generating interleaved video-text data for learning a video-centric dialogue system, advancing video-to-text and text-to-video generation research. These proposed resources provide a tool for researchers and practitioners interested in multimodal video understanding and generation. ","[{'version': 'v1', 'created': 'Thu, 13 Jul 2023 17:58:32 GMT'}]",2023-07-14,"[['Wang', 'Yi', ''], ['He', 'Yinan', ''], ['Li', 'Yizhuo', ''], ['Li', 'Kunchang', ''], ['Yu', 'Jiashuo', ''], ['Ma', 'Xin', ''], ['Chen', 'Xinyuan', ''], ['Wang', 'Yaohui', ''], ['Luo', 'Ping', ''], ['Liu', 'Ziwei', ''], ['Wang', 'Yali', ''], ['Wang', 'Limin', ''], ['Qiao', 'Yu', '']]",0,0,2023-07-13,1,13,1,0,0,0,369b449415d50387fba048bbd4d26ee890df84b5,259847783.0,https://www.semanticscholar.org/paper/369b449415d50387fba048bbd4d26ee890df84b5,arXiv.org,2023.0,79.0,3.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '46393411', 'name': 'Yi Wang'}, {'authorId': '2118918324', 'name': 'Yinan He'}, {'authorId': '2154568284', 'name': 'Yizhuo Li'}, {'authorId': '2115468491', 'name': 'Kunchang Li'}, {'authorId': '2116034742', 'name': 'Jiashuo Yu'}, {'authorId': '144533653', 'name': 'X. Ma'}, {'authorId': '2130193247', 'name': 'Xinyuan Chen'}, {'authorId': '2119049364', 'name': 'Yaohui Wang'}, {'authorId': '2143481782', 'name': 'Ping Luo'}, {'authorId': '2145253401', 'name': 'Ziwei Liu'}, {'authorId': '47903936', 'name': 'Yali Wang'}, {'authorId': '2141353278', 'name': 'Limin Wang'}, {'authorId': '145858545', 'name': 'Y. Qiao'}]","['Nanjing University', 'Shenzhen Institutes of Advanced Technology', 'OpenGVLab, Shanghai AI Laboratory', 'Monash University', 'Nanyang Technological University', 'University of Hong Kong']","['China', 'Singapore', 'Australia', 'Hong Kong']",2023-07
2307.07162,Licheng Wen,"Daocheng Fu, Xin Li, Licheng Wen, Min Dou, Pinlong Cai, Botian Shi, Yu
  Qiao",Drive Like a Human: Rethinking Autonomous Driving with Large Language Models,,,,,cs.RO cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we explore the potential of using a large language model (LLM) to understand the driving environment in a human-like manner and analyze its ability to reason, interpret, and memorize when facing complex scenarios. We argue that traditional optimization-based and modular autonomous driving (AD) systems face inherent performance limitations when dealing with long-tail corner cases. To address this problem, we propose that an ideal AD system should drive like a human, accumulating experience through continuous driving and using common sense to solve problems. To achieve this goal, we identify three key abilities necessary for an AD system: reasoning, interpretation, and memorization. We demonstrate the feasibility of employing an LLM in driving scenarios by building a closed-loop system to showcase its comprehension and environment-interaction abilities. Our extensive experiments show that the LLM exhibits the impressive ability to reason and solve long-tailed cases, providing valuable insights for the development of human-like autonomous driving. The related code are available at https://github.com/PJLab-ADG/DriveLikeAHuman . ","[{'version': 'v1', 'created': 'Fri, 14 Jul 2023 05:18:34 GMT'}]",2023-07-17,"[['Fu', 'Daocheng', ''], ['Li', 'Xin', ''], ['Wen', 'Licheng', ''], ['Dou', 'Min', ''], ['Cai', 'Pinlong', ''], ['Shi', 'Botian', ''], ['Qiao', 'Yu', '']]",0,0,2023-07-14,1,7,2,0,0,0,11bca2cafe89e14dc733504f97e2489de697ceab,259924488.0,https://www.semanticscholar.org/paper/11bca2cafe89e14dc733504f97e2489de697ceab,arXiv.org,2023.0,47.0,12.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '150967272', 'name': 'Daocheng Fu'}, {'authorId': '2153897378', 'name': 'Xin Li'}, {'authorId': '153109152', 'name': 'Licheng Wen'}, {'authorId': '2197075911', 'name': 'Min Dou'}, {'authorId': '26978261', 'name': 'Pinlong Cai'}, {'authorId': '119700639', 'name': 'Botian Shi'}, {'authorId': '145858545', 'name': 'Y. Qiao'}]","['Shanghai Artificial Intelligence Laboratory', 'East China Normal University']",['China'],2023-07
2307.07221,Yuchao Huang,"Junjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, Qing
  Wang","Software Testing with Large Language Model: Survey, Landscape, and Vision","20 pages, 11 figures",,,,cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, and making it an area ripe for innovative approaches such as the use of LLMs. This paper provides a comprehensive review of the utilization of LLMs in software testing. It analyzes 52 relevant studies that have used LLMs for software testing, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative ones. It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs. It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in software testing. ","[{'version': 'v1', 'created': 'Fri, 14 Jul 2023 08:26:12 GMT'}]",2023-07-17,"[['Wang', 'Junjie', ''], ['Huang', 'Yuchao', ''], ['Chen', 'Chunyang', ''], ['Liu', 'Zhe', ''], ['Wang', 'Song', ''], ['Wang', 'Qing', '']]",0,0,2023-07-14,1,6,1,0,0,0,f43b8a87a96f8abc2467b90538b643a6061416e9,259924919.0,https://www.semanticscholar.org/paper/f43b8a87a96f8abc2467b90538b643a6061416e9,arXiv.org,2023.0,103.0,8.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2109763128', 'name': 'Junjie Wang'}, {'authorId': '2115708808', 'name': 'Yuchao Huang'}, {'authorId': '46729152', 'name': 'Chunyang Chen'}, {'authorId': '2116746462', 'name': 'Zhe Liu'}, {'authorId': '2117074753', 'name': 'Song Wang'}, {'authorId': '2145464944', 'name': 'Qing Wang'}]","['Monash University', 'University of Chinese Academy of Sciences', 'Chinese Academy of Sciences', 'York University']","['Canada', 'China', 'Australia']",2023-07
2307.07306,Yuren Mao,"Xuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao, lu Chen,
  Jinshu Lin, Dongfang Lou",C3: Zero-shot Text-to-SQL with ChatGPT,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper proposes a ChatGPT-based zero-shot Text-to-SQL method, dubbed C3, which achieves 82.3\% in terms of execution accuracy on the holdout test set of Spider and becomes the state-of-the-art zero-shot Text-to-SQL method on the Spider Challenge. C3 consists of three key components: Clear Prompting (CP), Calibration with Hints (CH), and Consistent Output (CO), which are corresponding to the model input, model bias and model output respectively. It provides a systematic treatment for zero-shot Text-to-SQL. Extensive experiments have been conducted to verify the effectiveness and efficiency of our proposed method. ","[{'version': 'v1', 'created': 'Fri, 14 Jul 2023 12:30:41 GMT'}]",2023-07-17,"[['Dong', 'Xuemei', ''], ['Zhang', 'Chao', ''], ['Ge', 'Yuhang', ''], ['Mao', 'Yuren', ''], ['Gao', 'Yunjun', ''], ['Chen', 'lu', ''], ['Lin', 'Jinshu', ''], ['Lou', 'Dongfang', '']]",1,1,2023-07-14,1,8,2,1,0,1,01de0020c6a0f2d0aa73965e8dcfd9b8f38e05b6,259924856.0,https://www.semanticscholar.org/paper/01de0020c6a0f2d0aa73965e8dcfd9b8f38e05b6,arXiv.org,2023.0,23.0,4.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2223421628', 'name': 'Xuemei Dong'}, {'authorId': '50445747', 'name': 'C. Zhang'}, {'authorId': '2204667747', 'name': 'Yuhang Ge'}, {'authorId': '1753922779', 'name': 'Yuren Mao'}, {'authorId': '1409828392', 'name': 'Yunjun Gao'}, {'authorId': '2115384591', 'name': 'Lu Chen'}, {'authorId': '2223540001', 'name': 'Jinshu Lin'}, {'authorId': '104300808', 'name': 'Dongfang Lou'}]","['Hundsun Technologies INC., Zhejiang, China', 'Zhejiang University']",['China'],2023-07
2307.07319,Yuyang Du,"Yuyang Du, Soung Chang Liew, Kexin Chen, Yulin Shao",The Power of Large Language Models for Wireless Communication System Development: A Case Study on FPGA Platforms,,,,,eess.SP,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have garnered significant attention across various research disciplines, including the wireless communication community. There have been several heated discussions on the intersection of LLMs and wireless technologies. While recent studies have demonstrated the ability of LLMs to generate hardware description language (HDL) code for simple computation tasks, developing wireless prototypes and products via HDL poses far greater challenges because of the more complex computation tasks involved. In this paper, we aim to address this challenge by investigating the role of LLMs in FPGA-based hardware development for advanced wireless signal processing. We begin by exploring LLM-assisted code refactoring, reuse, and validation, using an open-source software-defined radio (SDR) project as a case study. Through the case study, we find that an LLM assistant can potentially yield substantial productivity gains for researchers and developers. We then examine the feasibility of using LLMs to generate HDL code for advanced wireless signal processing, using the Fast Fourier Transform (FFT) algorithm as an example. This task presents two unique challenges: the scheduling of subtasks within the overall task and the multi-step thinking required to solve certain arithmetic problem within the task. To address these challenges, we employ in-context learning (ICL) and Chain-of-Thought (CoT) prompting techniques, culminating in the successful generation of a 64-point Verilog FFT module. Our results demonstrate the potential of LLMs for generalization and imitation, affirming their usefulness in writing HDL code for wireless communication systems. Overall, this work contributes to understanding the role of LLMs in wireless communication and motivates further exploration of their capabilities. ","[{'version': 'v1', 'created': 'Fri, 14 Jul 2023 12:51:51 GMT'}, {'version': 'v2', 'created': 'Tue, 5 Sep 2023 02:04:07 GMT'}]",2023-09-06,"[['Du', 'Yuyang', ''], ['Liew', 'Soung Chang', ''], ['Chen', 'Kexin', ''], ['Shao', 'Yulin', '']]",0,0,2023-07-14,2,4,1,0,0,0,fdb5b1ade3dfcdb3f7deb62392416942c0effe65,259924486.0,https://www.semanticscholar.org/paper/fdb5b1ade3dfcdb3f7deb62392416942c0effe65,,2023.0,47.0,2.0,1.0,False,['Engineering'],"[{'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '23700162', 'name': 'Yuyang Du'}, {'authorId': '1736584', 'name': 'S. Liew'}, {'authorId': '2223537498', 'name': 'Kexin Chen'}, {'authorId': '8804170', 'name': 'Yulin Shao'}]","['Chinese University of Hong Kong', 'City University of Macau']",['China'],2023-07
2307.07443,Chen Qian,"Chen Qian, Huayi Tang, Zhirui Yang, Hong Liang, Yong Liu",Can Large Language Models Empower Molecular Property Prediction?,,,,,cs.LG cs.AI q-bio.QM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Molecular property prediction has gained significant attention due to its transformative potential in multiple scientific disciplines. Conventionally, a molecule graph can be represented either as a graph-structured data or a SMILES text. Recently, the rapid development of Large Language Models (LLMs) has revolutionized the field of NLP. Although it is natural to utilize LLMs to assist in understanding molecules represented by SMILES, the exploration of how LLMs will impact molecular property prediction is still in its early stage. In this work, we advance towards this objective through two perspectives: zero/few-shot molecular classification, and using the new explanations generated by LLMs as representations of molecules. To be specific, we first prompt LLMs to do in-context molecular classification and evaluate their performance. After that, we employ LLMs to generate semantically enriched explanations for the original SMILES and then leverage that to fine-tune a small-scale LM model for multiple downstream tasks. The experimental results highlight the superiority of text explanations as molecular representations across multiple benchmark datasets, and confirm the immense potential of LLMs in molecular property prediction tasks. Codes are available at \url{https://github.com/ChnQ/LLM4Mol}. ","[{'version': 'v1', 'created': 'Fri, 14 Jul 2023 16:06:42 GMT'}]",2023-07-17,"[['Qian', 'Chen', ''], ['Tang', 'Huayi', ''], ['Yang', 'Zhirui', ''], ['Liang', 'Hong', ''], ['Liu', 'Yong', '']]",0,0,2023-07-14,1,5,3,0,0,0,1212b1e44f7611d2017b246fd3d8e9c973c9d937,259924688.0,https://www.semanticscholar.org/paper/1212b1e44f7611d2017b246fd3d8e9c973c9d937,arXiv.org,2023.0,41.0,4.0,0.0,True,"['Computer Science', 'Biology']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Biology', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '144461220', 'name': 'Chen Qian'}, {'authorId': '2112701620', 'name': 'Huayi Tang'}, {'authorId': '1929221941', 'name': 'Zhi-Jiang Yang'}, {'authorId': '51227868', 'name': 'Hongsi Liang'}, {'authorId': '93006732', 'name': 'Y. Liu'}]","['Peking University', 'Renmin University of China']",['China'],2023-07
2307.07518,Lei Ma,"Lei Ma, Jincong Han, Zhaoxin Wang, Dian Zhang",CephGPT-4: An Interactive Multimodal Cephalometric Measurement and Diagnostic System with Visual Large Language Model,,,,,cs.AI cs.CL cs.CV eess.IV,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Large-scale multimodal language models (LMMs) have achieved remarkable success in general domains. However, the exploration of diagnostic language models based on multimodal cephalometric medical data remains limited. In this paper, we propose a novel multimodal cephalometric analysis and diagnostic dialogue model. Firstly, a multimodal orthodontic medical dataset is constructed, comprising cephalometric images and doctor-patient dialogue data, with automatic analysis of cephalometric landmarks using U-net and generation of diagnostic reports. Then, the cephalometric dataset and generated diagnostic reports are separately fine-tuned on Minigpt-4 and VisualGLM. Results demonstrate that the CephGPT-4 model exhibits excellent performance and has the potential to revolutionize orthodontic measurement and diagnostic applications. These innovations hold revolutionary application potential in the field of orthodontics. ","[{'version': 'v1', 'created': 'Sat, 1 Jul 2023 15:41:12 GMT'}]",2023-07-18,"[['Ma', 'Lei', ''], ['Han', 'Jincong', ''], ['Wang', 'Zhaoxin', ''], ['Zhang', 'Dian', '']]",0,1,2023-07-01,1,4,4,0,0,0,20c6d851621137bb762ae3a0f8a90a4b27e58e83,259937771.0,https://www.semanticscholar.org/paper/20c6d851621137bb762ae3a0f8a90a4b27e58e83,arXiv.org,2023.0,16.0,2.0,0.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '49198778', 'name': 'Lei Ma'}, {'authorId': '2223695035', 'name': 'Jincong Han'}, {'authorId': '2144719372', 'name': 'Zhaoxin Wang'}, {'authorId': '2223680148', 'name': 'Dian Zhang'}]",['Nantong University'],['China'],2023-07
2307.07697,Gasol Sun,"Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin,
  Yeyun Gong, Lionel M. Ni, Heung-Yeung Shum, Jian Guo",Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph,"30 pages, 13 figures, 20 tables",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although large language models (LLMs) have achieved significant success in various tasks, they often struggle with hallucination problems, especially in scenarios requiring deep and responsible reasoning. These issues could be partially addressed by introducing external knowledge graphs (KG) in LLM reasoning. In this paper, we propose a new LLM-KG integrating paradigm ``$\hbox{LLM}\otimes\hbox{KG}$'' which treats the LLM as an agent to interactively explore related entities and relations on KGs and perform reasoning based on the retrieved knowledge. We further implement this paradigm by introducing a new approach called Think-on-Graph (ToG), in which the LLM agent iteratively executes beam search on KG, discovers the most promising reasoning paths, and returns the most likely reasoning results. We use a number of well-designed experiments to examine and illustrate the following advantages of ToG: 1) compared with LLMs, ToG has better deep reasoning power; 2) ToG has the ability of knowledge traceability and knowledge correctability by leveraging LLMs reasoning and expert feedback; 3) ToG provides a flexible plug-and-play framework for different LLMs, KGs and prompting strategies without any additional training cost; 4) the performance of ToG with small LLM models could exceed large LLM such as GPT-4 in certain scenarios and this reduces the cost of LLM deployment and application. As a training-free method with lower computational cost and better generality, ToG achieves overall SOTA in 6 out of 9 datasets where most previous SOTAs rely on additional training. ","[{'version': 'v1', 'created': 'Sat, 15 Jul 2023 03:31:38 GMT'}, {'version': 'v2', 'created': 'Tue, 19 Sep 2023 08:39:04 GMT'}, {'version': 'v3', 'created': 'Sat, 30 Sep 2023 05:00:58 GMT'}]",2023-10-03,"[['Sun', 'Jiashuo', ''], ['Xu', 'Chengjin', ''], ['Tang', 'Lumingyuan', ''], ['Wang', 'Saizhuo', ''], ['Lin', 'Chen', ''], ['Gong', 'Yeyun', ''], ['Ni', 'Lionel M.', ''], ['Shum', 'Heung-Yeung', ''], ['Guo', 'Jian', '']]",0,1,2023-07-15,3,9,1,1,0,1,c7184f9a914dbfbad59faa5aeaf9ba7019dfcf74,263333907.0,https://www.semanticscholar.org/paper/c7184f9a914dbfbad59faa5aeaf9ba7019dfcf74,,2023.0,78.0,1.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2202454665', 'name': 'Jiashuo Sun'}, {'authorId': '2250617116', 'name': 'Chengjin Xu'}, {'authorId': '2253747495', 'name': 'Lumingyuan Tang'}, {'authorId': '2211588034', 'name': 'Sai Wang'}, {'authorId': '2249759096', 'name': 'Chen Lin'}, {'authorId': '2254121688', 'name': 'Yeyun Gong'}, {'authorId': '2249757348', 'name': 'Lionel M. Ni'}, {'authorId': '93596028', 'name': 'H. Shum'}, {'authorId': '2188226506', 'name': 'Jian Guo'}]","['University of Southern California', 'Xiamen University', 'Hong Kong University of Science and Technology', 'IDEA Research, International Digital Economy Academy', 'Microsoft']","['China', 'United States']",2023-07
2307.07705,Weilin Zhao,"Weilin Zhao, Yuxiang Huang, Xu Han, Zhiyuan Liu, Zhengyan Zhang,
  Maosong Sun",CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Parameter-efficient tuning (PET) has been widely explored in recent years because it tunes much fewer parameters (PET modules) than full-parameter fine-tuning (FT) while still stimulating sufficient knowledge from large language models (LLMs) for downstream tasks. Moreover, when PET is employed to serve multiple tasks, different task-specific PET modules can be built on a frozen LLM, avoiding redundant LLM deployments. Although PET significantly reduces the cost of tuning and deploying LLMs, its inference still suffers from the computational bottleneck of LLMs. To address the above issue, we propose an effective PET framework based on compressed LLMs, named ""CPET"". In CPET, we evaluate the impact of mainstream LLM compression techniques on PET performance and then introduce knowledge inheritance and recovery strategies to restore the knowledge loss caused by these compression techniques. Our experimental results demonstrate that, owing to the restoring strategies of CPET, collaborating task-specific PET modules with a compressed LLM can achieve comparable performance to collaborating PET modules with the original version of the compressed LLM and outperform directly applying vanilla PET methods to the compressed LLM. ","[{'version': 'v1', 'created': 'Sat, 15 Jul 2023 04:37:11 GMT'}]",2023-07-18,"[['Zhao', 'Weilin', ''], ['Huang', 'Yuxiang', ''], ['Han', 'Xu', ''], ['Liu', 'Zhiyuan', ''], ['Zhang', 'Zhengyan', ''], ['Sun', 'Maosong', '']]",0,0,2023-07-15,1,6,1,0,0,0,97f3cf88bc592a323e62dffe20e522de44957076,259937262.0,https://www.semanticscholar.org/paper/97f3cf88bc592a323e62dffe20e522de44957076,arXiv.org,2023.0,74.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2150606888', 'name': 'Weilin Zhao'}, {'authorId': '2214586078', 'name': 'Yuxiang Huang'}, {'authorId': '48506411', 'name': 'Xu Han'}, {'authorId': '2141313179', 'name': 'Zhiyuan Liu'}, {'authorId': '2621696', 'name': 'Zhengyan Zhang'}, {'authorId': '1753344', 'name': 'Maosong Sun'}]",['Tsinghua University'],['China'],2023-07
2307.07742,Yi-Syuan Chen,"Yi-Syuan Chen, Yun-Zhu Song, Cheng Yu Yeo, Bei Liu, Jianlong Fu,
  Hong-Han Shuai",SINC: Self-Supervised In-Context Learning for Vision-Language Tasks,Accepted by ICCV 2023; Camera Ready Version,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large Pre-trained Transformers exhibit an intriguing capacity for in-context learning. Without gradient updates, these models can rapidly construct new predictors from demonstrations presented in the inputs. Recent works promote this ability in the vision-language domain by incorporating visual information into large language models that can already make in-context predictions. However, these methods could inherit issues in the language domain, such as template sensitivity and hallucination. Also, the scale of these language models raises a significant demand for computations, making learning and operating these models resource-intensive. To this end, we raise a question: ``How can we enable in-context learning without relying on the intrinsic in-context ability of large language models?"". To answer it, we propose a succinct and general framework, Self-supervised IN-Context learning (SINC), that introduces a meta-model to learn on self-supervised prompts consisting of tailored demonstrations. The learned models can be transferred to downstream tasks for making in-context predictions on-the-fly. Extensive experiments show that SINC outperforms gradient-based methods in various vision-language tasks under few-shot settings. Furthermore, the designs of SINC help us investigate the benefits of in-context learning across different tasks, and the analysis further reveals the essential components for the emergence of in-context learning in the vision-language domain. ","[{'version': 'v1', 'created': 'Sat, 15 Jul 2023 08:33:08 GMT'}, {'version': 'v2', 'created': 'Sat, 19 Aug 2023 08:27:16 GMT'}]",2023-08-22,"[['Chen', 'Yi-Syuan', ''], ['Song', 'Yun-Zhu', ''], ['Yeo', 'Cheng Yu', ''], ['Liu', 'Bei', ''], ['Fu', 'Jianlong', ''], ['Shuai', 'Hong-Han', '']]",0,0,2023-07-15,2,6,2,0,0,0,7fc133b3a61e88338ae15a2bf72f08fdc2beb504,259937787.0,https://www.semanticscholar.org/paper/7fc133b3a61e88338ae15a2bf72f08fdc2beb504,arXiv.org,2023.0,86.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2109293164', 'name': 'Yi-Syuan Chen'}, {'authorId': '1491598459', 'name': 'Yun-Zhu Song'}, {'authorId': '2223591422', 'name': 'Cheng Yu Yeo'}, {'authorId': '2108662284', 'name': 'Bei Liu'}, {'authorId': '3247966', 'name': 'Jianlong Fu'}, {'authorId': '2426757', 'name': 'Hong-Han Shuai'}]","['National Yang Ming Chiao Tung University', 'Microsoft']","['China', 'Taiwan']",2023-07
2307.07909,Yao Wei,"Yao Wei and Yanchao Sun and Ruijie Zheng and Sai Vemprala and Rogerio
  Bonatti and Shuhang Chen and Ratnesh Madaan and Zhongjie Ba and Ashish Kapoor
  and Shuang Ma",Is Imitation All You Need? Generalized Decision-Making with Dual-Phase Training,,,,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  We introduce DualMind, a generalist agent designed to tackle various decision-making tasks that addresses challenges posed by current methods, such as overfitting behaviors and dependence on task-specific fine-tuning. DualMind uses a novel ""Dual-phase"" training strategy that emulates how humans learn to act in the world. The model first learns fundamental common knowledge through a self-supervised objective tailored for control tasks and then learns how to make decisions based on different contexts through imitating behaviors conditioned on given prompts. DualMind can handle tasks across domains, scenes, and embodiments using just a single set of model weights and can execute zero-shot prompting without requiring task-specific fine-tuning. We evaluate DualMind on MetaWorld and Habitat through extensive experiments and demonstrate its superior generalizability compared to previous techniques, outperforming other generalist agents by over 50$\%$ and 70$\%$ on Habitat and MetaWorld, respectively. On the 45 tasks in MetaWorld, DualMind achieves over 30 tasks at a 90$\%$ success rate. ","[{'version': 'v1', 'created': 'Sun, 16 Jul 2023 00:34:12 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Jul 2023 16:05:00 GMT'}]",2023-07-19,"[['Wei', 'Yao', ''], ['Sun', 'Yanchao', ''], ['Zheng', 'Ruijie', ''], ['Vemprala', 'Sai', ''], ['Bonatti', 'Rogerio', ''], ['Chen', 'Shuhang', ''], ['Madaan', 'Ratnesh', ''], ['Ba', 'Zhongjie', ''], ['Kapoor', 'Ashish', ''], ['Ma', 'Shuang', '']]",0,0,2023-07-16,2,10,1,0,0,0,283ab0486c77c9c9fccb060704fcdc559cae24ce,259937676.0,https://www.semanticscholar.org/paper/283ab0486c77c9c9fccb060704fcdc559cae24ce,arXiv.org,2023.0,65.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2211584665', 'name': 'Yao Wei'}, {'authorId': '120738683', 'name': 'Yanchao Sun'}, {'authorId': '2108815599', 'name': 'Ruijie Zheng'}, {'authorId': '8355354', 'name': 'Sai Vemprala'}, {'authorId': '3444893', 'name': 'Rogerio Bonatti'}, {'authorId': '122665192', 'name': 'Shuhang Chen'}, {'authorId': '31549065', 'name': 'Ratnesh Madaan'}, {'authorId': '36890675', 'name': 'Zhongjie Ba'}, {'authorId': '2189118', 'name': 'Ashish Kapoor'}, {'authorId': '2118867113', 'name': 'Shuang Ma'}]","['Scaled Foundations', 'University of Maryland, College Park', 'Zhejiang University']","['China', 'United States']",2023-07
2307.07924,Chen Qian,"Chen Qian and Xin Cong and Wei Liu and Cheng Yang and Weize Chen and
  Yusheng Su and Yufan Dang and Jiahao Li and Juyuan Xu and Dahai Li and
  Zhiyuan Liu and Maosong Sun",Communicative Agents for Software Development,https://github.com/OpenBMB/ChatDev,,,,cs.SE cs.CL cs.MA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Software engineering is a domain characterized by intricate decision-making processes, often relying on nuanced intuition and consultation. Recent advancements in deep learning have started to revolutionize software engineering practices through elaborate designs implemented at various stages of software development. In this paper, we present an innovative paradigm that leverages large language models (LLMs) throughout the entire software development process, streamlining and unifying key processes through natural language communication, thereby eliminating the need for specialized models at each phase. At the core of this paradigm lies ChatDev, a virtual chat-powered software development company that mirrors the established waterfall model, meticulously dividing the development process into four distinct chronological stages: designing, coding, testing, and documenting. Each stage engages a team of agents, such as programmers, code reviewers, and test engineers, fostering collaborative dialogue and facilitating a seamless workflow. The chat chain acts as a facilitator, breaking down each stage into atomic subtasks. This enables dual roles, allowing for proposing and validating solutions through context-aware communication, leading to efficient resolution of specific subtasks. The instrumental analysis of ChatDev highlights its remarkable efficacy in software generation, enabling the completion of the entire software development process in under seven minutes at a cost of less than one dollar. It not only identifies and alleviates potential vulnerabilities but also rectifies potential hallucinations while maintaining commendable efficiency and cost-effectiveness. The potential of ChatDev unveils fresh possibilities for integrating LLMs into the realm of software development. ","[{'version': 'v1', 'created': 'Sun, 16 Jul 2023 02:11:34 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Jul 2023 09:51:21 GMT'}, {'version': 'v3', 'created': 'Mon, 28 Aug 2023 08:38:38 GMT'}]",2023-08-29,"[['Qian', 'Chen', ''], ['Cong', 'Xin', ''], ['Liu', 'Wei', ''], ['Yang', 'Cheng', ''], ['Chen', 'Weize', ''], ['Su', 'Yusheng', ''], ['Dang', 'Yufan', ''], ['Li', 'Jiahao', ''], ['Xu', 'Juyuan', ''], ['Li', 'Dahai', ''], ['Liu', 'Zhiyuan', ''], ['Sun', 'Maosong', '']]",0,0,2023-07-16,3,12,3,0,0,0,4747e72c5bc706c50e76953188f0144df18992d0,259936967.0,https://www.semanticscholar.org/paper/4747e72c5bc706c50e76953188f0144df18992d0,arXiv.org,2023.0,45.0,35.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '40551843', 'name': 'Chen Qian'}, {'authorId': '2214579778', 'name': 'Xin Cong'}, {'authorId': '3443627', 'name': 'Cheng Yang'}, {'authorId': '2109136284', 'name': 'Weize Chen'}, {'authorId': '48576745', 'name': 'Yusheng Su'}, {'authorId': '2223695921', 'name': 'Juyuan Xu'}, {'authorId': '2141313179', 'name': 'Zhiyuan Liu'}, {'authorId': '1753344', 'name': 'Maosong Sun'}]",['Dalian University of Technology'],['China'],2023-07
2307.07951,Zhenwen Liang,"Zhenwen Liang, Dian Yu, Xiaoman Pan, Wenlin Yao, Qingkai Zeng,
  Xiangliang Zhang, Dong Yu",MinT: Boosting Generalization in Mathematical Reasoning via Multi-View Fine-Tuning,,,,,cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Reasoning in mathematical domains remains a significant challenge for relatively small language models (LMs). Many current methods focus on specializing LMs in mathematical reasoning and rely heavily on knowledge distillation from powerful but inefficient large LMs (LLMs). In this work, we explore a new direction that avoids over-reliance on LLM teachers, introducing a multi-view fine-tuning method that efficiently exploits existing mathematical problem datasets with diverse annotation styles. Our approach uniquely considers the various annotation formats as different ""views"" and leverages them in training the model. By postpending distinct instructions to input questions, models can learn to generate solutions in diverse formats in a flexible manner. Experimental results show that our strategy enables a LLaMA-7B model to outperform prior approaches that utilize knowledge distillation, as well as carefully established baselines. Additionally, the proposed method grants the models promising generalization ability across various views and datasets, and the capability to learn from inaccurate or incomplete noisy data. We hope our multi-view training paradigm could inspire future studies in other machine reasoning domains. ","[{'version': 'v1', 'created': 'Sun, 16 Jul 2023 05:41:53 GMT'}]",2023-07-18,"[['Liang', 'Zhenwen', ''], ['Yu', 'Dian', ''], ['Pan', 'Xiaoman', ''], ['Yao', 'Wenlin', ''], ['Zeng', 'Qingkai', ''], ['Zhang', 'Xiangliang', ''], ['Yu', 'Dong', '']]",0,0,2023-07-16,1,7,2,1,1,0,c1c93b4916aa5b8ce1d99a9e59c700e3d13ada36,259937550.0,https://www.semanticscholar.org/paper/c1c93b4916aa5b8ce1d99a9e59c700e3d13ada36,arXiv.org,2023.0,45.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '151474408', 'name': 'Zhenwen Liang'}, {'authorId': '41190054', 'name': 'Dian Yu'}, {'authorId': '34741133', 'name': 'Xiaoman Pan'}, {'authorId': '2087264100', 'name': 'Wenlin Yao'}, {'authorId': '1694209', 'name': 'Qingkai Zeng'}, {'authorId': '2928371', 'name': 'Xiangliang Zhang'}, {'authorId': '144580027', 'name': 'Dong Yu'}]","['Tencent', 'University of Notre Dame']","['China', 'United States']",2023-07
2307.08041,Yuying Ge,"Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang and Ying Shan",Planting a SEED of Vision in Large Language Model,"Technical Report; Project released at:
  https://github.com/AILab-CVC/SEED",,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  We present SEED, an elaborate image tokenizer that empowers Large Language Models (LLMs) with the emergent ability to SEE and Draw at the same time. Research on image tokenizers has previously reached an impasse, as frameworks employing quantized visual tokens have lost prominence due to subpar performance and convergence in multimodal comprehension (compared to BLIP-2, etc.) or generation (compared to Stable Diffusion, etc.). Despite the limitations, we remain confident in its natural capacity to unify visual and textual representations, facilitating scalable multimodal training with LLM's original recipe. In this study, we identify two crucial principles for the architecture and training of SEED that effectively ease subsequent alignment with LLMs. (1) Image tokens should be independent of 2D physical patch positions and instead be produced with a 1D causal dependency, exhibiting intrinsic interdependence that aligns with the left-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens should capture high-level semantics consistent with the degree of semantic abstraction in words, and be optimized for both discriminativeness and reconstruction during the tokenizer training phase. As a result, the off-the-shelf LLM is able to perform both image-to-text and text-to-image generation by incorporating our SEED through efficient LoRA tuning. Comprehensive multimodal pretraining and instruction tuning, which may yield improved results, are reserved for future investigation. This version of SEED was trained in 5.7 days using only 64 V100 GPUs and 5M publicly available image-text pairs. Our preliminary study emphasizes the great potential of discrete visual tokens in versatile multimodal LLMs and the importance of proper image tokenizers in broader research. ","[{'version': 'v1', 'created': 'Sun, 16 Jul 2023 13:41:39 GMT'}, {'version': 'v2', 'created': 'Sat, 12 Aug 2023 04:42:29 GMT'}]",2023-08-15,"[['Ge', 'Yuying', ''], ['Ge', 'Yixiao', ''], ['Zeng', 'Ziyun', ''], ['Wang', 'Xintao', ''], ['Shan', 'Ying', '']]",0,0,2023-07-16,2,5,1,0,0,0,40298b8d50109c52fc10763eddc64a07cf8acb31,259937351.0,https://www.semanticscholar.org/paper/40298b8d50109c52fc10763eddc64a07cf8acb31,arXiv.org,2023.0,31.0,11.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '51123495', 'name': 'Yuying Ge'}, {'authorId': '152988335', 'name': 'Yixiao Ge'}, {'authorId': '2150468893', 'name': 'Ziyun Zeng'}, {'authorId': '47119707', 'name': 'Xintao Wang'}, {'authorId': '1387190008', 'name': 'Ying Shan'}]",['Tencent'],['China'],2023-07
2307.08072,Peiyu Liu,"Peiyu Liu, Zikang Liu, Ze-Feng Gao, Dawei Gao, Wayne Xin Zhao, Yaliang
  Li, Bolin Ding, Ji-Rong Wen",Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study,"15 pages, 4 figures",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the superior performance, Large Language Models~(LLMs) require significant computational resources for deployment and use. To overcome this issue, quantization methods have been widely applied to reduce the memory footprint of LLMs as well as increasing the inference rate. However, a major challenge is that low-bit quantization methods often lead to performance degradation. It is important to understand how quantization impacts the capacity of LLMs. Different from previous studies focused on overall performance, this work aims to investigate the impact of quantization on \emph{emergent abilities}, which are important characteristics that distinguish LLMs from small language models. Specially, we examine the abilities of in-context learning, chain-of-thought reasoning, and instruction-following in quantized LLMs. Our empirical experiments show that these emergent abilities still exist in 4-bit quantization models, while 2-bit models encounter severe performance degradation on the test of these abilities. To improve the performance of low-bit models, we conduct two special experiments: (1) fine-gained impact analysis that studies which components (or substructures) are more sensitive to quantization, and (2) performance compensation through model fine-tuning. Our work derives a series of important findings to understand the impact of quantization on emergent abilities, and sheds lights on the possibilities of extremely low-bit quantization for LLMs. ","[{'version': 'v1', 'created': 'Sun, 16 Jul 2023 15:11:01 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Jul 2023 04:15:48 GMT'}]",2023-07-27,"[['Liu', 'Peiyu', ''], ['Liu', 'Zikang', ''], ['Gao', 'Ze-Feng', ''], ['Gao', 'Dawei', ''], ['Zhao', 'Wayne Xin', ''], ['Li', 'Yaliang', ''], ['Ding', 'Bolin', ''], ['Wen', 'Ji-Rong', '']]",0,0,2023-07-16,2,8,2,0,0,0,c48fc69b62c749e78928d6a3bae98ffe278f761a,259937594.0,https://www.semanticscholar.org/paper/c48fc69b62c749e78928d6a3bae98ffe278f761a,arXiv.org,2023.0,24.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108129670', 'name': 'Peiyu Liu'}, {'authorId': '2119618242', 'name': 'Zikang Liu'}, {'authorId': '9136116', 'name': 'Ze-Feng Gao'}, {'authorId': '2162036220', 'name': 'Dawei Gao'}, {'authorId': '2542603', 'name': 'Wayne Xin Zhao'}, {'authorId': '2110479359', 'name': 'Yaliang Li'}, {'authorId': '1696332', 'name': 'Bolin Ding'}, {'authorId': '153693432', 'name': 'Ji-rong Wen'}]","['Beijing Key Laboratory of Big', 'Renmin University of China']",['China'],2023-07
2307.08074,Longyue Wang,"Longyue Wang, Zefeng Du, Donghuai Liu, Deng Cai, Dian Yu, Haiyun
  Jiang, Yan Wang, Leyang Cui, Shuming Shi, Zhaopeng Tu",Disco-Bench: A Discourse-Aware Evaluation Benchmark for Language Modelling,Zhaopeng Tu is the corresponding author,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Modeling discourse -- the linguistic phenomena that go beyond individual sentences, is a fundamental yet challenging aspect of natural language processing (NLP). However, existing evaluation benchmarks primarily focus on the evaluation of inter-sentence properties and overlook critical discourse phenomena that cross sentences. To bridge the gap, we propose Disco-Bench, a benchmark that can evaluate intra-sentence discourse properties across a diverse set of NLP tasks, covering understanding, translation, and generation. Disco-Bench consists of 9 document-level testsets in the literature domain, which contain rich discourse phenomena (e.g. cohesion and coherence) in Chinese and/or English. For linguistic analysis, we also design a diagnostic test suite that can examine whether the target models learn discourse knowledge. We totally evaluate 20 general-, in-domain and commercial models based on Transformer, advanced pretraining architectures and large language models (LLMs). Our results show (1) the challenge and necessity of our evaluation benchmark; (2) fine-grained pretraining based on literary document-level training data consistently improves the modeling of discourse information. We will release the datasets, pretrained models, and leaderboard, which we hope can significantly facilitate research in this field: https://github.com/longyuewangdcu/Disco-Bench. ","[{'version': 'v1', 'created': 'Sun, 16 Jul 2023 15:18:25 GMT'}, {'version': 'v2', 'created': 'Sat, 22 Jul 2023 00:11:24 GMT'}]",2023-07-25,"[['Wang', 'Longyue', ''], ['Du', 'Zefeng', ''], ['Liu', 'Donghuai', ''], ['Cai', 'Deng', ''], ['Yu', 'Dian', ''], ['Jiang', 'Haiyun', ''], ['Wang', 'Yan', ''], ['Cui', 'Leyang', ''], ['Shi', 'Shuming', ''], ['Tu', 'Zhaopeng', '']]",0,0,2023-07-16,2,10,2,0,0,0,e1c957e0cb6098304deffb01e4428eb368f8e1ff,259937062.0,https://www.semanticscholar.org/paper/e1c957e0cb6098304deffb01e4428eb368f8e1ff,arXiv.org,2023.0,86.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1800190', 'name': 'Longyue Wang'}, {'authorId': '2112455515', 'name': 'Zefeng Du'}, {'authorId': '2152510456', 'name': 'Donghua Liu'}, {'authorId': '2223597668', 'name': 'Cai Deng'}, {'authorId': '150978762', 'name': 'Dian Yu'}, {'authorId': '48579460', 'name': 'Haiyun Jiang'}, {'authorId': '2159128007', 'name': 'Yan Wang'}, {'authorId': '152496687', 'name': 'Leyang Cui'}, {'authorId': '2072684668', 'name': 'Shuming Shi'}, {'authorId': '2909321', 'name': 'Zhaopeng Tu'}]","['Tencent', 'Viseen Building, Gaoxin 10th South Road, Nanshan District, Shenzhen, China.']",['China'],2023-07
2307.08152,Jingqing Zhang,"Jingqing Zhang, Kai Sun, Akshay Jagadeesh, Mahta Ghahfarokhi, Deepa
  Gupta, Ashok Gupta, Vibhor Gupta, Yike Guo",The Potential and Pitfalls of using a Large Language Model such as ChatGPT or GPT-4 as a Clinical Assistant,"This manuscript is pre-print and in peer review. Supplementary
  materials will be published later",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Recent studies have demonstrated promising performance of ChatGPT and GPT-4 on several medical domain tasks. However, none have assessed its performance using a large-scale real-world electronic health record database, nor have evaluated its utility in providing clinical diagnostic assistance for patients across a full range of disease presentation. We performed two analyses using ChatGPT and GPT-4, one to identify patients with specific medical diagnoses using a real-world large electronic health record database and the other, in providing diagnostic assistance to healthcare workers in the prospective evaluation of hypothetical patients. Our results show that GPT-4 across disease classification tasks with chain of thought and few-shot prompting can achieve performance as high as 96% F1 scores. For patient assessment, GPT-4 can accurately diagnose three out of four times. However, there were mentions of factually incorrect statements, overlooking crucial medical findings, recommendations for unnecessary investigations and overtreatment. These issues coupled with privacy concerns, make these models currently inadequate for real world clinical use. However, limited data and time needed for prompt engineering in comparison to configuration of conventional machine learning workflows highlight their potential for scalability across healthcare applications. ","[{'version': 'v1', 'created': 'Sun, 16 Jul 2023 21:19:47 GMT'}]",2023-07-18,"[['Zhang', 'Jingqing', ''], ['Sun', 'Kai', ''], ['Jagadeesh', 'Akshay', ''], ['Ghahfarokhi', 'Mahta', ''], ['Gupta', 'Deepa', ''], ['Gupta', 'Ashok', ''], ['Gupta', 'Vibhor', ''], ['Guo', 'Yike', '']]",1,1,2023-07-16,1,8,1,2,0,2,b3d6fec3f1a878b0c612f0ffed820b045c2c46d8,259937819.0,https://www.semanticscholar.org/paper/b3d6fec3f1a878b0c612f0ffed820b045c2c46d8,arXiv.org,2023.0,33.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Medicine', 'source': 's2-fos-model'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '47540100', 'name': 'Jingqing Zhang'}, {'authorId': '145932999', 'name': 'K. Sun'}, {'authorId': '66890764', 'name': 'A. Jagadeesh'}, {'authorId': '2223592134', 'name': 'Mahta Ghahfarokhi'}, {'authorId': '143615116', 'name': 'Deepa Gupta'}, {'authorId': '2223694395', 'name': 'Ashok Gupta'}, {'authorId': '2110652718', 'name': 'Vibhor Gupta'}, {'authorId': '2118269437', 'name': 'Yike Guo'}]","['Imperial College London', 'Pangaea Data Limited, UK, USA', 'Hong Kong University of Science and Technology']","['China', 'United States', 'United Kingdom']",2023-07
2307.08321,Cong Jiang,Cong Jiang and Xiaolei Yang,Legal Syllogism Prompting: Teaching Large Language Models for Legal Judgment Prediction,"Nineteenth International Conference on Artificial Intelligence and
  Law (ICAIL 2023)",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Legal syllogism is a form of deductive reasoning commonly used by legal professionals to analyze cases. In this paper, we propose legal syllogism prompting (LoT), a simple prompting method to teach large language models (LLMs) for legal judgment prediction. LoT teaches only that in the legal syllogism the major premise is law, the minor premise is the fact, and the conclusion is judgment. Then the models can produce a syllogism reasoning of the case and give the judgment without any learning, fine-tuning, or examples. On CAIL2018, a Chinese criminal case dataset, we performed zero-shot judgment prediction experiments with GPT-3 models. Our results show that LLMs with LoT achieve better performance than the baseline and chain of thought prompting, the state-of-art prompting method on diverse reasoning tasks. LoT enables the model to concentrate on the key information relevant to the judgment and to correctly understand the legal meaning of acts, as compared to other methods. Our method enables LLMs to predict judgment along with law articles and justification, which significantly enhances the explainability of models. ","[{'version': 'v1', 'created': 'Mon, 17 Jul 2023 08:38:46 GMT'}]",2023-07-18,"[['Jiang', 'Cong', ''], ['Yang', 'Xiaolei', '']]",0,1,2023-07-17,1,2,1,1,0,1,1640bda76a52f8b29e012b4e98b785882fb011c2,259937541.0,https://www.semanticscholar.org/paper/1640bda76a52f8b29e012b4e98b785882fb011c2,International Conference on Artificial Intelligence and Law,2023.0,27.0,3.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Law', 'source': 's2-fos-model'}]","[{'authorId': '2189380054', 'name': 'Cong Jiang'}, {'authorId': '2223684923', 'name': 'Xiaolei Yang'}]",['Peking University'],['China'],2023-07
2307.08487,Huachuan Qiu,"Huachuan Qiu, Shuai Zhang, Anqi Li, Hongliang He, Zhenzhong Lan",Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models,"Code and data are available at
  https://github.com/qiuhuachuan/latent-jailbreak",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Considerable research efforts have been devoted to ensuring that large language models (LLMs) align with human values and generate safe text. However, an excessive focus on sensitivity to certain topics can compromise the model's robustness in following instructions, thereby impacting its overall performance in completing tasks. Previous benchmarks for jailbreaking LLMs have primarily focused on evaluating the safety of the models without considering their robustness. In this paper, we propose a benchmark that assesses both the safety and robustness of LLMs, emphasizing the need for a balanced approach. To comprehensively study text safety and output robustness, we introduce a latent jailbreak prompt dataset, each involving malicious instruction embedding. Specifically, we instruct the model to complete a regular task, such as translation, with the text to be translated containing malicious instructions. To further analyze safety and robustness, we design a hierarchical annotation framework. We present a systematic analysis of the safety and robustness of LLMs regarding the position of explicit normal instructions, word replacements (verbs in explicit normal instructions, target groups in malicious instructions, cue words for explicit normal instructions), and instruction replacements (different explicit normal instructions). Our results demonstrate that current LLMs not only prioritize certain instruction verbs but also exhibit varying jailbreak rates for different instruction verbs in explicit normal instructions. Code and data are available at https://github.com/qiuhuachuan/latent-jailbreak. ","[{'version': 'v1', 'created': 'Mon, 17 Jul 2023 13:49:52 GMT'}, {'version': 'v2', 'created': 'Fri, 18 Aug 2023 07:52:53 GMT'}, {'version': 'v3', 'created': 'Mon, 28 Aug 2023 08:35:28 GMT'}]",2023-08-29,"[['Qiu', 'Huachuan', ''], ['Zhang', 'Shuai', ''], ['Li', 'Anqi', ''], ['He', 'Hongliang', ''], ['Lan', 'Zhenzhong', '']]",0,0,2023-07-17,3,5,1,0,0,0,ace98e1e58bcc364afbb2feff6d136232f5f47da,259937347.0,https://www.semanticscholar.org/paper/ace98e1e58bcc364afbb2feff6d136232f5f47da,arXiv.org,2023.0,29.0,9.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2106604080', 'name': 'Huachuan Qiu'}, {'authorId': '143893983', 'name': 'Shuai Zhang'}, {'authorId': '2141519878', 'name': 'Anqi Li'}, {'authorId': '144987681', 'name': 'Hongliang He'}, {'authorId': '2362534', 'name': 'Zhenzhong Lan'}]","['Westlake University', 'Zhejiang University']",['China'],2023-07
2307.08576,Sijin Cai,"Sijin Cai, Nanfeng Zhang, Jiaying Zhu, Yanjie Liu, Yongjin Zhou",A Study on the Performance of Generative Pre-trained Transformer (GPT) in Simulating Depressed Individuals on the Standardized Depressive Symptom Scale,,,,,q-bio.NC cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Background: Depression is a common mental disorder with societal and economic burden. Current diagnosis relies on self-reports and assessment scales, which have reliability issues. Objective approaches are needed for diagnosing depression. Objective: Evaluate the potential of GPT technology in diagnosing depression. Assess its ability to simulate individuals with depression and investigate the influence of depression scales. Methods: Three depression-related assessment tools (HAMD-17, SDS, GDS-15) were used. Two experiments simulated GPT responses to normal individuals and individuals with depression. Compare GPT's responses with expected results, assess its understanding of depressive symptoms, and performance differences under different conditions. Results: GPT's performance in depression assessment was evaluated. It aligned with scoring criteria for both individuals with depression and normal individuals. Some performance differences were observed based on depression severity. GPT performed better on scales with higher sensitivity. Conclusion: GPT accurately simulates individuals with depression and normal individuals during depression-related assessments. Deviations occur when simulating different degrees of depression, limiting understanding of mild and moderate cases. GPT performs better on scales with higher sensitivity, indicating potential for developing more effective depression scales. GPT has important potential in depression assessment, supporting clinicians and patients. ","[{'version': 'v1', 'created': 'Mon, 17 Jul 2023 15:44:13 GMT'}]",2023-07-18,"[['Cai', 'Sijin', ''], ['Zhang', 'Nanfeng', ''], ['Zhu', 'Jiaying', ''], ['Liu', 'Yanjie', ''], ['Zhou', 'Yongjin', '']]",0,1,2023-07-17,1,5,2,0,0,0,2bb69e71a4cb641ca9426c6cda30c9f7ded8edac,259937110.0,https://www.semanticscholar.org/paper/2bb69e71a4cb641ca9426c6cda30c9f7ded8edac,arXiv.org,2023.0,17.0,0.0,0.0,True,"['Computer Science', 'Biology']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Biology', 'source': 'external'}, {'category': 'Psychology', 'source': 's2-fos-model'}, {'category': 'Medicine', 'source': 's2-fos-model'}]","[{'authorId': '92980448', 'name': 'Sijin Cai'}, {'authorId': '2223713591', 'name': 'Nanfeng Zhang'}, {'authorId': '2223692073', 'name': 'Jiaying Zhu'}, {'authorId': '2155419251', 'name': 'Yanjie Liu'}, {'authorId': '152641149', 'name': 'Yongjin Zhou'}]",['Shenzhen University'],['China'],2023-07
2307.08621,Li Dong,"Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue,
  Jianyong Wang, Furu Wei",Retentive Network: A Successor to Transformer for Large Language Models,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet. ","[{'version': 'v1', 'created': 'Mon, 17 Jul 2023 16:40:01 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Jul 2023 05:56:42 GMT'}, {'version': 'v3', 'created': 'Tue, 25 Jul 2023 06:47:43 GMT'}, {'version': 'v4', 'created': 'Wed, 9 Aug 2023 08:53:08 GMT'}]",2023-08-10,"[['Sun', 'Yutao', ''], ['Dong', 'Li', ''], ['Huang', 'Shaohan', ''], ['Ma', 'Shuming', ''], ['Xia', 'Yuqing', ''], ['Xue', 'Jilong', ''], ['Wang', 'Jianyong', ''], ['Wei', 'Furu', '']]",0,0,2023-07-17,4,8,2,0,0,0,240103933ffe3dac2179cc160a2bd91299357a53,259937453.0,https://www.semanticscholar.org/paper/240103933ffe3dac2179cc160a2bd91299357a53,arXiv.org,2023.0,40.0,24.0,7.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108540694', 'name': 'Yutao Sun'}, {'authorId': '145307652', 'name': 'Li Dong'}, {'authorId': '3110003', 'name': 'Shaohan Huang'}, {'authorId': '2118866998', 'name': 'Shuming Ma'}, {'authorId': '4866834', 'name': 'Yuqing Xia'}, {'authorId': '2870618', 'name': 'Jilong Xue'}, {'authorId': '2115642141', 'name': 'Jianyong Wang'}, {'authorId': '49807919', 'name': 'Furu Wei'}]",['Tsinghua University'],['China'],2023-07
2307.08925,Xiaohua Feng,"Chaochao Chen, Xiaohua Feng, Jun Zhou, Jianwei Yin, Xiaolin Zheng",Federated Large Language Model: A Position Paper,"11 pages, 4 figures",,,,cs.LG cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large scale language models (LLM) have received significant attention and found diverse applications across various domains, but their development encounters challenges in real-world scenarios. These challenges arise due to the scarcity of public domain data availability and the need to maintain privacy with respect to private domain data. To address these issues, federated learning (FL) has emerged as a promising technology that enables collaborative training of shared models while preserving decentralized data. We propose the concept of federated LLM, which comprises three key components, i.e., federated LLM pre-training, federated LLM fine-tuning, and federated LLM prompt engineering. For each component, we discuss its advantage over traditional LLM training methods and propose specific engineering strategies for implementation. Furthermore, we explore the novel challenges introduced by the integration of FL and LLM. We analyze existing solutions and identify potential obstacles faced by these solutions within the context of federated LLM. ","[{'version': 'v1', 'created': 'Tue, 18 Jul 2023 02:09:14 GMT'}]",2023-07-19,"[['Chen', 'Chaochao', ''], ['Feng', 'Xiaohua', ''], ['Zhou', 'Jun', ''], ['Yin', 'Jianwei', ''], ['Zheng', 'Xiaolin', '']]",0,0,2023-07-18,1,5,3,0,0,0,7aad760762c4a10dfbc2d3391eb8bdb28c80b236,259950775.0,https://www.semanticscholar.org/paper/7aad760762c4a10dfbc2d3391eb8bdb28c80b236,arXiv.org,2023.0,51.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1694815', 'name': 'Chaochao Chen'}, {'authorId': '47460280', 'name': 'Xiaohua Feng'}, {'authorId': '2151548940', 'name': 'Jun Zhou'}, {'authorId': '2153419047', 'name': 'Jianwei Yin'}, {'authorId': '2230959408', 'name': 'Xiaolin Zheng'}]",['Zhejiang University'],['China'],2023-07
2307.09007,Yinghui Li,"Yinghui Li, Haojing Huang, Shirong Ma, Yong Jiang, Yangning Li, Feng
  Zhou, Hai-Tao Zheng, Qingyu Zhou",On the (In)Effectiveness of Large Language Models for Chinese Text Correction,Work in progress!,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, the development and progress of Large Language Models (LLMs) have amazed the entire Artificial Intelligence community. As an outstanding representative of LLMs and the foundation model that set off this wave of research on LLMs, ChatGPT has attracted more and more researchers to study its capabilities and performance on various downstream Natural Language Processing (NLP) tasks. While marveling at ChatGPT's incredible performance on kinds of tasks, we notice that ChatGPT also has excellent multilingual processing capabilities, such as Chinese. To explore the Chinese processing ability of ChatGPT, we focus on Chinese Text Correction, a fundamental and challenging Chinese NLP task. Specifically, we evaluate ChatGPT on the Chinese Grammatical Error Correction (CGEC) and Chinese Spelling Check (CSC) tasks, which are two main Chinese Text Correction scenarios. From extensive analyses and comparisons with previous state-of-the-art fine-tuned models, we empirically find that the ChatGPT currently has both amazing performance and unsatisfactory behavior for Chinese Text Correction. We believe our findings will promote the landing and application of LLMs in the Chinese NLP community. ","[{'version': 'v1', 'created': 'Tue, 18 Jul 2023 06:48:52 GMT'}]",2023-07-19,"[['Li', 'Yinghui', ''], ['Huang', 'Haojing', ''], ['Ma', 'Shirong', ''], ['Jiang', 'Yong', ''], ['Li', 'Yangning', ''], ['Zhou', 'Feng', ''], ['Zheng', 'Hai-Tao', ''], ['Zhou', 'Qingyu', '']]",1,1,2023-07-18,1,8,1,1,0,1,baf0a52f7a3c4539c08eceb9e31b77eb397159df,259951327.0,https://www.semanticscholar.org/paper/baf0a52f7a3c4539c08eceb9e31b77eb397159df,arXiv.org,2023.0,49.0,11.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2110503552', 'name': 'Yinghui Li'}, {'authorId': '3042049', 'name': 'Haojing Huang'}, {'authorId': '2118867306', 'name': 'Shirong Ma'}, {'authorId': '2118476056', 'name': 'Yong Jiang'}, {'authorId': '98177814', 'name': 'Y. Li'}, {'authorId': '40242610', 'name': 'F. Zhou'}, {'authorId': '16215052', 'name': 'Haitao Zheng'}, {'authorId': '50656875', 'name': 'Qingyu Zhou'}]","['Alibaba', 'Tsinghua University', 'OPPO']",['China'],2023-07
2307.09025,Pan Zhang,"Hanyan Cao, Feng Pan, Yijia Wang, Pan Zhang",qecGPT: decoding Quantum Error-correcting Codes with Generative Pre-trained Transformers,Comments are welcome,,,,quant-ph cond-mat.stat-mech cs.LG stat.ML,http://creativecommons.org/licenses/by/4.0/,"  We propose a general framework for decoding quantum error-correcting codes with generative modeling. The model utilizes autoregressive neural networks, specifically Transformers, to learn the joint probability of logical operators and syndromes. This training is in an unsupervised way, without the need for labeled training data, and is thus referred to as pre-training. After the pre-training, the model can efficiently compute the likelihood of logical operators for any given syndrome, using maximum likelihood decoding. It can directly generate the most-likely logical operators with computational complexity $\mathcal O(2k)$ in the number of logical qubits $k$, which is significantly better than the conventional maximum likelihood decoding algorithms that require $\mathcal O(4^k)$ computation. Based on the pre-trained model, we further propose refinement to achieve more accurately the likelihood of logical operators for a given syndrome by directly sampling the stabilizer operators. We perform numerical experiments on stabilizer codes with small code distances, using both depolarizing error models and error models with correlated noise. The results show that our approach provides significantly better decoding accuracy than the minimum weight perfect matching and belief-propagation-based algorithms. Our framework is general and can be applied to any error model and quantum codes with different topologies such as surface codes and quantum LDPC codes. Furthermore, it leverages the parallelization capabilities of GPUs, enabling simultaneous decoding of a large number of syndromes. Our approach sheds light on the efficient and accurate decoding of quantum error-correcting codes using generative artificial intelligence and modern computational power. ","[{'version': 'v1', 'created': 'Tue, 18 Jul 2023 07:34:02 GMT'}]",2023-07-19,"[['Cao', 'Hanyan', ''], ['Pan', 'Feng', ''], ['Wang', 'Yijia', ''], ['Zhang', 'Pan', '']]",0,1,2023-07-18,1,4,4,0,0,0,71ee84cdde7ff06b3d796bb940be31f59d6992a0,259951030.0,https://www.semanticscholar.org/paper/71ee84cdde7ff06b3d796bb940be31f59d6992a0,arXiv.org,2023.0,29.0,1.0,0.0,True,"['Physics', 'Computer Science', 'Mathematics']","[{'category': 'Physics', 'source': 'external'}, {'category': 'Computer Science', 'source': 'external'}, {'category': 'Mathematics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2114856926', 'name': 'Han-Yu Cao'}, {'authorId': '2068792551', 'name': 'Feng Pan'}, {'authorId': '2187046250', 'name': 'Yijia Wang'}, {'authorId': '2143362179', 'name': 'Pan Zhang'}]","['International Centre for Theoretical Physics Asia-Pacific', 'University of Chinese Academy of Sciences', 'Chinese Academy of Sciences']",['China'],2023-07
2307.09042,Xuena Wang,"Xuena Wang, Xueting Li, Zi Yin, Yue Wu and Liu Jia",Emotional Intelligence of Large Language Models,"36 pages, 5 figures",,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have demonstrated remarkable abilities across numerous disciplines, primarily assessed through tasks in language generation, knowledge utilization, and complex reasoning. However, their alignment with human emotions and values, which is critical for real-world applications, has not been systematically evaluated. Here, we assessed LLMs' Emotional Intelligence (EI), encompassing emotion recognition, interpretation, and understanding, which is necessary for effective communication and social interactions. Specifically, we first developed a novel psychometric assessment focusing on Emotion Understanding (EU), a core component of EI, suitable for both humans and LLMs. This test requires evaluating complex emotions (e.g., surprised, joyful, puzzled, proud) in realistic scenarios (e.g., despite feeling underperformed, John surprisingly achieved a top score). With a reference frame constructed from over 500 adults, we tested a variety of mainstream LLMs. Most achieved above-average EQ scores, with GPT-4 exceeding 89% of human participants with an EQ of 117. Interestingly, a multivariate pattern analysis revealed that some LLMs apparently did not reply on the human-like mechanism to achieve human-level performance, as their representational patterns were qualitatively distinct from humans. In addition, we discussed the impact of factors such as model size, training method, and architecture on LLMs' EQ. In summary, our study presents one of the first psychometric evaluations of the human-like characteristics of LLMs, which may shed light on the future development of LLMs aiming for both high intellectual and emotional intelligence. Project website: https://emotional-intelligence.github.io/ ","[{'version': 'v1', 'created': 'Tue, 18 Jul 2023 07:49:38 GMT'}, {'version': 'v2', 'created': 'Fri, 28 Jul 2023 06:29:07 GMT'}]",2023-07-31,"[['Wang', 'Xuena', ''], ['Li', 'Xueting', ''], ['Yin', 'Zi', ''], ['Wu', 'Yue', ''], ['Jia', 'Liu', '']]",0,1,2023-07-18,2,5,1,1,0,1,bff499d51b002fd0b1aa05ba151a4a515e5bf36f,259951557.0,https://www.semanticscholar.org/paper/bff499d51b002fd0b1aa05ba151a4a515e5bf36f,Journal of Pacific Rim Psychology,2023.0,43.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Psychology', 'source': 's2-fos-model'}]","[{'authorId': '48630984', 'name': 'Xuena Wang'}, {'authorId': '2816471', 'name': 'Xueting Li'}, {'authorId': '2069532941', 'name': 'Zi Yin'}, {'authorId': '46220633', 'name': 'Yue Wu'}, {'authorId': '2223759460', 'name': 'Liu Jia Department of PsychologyTsinghua Laboratory of Brain'}, {'authorId': '2223775536', 'name': 'Intelligence'}, {'authorId': '102392556', 'name': 'Tsinghua University'}, {'authorId': '116398212', 'name': 'Departmentof Psychology'}, {'authorId': '2223749060', 'name': 'Renmin University'}]","['Tsinghua University', 'Renmin University of China']",['China'],2023-07
2307.09163,Yun Peng,"Yun Peng, Chaozheng Wang, Wenxuan Wang, Cuiyun Gao, Michael R. Lyu",Generative Type Inference for Python,This paper has been accepted by ASE'23,,,,cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Python is a popular dynamic programming language, evidenced by its ranking as the second most commonly used language on GitHub. However, its dynamic type system can lead to potential type errors, leading researchers to explore automatic type inference approaches for Python programs. The rule-based type inference approaches can ensure the accuracy of predicted variable types, but they suffer from low coverage problems. Supervised type inference approaches, while feature-agnostic, require large, high-quality annotated datasets and are limited to pre-defined types. As zero-shot approaches, the cloze-style approaches reformulate the type inference problem into a fill-in-the-blank problem. However, their performance is limited.   This paper introduces TypeGen, a few-shot generative type inference approach that incorporates static domain knowledge from static analysis. TypeGen creates chain-of-thought (COT) prompts by translating the type inference steps of static analysis into prompts based on the type dependency graphs (TDGs), enabling language models to learn from how static analysis infers types. By combining COT prompts with code slices and type hints, TypeGen constructs example prompts from human annotations. TypeGen only requires very few annotated examples to teach language models to generate similar COT prompts via in-context learning. Moreover, TypeGen enhances the interpretability of results through the use of the input-explanation-output strategy. Experiments show that TypeGen outperforms the best baseline Type4Py by 10.0% for argument type prediction and 22.5% in return value type prediction in terms of top-1 Exact Match by using only five examples. Furthermore, TypeGen achieves substantial improvements of 27% to 84% compared to the zero-shot performance of large language models with parameter sizes ranging from 1.3B to 175B in terms of top-1 Exact Match. ","[{'version': 'v1', 'created': 'Tue, 18 Jul 2023 11:40:31 GMT'}]",2023-07-19,"[['Peng', 'Yun', ''], ['Wang', 'Chaozheng', ''], ['Wang', 'Wenxuan', ''], ['Gao', 'Cuiyun', ''], ['Lyu', 'Michael R.', '']]",0,0,2023-07-18,1,5,1,0,0,0,ddf2e20427e24b422cc11f58a27458b75e1d3cca,259951409.0,https://www.semanticscholar.org/paper/ddf2e20427e24b422cc11f58a27458b75e1d3cca,International Conference on Automated Software Engineering,2023.0,46.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2111035155', 'name': 'Yun Peng'}, {'authorId': '2135764153', 'name': 'Chaozheng Wang'}, {'authorId': '2144328160', 'name': 'Wenxuan Wang'}, {'authorId': '2337550', 'name': 'Cuiyun Gao'}, {'authorId': '2146840128', 'name': 'Michael R. Lyu'}]","['Harbin Institute of Technology', 'Chinese University of Hong Kong']",['China'],2023-07
2307.09474,En Yu,"Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei, Hongyu Zhou,
  Jianjian Sun, Yuang Peng, Runpei Dong, Chunrui Han, Xiangyu Zhang",ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning,"15 pages, 8 figures",,,,cs.CL cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs). However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency. In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region. This enables MLLMs to focus on the region of interest and achieve finer-grained interaction. Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience. We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating. Furthermore, we design a series of evaluation tasks to assess the effectiveness of region recognition and interaction. Experimental results showcase ChatSpot's promising performance. ","[{'version': 'v1', 'created': 'Tue, 18 Jul 2023 17:56:06 GMT'}]",2023-07-19,"[['Zhao', 'Liang', ''], ['Yu', 'En', ''], ['Ge', 'Zheng', ''], ['Yang', 'Jinrong', ''], ['Wei', 'Haoran', ''], ['Zhou', 'Hongyu', ''], ['Sun', 'Jianjian', ''], ['Peng', 'Yuang', ''], ['Dong', 'Runpei', ''], ['Han', 'Chunrui', ''], ['Zhang', 'Xiangyu', '']]",0,1,2023-07-18,1,11,2,1,0,1,2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f,259951197.0,https://www.semanticscholar.org/paper/2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f,arXiv.org,2023.0,39.0,6.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '48096671', 'name': 'Liang Zhao'}, {'authorId': '2054414548', 'name': 'En Yu'}, {'authorId': '151479828', 'name': 'Zheng Ge'}, {'authorId': '2161319190', 'name': 'Jinrong Yang'}, {'authorId': '134085586', 'name': 'Hao-Ran Wei'}, {'authorId': '2157473950', 'name': 'Hongyu Zhou'}, {'authorId': '26913717', 'name': 'JianYuan Sun'}, {'authorId': '2211415443', 'name': 'Yuang Peng'}, {'authorId': '2056965063', 'name': 'Runpei Dong'}, {'authorId': '2118643247', 'name': 'Chunrui Han'}, {'authorId': '2185865433', 'name': 'Xiangyu Zhang'}]","['MEGVII Technology,', 'Huazhong University of Science and Technology', 'Tsinghua University', ""Xi'an Jiaotong University""]",['China'],2023-07
2307.09705,Guohai Xu,"Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou,
  Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, Ji Zhang, Chao Peng, Fei Huang,
  Jingren Zhou",CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility,Working in Process,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the rapid evolution of large language models (LLMs), there is a growing concern that they may pose risks or have negative social impacts. Therefore, evaluation of human values alignment is becoming increasingly important. Previous work mainly focuses on assessing the performance of LLMs on certain knowledge and reasoning abilities, while neglecting the alignment to human values, especially in a Chinese context. In this paper, we present CValues, the first Chinese human values evaluation benchmark to measure the alignment ability of LLMs in terms of both safety and responsibility criteria. As a result, we have manually collected adversarial safety prompts across 10 scenarios and induced responsibility prompts from 8 domains by professional experts. To provide a comprehensive values evaluation of Chinese LLMs, we not only conduct human evaluation for reliable comparison, but also construct multi-choice prompts for automatic evaluation. Our findings suggest that while most Chinese LLMs perform well in terms of safety, there is considerable room for improvement in terms of responsibility. Moreover, both the automatic and human evaluation are important for assessing the human values alignment in different aspects. The benchmark and code is available on ModelScope and Github. ","[{'version': 'v1', 'created': 'Wed, 19 Jul 2023 01:22:40 GMT'}]",2023-07-20,"[['Xu', 'Guohai', ''], ['Liu', 'Jiayi', ''], ['Yan', 'Ming', ''], ['Xu', 'Haotian', ''], ['Si', 'Jinghui', ''], ['Zhou', 'Zhuoran', ''], ['Yi', 'Peng', ''], ['Gao', 'Xing', ''], ['Sang', 'Jitao', ''], ['Zhang', 'Rong', ''], ['Zhang', 'Ji', ''], ['Peng', 'Chao', ''], ['Huang', 'Fei', ''], ['Zhou', 'Jingren', '']]",0,0,2023-07-19,1,14,1,0,0,0,bc0549a5f07474c18987c219ecf367fb73a1b79c,259983087.0,https://www.semanticscholar.org/paper/bc0549a5f07474c18987c219ecf367fb73a1b79c,arXiv.org,2023.0,37.0,12.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2115723816', 'name': 'Guohai Xu'}, {'authorId': '2108357836', 'name': 'Jiayi Liu'}, {'authorId': '2114009661', 'name': 'Mingshi Yan'}, {'authorId': '2141375220', 'name': 'Haotian Xu'}, {'authorId': '2057225626', 'name': 'Jinghui Si'}, {'authorId': '2142552684', 'name': 'Zhuoran Zhou'}, {'authorId': '2223953100', 'name': 'Peng Yi'}, {'authorId': '2185575749', 'name': 'Xing Gao'}, {'authorId': '1798398', 'name': 'J. Sang'}, {'authorId': '2223973456', 'name': 'Rong Zhang'}, {'authorId': '2116921824', 'name': 'Ji Zhang'}, {'authorId': '2224445396', 'name': 'Chao Peng'}, {'authorId': '2194508991', 'name': 'Feiyan Huang'}, {'authorId': '1709595', 'name': 'Jingren Zhou'}]","['Alibaba', 'Beijing Jiaotong University']",['China'],2023-07
2307.09751,Peng Zhang,"Qingyao Ai, Ting Bai, Zhao Cao, Yi Chang, Jiawei Chen, Zhumin Chen,
  Zhiyong Cheng, Shoubin Dong, Zhicheng Dou, Fuli Feng, Shen Gao, Jiafeng Guo,
  Xiangnan He, Yanyan Lan, Chenliang Li, Yiqun Liu, Ziyu Lyu, Weizhi Ma, Jun
  Ma, Zhaochun Ren, Pengjie Ren, Zhiqiang Wang, Mingwen Wang, Ji-Rong Wen, Le
  Wu, Xin Xin, Jun Xu, Dawei Yin, Peng Zhang, Fan Zhang, Weinan Zhang, Min
  Zhang and Xiaofei Zhu",Information Retrieval Meets Large Language Models: A Strategic Report from Chinese IR Community,17 pages,,,,cs.IR cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The research field of Information Retrieval (IR) has evolved significantly, expanding beyond traditional search to meet diverse user information needs. Recently, Large Language Models (LLMs) have demonstrated exceptional capabilities in text understanding, generation, and knowledge inference, opening up exciting avenues for IR research. LLMs not only facilitate generative retrieval but also offer improved solutions for user understanding, model evaluation, and user-system interactions. More importantly, the synergistic relationship among IR models, LLMs, and humans forms a new technical paradigm that is more powerful for information seeking. IR models provide real-time and relevant information, LLMs contribute internal knowledge, and humans play a central role of demanders and evaluators to the reliability of information services. Nevertheless, significant challenges exist, including computational costs, credibility concerns, domain-specific limitations, and ethical considerations. To thoroughly discuss the transformative impact of LLMs on IR research, the Chinese IR community conducted a strategic workshop in April 2023, yielding valuable insights. This paper provides a summary of the workshop's outcomes, including the rethinking of IR's core values, the mutual enhancement of LLMs and IR, the proposal of a novel IR technical paradigm, and open challenges. ","[{'version': 'v1', 'created': 'Wed, 19 Jul 2023 05:23:43 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Jul 2023 02:32:08 GMT'}]",2023-07-28,"[['Ai', 'Qingyao', ''], ['Bai', 'Ting', ''], ['Cao', 'Zhao', ''], ['Chang', 'Yi', ''], ['Chen', 'Jiawei', ''], ['Chen', 'Zhumin', ''], ['Cheng', 'Zhiyong', ''], ['Dong', 'Shoubin', ''], ['Dou', 'Zhicheng', ''], ['Feng', 'Fuli', ''], ['Gao', 'Shen', ''], ['Guo', 'Jiafeng', ''], ['He', 'Xiangnan', ''], ['Lan', 'Yanyan', ''], ['Li', 'Chenliang', ''], ['Liu', 'Yiqun', ''], ['Lyu', 'Ziyu', ''], ['Ma', 'Weizhi', ''], ['Ma', 'Jun', ''], ['Ren', 'Zhaochun', ''], ['Ren', 'Pengjie', ''], ['Wang', 'Zhiqiang', ''], ['Wang', 'Mingwen', ''], ['Wen', 'Ji-Rong', ''], ['Wu', 'Le', ''], ['Xin', 'Xin', ''], ['Xu', 'Jun', ''], ['Yin', 'Dawei', ''], ['Zhang', 'Peng', ''], ['Zhang', 'Fan', ''], ['Zhang', 'Weinan', ''], ['Zhang', 'Min', ''], ['Zhu', 'Xiaofei', '']]",0,0,2023-07-19,2,33,2,0,0,0,4f03688df8997e1f4b5aeaf81f44242390ea5c27,259982533.0,https://www.semanticscholar.org/paper/4f03688df8997e1f4b5aeaf81f44242390ea5c27,AI Open,2023.0,118.0,8.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '144922928', 'name': 'Qingyao Ai'}, {'authorId': '143831387', 'name': 'Ting Bai'}, {'authorId': '2106400572', 'name': 'Zhao Cao'}, {'authorId': '2131636065', 'name': 'Yi Chang'}, {'authorId': '1452347263', 'name': 'Jiawei Chen'}, {'authorId': '1721165', 'name': 'Zhumin Chen'}, {'authorId': '13167100', 'name': 'Zhiyong Cheng'}, {'authorId': '1752810', 'name': 'Shoubin Dong'}, {'authorId': '1897235', 'name': 'Zhicheng Dou'}, {'authorId': '2163400298', 'name': 'Fuli Feng'}, {'authorId': '2112313769', 'name': 'Shengling Gao'}, {'authorId': '1777025', 'name': 'J. Guo'}, {'authorId': '7792071', 'name': 'Xiangnan He'}, {'authorId': '37510256', 'name': 'Yanyan Lan'}, {'authorId': '2829009', 'name': 'Chenliang Li'}, {'authorId': '1783406', 'name': 'Yiqun Liu'}, {'authorId': '1920802076', 'name': 'Ziyu Lyu'}, {'authorId': '2903964', 'name': 'Weizhi Ma'}, {'authorId': '2152611495', 'name': 'Jun Ma'}, {'authorId': '2780667', 'name': 'Z. Ren'}, {'authorId': '1749477', 'name': 'Pengjie Ren'}, {'authorId': '2108331735', 'name': 'Zhiqiang Wang'}, {'authorId': '143894230', 'name': 'Min Wang'}, {'authorId': '2113341834', 'name': 'Jirong Wen'}, {'authorId': '1734221', 'name': 'Lei Wu'}, {'authorId': '2113821128', 'name': 'Xin Xin'}, {'authorId': '2150636233', 'name': 'Jun Xu'}, {'authorId': '2136400100', 'name': 'Dawei Yin'}, {'authorId': '2151333116', 'name': 'Peng Zhang'}, {'authorId': '50212920', 'name': 'Fan Zhang'}, {'authorId': '2157434730', 'name': 'Wei-na Zhang'}, {'authorId': '39767557', 'name': 'M. Zhang'}, {'authorId': '2125161115', 'name': 'Xiaofei Zhu'}]","['Beijing University of Posts and Telecommunications', 'South China University of Technology', 'Jiangxi Normal University', 'Shandong University', 'Huawei Technologies (China)', 'Baidu', 'Wuhan University', 'University of Science and Technology of China', 'Chongqing University of Technology', 'Shenzhen Institutes of Advanced Technology', 'Tsinghua University', 'Shandong Artificial Intelligence Institute,', 'Jilin University', 'Zhejiang University', 'Shanghai Jiao Tong University', 'Hefei University of Technology', 'Institute of Computing Technology', 'Tianjin University', 'Renmin University of China']",['China'],2023-07
2307.09765,Bowen Xu,"Bowen Xu, Thanh-Dat Nguyen, Thanh Le-Cong, Thong Hoang, Jiakun Liu,
  Kisub Kim, Chen Gong, Changan Niu, Chenyu Wang, Bach Le, David Lo",Are We Ready to Embrace Generative AI for Software Q&A?,"Accepted by the New Ideas and Emerging Results (NIER) track at The
  IEEE/ACM Automated Software Engineering (ASE) Conference",,,,cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Stack Overflow, the world's largest software Q&A (SQA) website, is facing a significant traffic drop due to the emergence of generative AI techniques. ChatGPT is banned by Stack Overflow after only 6 days from its release. The main reason provided by the official Stack Overflow is that the answers generated by ChatGPT are of low quality. To verify this, we conduct a comparative evaluation of human-written and ChatGPT-generated answers. Our methodology employs both automatic comparison and a manual study. Our results suggest that human-written and ChatGPT-generated answers are semantically similar, however, human-written answers outperform ChatGPT-generated ones consistently across multiple aspects, specifically by 10% on the overall score. We release the data, analysis scripts, and detailed results at https://anonymous.4open.science/r/GAI4SQA-FD5C. ","[{'version': 'v1', 'created': 'Wed, 19 Jul 2023 05:54:43 GMT'}, {'version': 'v2', 'created': 'Sat, 12 Aug 2023 13:10:02 GMT'}]",2023-08-15,"[['Xu', 'Bowen', ''], ['Nguyen', 'Thanh-Dat', ''], ['Le-Cong', 'Thanh', ''], ['Hoang', 'Thong', ''], ['Liu', 'Jiakun', ''], ['Kim', 'Kisub', ''], ['Gong', 'Chen', ''], ['Niu', 'Changan', ''], ['Wang', 'Chenyu', ''], ['Le', 'Bach', ''], ['Lo', 'David', '']]",1,1,2023-07-19,2,11,1,1,0,1,43633a1a90a3991ffcf108859bb25720330c145f,259982596.0,https://www.semanticscholar.org/paper/43633a1a90a3991ffcf108859bb25720330c145f,International Conference on Automated Software Engineering,2023.0,19.0,2.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2203459', 'name': 'Bowen Xu'}, {'authorId': '2174165242', 'name': 'Thanh-Dat Nguyen'}, {'authorId': '1811413300', 'name': 'Thanh Le-Cong'}, {'authorId': '9719656', 'name': 'Thong Hoang'}, {'authorId': '120809900', 'name': 'Jiakun Liu'}, {'authorId': '35276441', 'name': 'Kisub Kim'}, {'authorId': '2223951228', 'name': 'Chen Gong'}, {'authorId': '2069657833', 'name': 'Changan Niu'}, {'authorId': '2211991264', 'name': 'Chenyu Wang'}, {'authorId': '2248993736', 'name': 'Xuan-Bach Dinh Le'}, {'authorId': '2150912791', 'name': 'David Lo'}]","['Nanjing University', 'Data61', 'University of Virginia', 'Singapore Management University', 'University of Melbourne', 'North Carolina State University']","['China', 'United States', 'Singapore', 'Australia']",2023-07
2307.10337,Siyu Li,"Siyu Li, Jin Yang, Kui Zhao",Are you in a Masquerade? Exploring the Behavior and Impact of Large Language Model Driven Social Bots in Online Social Networks,"18 pages, 7 figures",,,,cs.SI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As the capabilities of Large Language Models (LLMs) emerge, they not only assist in accomplishing traditional tasks within more efficient paradigms but also stimulate the evolution of social bots. Researchers have begun exploring the implementation of LLMs as the driving core of social bots, enabling more efficient and user-friendly completion of tasks like profile completion, social behavior decision-making, and social content generation. However, there is currently a lack of systematic research on the behavioral characteristics of LLMs-driven social bots and their impact on social networks. We have curated data from Chirper, a Twitter-like social network populated by LLMs-driven social bots and embarked on an exploratory study. Our findings indicate that: (1) LLMs-driven social bots possess enhanced individual-level camouflage while exhibiting certain collective characteristics; (2) these bots have the ability to exert influence on online communities through toxic behaviors; (3) existing detection methods are applicable to the activity environment of LLMs-driven social bots but may be subject to certain limitations in effectiveness. Moreover, we have organized the data collected in our study into the Masquerade-23 dataset, which we have publicly released, thus addressing the data void in the subfield of LLMs-driven social bots behavior datasets. Our research outcomes provide primary insights for the research and governance of LLMs-driven social bots within the research community. ","[{'version': 'v1', 'created': 'Wed, 19 Jul 2023 15:18:21 GMT'}]",2023-07-21,"[['Li', 'Siyu', ''], ['Yang', 'Jin', ''], ['Zhao', 'Kui', '']]",0,0,2023-07-19,1,3,1,0,0,0,a44583682ce7be93e2dd1fbc08a8f137b67e4645,259991646.0,https://www.semanticscholar.org/paper/a44583682ce7be93e2dd1fbc08a8f137b67e4645,arXiv.org,2023.0,61.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2164758726', 'name': 'Siyu Li'}, {'authorId': '2144539822', 'name': 'Jin Yang'}, {'authorId': '2074109522', 'name': 'Kui Zhao'}]",['Sichuan University'],['China'],2023-07
2307.10432,Zhengliang Liu,"Zhengliang Liu, Zihao Wu, Mengxuan Hu, Bokai Zhao, Lin Zhao, Tianyi
  Zhang, Haixing Dai, Xianyan Chen, Ye Shen, Sheng Li, Brian Murray, Tianming
  Liu, Andrea Sikora",PharmacyGPT: The AI Pharmacist,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  In this study, we introduce PharmacyGPT, a novel framework to assess the capabilities of large language models (LLMs) such as ChatGPT and GPT-4 in emulating the role of clinical pharmacists. Our methodology encompasses the utilization of LLMs to generate comprehensible patient clusters, formulate medication plans, and forecast patient outcomes. We conduct our investigation using real data acquired from the intensive care unit (ICU) at the University of North Carolina Chapel Hill (UNC) Hospital. Our analysis offers valuable insights into the potential applications and limitations of LLMs in the field of clinical pharmacy, with implications for both patient care and the development of future AI-driven healthcare solutions. By evaluating the performance of PharmacyGPT, we aim to contribute to the ongoing discourse surrounding the integration of artificial intelligence in healthcare settings, ultimately promoting the responsible and efficacious use of such technologies. ","[{'version': 'v1', 'created': 'Wed, 19 Jul 2023 19:40:34 GMT'}, {'version': 'v2', 'created': 'Fri, 21 Jul 2023 02:22:14 GMT'}]",2023-07-24,"[['Liu', 'Zhengliang', ''], ['Wu', 'Zihao', ''], ['Hu', 'Mengxuan', ''], ['Zhao', 'Bokai', ''], ['Zhao', 'Lin', ''], ['Zhang', 'Tianyi', ''], ['Dai', 'Haixing', ''], ['Chen', 'Xianyan', ''], ['Shen', 'Ye', ''], ['Li', 'Sheng', ''], ['Murray', 'Brian', ''], ['Liu', 'Tianming', ''], ['Sikora', 'Andrea', '']]",1,1,2023-07-19,2,13,1,2,0,2,8ef83aec0d772ebb9fb22e49d15aa979d38d3ef4,259991555.0,https://www.semanticscholar.org/paper/8ef83aec0d772ebb9fb22e49d15aa979d38d3ef4,arXiv.org,2023.0,74.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Medicine', 'source': 's2-fos-model'}]","[{'authorId': '2145977326', 'name': 'Zheng Liu'}, {'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '2215174877', 'name': 'Mengxuan Hu'}, {'authorId': '2112525162', 'name': 'Bokai Zhao'}, {'authorId': '2111641126', 'name': 'Lin Zhao'}, {'authorId': '2221113406', 'name': 'Tianyi Zhang'}, {'authorId': '29944950', 'name': 'Haixing Dai'}, {'authorId': '2167589230', 'name': 'Xianyan Chen'}, {'authorId': '2196205696', 'name': 'Ye Shen'}, {'authorId': '2153702893', 'name': 'Sheng Li'}, {'authorId': '2156692365', 'name': 'Brian Murray'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}, {'authorId': '84406057', 'name': 'A. Sikora'}]","['University of Virginia', 'University of Georgia', 'Northwest University']","['China', 'United States']",2023-07
2307.10512,Rongsheng Wang,"Rongsheng Wang and Yaofei Duan and ChanTong Lam and Jiexi Chen and
  Jiangsheng Xu and Haoming Chen and Xiaohong Liu and Patrick Cheong-Iao Pang
  and Tao Tan",IvyGPT: InteractiVe Chinese pathwaY language model in medical domain,"5 pages, 3 figures",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  General large language models (LLMs) such as ChatGPT have shown remarkable success. However, such LLMs have not been widely adopted for medical purposes, due to poor accuracy and inability to provide medical advice. We propose IvyGPT, an LLM based on LLaMA that is trained and fine-tuned with high-quality medical question-answer (QA) instances and Reinforcement Learning from Human Feedback (RLHF). After supervised fine-tuning, IvyGPT has good multi-turn conversation capabilities, but it cannot perform like a doctor in other aspects, such as comprehensive diagnosis. Through RLHF, IvyGPT can output richer diagnosis and treatment answers that are closer to human. In the training, we used QLoRA to train 33 billion parameters on a small number of NVIDIA A100 (80GB) GPUs. Experimental results show that IvyGPT has outperformed other medical GPT models. ","[{'version': 'v1', 'created': 'Thu, 20 Jul 2023 01:11:14 GMT'}]",2023-07-21,"[['Wang', 'Rongsheng', ''], ['Duan', 'Yaofei', ''], ['Lam', 'ChanTong', ''], ['Chen', 'Jiexi', ''], ['Xu', 'Jiangsheng', ''], ['Chen', 'Haoming', ''], ['Liu', 'Xiaohong', ''], ['Pang', 'Patrick Cheong-Iao', ''], ['Tan', 'Tao', '']]",1,1,2023-07-20,1,9,2,2,1,1,972b2c4dcb9712d5cd1c5d9f06a5ac0c5e084350,259991244.0,https://www.semanticscholar.org/paper/972b2c4dcb9712d5cd1c5d9f06a5ac0c5e084350,arXiv.org,2023.0,13.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '48397286', 'name': 'Rongsheng Wang'}, {'authorId': '2211647185', 'name': 'Yaofei Duan'}, {'authorId': '144325848', 'name': 'C. Lam'}, {'authorId': '47740650', 'name': 'Jing Chen'}, {'authorId': '2143509804', 'name': 'Jiangsheng Xu'}, {'authorId': '2221531915', 'name': 'Hao Chen'}, {'authorId': '2211597451', 'name': 'Xiaohong Liu'}, {'authorId': '1863458', 'name': 'P. Pang'}, {'authorId': '2184837813', 'name': 'Tao Tan'}]","['Shanghai Jiao Tong University', 'Macao Polytechnic University']","['China', 'Macao']",2023-07
2307.10549,Yuanhao Gong,Yuanhao Gong,Dynamic Large Language Models on Blockchains,,,,,cs.CV cs.AI cs.CL econ.GN q-fin.EC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Training and deploying the large language models requires a large mount of computational resource because the language models contain billions of parameters and the text has thousands of tokens. Another problem is that the large language models are static. They are fixed after the training process. To tackle these issues, in this paper, we propose to train and deploy the dynamic large language model on blockchains, which have high computation performance and are distributed across a network of computers. A blockchain is a secure, decentralized, and transparent system that allows for the creation of a tamper-proof ledger for transactions without the need for intermediaries. The dynamic large language models can continuously learn from the user input after the training process. Our method provides a new way to develop the large language models and also sheds a light on the next generation artificial intelligence systems. ","[{'version': 'v1', 'created': 'Thu, 20 Jul 2023 03:26:57 GMT'}]",2023-07-21,"[['Gong', 'Yuanhao', '']]",0,0,2023-07-20,1,1,5,0,0,0,d4922e165ce55c27d33350ff808a1bc90d367078,259991771.0,https://www.semanticscholar.org/paper/d4922e165ce55c27d33350ff808a1bc90d367078,arXiv.org,2023.0,43.0,3.0,0.0,True,"['Computer Science', 'Economics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Economics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2184586977', 'name': 'Yuanhao Gong'}]",['Shenzhen University'],['China'],2023-07
2307.10747,Yingpeng Du,"Yingpeng Du, Di Luo, Rui Yan, Hongzhi Liu, Yang Song, Hengshu Zhu, Jie
  Zhang",Enhancing Job Recommendation through LLM-based Generative Adversarial Networks,"13 pages, 6 figures, 3 tables",,,,cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recommending suitable jobs to users is a critical task in online recruitment platforms, as it can enhance users' satisfaction and the platforms' profitability. While existing job recommendation methods encounter challenges such as the low quality of users' resumes, which hampers their accuracy and practical effectiveness. With the rapid development of large language models (LLMs), utilizing the rich external knowledge encapsulated within them, as well as their powerful capabilities of text processing and reasoning, is a promising way to complete users' resumes for more accurate recommendations. However, directly leveraging LLMs to enhance recommendation results is not a one-size-fits-all solution, as LLMs may suffer from fabricated generation and few-shot problems, which degrade the quality of resume completion. In this paper, we propose a novel LLM-based approach for job recommendation. To alleviate the limitation of fabricated generation for LLMs, we extract accurate and valuable information beyond users' self-description, which helps the LLMs better profile users for resume completion. Specifically, we not only extract users' explicit properties (e.g., skills, interests) from their self-description but also infer users' implicit characteristics from their behaviors for more accurate and meaningful resume completion. Nevertheless, some users still suffer from few-shot problems, which arise due to scarce interaction records, leading to limited guidance for the models in generating high-quality resumes. To address this issue, we propose aligning unpaired low-quality with high-quality generated resumes by Generative Adversarial Networks (GANs), which can refine the resume representations for better recommendation results. Extensive experiments on three large real-world recruitment datasets demonstrate the effectiveness of our proposed method. ","[{'version': 'v1', 'created': 'Thu, 20 Jul 2023 10:19:47 GMT'}]",2023-07-21,"[['Du', 'Yingpeng', ''], ['Luo', 'Di', ''], ['Yan', 'Rui', ''], ['Liu', 'Hongzhi', ''], ['Song', 'Yang', ''], ['Zhu', 'Hengshu', ''], ['Zhang', 'Jie', '']]",0,0,2023-07-20,1,7,1,0,0,0,e82348d329bedb4545ac04d405cce07a2a4ede9b,259991305.0,https://www.semanticscholar.org/paper/e82348d329bedb4545ac04d405cce07a2a4ede9b,arXiv.org,2023.0,43.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '51123128', 'name': 'Yingpeng Du'}, {'authorId': '2215612529', 'name': 'Di Luo'}, {'authorId': '2055864893', 'name': 'Rui Yan'}, {'authorId': '2109503105', 'name': 'Hongzhi Liu'}, {'authorId': '144404428', 'name': 'Yang Song'}, {'authorId': '1968806', 'name': 'Hengshu Zhu'}, {'authorId': '2210549980', 'name': 'Jiehan Zhang'}]","['Peking University', 'Equal Contribution.', 'Renmin University of China', 'Nanyang Technological University']","['China', 'Singapore']",2023-07
2307.10811,Zhicong Lu,"Qian Wan, Siying Hu, Yu Zhang, Piaohong Wang, Bo Wen, Zhicong Lu","""It Felt Like Having a Second Mind"": Investigating Human-AI Co-creativity in Prewriting with Large Language Models","Under Review; 25 pages, 2 figures",,,,cs.HC cs.AI cs.CL,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Prewriting is the process of discovering and developing ideas before a first draft, which requires divergent thinking and often implies unstructured strategies such as diagramming, outlining, free-writing, etc. Although large language models (LLMs) have been demonstrated to be useful for a variety of tasks including creative writing, little is known about how users would collaborate with LLMs to support prewriting. The preferred collaborative role and initiative of LLMs during such a creativity process is also unclear. To investigate human-LLM collaboration patterns and dynamics during prewriting, we conducted a three-session qualitative study with 15 participants in two creative tasks: story writing and slogan writing. The findings indicated that during collaborative prewriting, there appears to be a three-stage iterative Human-AI Co-creativity process that includes Ideation, Illumination, and Implementation stages. This collaborative process champions the human in a dominant role, in addition to mixed and shifting levels of initiative that exist between humans and LLMs. This research also reports on collaboration breakdowns that occur during this process, user perceptions of using existing LLMs during Human-AI Co-creativity, and discusses design implications to support this co-creativity process. ","[{'version': 'v1', 'created': 'Thu, 20 Jul 2023 16:55:25 GMT'}, {'version': 'v2', 'created': 'Thu, 31 Aug 2023 14:13:31 GMT'}]",2023-09-01,"[['Wan', 'Qian', ''], ['Hu', 'Siying', ''], ['Zhang', 'Yu', ''], ['Wang', 'Piaohong', ''], ['Wen', 'Bo', ''], ['Lu', 'Zhicong', '']]",0,0,2023-07-20,2,6,3,0,0,0,a392d1089133982eb4102d67dfe28ecbc1548785,259991355.0,https://www.semanticscholar.org/paper/a392d1089133982eb4102d67dfe28ecbc1548785,arXiv.org,2023.0,82.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Psychology', 'source': 's2-fos-model'}]","[{'authorId': '2055596232', 'name': 'Qian Wan'}, {'authorId': '2154288361', 'name': 'Si-Yuan Hu'}, {'authorId': '2214904615', 'name': 'Yu Zhang'}, {'authorId': '1400309983', 'name': 'Pi-Hui Wang'}, {'authorId': '2224015857', 'name': 'Bo Wen'}, {'authorId': '2110326898', 'name': 'Zhicong Lu'}]",['City University of Hong Kong'],['China'],2023-07
2307.11019,Ruiyang Ren,"Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao
  Tian, Hua Wu, Ji-Rong Wen, Haifeng Wang",Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation,,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge-intensive tasks (e.g., open-domain question answering (QA)) require a substantial amount of factual knowledge and often rely on external information for assistance. Recently, large language models (LLMs) (e.g., ChatGPT), have demonstrated impressive prowess in solving a wide range of tasks with world knowledge, including knowledge-intensive tasks. However, it remains unclear how well LLMs are able to perceive their factual knowledge boundaries, particularly how they behave when incorporating retrieval augmentation. In this study, we present an initial analysis of the factual knowledge boundaries of LLMs and how retrieval augmentation affects LLMs on open-domain QA. Specially, we focus on three primary research questions and analyze them by examining QA performance, priori judgement and posteriori judgement of LLMs. We show evidence that LLMs possess unwavering confidence in their capabilities to respond to questions and the accuracy of their responses. Furthermore, retrieval augmentation proves to be an effective approach in enhancing LLMs' awareness of knowledge boundaries, thereby improving their judgemental abilities. Additionally, we also find that LLMs have a propensity to rely on the provided retrieval results when formulating answers, while the quality of these results significantly impacts their reliance. The code to reproduce this work is available at https://github.com/RUCAIBox/LLM-Knowledge-Boundary. ","[{'version': 'v1', 'created': 'Thu, 20 Jul 2023 16:46:10 GMT'}, {'version': 'v2', 'created': 'Sun, 23 Jul 2023 16:52:59 GMT'}]",2023-07-25,"[['Ren', 'Ruiyang', ''], ['Wang', 'Yuhao', ''], ['Qu', 'Yingqi', ''], ['Zhao', 'Wayne Xin', ''], ['Liu', 'Jing', ''], ['Tian', 'Hao', ''], ['Wu', 'Hua', ''], ['Wen', 'Ji-Rong', ''], ['Wang', 'Haifeng', '']]",1,1,2023-07-20,2,9,2,1,0,1,84b77180228051040286423cec82b62c323a8fda,259991467.0,https://www.semanticscholar.org/paper/84b77180228051040286423cec82b62c323a8fda,arXiv.org,2023.0,34.0,10.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1708171825', 'name': 'Ruiyang Ren'}, {'authorId': '2198466349', 'name': 'Yuhao Wang'}, {'authorId': '51281403', 'name': 'Yingqi Qu'}, {'authorId': '2542603', 'name': 'Wayne Xin Zhao'}, {'authorId': '46701354', 'name': 'J. Liu'}, {'authorId': '50007795', 'name': 'Hao Tian'}, {'authorId': '46477059', 'name': 'Huaqin Wu'}, {'authorId': '153693432', 'name': 'Ji-rong Wen'}, {'authorId': '2155569278', 'name': 'Haifeng Wang'}]","['Beijing Key Laboratory of Big Data Management and Analysis Methods', 'Renmin University of China', 'Baidu']",['China'],2023-07
2307.12573,Yuanzhi Liang,"Yuanzhi Liang, Linchao Zhu, Yi Yang",Tachikuma: Understading Complex Interactions with Multi-Character and Novel Objects by Large Language Models,Preliminary version of an ongoing work,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advancements in natural language and Large Language Models (LLMs) have enabled AI agents to simulate human-like interactions within virtual worlds. However, these interactions still face limitations in complexity and flexibility, particularly in scenarios involving multiple characters and novel objects. Pre-defining all interactable objects in the agent's world model presents challenges, and conveying implicit intentions to multiple characters through complex interactions remains difficult. To address these issues, we propose integrating virtual Game Masters (GMs) into the agent's world model, drawing inspiration from Tabletop Role-Playing Games (TRPGs). GMs play a crucial role in overseeing information, estimating players' intentions, providing environment descriptions, and offering feedback, compensating for current world model deficiencies. To facilitate future explorations for complex interactions, we introduce a benchmark named Tachikuma, comprising a Multiple character and novel Object based interaction Estimation (MOE) task and a supporting dataset. MOE challenges models to understand characters' intentions and accurately determine their actions within intricate contexts involving multi-character and novel object interactions. Besides, the dataset captures log data from real-time communications during gameplay, providing diverse, grounded, and complex interactions for further explorations. Finally, we present a simple prompting baseline and evaluate its performance, demonstrating its effectiveness in enhancing interaction understanding. We hope that our dataset and task will inspire further research in complex interactions with natural language, fostering the development of more advanced AI agents. ","[{'version': 'v1', 'created': 'Mon, 24 Jul 2023 07:40:59 GMT'}]",2023-07-25,"[['Liang', 'Yuanzhi', ''], ['Zhu', 'Linchao', ''], ['Yang', 'Yi', '']]",0,0,2023-07-24,1,3,1,0,0,0,4dcbc14301d048545cd83ca4fd57f110fc995c48,260125274.0,https://www.semanticscholar.org/paper/4dcbc14301d048545cd83ca4fd57f110fc995c48,arXiv.org,2023.0,26.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '72322311', 'name': 'Yuanzhi Liang'}, {'authorId': '2948393', 'name': 'Linchao Zhu'}, {'authorId': '7607499', 'name': 'Yezhou Yang'}]","['University of Technology Sydney', 'Zhejiang University']","['China', 'Australia']",2023-07
2307.12596,Yue Liu,"Yue Liu, Thanh Le-Cong, Ratnadira Widyasari, Chakkrit
  Tantithamthavorn, Li Li, Xuan-Bach D. Le, David Lo",Refining ChatGPT-Generated Code: Characterizing and Mitigating Code Quality Issues,,,,,cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we systematically study the quality of 4,066 ChatGPT-generated code implemented in two popular programming languages, i.e., Java and Python, for 2,033 programming tasks. The goal of this work is three folds. First, we analyze the correctness of ChatGPT on code generation tasks and uncover the factors that influence its effectiveness, including task difficulty, programming language, time that tasks are introduced, and program size. Second, we identify and characterize potential issues with the quality of ChatGPT-generated code. Last, we provide insights into how these issues can be mitigated. Experiments highlight that out of 4,066 programs generated by ChatGPT, 2,757 programs are deemed correct, 1,081 programs provide wrong outputs, and 177 programs contain compilation or runtime errors. Additionally, we further analyze other characteristics of the generated code through static analysis tools, such as code style and maintainability, and find that 1,933 ChatGPT-generated code snippets suffer from maintainability issues. Subsequently, we investigate ChatGPT's self-debugging ability and its interaction with static analysis tools to fix the errors uncovered in the previous step. Experiments suggest that ChatGPT can partially address these challenges, improving code quality by more than 20%, but there are still limitations and opportunities for improvement. Overall, our study provides valuable insights into the current limitations of ChatGPT and offers a roadmap for future research and development efforts to enhance the code generation capabilities of AI models like ChatGPT. ","[{'version': 'v1', 'created': 'Mon, 24 Jul 2023 08:14:22 GMT'}]",2023-07-25,"[['Liu', 'Yue', ''], ['Le-Cong', 'Thanh', ''], ['Widyasari', 'Ratnadira', ''], ['Tantithamthavorn', 'Chakkrit', ''], ['Li', 'Li', ''], ['Le', 'Xuan-Bach D.', ''], ['Lo', 'David', '']]",1,1,2023-07-24,1,7,1,1,0,1,41a2e7c079179ae94557d3198de674a16a5987a6,260126031.0,https://www.semanticscholar.org/paper/41a2e7c079179ae94557d3198de674a16a5987a6,arXiv.org,2023.0,49.0,7.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '100576986', 'name': 'Yue Liu'}, {'authorId': '1811413300', 'name': 'Thanh Le-Cong'}, {'authorId': '2008294024', 'name': 'Ratnadira Widyasari'}, {'authorId': '1957122', 'name': 'C. Tantithamthavorn'}, {'authorId': '2156059416', 'name': 'Li Li'}, {'authorId': '2052967173', 'name': 'Xu Le'}, {'authorId': '2150912791', 'name': 'David Lo'}]","['Shanghai Advanced Research Institute', 'Thanh Le-Cong,', 'Singapore Management University', 'Monash University', 'University of Melbourne', 'Beihang University', 'RATNADIRA WIDYASARI, Singapore']","['China', 'Singapore', 'Australia']",2023-07
2307.13221,Yuanhao Gong,Yuanhao Gong,Multilevel Large Language Models for Everyone,,,,,cs.CV cs.AI cs.CE cs.DC econ.GN q-fin.EC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models have made significant progress in the past few years. However, they are either generic {\it or} field specific, splitting the community into different groups. In this paper, we unify these large language models into a larger map, where the generic {\it and} specific models are linked together and can improve each other, based on the user personal input and information from the internet. The idea of linking several large language models together is inspired by the functionality of human brain. The specific regions on the brain cortex are specific for certain low level functionality. And these regions can jointly work together to achieve more complex high level functionality. Such behavior on human brain cortex sheds the light to design the multilevel large language models that contain global level, field level and user level models. The user level models run on local machines to achieve efficient response and protect the user's privacy. Such multilevel models reduce some redundancy and perform better than the single level models. The proposed multilevel idea can be applied in various applications, such as natural language processing, computer vision tasks, professional assistant, business and healthcare. ","[{'version': 'v1', 'created': 'Tue, 25 Jul 2023 03:18:04 GMT'}]",2023-07-26,"[['Gong', 'Yuanhao', '']]",0,0,2023-07-25,1,1,6,0,0,0,459abe7efef562908de01c3d256fa2854d509c2d,260154907.0,https://www.semanticscholar.org/paper/459abe7efef562908de01c3d256fa2854d509c2d,arXiv.org,2023.0,45.0,4.0,0.0,True,"['Computer Science', 'Economics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Economics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2222514', 'name': 'Yuanhao Gong'}]",['Shenzhen University'],['China'],2023-07
2307.13365,Yifei Gao,"Yifei Gao, Lei Wang, Jun Fang, Longhua Hu, Jun Cheng",Empower Your Model with Longer and Better Context Comprehension,LLM for long context comprehension,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, with the emergence of numerous Large Language Models (LLMs), the implementation of AI has entered a new era. Irrespective of these models' own capacity and structure, there is a growing demand for LLMs to possess enhanced comprehension of longer and more complex contexts with relatively smaller sizes. Models often encounter an upper limit when processing sequences of sentences that extend beyond their comprehension capacity and result in off-topic or even chaotic responses. While several recent works attempt to address this issue in various ways, they rarely focus on ""why models are unable to compensate or strengthen their capabilities on their own"". In this paper, we thoroughly investigate the nature of information transfer within LLMs and propose a novel technique called Attention Transition. This technique empowers models to achieve longer and better context comprehension with minimal additional training or impact on generation fluency. Our experiments are conducted on the challenging XSum dataset using LLaMa-7b model with context token length ranging from 800 to 1900. Results demonstrate that we achieve substantial improvements compared with the original generation results evaluated by GPT4. ","[{'version': 'v1', 'created': 'Tue, 25 Jul 2023 09:34:42 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Jul 2023 10:17:18 GMT'}]",2023-07-28,"[['Gao', 'Yifei', ''], ['Wang', 'Lei', ''], ['Fang', 'Jun', ''], ['Hu', 'Longhua', ''], ['Cheng', 'Jun', '']]",0,1,2023-07-25,2,5,2,2,1,1,58d1a002a0ff0aa40b6633f0a7073d48f1cdff53,260155058.0,https://www.semanticscholar.org/paper/58d1a002a0ff0aa40b6633f0a7073d48f1cdff53,arXiv.org,2023.0,22.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2223147456', 'name': 'Yifei Gao'}, {'authorId': '2152509578', 'name': 'Lei Wang'}, {'authorId': '2216793352', 'name': 'Jun Fang'}, {'authorId': '2191398292', 'name': 'Longhua Hu'}, {'authorId': '2157745217', 'name': 'Jun Cheng'}]","['Shenzhen University', 'Xidian University', 'Shenzhen Institutes of Advanced Technology']",['China'],2023-07
2307.13424,Hexuan Deng,"Hexuan Deng, Xin Zhang, Meishan Zhang, Xuebo Liu, Min Zhang","Holistic Exploration on Universal Decompositional Semantic Parsing: Architecture, Data Augmentation, and LLM Paradigm","12 pages, 7 figures, 3 tables",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we conduct a holistic exploration of the Universal Decompositional Semantic (UDS) Parsing. We first introduce a cascade model for UDS parsing that decomposes the complex parsing task into semantically appropriate subtasks. Our approach outperforms the prior models, while significantly reducing inference time. We also incorporate syntactic information and further optimized the architecture. Besides, different ways for data augmentation are explored, which further improve the UDS Parsing. Lastly, we conduct experiments to investigate the efficacy of ChatGPT in handling the UDS task, revealing that it excels in attribute parsing but struggles in relation parsing, and using ChatGPT for data augmentation yields suboptimal results. Our code is available at https://github.com/hexuandeng/HExp4UDS. ","[{'version': 'v1', 'created': 'Tue, 25 Jul 2023 11:44:28 GMT'}]",2023-07-26,"[['Deng', 'Hexuan', ''], ['Zhang', 'Xin', ''], ['Zhang', 'Meishan', ''], ['Liu', 'Xuebo', ''], ['Zhang', 'Min', '']]",1,1,2023-07-25,1,5,1,1,0,1,092c560726b253f3e3a85e01a176fe96b1c4b451,260155177.0,https://www.semanticscholar.org/paper/092c560726b253f3e3a85e01a176fe96b1c4b451,arXiv.org,2023.0,41.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2193388247', 'name': 'Hexuan Deng'}, {'authorId': '2149173726', 'name': 'Xin Zhang'}, {'authorId': '2117849151', 'name': 'Meishan Zhang'}, {'authorId': '1390611971', 'name': 'Xuebo Liu'}, {'authorId': '50495870', 'name': 'M. Zhang'}]",['Harbin Institute of Technology'],['China'],2023-07
2307.13693,Zhengliang Liu,"Zhengliang Liu, Tianyang Zhong, Yiwei Li, Yutong Zhang, Yi Pan, Zihao
  Zhao, Peixin Dong, Chao Cao, Yuxiao Liu, Peng Shu, Yaonai Wei, Zihao Wu,
  Chong Ma, Jiaqi Wang, Sheng Wang, Mengyue Zhou, Zuowei Jiang, Chunlin Li,
  Jason Holmes, Shaochen Xu, Lu Zhang, Haixing Dai, Kai Zhang, Lin Zhao,
  Yuanhao Chen, Xu Liu, Peilong Wang, Pingkun Yan, Jun Liu, Bao Ge, Lichao Sun,
  Dajiang Zhu, Xiang Li, Wei Liu, Xiaoyan Cai, Xintao Hu, Xi Jiang, Shu Zhang,
  Xin Zhang, Tuo Zhang, Shijie Zhao, Quanzheng Li, Hongtu Zhu, Dinggang Shen,
  Tianming Liu",Evaluating Large Language Models for Radiology Natural Language Processing,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The rise of large language models (LLMs) has marked a pivotal shift in the field of natural language processing (NLP). LLMs have revolutionized a multitude of domains, and they have made a significant impact in the medical field. Large language models are now more abundant than ever, and many of these models exhibit bilingual capabilities, proficient in both English and Chinese. However, a comprehensive evaluation of these models remains to be conducted. This lack of assessment is especially apparent within the context of radiology NLP. This study seeks to bridge this gap by critically evaluating thirty two LLMs in interpreting radiology reports, a crucial component of radiology NLP. Specifically, the ability to derive impressions from radiologic findings is assessed. The outcomes of this evaluation provide key insights into the performance, strengths, and weaknesses of these LLMs, informing their practical applications within the medical domain. ","[{'version': 'v1', 'created': 'Tue, 25 Jul 2023 17:57:18 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Jul 2023 12:58:59 GMT'}]",2023-07-28,"[['Liu', 'Zhengliang', ''], ['Zhong', 'Tianyang', ''], ['Li', 'Yiwei', ''], ['Zhang', 'Yutong', ''], ['Pan', 'Yi', ''], ['Zhao', 'Zihao', ''], ['Dong', 'Peixin', ''], ['Cao', 'Chao', ''], ['Liu', 'Yuxiao', ''], ['Shu', 'Peng', ''], ['Wei', 'Yaonai', ''], ['Wu', 'Zihao', ''], ['Ma', 'Chong', ''], ['Wang', 'Jiaqi', ''], ['Wang', 'Sheng', ''], ['Zhou', 'Mengyue', ''], ['Jiang', 'Zuowei', ''], ['Li', 'Chunlin', ''], ['Holmes', 'Jason', ''], ['Xu', 'Shaochen', ''], ['Zhang', 'Lu', ''], ['Dai', 'Haixing', ''], ['Zhang', 'Kai', ''], ['Zhao', 'Lin', ''], ['Chen', 'Yuanhao', ''], ['Liu', 'Xu', ''], ['Wang', 'Peilong', ''], ['Yan', 'Pingkun', ''], ['Liu', 'Jun', ''], ['Ge', 'Bao', ''], ['Sun', 'Lichao', ''], ['Zhu', 'Dajiang', ''], ['Li', 'Xiang', ''], ['Liu', 'Wei', ''], ['Cai', 'Xiaoyan', ''], ['Hu', 'Xintao', ''], ['Jiang', 'Xi', ''], ['Zhang', 'Shu', ''], ['Zhang', 'Xin', ''], ['Zhang', 'Tuo', ''], ['Zhao', 'Shijie', ''], ['Li', 'Quanzheng', ''], ['Zhu', 'Hongtu', ''], ['Shen', 'Dinggang', ''], ['Liu', 'Tianming', '']]",0,0,2023-07-25,2,45,1,0,0,0,2525f8185ff4eee23dea96f2a820714a1619fb89,260209167.0,https://www.semanticscholar.org/paper/2525f8185ff4eee23dea96f2a820714a1619fb89,,2023.0,135.0,3.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Medicine', 'source': 's2-fos-model'}]","[{'authorId': '2145977326', 'name': 'Zheng Liu'}, {'authorId': '2215167446', 'name': 'Tianyang Zhong'}, {'authorId': '2111161355', 'name': 'Yiwei Li'}, {'authorId': '2224856036', 'name': 'Yutong Zhang'}, {'authorId': '19308245', 'name': 'Yirong Pan'}, {'authorId': '15594254', 'name': 'Zihao Zhao'}, {'authorId': '11628039', 'name': 'Pei Dong'}, {'authorId': '5762440', 'name': 'Chao-Yang Cao'}, {'authorId': '2108135727', 'name': 'YuXin Liu'}, {'authorId': '2220096705', 'name': 'Peng Shu'}, {'authorId': '2163675860', 'name': 'Yaonai Wei'}, {'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '120688117', 'name': 'Chong-Yi Ma'}, {'authorId': '2136025369', 'name': 'Jiaqi Wang'}, {'authorId': '2151485716', 'name': 'Shengming Wang'}, {'authorId': '2212701018', 'name': 'Mengyue Zhou'}, {'authorId': '2184750732', 'name': 'Zuowei Jiang'}, {'authorId': '48162177', 'name': 'Chunlin Li'}, {'authorId': '2177342883', 'name': 'J. Holmes'}, {'authorId': '2211904452', 'name': 'Shaochen Xu'}, {'authorId': '2156146823', 'name': 'Lu Zhang'}, {'authorId': '29944950', 'name': 'Haixing Dai'}, {'authorId': '2158521525', 'name': 'Kailiang Zhang'}, {'authorId': '2111641126', 'name': 'Lin Zhao'}, {'authorId': '84735131', 'name': 'Yuanhao'}, {'authorId': '2117444859', 'name': 'Chen'}, {'authorId': '40913460', 'name': 'X. Liu'}, {'authorId': '2118952169', 'name': 'Pei Wang'}, {'authorId': '1690097', 'name': 'Pingkun Yan'}, {'authorId': '2157176152', 'name': 'Jun Liu'}, {'authorId': '144691205', 'name': 'Bao Ge'}, {'authorId': '2219704055', 'name': 'Lichao Sun'}, {'authorId': '2081561953', 'name': 'Dajiang'}, {'authorId': '2054765028', 'name': 'Zhu'}, {'authorId': '113075935', 'name': 'Xiang Li'}, {'authorId': '46641573', 'name': 'W. Liu'}, {'authorId': '2149789733', 'name': 'Xiaoyan Cai'}, {'authorId': '1742535', 'name': 'Xintao Hu'}, {'authorId': '143796247', 'name': 'Xi Jiang'}, {'authorId': '2108086798', 'name': 'Shu Zhang'}, {'authorId': '2149174952', 'name': 'Xin Zhang'}, {'authorId': '49104946', 'name': 'Tuo Zhang'}, {'authorId': '2946035', 'name': 'Shijie Zhao'}, {'authorId': '1762919', 'name': 'Quanzheng Li'}, {'authorId': '3280236', 'name': 'Hongtu Zhu'}, {'authorId': '2150038187', 'name': 'Dinggang Shen'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}]","['University of Electronic Science and Technology of China', 'Lingang Laboratory, Shanghai, 200031, China', 'Second Xiangya Hospital of Central South University', 'Mayo Clinic', 'University of North Carolina at Chapel Hill', 'Shanghai United Imaging Intelligence Co., Ltd.', 'Lehigh University', 'ShanghaiTech University', 'Shanghai Jiao Tong University', 'Dartmouth College', 'Shanghai Clinical Research Center', 'Massachusetts General Hospital', 'Shaanxi Normal University', 'The University of Texas at Arlington', 'University of Georgia', 'Rensselaer Polytechnic Institute', 'Northwestern Polytechnical University']","['China', 'United States']",2023-07
2308.10025,Kaihang Pan,"Kaihang Pan, Juncheng Li, Hongye Song, Hao Fei, Wei Ji, Shuo Zhang,
  Jun Lin, Xiaozhong Liu, Siliang Tang",ControlRetriever: Harnessing the Power of Instructions for Controllable Retrieval,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent studies have shown that dense retrieval models, lacking dedicated training data, struggle to perform well across diverse retrieval tasks, as different retrieval tasks often entail distinct search intents. To address this challenge, in this work we introduce ControlRetriever, a generic and efficient approach with a parameter isolated architecture, capable of controlling dense retrieval models to directly perform varied retrieval tasks, harnessing the power of instructions that explicitly describe retrieval intents in natural language. Leveraging the foundation of ControlNet, which has proven powerful in text-to-image generation, ControlRetriever imbues different retrieval models with the new capacity of controllable retrieval, all while being guided by task-specific instructions. Furthermore, we propose a novel LLM guided Instruction Synthesizing and Iterative Training strategy, which iteratively tunes ControlRetriever based on extensive automatically-generated retrieval data with diverse instructions by capitalizing the advancement of large language models. Extensive experiments show that in the BEIR benchmark, with only natural language descriptions of specific retrieval intent for each task, ControlRetriever, as a unified multi-task retrieval system without task-specific tuning, significantly outperforms baseline methods designed with task-specific retrievers and also achieves state-of-the-art zero-shot performance. ","[{'version': 'v1', 'created': 'Sat, 19 Aug 2023 14:17:57 GMT'}]",2023-08-22,"[['Pan', 'Kaihang', ''], ['Li', 'Juncheng', ''], ['Song', 'Hongye', ''], ['Fei', 'Hao', ''], ['Ji', 'Wei', ''], ['Zhang', 'Shuo', ''], ['Lin', 'Jun', ''], ['Liu', 'Xiaozhong', ''], ['Tang', 'Siliang', '']]",0,0,2023-08-19,1,9,1,0,0,0,ab3f7615d9ed62483f98048b62ba1e4cbf0fada2,261048776.0,https://www.semanticscholar.org/paper/ab3f7615d9ed62483f98048b62ba1e4cbf0fada2,arXiv.org,2023.0,52.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2212175601', 'name': 'Kaihang Pan'}, {'authorId': '2108998895', 'name': 'Juncheng Li'}, {'authorId': '2122383983', 'name': 'Hongye Song'}, {'authorId': '2142672912', 'name': 'Hao Fei'}, {'authorId': '2150078659', 'name': 'Wei Ji'}, {'authorId': '2144324539', 'name': 'Shuo Zhang'}, {'authorId': '2110808728', 'name': 'Jun Lin'}, {'authorId': '1713802', 'name': 'Xiaozhong Liu'}, {'authorId': '2118071462', 'name': 'Siliang Tang'}]","['Alibaba', 'National University of Singapore', 'Worcester Polytechnic Institute', 'Zhejiang University']","['China', 'United States', 'Singapore']",2023-08
2308.10032,Chenfei Wu,"Dan Qiao, Chenfei Wu, Yaobo Liang, Juntao Li, Nan Duan",GameEval: Evaluating LLMs on Conversational Games,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The rapid advancements in large language models (LLMs) have presented challenges in evaluating those models. Existing evaluation methods are either reference-based or preference based, which inevitably need human intervention or introduce test bias caused by evaluator models. In this paper, we propose GameEval, a novel approach to evaluating LLMs through goal-driven conversational games, overcoming the limitations of previous methods. GameEval treats LLMs as game players and assigns them distinct roles with specific goals achieved by launching conversations of various forms, including discussion, question answering, and voting. We design three unique games with cooperative or adversarial objectives, accompanied by corresponding evaluation metrics, to show how this new paradigm comprehensively evaluates model performance.Through extensive experiments, we show that GameEval can effectively differentiate the capabilities of various LLMs, providing a comprehensive assessment of their integrated abilities to solve complex problems. Our public anonymous code is available at https://github.com/GameEval/GameEval. ","[{'version': 'v1', 'created': 'Sat, 19 Aug 2023 14:33:40 GMT'}]",2023-08-22,"[['Qiao', 'Dan', ''], ['Wu', 'Chenfei', ''], ['Liang', 'Yaobo', ''], ['Li', 'Juntao', ''], ['Duan', 'Nan', '']]",0,0,2023-08-19,1,5,1,0,0,0,6f28ca1e6c007c46cbc30aad531d800b8e6bc405,261048971.0,https://www.semanticscholar.org/paper/6f28ca1e6c007c46cbc30aad531d800b8e6bc405,arXiv.org,2023.0,27.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2187298413', 'name': 'Dan Qiao'}, {'authorId': '2151101534', 'name': 'Chenfei Wu'}, {'authorId': '3887469', 'name': 'Yaobo Liang'}, {'authorId': '2109013629', 'name': 'Juntao Li'}, {'authorId': '2072609829', 'name': 'Nan Duan'}]","['Soochow University', 'Microsoft']",['China'],2023-08
2308.10088,Yihong Dong,"Yihong Dong, Kangcheng Luo, Xue Jiang, Zhi Jin, and Ge Li",PACE: Improving Prompt with Actor-Critic Editing for Large Language Model,,,,,cs.CL cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have showcased remarkable potential across various tasks by conditioning on prompts. However, the quality of different human-written prompts leads to substantial discrepancies in LLMs' performance, and improving prompts usually necessitates considerable human effort and expertise. To this end, this paper proposes Prompt with Actor-Critic Editing (PACE) for LLMs to enable automatic prompt editing. Drawing inspiration from the actor-critic algorithm in reinforcement learning, PACE leverages LLMs as the dual roles of actors and critics, conceptualizing prompt as a type of policy. PACE refines prompt, taking into account the feedback from both actors performing prompt and critics criticizing response. This process helps LLMs better align prompt to a specific task, thanks to real responses and thinking from LLMs. We conduct extensive experiments on 24 instruction induction tasks and 21 big-bench tasks. Experimental results indicate that PACE elevates the relative performance of medium/low-quality human-written prompts by up to 98\%, which has comparable performance to high-quality human-written prompts. Moreover, PACE also exhibits notable efficacy for prompt generation. ","[{'version': 'v1', 'created': 'Sat, 19 Aug 2023 18:47:44 GMT'}]",2023-08-22,"[['Dong', 'Yihong', ''], ['Luo', 'Kangcheng', ''], ['Jiang', 'Xue', ''], ['Jin', 'Zhi', ''], ['Li', 'Ge', '']]",0,0,2023-08-19,1,5,2,0,0,0,d8ecaac57de593207c85461ccfc7d10c8807d78d,261049382.0,https://www.semanticscholar.org/paper/d8ecaac57de593207c85461ccfc7d10c8807d78d,arXiv.org,2023.0,33.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '26845858', 'name': 'Yihong Dong'}, {'authorId': '2232781058', 'name': 'Kangcheng Luo'}, {'authorId': '2199808863', 'name': 'Xue Jiang'}, {'authorId': '2152843753', 'name': 'Zhi Jin'}, {'authorId': '2154591375', 'name': 'Ge Li'}]",['Peking University'],['China'],2023-08
2308.10141,Yanyuan Qiao,"Yanyuan Qiao, Yuankai Qi, Zheng Yu, Jing Liu, Qi Wu",March in Chat: Interactive Prompting for Remote Embodied Referring Expression,Accepted by ICCV 2023,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many Vision-and-Language Navigation (VLN) tasks have been proposed in recent years, from room-based to object-based and indoor to outdoor. The REVERIE (Remote Embodied Referring Expression) is interesting since it only provides high-level instructions to the agent, which are closer to human commands in practice. Nevertheless, this poses more challenges than other VLN tasks since it requires agents to infer a navigation plan only based on a short instruction. Large Language Models (LLMs) show great potential in robot action planning by providing proper prompts. Still, this strategy has not been explored under the REVERIE settings. There are several new challenges. For example, the LLM should be environment-aware so that the navigation plan can be adjusted based on the current visual observation. Moreover, the LLM planned actions should be adaptable to the much larger and more complex REVERIE environment. This paper proposes a March-in-Chat (MiC) model that can talk to the LLM on the fly and plan dynamically based on a newly proposed Room-and-Object Aware Scene Perceiver (ROASP). Our MiC model outperforms the previous state-of-the-art by large margins by SPL and RGSPL metrics on the REVERIE benchmark. ","[{'version': 'v1', 'created': 'Sun, 20 Aug 2023 03:00:20 GMT'}]",2023-08-22,"[['Qiao', 'Yanyuan', ''], ['Qi', 'Yuankai', ''], ['Yu', 'Zheng', ''], ['Liu', 'Jing', ''], ['Wu', 'Qi', '']]",0,0,2023-08-20,1,5,1,0,0,0,c3925ef53f864e5c30189272f63801248ff1406f,261048775.0,https://www.semanticscholar.org/paper/c3925ef53f864e5c30189272f63801248ff1406f,arXiv.org,2023.0,41.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '80526284', 'name': 'Yanyuan Qiao'}, {'authorId': '35653798', 'name': 'Yuankai Qi'}, {'authorId': '2116731898', 'name': 'Zheng Yu'}, {'authorId': '46701354', 'name': 'J. Liu'}, {'authorId': '1715610', 'name': 'Qi Wu'}]","['University of Adelaide', 'University of Chinese Academy of Sciences', 'Chinese Academy of Sciences']","['China', 'Australia']",2023-08
2308.10144,Andrew Zhao,"Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, Gao
  Huang",ExpeL: LLM Agents Are Experiential Learners,,,,,cs.LG cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The recent surge in research interest in applying large language models (LLMs) to decision-making tasks has flourished by leveraging the extensive world knowledge embedded in LLMs. While there is a growing demand to tailor LLMs for custom decision-making tasks, finetuning them for specific tasks is resource-intensive and may diminish the model's generalization capabilities. Moreover, state-of-the-art language models like GPT-4 and Claude are primarily accessible through API calls, with their parametric weights remaining proprietary and unavailable to the public. This scenario emphasizes the growing need for new methodologies that allow learning from agent experiences without requiring parametric updates. To address these problems, we introduce the Experiential Learning (ExpeL) agent. Our agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks. At inference, the agent recalls its extracted insights and past experiences to make informed decisions. Our empirical results highlight the robust learning efficacy of the ExpeL agent, indicating a consistent enhancement in its performance as it accumulates experiences. We further explore the emerging capabilities and transfer learning potential of the ExpeL agent through qualitative observations and additional experiments. ","[{'version': 'v1', 'created': 'Sun, 20 Aug 2023 03:03:34 GMT'}]",2023-08-22,"[['Zhao', 'Andrew', ''], ['Huang', 'Daniel', ''], ['Xu', 'Quentin', ''], ['Lin', 'Matthieu', ''], ['Liu', 'Yong-Jin', ''], ['Huang', 'Gao', '']]",0,1,2023-08-20,1,6,3,2,0,2,5e4597eb21a393b23e473cf66cb5ae8b27cab03e,261048772.0,https://www.semanticscholar.org/paper/5e4597eb21a393b23e473cf66cb5ae8b27cab03e,arXiv.org,2023.0,64.0,9.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2136104377', 'name': 'Andrew Zhao'}, {'authorId': '46307329', 'name': 'Daniel Huang'}, {'authorId': '2232847317', 'name': 'Quentin Xu'}, {'authorId': '2036238525', 'name': 'Matthieu Lin'}, {'authorId': '1679704', 'name': 'Y. Liu'}, {'authorId': '2115218570', 'name': 'Gao Huang'}]",['Tsinghua University'],['China'],2023-08
2308.10149,Yingji Li,"Yingji Li, Mengnan Du, Rui Song, Xin Wang, Ying Wang",A Survey on Fairness in Large Language Models,"12 pages, 2 figures, 101 references",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have shown powerful performance and development prospect and are widely deployed in the real world. However, LLMs can capture social biases from unprocessed training data and propagate the biases to downstream tasks. Unfair LLM systems have undesirable social impacts and potential harms. In this paper, we provide a comprehensive review of related research on fairness in LLMs. First, for medium-scale LLMs, we introduce evaluation metrics and debiasing methods from the perspectives of intrinsic bias and extrinsic bias, respectively. Then, for large-scale LLMs, we introduce recent fairness research, including fairness evaluation, reasons for bias, and debiasing methods. Finally, we discuss and provide insight on the challenges and future directions for the development of fairness in LLMs. ","[{'version': 'v1', 'created': 'Sun, 20 Aug 2023 03:30:22 GMT'}]",2023-08-22,"[['Li', 'Yingji', ''], ['Du', 'Mengnan', ''], ['Song', 'Rui', ''], ['Wang', 'Xin', ''], ['Wang', 'Ying', '']]",0,0,2023-08-20,1,5,2,0,0,0,03bf28df6e282a7e36e1686edeb9c624e6ffb13b,261049466.0,https://www.semanticscholar.org/paper/03bf28df6e282a7e36e1686edeb9c624e6ffb13b,arXiv.org,2023.0,104.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2116202874', 'name': 'Yingji Li'}, {'authorId': '3432460', 'name': 'Mengnan Du'}, {'authorId': '145401373', 'name': 'Rui Song'}, {'authorId': '2153687737', 'name': 'Xin Wang'}, {'authorId': '49416173', 'name': 'Y. Wang'}]","['Jilin University', 'New Jersey Institute of Technology']","['China', 'United States']",2023-08
2308.10173,Zhixiao Qi,"Zhixiao Qi, Yijiong Yu, Meiqi Tu, Junyi Tan, Yongfeng Huang",FoodGPT: A Large Language Model in Food Testing Domain with Incremental Pre-training and Knowledge Graph Prompt,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Currently, the construction of large language models in specific domains is done by fine-tuning on a base model. Some models also incorporate knowledge bases without the need for pre-training. This is because the base model already contains domain-specific knowledge during the pre-training process. We build a large language model for food testing. Unlike the above approach, a significant amount of data in this domain exists in Scanning format for domain standard documents. In addition, there is a large amount of untrained structured knowledge. Therefore, we introduce an incremental pre-training step to inject this knowledge into a large language model. In this paper, we propose a method for handling structured knowledge and scanned documents in incremental pre-training. To overcome the problem of machine hallucination, we constructe a knowledge graph to serve as an external knowledge base for supporting retrieval in the large language model. It is worth mentioning that this paper is a technical report of our pre-release version, and we will report our specific experimental data in future versions. ","[{'version': 'v1', 'created': 'Sun, 20 Aug 2023 05:58:33 GMT'}]",2023-08-22,"[['Qi', 'Zhixiao', ''], ['Yu', 'Yijiong', ''], ['Tu', 'Meiqi', ''], ['Tan', 'Junyi', ''], ['Huang', 'Yongfeng', '']]",0,1,2023-08-20,1,5,1,0,0,0,ac4ab9b23a002e81c9419c520747a603fbcbb40d,261048937.0,https://www.semanticscholar.org/paper/ac4ab9b23a002e81c9419c520747a603fbcbb40d,arXiv.org,2023.0,13.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2221746779', 'name': 'Zhi Qi'}, {'authorId': '47111778', 'name': 'Yi Yu'}, {'authorId': '2232780428', 'name': 'Meiqi Tu'}, {'authorId': '2110450041', 'name': 'Junyi Tan'}, {'authorId': '2145525387', 'name': 'Yongfeng Huang'}]","['Beijing University of Posts and Telecommunications', 'Tsinghua University']",['China'],2023-08
2308.10204,Haoyuan Wu,"Zhuolun He, Haoyuan Wu, Xinyun Zhang, Xufeng Yao, Su Zheng, Haisheng
  Zheng, Bei Yu",ChatEDA: A Large Language Model Powered Autonomous Agent for EDA,,,,,cs.AR cs.AI,http://creativecommons.org/licenses/by/4.0/,"  The integration of a complex set of Electronic Design Automation (EDA) tools to enhance interoperability is a critical concern for circuit designers. Recent advancements in large language models (LLMs) have showcased their exceptional capabilities in natural language processing and comprehension, offering a novel approach to interfacing with EDA tools. This research paper introduces ChatEDA, an autonomous agent for EDA empowered by a large language model, AutoMage, complemented by EDA tools serving as executors. ChatEDA streamlines the design flow from the Register-Transfer Level (RTL) to the Graphic Data System Version II (GDSII) by effectively managing task planning, script generation, and task execution. Through comprehensive experimental evaluations, ChatEDA has demonstrated its proficiency in handling diverse requirements, and our fine-tuned AutoMage model has exhibited superior performance compared to GPT-4 and other similar LLMs. ","[{'version': 'v1', 'created': 'Sun, 20 Aug 2023 08:32:13 GMT'}]",2023-08-22,"[['He', 'Zhuolun', ''], ['Wu', 'Haoyuan', ''], ['Zhang', 'Xinyun', ''], ['Yao', 'Xufeng', ''], ['Zheng', 'Su', ''], ['Zheng', 'Haisheng', ''], ['Yu', 'Bei', '']]",0,1,2023-08-20,1,7,2,1,0,1,b89c47a3b16196a73b47f41bfb497a56c32c217c,261049069.0,https://www.semanticscholar.org/paper/b89c47a3b16196a73b47f41bfb497a56c32c217c,Workshop on Machine Learning for CAD,2023.0,25.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '8822971', 'name': 'Zhuolun He'}, {'authorId': '2149253203', 'name': 'Haoyuan Wu'}, {'authorId': '2155544084', 'name': 'Xinyun Zhang'}, {'authorId': '2115585295', 'name': 'Xufeng Yao'}, {'authorId': '2218461490', 'name': 'Su Zheng'}, {'authorId': '67219756', 'name': 'Haisheng Zheng'}, {'authorId': '2148266888', 'name': 'Bei Yu'}]","['Shanghai Artificial Intelligence Laboratory', 'Chinese University of Hong Kong']",['China'],2023-08
2308.10252,Yixuan Weng,"Yixuan Weng, Zhiqi Wang, Huanxuan Liao, Shizhu He, Shengping Liu, Kang
  Liu, Jun Zhao",LMTuner: An user-friendly and highly-integrable Training Framework for fine-tuning Large Language Models,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the burgeoning development in the realm of large language models (LLMs), the demand for efficient incremental training tailored to specific industries and domains continues to increase. Currently, the predominantly employed frameworks lack modular design, it often takes a lot of coding work to kickstart the training of LLM. To address this, we present ""LMTuner"", a highly usable, integrable, and scalable system for training LLMs expeditiously and with minimal user-input. LMTuner comprises three main modules - the Interaction, Training, and Inference Modules. We advocate that LMTuner's usability and integrality alleviate the complexities in training large language models. Remarkably, even a novice user could commence training large language models within five minutes. Furthermore, it integrates DeepSpeed frameworks and supports Efficient Fine-Tuning methodologies like Low Rank Adaptation (LoRA), Quantized LoRA (QLoRA), etc., enabling the training of language models scaling from 300M to a whopping 130B parameters using a single server. The LMTuner's homepage (https://wengsyx.github.io/LMTuner/)and screencast video (https://youtu.be/nsXmWOmN3rE) are now publicly available. ","[{'version': 'v1', 'created': 'Sun, 20 Aug 2023 12:42:19 GMT'}]",2023-08-22,"[['Weng', 'Yixuan', ''], ['Wang', 'Zhiqi', ''], ['Liao', 'Huanxuan', ''], ['He', 'Shizhu', ''], ['Liu', 'Shengping', ''], ['Liu', 'Kang', ''], ['Zhao', 'Jun', '']]",0,0,2023-08-20,1,7,2,0,0,0,ed3873864a14ed4f9ee09d3cf70d1ead2fa2be10,261048891.0,https://www.semanticscholar.org/paper/ed3873864a14ed4f9ee09d3cf70d1ead2fa2be10,arXiv.org,2023.0,62.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2142839441', 'name': 'Yixuan Weng'}, {'authorId': '2108332426', 'name': 'Zhiqi Wang'}, {'authorId': '2232817310', 'name': 'Huanxuan Liao'}, {'authorId': '1954845', 'name': 'Shizhu He'}, {'authorId': '2035396', 'name': 'Shengping Liu'}, {'authorId': '2200096', 'name': 'Kang Liu'}, {'authorId': '11447228', 'name': 'Jun Zhao'}]","['University of Chinese Academy of Sciences', 'Chinese Academy of Sciences', 'Tianjin University']",['China'],2023-08
2308.10253,Chi Zhang,"Yanda Li, Chi Zhang, Gang Yu, Zhibin Wang, Bin Fu, Guosheng Lin,
  Chunhua Shen, Ling Chen, Yunchao Wei",StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data,Project page: https://github.com/icoz69/StableLLAVA,,,,cs.CV cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have sparked significant interest in the development of multimodal Large Language Models (LLMs). A primary research objective of such models is to align visual and textual modalities effectively while comprehending human instructions. Current methodologies often rely on annotations derived from benchmark datasets to construct image-dialogue datasets for training purposes, akin to instruction tuning in LLMs. However, these datasets often exhibit domain bias, potentially constraining the generative capabilities of the models. In an effort to mitigate these limitations, we propose a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning. This approach harnesses the power of generative models, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content. This not only provides greater flexibility compared to existing methodologies but also significantly enhances several model capabilities. Our research includes comprehensive experiments conducted on various datasets using the open-source LLAVA model as a testbed for our proposed pipeline. Our results underscore marked enhancements across more than ten commonly assessed capabilities, ","[{'version': 'v1', 'created': 'Sun, 20 Aug 2023 12:43:52 GMT'}]",2023-08-22,"[['Li', 'Yanda', ''], ['Zhang', 'Chi', ''], ['Yu', 'Gang', ''], ['Wang', 'Zhibin', ''], ['Fu', 'Bin', ''], ['Lin', 'Guosheng', ''], ['Shen', 'Chunhua', ''], ['Chen', 'Ling', ''], ['Wei', 'Yunchao', '']]",1,1,2023-08-20,1,9,3,2,0,2,da96ec9c32d63292e506ba8f8ea8e838df998c02,261049617.0,https://www.semanticscholar.org/paper/da96ec9c32d63292e506ba8f8ea8e838df998c02,arXiv.org,2023.0,29.0,2.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2232930725', 'name': 'Yanda Li'}, {'authorId': '144876211', 'name': 'Chi Zhang'}, {'authorId': '2116565951', 'name': 'Gang Yu'}, {'authorId': '2051262469', 'name': 'Zhibin Wang'}, {'authorId': '2107058893', 'name': 'Bin Fu'}, {'authorId': '2604251', 'name': 'Guosheng Lin'}, {'authorId': '12459603', 'name': 'Chunhua Shen'}, {'authorId': '2232790293', 'name': 'Ling Chen'}, {'authorId': '49020088', 'name': 'Yunchao Wei'}]","['Beijing Jiaotong University', 'Zhejiang University', 'Tencent', 'Nanyang Technological University', 'University of Technology Sydney']","['China', 'Singapore', 'Australia']",2023-08
2308.10261,Bo Liu,"Bo Liu, Liming Zhan, Zexin Lu, Yujie Feng, Lei Xue, Xiao-Ming Wu",How Good Are Large Language Models at Out-of-Distribution Detection?,Work in progress,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Out-of-distribution (OOD) detection plays a vital role in enhancing the reliability of machine learning (ML) models. The emergence of large language models (LLMs) has catalyzed a paradigm shift within the ML community, showcasing their exceptional capabilities across diverse natural language processing tasks. While existing research has probed OOD detection with relative small-scale Transformers like BERT, RoBERTa and GPT-2, the stark differences in scales, pre-training objectives, and inference paradigms call into question the applicability of these findings to LLMs. This paper embarks on a pioneering empirical investigation of OOD detection in the domain of LLMs, focusing on LLaMA series ranging from 7B to 65B in size. We thoroughly evaluate commonly-used OOD detectors, scrutinizing their performance in both zero-grad and fine-tuning scenarios. Notably, we alter previous discriminative in-distribution fine-tuning into generative fine-tuning, aligning the pre-training objective of LLMs with downstream tasks. Our findings unveil that a simple cosine distance OOD detector demonstrates superior efficacy, outperforming other OOD detectors. We provide an intriguing explanation for this phenomenon by highlighting the isotropic nature of the embedding spaces of LLMs, which distinctly contrasts with the anisotropic property observed in smaller BERT family models. The new insight enhances our understanding of how LLMs detect OOD data, thereby enhancing their adaptability and reliability in dynamic environments. ","[{'version': 'v1', 'created': 'Sun, 20 Aug 2023 13:15:18 GMT'}, {'version': 'v2', 'created': 'Wed, 23 Aug 2023 16:49:33 GMT'}]",2023-08-24,"[['Liu', 'Bo', ''], ['Zhan', 'Liming', ''], ['Lu', 'Zexin', ''], ['Feng', 'Yujie', ''], ['Xue', 'Lei', ''], ['Wu', 'Xiao-Ming', '']]",0,1,2023-08-20,2,6,1,2,2,0,3cb7621a034f85a7afb3526f6e2570ee6f7e2340,261049411.0,https://www.semanticscholar.org/paper/3cb7621a034f85a7afb3526f6e2570ee6f7e2340,arXiv.org,2023.0,47.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '73548014', 'name': 'Bo Liu'}, {'authorId': '2061239456', 'name': 'Li-Ming Zhan'}, {'authorId': '2220034673', 'name': 'Zexin Lu'}, {'authorId': '2150671546', 'name': 'Yu Feng'}, {'authorId': '2215151344', 'name': 'Lei Xue'}, {'authorId': '2187512110', 'name': 'Xiao-Ming Wu'}]","['Sun Yat-sen University', 'Hong Kong Polytechnic University']","['China', 'Hong Kong']",2023-08
2308.10444,Qi Chen,"Qi Chen, Dexi Liu",Dynamic Strategy Chain: Dynamic Zero-Shot CoT for Long Mental Health Support Generation,,,,,cs.CL cs.AI cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Long counseling Text Generation for Mental health support (LTGM), an innovative and challenging task, aims to provide help-seekers with mental health support through a comprehensive and more acceptable response. The combination of chain-of-thought (CoT) prompting and Large Language Models (LLMs) is employed and get the SOTA performance on various NLP tasks, especially on text generation tasks. Zero-shot CoT prompting is one of the most common methods in CoT prompting. However, in the LTGM task, Zero-shot CoT prompting can not simulate a counselor or provide personalized strategies without effective mental health counseling strategy prompts. To tackle this challenge, we propose a zero-shot Dynamic Strategy Chain (DSC) prompting method. Firstly, we utilize GPT2 to learn the responses written by mental health counselors and dynamically generate mental health counseling strategies tailored to the help-seekers' needs. Secondly, the Zero-shot DSC prompting is constructed according to mental health counseling strategies and the help-seekers' post. Finally, the Zero-shot DSC prompting is employed to guide LLMs in generating more human-like responses for the help-seekers. Both automatic and manual evaluations demonstrate that Zero-shot DSC prompting can deliver more human-like responses than CoT prompting methods on LTGM tasks. ","[{'version': 'v1', 'created': 'Mon, 21 Aug 2023 03:31:20 GMT'}]",2023-08-22,"[['Chen', 'Qi', ''], ['Liu', 'Dexi', '']]",0,1,2023-08-21,1,2,3,1,1,0,96599abdbac3106b89f3d8dd3b26fe9c38a7624f,261048659.0,https://www.semanticscholar.org/paper/96599abdbac3106b89f3d8dd3b26fe9c38a7624f,arXiv.org,2023.0,20.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Psychology', 'source': 's2-fos-model'}]","[{'authorId': '2157955974', 'name': 'Qi Chen'}, {'authorId': '48929153', 'name': 'Dexi Liu'}]",['Jiangxi University of Finance and Economics'],['China'],2023-08
2308.10509,Rong Li,"Teli Ma, Rong Li, Junwei Liang",An Examination of the Compositionality of Large Generative Vision-Language Models,,,,,cs.CL cs.AI cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the success of Large Language Models (LLMs), a surge of Generative Vision-Language Models (GVLMs) have been constructed via multimodal instruction tuning. The tuning recipe substantially deviates from the common contrastive vision-language learning. However, the performance of GVLMs in multimodal compositional reasoning remains largely unexplored, as existing evaluation metrics and benchmarks focus predominantly on assessing contrastive models like CLIP. In this paper, we examine the potential evaluation metrics to assess the GVLMs and hypothesize generative score methods are suitable for evaluating compositionality. In addition, current benchmarks tend to prioritize syntactic correctness over semantics. The presence of morphological bias in these benchmarks can be exploited by GVLMs, leading to ineffective evaluations. To combat this, we define a MorphoBias Score to quantify the morphological bias and propose a novel LLM-based strategy to calibrate the bias. Moreover, a challenging task is added to evaluate the robustness of GVLMs against inherent inclination toward syntactic correctness. We include the calibrated dataset and the task into a new benchmark, namely MOrphologicall De-biased Benchmark (MODE). Our study provides the first unbiased benchmark for the compositionality of GVLMs, facilitating future research in this direction. We will release our code and datasets. ","[{'version': 'v1', 'created': 'Mon, 21 Aug 2023 06:50:29 GMT'}]",2023-08-22,"[['Ma', 'Teli', ''], ['Li', 'Rong', ''], ['Liang', 'Junwei', '']]",0,0,2023-08-21,1,3,3,0,0,0,66d3b7a6561148fd21c364315e67bf9373f50ef7,261049335.0,https://www.semanticscholar.org/paper/66d3b7a6561148fd21c364315e67bf9373f50ef7,arXiv.org,2023.0,48.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1390452961', 'name': 'Teli Ma'}, {'authorId': '2233049504', 'name': 'Rong Li'}, {'authorId': '118150711', 'name': 'Junwei Liang'}]",['Hong Kong University of Science and Technology'],['China'],2023-08
2308.10524,Zhou Daquan,"Daquan Zhou, Kai Wang, Jianyang Gu, Xiangyu Peng, Dongze Lian, Yifan
  Zhang, Yang You, Jiashi Feng",Dataset Quantization,9 pages,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  State-of-the-art deep neural networks are trained with large amounts (millions or even billions) of data. The expensive computation and memory costs make it difficult to train them on limited hardware resources, especially for recent popular large language models (LLM) and computer vision models (CV). Recent popular dataset distillation methods are thus developed, aiming to reduce the number of training samples via synthesizing small-scale datasets via gradient matching. However, as the gradient calculation is coupled with the specific network architecture, the synthesized dataset is biased and performs poorly when used for training unseen architectures. To address these limitations, we present dataset quantization (DQ), a new framework to compress large-scale datasets into small subsets which can be used for training any neural network architectures. Extensive experiments demonstrate that DQ is able to generate condensed small datasets for training unseen network architectures with state-of-the-art compression ratios for lossless model training. To the best of our knowledge, DQ is the first method that can successfully distill large-scale datasets such as ImageNet-1k with a state-of-the-art compression ratio. Notably, with 60% data from ImageNet and 20% data from Alpaca's instruction tuning data, the models can be trained with negligible or no performance drop for both vision tasks (including classification, semantic segmentation, and object detection) as well as language tasks (including instruction tuning tasks such as BBH and DROP). ","[{'version': 'v1', 'created': 'Mon, 21 Aug 2023 07:24:29 GMT'}]",2023-08-22,"[['Zhou', 'Daquan', ''], ['Wang', 'Kai', ''], ['Gu', 'Jianyang', ''], ['Peng', 'Xiangyu', ''], ['Lian', 'Dongze', ''], ['Zhang', 'Yifan', ''], ['You', 'Yang', ''], ['Feng', 'Jiashi', '']]",0,0,2023-08-21,1,8,2,1,0,1,51ef336bb1bb9875d715abb78be93b58f952cb5c,261049434.0,https://www.semanticscholar.org/paper/51ef336bb1bb9875d715abb78be93b58f952cb5c,arXiv.org,2023.0,66.0,7.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '18119920', 'name': 'Daquan Zhou'}, {'authorId': '151491771', 'name': 'Kaixin Wang'}, {'authorId': '123902785', 'name': 'Jianyang Gu'}, {'authorId': '2115815502', 'name': 'Xiang Peng'}, {'authorId': '35180251', 'name': 'Dongze Lian'}, {'authorId': '2108464858', 'name': 'Yifan Zhang'}, {'authorId': '2147330258', 'name': 'Yang You'}, {'authorId': '1698982', 'name': 'Jiashi Feng'}]","['National University of Singapore', 'ByteDance']","['China', 'Singapore']",2023-08
2308.10529,Tianyu Yu,"Tianyu Yu, Chengyue Jiang, Chao Lou, Shen Huang, Xiaobin Wang, Wei
  Liu, Jiong Cai, Yangning Li, Yinghui Li, Kewei Tu, Hai-Tao Zheng, Ningyu
  Zhang, Pengjun Xie, Fei Huang, Yong Jiang",SeqGPT: An Out-of-the-box Large Language Model for Open Domain Sequence Understanding,Initial version of SeqGPT,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have shown impressive ability for open-domain NLP tasks. However, LLMs are sometimes too footloose for natural language understanding (NLU) tasks which always have restricted output and input format. Their performances on NLU tasks are highly related to prompts or demonstrations and are shown to be poor at performing several representative NLU tasks, such as event extraction and entity typing. To this end, we present SeqGPT, a bilingual (i.e., English and Chinese) open-source autoregressive model specially enhanced for open-domain natural language understanding. We express all NLU tasks with two atomic tasks, which define fixed instructions to restrict the input and output format but still ``open'' for arbitrarily varied label sets. The model is first instruction-tuned with extremely fine-grained labeled data synthesized by ChatGPT and then further fine-tuned by 233 different atomic tasks from 152 datasets across various domains. The experimental results show that SeqGPT has decent classification and extraction ability, and is capable of performing language understanding tasks on unseen domains. We also conduct empirical studies on the scaling of data and model size as well as on the transfer across tasks. Our model is accessible at https://github.com/Alibaba-NLP/SeqGPT. ","[{'version': 'v1', 'created': 'Mon, 21 Aug 2023 07:31:19 GMT'}]",2023-08-22,"[['Yu', 'Tianyu', ''], ['Jiang', 'Chengyue', ''], ['Lou', 'Chao', ''], ['Huang', 'Shen', ''], ['Wang', 'Xiaobin', ''], ['Liu', 'Wei', ''], ['Cai', 'Jiong', ''], ['Li', 'Yangning', ''], ['Li', 'Yinghui', ''], ['Tu', 'Kewei', ''], ['Zheng', 'Hai-Tao', ''], ['Zhang', 'Ningyu', ''], ['Xie', 'Pengjun', ''], ['Huang', 'Fei', ''], ['Jiang', 'Yong', '']]",1,1,2023-08-21,1,15,1,1,0,1,8cf37154fc183d16e4c17c86309855248662b709,261048926.0,https://www.semanticscholar.org/paper/8cf37154fc183d16e4c17c86309855248662b709,arXiv.org,2023.0,144.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2256865435', 'name': 'Tianyu Yu'}, {'authorId': '2115486297', 'name': 'Chengyue Jiang'}, {'authorId': '2061806821', 'name': 'Chao Lou'}, {'authorId': '2186268584', 'name': 'Shen Huang'}, {'authorId': '2108114693', 'name': 'Xiaobin Wang'}, {'authorId': '2157222998', 'name': 'Wei Liu'}, {'authorId': '4442130', 'name': 'Jiong Cai'}, {'authorId': '98177814', 'name': 'Y. Li'}, {'authorId': '2110503552', 'name': 'Yinghui Li'}, {'authorId': '40341553', 'name': 'Kewei Tu'}, {'authorId': '16215052', 'name': 'Haitao Zheng'}, {'authorId': '2608639', 'name': 'Ningyu Zhang'}, {'authorId': '35930962', 'name': 'Pengjun Xie'}, {'authorId': '2117426607', 'name': 'Fei Huang'}, {'authorId': '50262192', 'name': 'Yong Jiang'}]","['Alibaba', 'Tsinghua University', 'Zhejiang University', 'ShanghaiTech University']",['China'],2023-08
2308.10585,Dingzirui Wang,"Dingzirui Wang, Longxu Dou, Wenbin Zhang, Junyu Zeng, Wanxiang Che",Exploring Equation as a Better Intermediate Meaning Representation for Numerical Reasoning,,,,,cs.CL,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Numerical reasoning is vital for natural language processing models to understand and process numerical information in real-world scenarios. Most current methods first generate the Intermediate Meaning Representations (IMRs) of questions and then generate answers. Current SOTA methods generate programs as IMRs with large language models (LLMs). Intuitively, equations have fewer restrictions and closer semantics to the question than programs, leading to higher generation accuracy. However, current LLMs generate equations worse than programs, where we assume that the equation data is rare in pre-training data compared to programs. So in this paper, we try to use equations as IMRs to solve the numerical reasoning task by addressing two problems: (1) Theoretically, how to prove that the equation is an IMR with higher generation accuracy than programs; (2) Empirically, how to improve the generation accuracy of equations with LLMs. For the first problem, we propose and prove a proposition to theoretically compare the generation accuracy of different IMRs. For the second problem, we present a method called Boosting Numerical Reason\textbfing by Decomposing the Generation of Equations (Bridge), which can improve the accuracy of LLMs in generating equations as IMRs by reducing the tendency of generating constant expressions and programs. Our method improves the performance by 2.2%, 0.9%, and 1.7% on GSM8K, SVAMP, and Algebra datasets compared to the previous state-of-the-art methods under the single reasoning path setting. Our codes and prompts are released in https://github.com/zirui-HIT/Bridge_for_Numerical_Reasoning. ","[{'version': 'v1', 'created': 'Mon, 21 Aug 2023 09:35:33 GMT'}]",2023-08-22,"[['Wang', 'Dingzirui', ''], ['Dou', 'Longxu', ''], ['Zhang', 'Wenbin', ''], ['Zeng', 'Junyu', ''], ['Che', 'Wanxiang', '']]",0,0,2023-08-21,1,5,1,0,0,0,13ac3b7f1ace63c76c1e081898369f8e0505411c,261049149.0,https://www.semanticscholar.org/paper/13ac3b7f1ace63c76c1e081898369f8e0505411c,arXiv.org,2023.0,43.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2159084043', 'name': 'Dingzirui Wang'}, {'authorId': '49093992', 'name': 'Longxu Dou'}, {'authorId': '2157260786', 'name': 'Wenbin Zhang'}, {'authorId': '2072983550', 'name': 'Junyu Zeng'}, {'authorId': '2256319', 'name': 'Wanxiang Che'}]","['Harbin Institute of Technology', 'University of Science and Technology Beijing']",['China'],2023-08
2308.10755,Bin Wang,"Conghui He, Zhenjiang Jin, Chao Xu, Jiantao Qiu, Bin Wang, Wei Li,
  Hang Yan, Jiaqi Wang, Dahua Lin",WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models,Technical Report,,,,cs.CL cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the development of large models, leading to the creation of numerous impressive large language models(LLMs) and multimodal large language models (MLLMs). These cutting-edge models owe their remarkable performance to high-quality data. However, the details of the training data used in leading paradigms are often kept confidential. This lack of transparency, coupled with the scarcity of open-source data, impedes further developments within the community. As a response, this paper presents ""Wan Juan"", a large-scale multimodal dataset composed of both Chinese and English data, collected from a wide range of web sources. The dataset incorporates text, image-text, and video modalities, with a total volume exceeding 2TB. It was utilized in the training of InternLM, a model that demonstrated significant advantages in multi-dimensional evaluations when compared to models of a similar scale. All data can be accessed at https://opendatalab.org.cn/WanJuan1.0. ","[{'version': 'v1', 'created': 'Mon, 21 Aug 2023 14:40:48 GMT'}, {'version': 'v2', 'created': 'Tue, 22 Aug 2023 02:57:45 GMT'}, {'version': 'v3', 'created': 'Fri, 15 Sep 2023 09:52:14 GMT'}]",2023-09-18,"[['He', 'Conghui', ''], ['Jin', 'Zhenjiang', ''], ['Xu', 'Chao', ''], ['Qiu', 'Jiantao', ''], ['Wang', 'Bin', ''], ['Li', 'Wei', ''], ['Yan', 'Hang', ''], ['Wang', 'Jiaqi', ''], ['Lin', 'Dahua', '']]",1,1,2023-08-21,3,9,2,2,0,2,afb39ed837db8750dd1c3b2a54ad442372c106b2,261049100.0,https://www.semanticscholar.org/paper/afb39ed837db8750dd1c3b2a54ad442372c106b2,arXiv.org,2023.0,18.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '3486481', 'name': 'Conghui He'}, {'authorId': '2232929856', 'name': 'Zhenjiang Jin'}, {'authorId': '46200183', 'name': 'Chaoxi Xu'}, {'authorId': '1848503', 'name': 'Jiantao Qiu'}, {'authorId': '2256857728', 'name': 'Bin Wang'}, {'authorId': '2256598803', 'name': 'Wei Li'}, {'authorId': '146948229', 'name': 'Hang Yan'}, {'authorId': '2143074003', 'name': 'Jiaqi Wang'}, {'authorId': '2116442295', 'name': 'Da Lin'}]",['Shanghai Artificial Intelligence Laboratory'],['China'],2023-08
2308.10848,Weize Chen,"Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen
  Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, Zhiyuan Liu, Maosong
  Sun, Jie Zhou",AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors in Agents,Work in progress,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment. Hence, inspired by human group dynamics, we propose a multi-agent framework \framework that can collaboratively and dynamically adjust its composition as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that \framework framework can effectively deploy multi-agent groups that outperform a single agent. Furthermore, we delve into the emergence of social behaviors among individual agents within a group during collaborative task accomplishment. In view of these behaviors, we discuss some possible strategies to leverage positive ones and mitigate negative ones for improving the collaborative potential of multi-agent groups. Our codes for \framework will soon be released at \url{https://github.com/OpenBMB/AgentVerse}. ","[{'version': 'v1', 'created': 'Mon, 21 Aug 2023 16:47:11 GMT'}]",2023-08-22,"[['Chen', 'Weize', ''], ['Su', 'Yusheng', ''], ['Zuo', 'Jingwei', ''], ['Yang', 'Cheng', ''], ['Yuan', 'Chenfei', ''], ['Qian', 'Chen', ''], ['Chan', 'Chi-Min', ''], ['Qin', 'Yujia', ''], ['Lu', 'Yaxi', ''], ['Xie', 'Ruobing', ''], ['Liu', 'Zhiyuan', ''], ['Sun', 'Maosong', ''], ['Zhou', 'Jie', '']]",0,0,2023-08-21,1,13,1,0,0,0,0a69b914499000e0a5b1a552164bc9ebefdc5701,261048935.0,https://www.semanticscholar.org/paper/0a69b914499000e0a5b1a552164bc9ebefdc5701,arXiv.org,2023.0,56.0,15.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2109136284', 'name': 'Weize Chen'}, {'authorId': '48576745', 'name': 'Yusheng Su'}, {'authorId': '2057461656', 'name': 'Jingwei Zuo'}, {'authorId': '3443627', 'name': 'Cheng Yang'}, {'authorId': '2232928180', 'name': 'Chenfei Yuan'}, {'authorId': '2214580084', 'name': 'Cheng Qian'}, {'authorId': '2151547817', 'name': 'Chi-Min Chan'}, {'authorId': '50625437', 'name': 'Yujia Qin'}, {'authorId': '2191753738', 'name': 'Ya-Ting Lu'}, {'authorId': '3360722', 'name': 'Ruobing Xie'}, {'authorId': '2141313179', 'name': 'Zhiyuan Liu'}, {'authorId': '1753344', 'name': 'Maosong Sun'}, {'authorId': '49640256', 'name': 'Jie Zhou'}]","['Beijing University of Posts and Telecommunications', 'Tencent', 'Tsinghua University']",['China'],2023-08
2308.10855,Shulin Huang,"Shulin Huang, Shirong Ma, Yinghui Li, Mengzuo Huang, Wuhe Zou, Weidong
  Zhang, Hai-Tao Zheng",LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles,Work in progress,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the continuous evolution and refinement of LLMs, they are endowed with impressive logical reasoning or vertical thinking capabilities. But can they think out of the box? Do they possess proficient lateral thinking abilities? Following the setup of Lateral Thinking Puzzles, we propose a novel evaluation benchmark, LatEval, which assesses the model's lateral thinking within an interactive framework. In our benchmark, we challenge LLMs with 2 aspects: the quality of questions posed by the model and the model's capability to integrate information for problem-solving. We find that nearly all LLMs struggle with employing lateral thinking during interactions. For example, even the most advanced model, GPT-4, exhibits the advantage to some extent, yet still maintain a noticeable gap when compared to human. This evaluation benchmark provides LLMs with a highly challenging and distinctive task that is crucial to an effective AI assistant. ","[{'version': 'v1', 'created': 'Mon, 21 Aug 2023 16:49:40 GMT'}, {'version': 'v2', 'created': 'Thu, 21 Sep 2023 17:14:13 GMT'}]",2023-09-22,"[['Huang', 'Shulin', ''], ['Ma', 'Shirong', ''], ['Li', 'Yinghui', ''], ['Huang', 'Mengzuo', ''], ['Zou', 'Wuhe', ''], ['Zhang', 'Weidong', ''], ['Zheng', 'Hai-Tao', '']]",0,1,2023-08-21,2,7,1,1,0,1,194e8cb8f2b4673d467c01d010afba31551cb0da,261049428.0,https://www.semanticscholar.org/paper/194e8cb8f2b4673d467c01d010afba31551cb0da,arXiv.org,2023.0,48.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2177124783', 'name': 'Shulin Huang'}, {'authorId': '2118867306', 'name': 'Shirong Ma'}, {'authorId': '2110503552', 'name': 'Yinghui Li'}, {'authorId': '138270843', 'name': 'Mengzuo Huang'}, {'authorId': '2057329020', 'name': 'Wuhe Zou'}, {'authorId': '2108134355', 'name': 'Weidong Zhang'}, {'authorId': '16215052', 'name': 'Haitao Zheng'}]","['NetEase', 'Peng Cheng Laboratory', 'Tsinghua University']",['China'],2023-08
2308.11131,Jianghao Lin,"Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang
  Quan, Ruiming Tang, Yong Yu, Weinan Zhang",ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation,Under Review,,,,cs.IR cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful information from a textual context of long user behavior sequence, even if the length of context is far from reaching the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval-enhanced Large Language models (ReLLa) for recommendation tasks in both zero-shot and few-shot settings. For zero-shot recommendation, we perform semantic user behavior retrieval (SUBR) to improve the data quality of testing samples, which greatly reduces the difficulty for LLMs to extract the essential knowledge from user behavior sequences. As for few-shot recommendation, we further design retrieval-enhanced instruction tuning (ReiT) by adopting SUBR as a data augmentation technique for training samples. Specifically, we develop a mixed training dataset consisting of both the original data samples and their retrieval-enhanced counterparts. We conduct extensive experiments on a real-world public dataset (i.e., MovieLens-1M) to demonstrate the superiority of ReLLa compared with existing baseline models, as well as its capability for lifelong sequential behavior comprehension. ","[{'version': 'v1', 'created': 'Tue, 22 Aug 2023 02:25:04 GMT'}]",2023-08-23,"[['Lin', 'Jianghao', ''], ['Shan', 'Rong', ''], ['Zhu', 'Chenxu', ''], ['Du', 'Kounianhua', ''], ['Chen', 'Bo', ''], ['Quan', 'Shigang', ''], ['Tang', 'Ruiming', ''], ['Yu', 'Yong', ''], ['Zhang', 'Weinan', '']]",0,0,2023-08-22,1,9,2,0,0,0,429e6c09eeadf54e2b245b8f2cddfbf157f9da4c,261065228.0,https://www.semanticscholar.org/paper/429e6c09eeadf54e2b245b8f2cddfbf157f9da4c,arXiv.org,2023.0,98.0,6.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2144908858', 'name': 'Jianghao Lin'}, {'authorId': '122215415', 'name': 'Rongjie Shan'}, {'authorId': '2115802321', 'name': 'Chenxu Zhu'}, {'authorId': '1780721965', 'name': 'Kounianhua Du'}, {'authorId': '92633145', 'name': 'Bo Chen'}, {'authorId': '2232956640', 'name': 'Shigang Quan'}, {'authorId': '2824766', 'name': 'Ruiming Tang'}, {'authorId': '2156098229', 'name': 'Yong Yu'}, {'authorId': '2108309275', 'name': 'Weinan Zhang'}]","['Shanghai Jiao Tong University', 'Huawei Technologies (China)', 'Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, and Weinan Zhang.']",['China'],2023-08
2308.11136,Simiao Zhang,"Jitao Bai, Simiao Zhang, Zhonghao Chen",Is There Any Social Principle for LLM-Based Agents?,"3 pages, 1 figure",,,,cs.CY cs.AI cs.SI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Focus on Large Language Model based agents should involve more than ""human-centered"" alignment or application. We argue that more attention should be paid to the agent itself and discuss the potential of social sciences for agents. ","[{'version': 'v1', 'created': 'Tue, 22 Aug 2023 02:32:14 GMT'}]",2023-08-23,"[['Bai', 'Jitao', ''], ['Zhang', 'Simiao', ''], ['Chen', 'Zhonghao', '']]",0,0,2023-08-22,1,3,3,0,0,0,39ac5440871989b0f9a00bab2ad7fc5c00bd81ee,261064903.0,https://www.semanticscholar.org/paper/39ac5440871989b0f9a00bab2ad7fc5c00bd81ee,arXiv.org,2023.0,18.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Biology', 'source': 's2-fos-model'}]","[{'authorId': '2106583832', 'name': 'Jitao Bai'}, {'authorId': '2108339438', 'name': 'Simiao Zhang'}, {'authorId': '2144174222', 'name': 'Zhong Chen'}]","['East China Normal University', 'Tianjin University']",['China'],2023-08
2308.11148,Junyi Lu,"Junyi Lu, Lei Yu, Xiaojia Li, Li Yang, Chun Zuo",LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning,"Accepted to the 34th IEEE International Symposium on Software
  Reliability Engineering (ISSRE 2023)",,,,cs.SE cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The automation of code review activities, a long-standing pursuit in software engineering, has been primarily addressed by numerous domain-specific pre-trained models. Despite their success, these models frequently demand extensive resources for pre-training from scratch. In contrast, Large Language Models (LLMs) provide an intriguing alternative, given their remarkable capabilities when supplemented with domain-specific knowledge. However, their potential for automating code review tasks remains largely unexplored.   In response to this research gap, we present LLaMA-Reviewer, an innovative framework that leverages the capabilities of LLaMA, a popular LLM, in the realm of code review. Mindful of resource constraints, this framework employs parameter-efficient fine-tuning (PEFT) methods, delivering high performance while using less than 1% of trainable parameters.   An extensive evaluation of LLaMA-Reviewer is conducted on two diverse, publicly available datasets. Notably, even with the smallest LLaMA base model consisting of 6.7B parameters and a limited number of tuning epochs, LLaMA-Reviewer equals the performance of existing code-review-focused models.   The ablation experiments provide insights into the influence of various fine-tuning process components, including input representation, instruction tuning, and different PEFT methods. To foster continuous progress in this field, the code and all PEFT-weight plugins have been made open-source. ","[{'version': 'v1', 'created': 'Tue, 22 Aug 2023 03:10:40 GMT'}, {'version': 'v2', 'created': 'Tue, 5 Sep 2023 02:28:49 GMT'}]",2023-09-06,"[['Lu', 'Junyi', ''], ['Yu', 'Lei', ''], ['Li', 'Xiaojia', ''], ['Yang', 'Li', ''], ['Zuo', 'Chun', '']]",0,0,2023-08-22,2,5,3,1,1,0,d955956378b40b23fa4b34098662c54a3b1fd64d,261064667.0,https://www.semanticscholar.org/paper/d955956378b40b23fa4b34098662c54a3b1fd64d,IEEE International Symposium on Software Reliability Engineering,2023.0,68.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2151165499', 'name': 'Jun Lu'}, {'authorId': '2109352263', 'name': 'Lei Yu'}, {'authorId': '2232971540', 'name': 'Xiaojia Li'}, {'authorId': '2153204688', 'name': 'Li Yang'}, {'authorId': '31841234', 'name': 'Chun Zuo'}]","['Tsinghua University', 'University of Chinese Academy of Sciences', 'Sinosoft Company Limited, Beijing, China', 'Chinese Academy of Sciences']",['China'],2023-08
2308.11276,Shansong Liu,"Shansong Liu, Atin Sakkeer Hussain, Chenshuo Sun, Ying Shan",Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning,,,,,cs.SD cs.AI cs.CL cs.MM eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity of large-scale publicly available music datasets with natural language captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA), capable of answering music-related questions and generating captions for music files. Our model utilizes audio representations from a pretrained MERT model to extract music features. However, obtaining a suitable dataset for training the MU-LLaMA model remains challenging, as existing publicly accessible audio question answering datasets lack the necessary depth for open-ended music question answering. To fill this gap, we present a methodology for generating question-answer pairs from existing audio captioning datasets and introduce the MusicQA Dataset designed for answering open-ended music-related questions. The experiments demonstrate that the proposed MU-LLaMA model, trained on our designed MusicQA dataset, achieves outstanding performance in both music question answering and music caption generation across various metrics, outperforming current state-of-the-art (SOTA) models in both fields and offering a promising advancement in the T2M-Gen research field. ","[{'version': 'v1', 'created': 'Tue, 22 Aug 2023 08:43:33 GMT'}]",2023-08-23,"[['Liu', 'Shansong', ''], ['Hussain', 'Atin Sakkeer', ''], ['Sun', 'Chenshuo', ''], ['Shan', 'Ying', '']]",0,0,2023-08-22,1,4,5,1,1,0,a33b437618be733fea7176bd98e18b6362af0838,261064622.0,https://www.semanticscholar.org/paper/a33b437618be733fea7176bd98e18b6362af0838,arXiv.org,2023.0,30.0,6.0,0.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '9511592', 'name': 'Shansong Liu'}, {'authorId': '2232951291', 'name': 'Atin Sakkeer Hussain'}, {'authorId': '49322048', 'name': 'Chenshuo Sun'}, {'authorId': '2193802892', 'name': 'Yin Shan'}]","['National University of Singapore', 'Tencent']","['China', 'Singapore']",2023-08
2308.11339,Siyi Hu,"Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun,
  Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang, Junge
  Zhang, Feng Yin, Yitao Liang, Yaodong Yang",ProAgent: Building Proactive Cooperative AI with Large Language Models,,,,,cs.AI cs.LG cs.MA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Building AIs with adaptive behaviors in human-AI cooperation stands as a pivotal focus in AGI research. Current methods for developing cooperative agents predominantly rely on learning-based methods, where policy generalization heavily hinges on past interactions with specific teammates. These approaches constrain the agent's capacity to recalibrate its strategy when confronted with novel teammates. We propose \textbf{ProAgent}, a novel framework that harnesses large language models (LLMs) to fashion a \textit{pro}active \textit{agent} empowered with the ability to anticipate teammates' forthcoming decisions and formulate enhanced plans for itself. ProAgent excels at cooperative reasoning with the capacity to dynamically adapt its behavior to enhance collaborative efforts with teammates. Moreover, the ProAgent framework exhibits a high degree of modularity and interpretability, facilitating seamless integration to address a wide array of coordination scenarios. Experimental evaluations conducted within the framework of \textit{Overcook-AI} unveil the remarkable performance superiority of ProAgent, outperforming five methods based on self-play and population-based training in cooperation with AI agents. Further, when cooperating with human proxy models, its performance exhibits an average improvement exceeding 10\% compared to the current state-of-the-art, COLE. The advancement was consistently observed across diverse scenarios involving interactions with both AI agents of varying characteristics and human counterparts. These findings inspire future research for human-robot collaborations. For a hands-on demonstration, please visit \url{https://pku-proagent.github.io}. ","[{'version': 'v1', 'created': 'Tue, 22 Aug 2023 10:36:56 GMT'}, {'version': 'v2', 'created': 'Mon, 28 Aug 2023 01:50:00 GMT'}]",2023-08-29,"[['Zhang', 'Ceyao', ''], ['Yang', 'Kaijie', ''], ['Hu', 'Siyi', ''], ['Wang', 'Zihao', ''], ['Li', 'Guanghe', ''], ['Sun', 'Yihang', ''], ['Zhang', 'Cheng', ''], ['Zhang', 'Zhaowei', ''], ['Liu', 'Anji', ''], ['Zhu', 'Song-Chun', ''], ['Chang', 'Xiaojun', ''], ['Zhang', 'Junge', ''], ['Yin', 'Feng', ''], ['Liang', 'Yitao', ''], ['Yang', 'Yaodong', '']]",0,0,2023-08-22,2,15,3,0,0,0,33a4ca94b6fb91ef688767f42a4692822a947996,261064959.0,https://www.semanticscholar.org/paper/33a4ca94b6fb91ef688767f42a4692822a947996,arXiv.org,2023.0,58.0,10.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2000868582', 'name': 'Ceyao Zhang'}, {'authorId': '2233046737', 'name': 'Kaijie Yang'}, {'authorId': '4443613', 'name': 'Siyi Hu'}, {'authorId': '47196237', 'name': 'Zihao Wang'}, {'authorId': '2151299337', 'name': 'Guanghe Li'}, {'authorId': '2196448510', 'name': 'Y. Sun'}, {'authorId': '145107889', 'name': 'Chen Zhang'}, {'authorId': '2174174943', 'name': 'Zhaowei Zhang'}, {'authorId': '70097297', 'name': 'Anji Liu'}, {'authorId': '2148575818', 'name': 'Song-Chun Zhu'}, {'authorId': '2232955435', 'name': 'Xiaojun Chang'}, {'authorId': '2136182354', 'name': 'Junge Zhang'}, {'authorId': '5951460', 'name': 'F. Yin'}, {'authorId': '2397352', 'name': 'Yitao Liang'}, {'authorId': '47796324', 'name': 'Yaodong Yang'}]","['Chinese Academy of Sciences', 'Chinese University of Hong Kong, Shenzhen', 'University of Technology Sydney', 'National Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI)', 'Peking University']","['China', 'Australia']",2023-08
2308.11396,Kaiwen Ning,"Zibin Zheng, Kaiwen Ning, Jiachi Chen, Yanlin Wang, Wenqing Chen,
  Lianghong Guo and Weicheng Wang",Towards an Understanding of Large Language Models in Software Engineering Tasks,,,,,cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have drawn widespread attention and research due to their astounding performance in tasks such as text generation and reasoning. Derivative products, like ChatGPT, have been extensively deployed and highly sought after. Meanwhile, the evaluation and optimization of LLMs in software engineering tasks, such as code generation, have become a research focus. However, there is still a lack of systematic research on the application and evaluation of LLMs in the field of software engineering. Therefore, this paper is the first to comprehensively investigate and collate the research and products combining LLMs with software engineering, aiming to answer two questions: (1) What are the current integrations of LLMs with software engineering? (2) Can LLMs effectively handle software engineering tasks? To find the answers, we have collected related literature as extensively as possible from seven mainstream databases, and selected 123 papers for analysis. We have categorized these papers in detail and reviewed the current research status of LLMs from the perspective of seven major software engineering tasks, hoping this will help researchers better grasp the research trends and address the issues when applying LLMs. Meanwhile, we have also organized and presented papers with evaluation content to reveal the performance and effectiveness of LLMs in various software engineering tasks, providing guidance for researchers and developers to optimize. ","[{'version': 'v1', 'created': 'Tue, 22 Aug 2023 12:37:29 GMT'}]",2023-08-23,"[['Zheng', 'Zibin', ''], ['Ning', 'Kaiwen', ''], ['Chen', 'Jiachi', ''], ['Wang', 'Yanlin', ''], ['Chen', 'Wenqing', ''], ['Guo', 'Lianghong', ''], ['Wang', 'Weicheng', '']]",1,1,2023-08-22,1,7,1,1,0,1,681f9009e22c947007b53455e9f8f22e29209010,261064777.0,https://www.semanticscholar.org/paper/681f9009e22c947007b53455e9f8f22e29209010,arXiv.org,2023.0,174.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2148256392', 'name': 'Zibin Zheng'}, {'authorId': '2115304', 'name': 'Kai-Chun Ning'}, {'authorId': '3413969', 'name': 'Jiachi Chen'}, {'authorId': '2214155529', 'name': 'Yanlin Wang'}, {'authorId': '2108997748', 'name': 'Wenqing Chen'}, {'authorId': '2217902484', 'name': 'Lianghong Guo'}, {'authorId': '2233023641', 'name': 'Weicheng Wang'}]",['Sun Yat-sen University'],['China'],2023-08
2308.11432,Xu Chen,"Lei Wang and Chen Ma and Xueyang Feng and Zeyu Zhang and Hao Yang and
  Jingsen Zhang and Zhiyuan Chen and Jiakai Tang and Xu Chen and Yankai Lin and
  Wayne Xin Zhao and Zhewei Wei and Ji-Rong Wen",A Survey on Large Language Model based Autonomous Agents,"35 pages, 5 figures, 3 tables",,,,cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM-based autonomous agents from a holistic perspective. More specifically, we first discuss the construction of LLM-based autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository of relevant references at https://github.com/Paitesanshi/LLM-Agent-Survey. ","[{'version': 'v1', 'created': 'Tue, 22 Aug 2023 13:30:37 GMT'}, {'version': 'v2', 'created': 'Thu, 7 Sep 2023 04:42:48 GMT'}]",2023-09-08,"[['Wang', 'Lei', ''], ['Ma', 'Chen', ''], ['Feng', 'Xueyang', ''], ['Zhang', 'Zeyu', ''], ['Yang', 'Hao', ''], ['Zhang', 'Jingsen', ''], ['Chen', 'Zhiyuan', ''], ['Tang', 'Jiakai', ''], ['Chen', 'Xu', ''], ['Lin', 'Yankai', ''], ['Zhao', 'Wayne Xin', ''], ['Wei', 'Zhewei', ''], ['Wen', 'Ji-Rong', '']]",0,0,2023-08-22,2,13,2,0,0,0,28c6ac721f54544162865f41c5692e70d61bccab,261064713.0,https://www.semanticscholar.org/paper/28c6ac721f54544162865f41c5692e70d61bccab,arXiv.org,2023.0,187.0,53.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2152509786', 'name': 'Lei Wang'}, {'authorId': '33754380', 'name': 'Chengbang Ma'}, {'authorId': '2163940136', 'name': 'Xueyang Feng'}, {'authorId': '2223760889', 'name': 'Zeyu Zhang'}, {'authorId': '11325431', 'name': 'Hao-ran Yang'}, {'authorId': '2144163813', 'name': 'Jingsen Zhang'}, {'authorId': '2128141801', 'name': 'Zhi-Yang Chen'}, {'authorId': '144010962', 'name': 'Jiakai Tang'}, {'authorId': '2144230136', 'name': 'Xu Chen'}, {'authorId': '2149202150', 'name': 'Yankai Lin'}, {'authorId': '2542603', 'name': 'Wayne Xin Zhao'}, {'authorId': '12457830', 'name': 'Zhewei Wei'}, {'authorId': '153693432', 'name': 'Ji-rong Wen'}]",['Renmin University of China'],['China'],2023-08
2308.11521,Zhenhua Wang,"Zhenhua Wang, Wei Xie, Kai Chen, Baosheng Wang, Zhiwen Gui, Enze Wang",Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models,"Serious errors were found in the experiment, which may lead to the
  overturning of the overall conclusions of the paper",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs), such as ChatGPT, have emerged with astonishing capabilities approaching artificial general intelligence. While providing convenience for various societal needs, LLMs have also lowered the cost of generating harmful content. Consequently, LLM developers have deployed semantic-level defenses to recognize and reject prompts that may lead to inappropriate content. Unfortunately, these defenses are not foolproof, and some attackers have crafted ""jailbreak"" prompts that temporarily hypnotize the LLM into forgetting content defense rules and answering any improper questions. To date, there is no clear explanation of the principles behind these semantic-level attacks and defenses in both industry and academia.   This paper investigates the LLM jailbreak problem and proposes an automatic jailbreak method for the first time. We propose the concept of a semantic firewall and provide three technical implementation approaches. Inspired by the attack that penetrates traditional firewalls through reverse tunnels, we introduce a ""self-deception"" attack that can bypass the semantic firewall by inducing LLM to generate prompts that facilitate jailbreak. We generated a total of 2,520 attack payloads in six languages (English, Russian, French, Spanish, Chinese, and Arabic) across seven virtual scenarios, targeting the three most common types of violations: violence, hate, and pornography. The experiment was conducted on two models, namely the GPT-3.5-Turbo and GPT-4. The success rates on the two models were 86.2% and 67%, while the failure rates were 4.7% and 2.2%, respectively. This highlighted the effectiveness of the proposed attack method. All experimental code and raw data will be released as open-source to inspire future research. We believe that manipulating AI behavior through carefully crafted prompts will become an important research direction in the future. ","[{'version': 'v1', 'created': 'Wed, 16 Aug 2023 09:04:36 GMT'}, {'version': 'v2', 'created': 'Fri, 25 Aug 2023 00:25:06 GMT'}]",2023-08-28,"[['Wang', 'Zhenhua', ''], ['Xie', 'Wei', ''], ['Chen', 'Kai', ''], ['Wang', 'Baosheng', ''], ['Gui', 'Zhiwen', ''], ['Wang', 'Enze', '']]",1,1,2023-08-16,2,6,3,3,0,3,13ee4f41413ff8e48a33390225baa0ce53c3a392,261065299.0,https://www.semanticscholar.org/paper/13ee4f41413ff8e48a33390225baa0ce53c3a392,arXiv.org,2023.0,62.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108330405', 'name': 'Zhenhua Wang'}, {'authorId': '2113722106', 'name': 'Wei Xie'}, {'authorId': '2208124975', 'name': 'Kai Chen'}, {'authorId': '2118640332', 'name': 'Baosheng Wang'}, {'authorId': '2232958249', 'name': 'Zhiwen Gui'}, {'authorId': '152482017', 'name': 'Enze Wang'}]","['National University of Defense Technology', 'Institute of Information Engineering']",['China'],2023-08
2308.11561,Yifei Su,"Yifei Su, Dong An, Yuan Xu, Kehan Chen, Yan Huang",Target-Grounded Graph-Aware Transformer for Aerial Vision-and-Dialog Navigation,,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  This report details the methods of the winning entry of the AVDN Challenge in ICCV CLVL 2023. The competition addresses the Aerial Navigation from Dialog History (ANDH) task, which requires a drone agent to associate dialog history with aerial observations to reach the destination. For better cross-modal grounding abilities of the drone agent, we propose a Target-Grounded Graph-Aware Transformer (TG-GAT) framework. Concretely, TG-GAT first leverages a graph-aware transformer to capture spatiotemporal dependency, which benefits navigation state tracking and robust action planning. In addition,an auxiliary visual grounding task is devised to boost the agent's awareness of referred landmarks. Moreover, a hybrid augmentation strategy based on large language models is utilized to mitigate data scarcity limitations. Our TG-GAT framework won the AVDN Challenge, with 2.2% and 3.0% absolute improvements over the baseline on SPL and SR metrics, respectively. The code is available at https://github.com/yifeisu/TG-GAT. ","[{'version': 'v1', 'created': 'Tue, 22 Aug 2023 16:45:35 GMT'}, {'version': 'v2', 'created': 'Wed, 23 Aug 2023 05:53:43 GMT'}, {'version': 'v3', 'created': 'Fri, 25 Aug 2023 13:57:28 GMT'}, {'version': 'v4', 'created': 'Mon, 4 Sep 2023 09:33:35 GMT'}]",2023-09-06,"[['Su', 'Yifei', ''], ['An', 'Dong', ''], ['Xu', 'Yuan', ''], ['Chen', 'Kehan', ''], ['Huang', 'Yan', '']]",0,0,2023-08-22,4,5,1,0,0,0,d09ca4cbff21d4aaefbd3eb84eb073f1847190aa,261064816.0,https://www.semanticscholar.org/paper/d09ca4cbff21d4aaefbd3eb84eb073f1847190aa,arXiv.org,2023.0,31.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2157302359', 'name': 'Yi-Chiao Su'}, {'authorId': '65990035', 'name': 'Dongyan An'}, {'authorId': '2110356235', 'name': 'Yuan Xu'}, {'authorId': '2152955178', 'name': 'Kehan Chen'}, {'authorId': '2232951530', 'name': 'Yan Huang'}]","['University of Chinese Academy of Sciences', 'Chinese Academy of Sciences']",['China'],2023-08
2308.11578,Zixing Zhang,"Zixing Zhang, Liyizhe Peng, Tao Pang, Jing Han, Huan Zhao, Bjorn W.
  Schuller",Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  After the inception of emotion recognition or affective computing, it has increasingly become an active research topic due to its broad applications. Over the past couple of decades, emotion recognition models have gradually migrated from statistically shallow models to neural network-based deep models, which can significantly boost the performance of emotion recognition models and consistently achieve the best results on different benchmarks. Therefore, in recent years, deep models have always been considered the first option for emotion recognition. However, the debut of large language models (LLMs), such as ChatGPT, has remarkably astonished the world due to their emerged capabilities of zero/few-shot learning, in-context learning, chain-of-thought, and others that are never shown in previous deep models. In the present paper, we comprehensively investigate how the LLMs perform in emotion recognition in terms of diverse aspects, including in-context learning, few-short learning, accuracy, generalisation, and explanation. Moreover, we offer some insights and pose other potential challenges, hoping to ignite broader discussions about enhancing emotion recognition in the new era of advanced and generalised large models. ","[{'version': 'v1', 'created': 'Mon, 21 Aug 2023 13:14:32 GMT'}]",2023-08-23,"[['Zhang', 'Zixing', ''], ['Peng', 'Liyizhe', ''], ['Pang', 'Tao', ''], ['Han', 'Jing', ''], ['Zhao', 'Huan', ''], ['Schuller', 'Bjorn W.', '']]",1,1,2023-08-21,1,6,3,1,0,1,3fecb6df989c51592cb1a1a737f259c5e3b0eab1,261065047.0,https://www.semanticscholar.org/paper/3fecb6df989c51592cb1a1a737f259c5e3b0eab1,arXiv.org,2023.0,69.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1742291', 'name': 'Zixing Zhang'}, {'authorId': '2233057038', 'name': 'Liyizhe Peng'}, {'authorId': '2232956590', 'name': 'Tao Pang'}, {'authorId': '144610333', 'name': 'Jing Han'}, {'authorId': '2153034438', 'name': 'Huan Zhao'}, {'authorId': '145411696', 'name': 'Bjrn Schuller'}]","['Huxley Bldg., London SW7 2AZ, UK,', 'Hunan University', 'Imperial College London', 'University of Cambridge', 'University of Augsburg']","['Germany', 'China', 'United Kingdom']",2023-08
2308.11584,Zhonghua Zheng,"Zhonghua Zheng, Lizi Liao, Yang Deng, Liqiang Nie",Building Emotional Support Chatbots in the Era of LLMs,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The integration of emotional support into various conversational scenarios presents profound societal benefits, such as social interactions, mental health counseling, and customer service. However, there are unsolved challenges that hinder real-world applications in this field, including limited data availability and the absence of well-accepted model training paradigms. This work endeavors to navigate these challenges by harnessing the capabilities of Large Language Models (LLMs). We introduce an innovative methodology that synthesizes human insights with the computational prowess of LLMs to curate an extensive emotional support dialogue dataset. Our approach is initiated with a meticulously designed set of dialogues spanning diverse scenarios as generative seeds. By utilizing the in-context learning potential of ChatGPT, we recursively generate an ExTensible Emotional Support dialogue dataset, named ExTES. Following this, we deploy advanced tuning techniques on the LLaMA model, examining the impact of diverse training strategies, ultimately yielding an LLM meticulously optimized for emotional support interactions. An exhaustive assessment of the resultant model showcases its proficiency in offering emotional support, marking a pivotal step in the realm of emotional support bots and paving the way for subsequent research and implementations. ","[{'version': 'v1', 'created': 'Thu, 17 Aug 2023 10:49:18 GMT'}]",2023-08-23,"[['Zheng', 'Zhonghua', ''], ['Liao', 'Lizi', ''], ['Deng', 'Yang', ''], ['Nie', 'Liqiang', '']]",1,1,2023-08-17,1,4,2,2,1,1,f77c114553bc851aab4fff535570a9e83a18227e,261065100.0,https://www.semanticscholar.org/paper/f77c114553bc851aab4fff535570a9e83a18227e,arXiv.org,2023.0,69.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2115549719', 'name': 'Zhonghua Zheng'}, {'authorId': '2053831159', 'name': 'Lizi Liao'}, {'authorId': '145843537', 'name': 'Yang Deng'}, {'authorId': '143982887', 'name': 'Liqiang Nie'}]","['Harbin Institute of Technology', 'National University of Singapore', 'Singapore Management University']","['China', 'Singapore']",2023-08
2308.11592,Hao Feng Mr.,"Hao Feng, Zijian Wang, Jingqun Tang, Jinghui Lu, Wengang Zhou,
  Houqiang Li, Can Huang","UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding",,,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the era of Large Language Models (LLMs), tremendous strides have been made in the field of multimodal understanding. However, existing advanced algorithms are limited to effectively utilizing the immense representation capabilities and rich world knowledge inherent to these large pre-trained models, and the beneficial connections among tasks within the context of text-rich scenarios have not been sufficiently explored. In this work, we introduce UniDoc, a novel multimodal model equipped with text detection and recognition capabilities, which are deficient in existing approaches. Moreover, UniDoc capitalizes on the beneficial interactions among tasks to enhance the performance of each individual task. To implement UniDoc, we perform unified multimodal instruct tuning on the contributed large-scale instruction following datasets. Quantitative and qualitative experimental results show that UniDoc sets state-of-the-art scores across multiple challenging benchmarks. To the best of our knowledge, this is the first large multimodal model capable of simultaneous text detection, recognition, spotting, and understanding. ","[{'version': 'v1', 'created': 'Sat, 19 Aug 2023 17:32:34 GMT'}, {'version': 'v2', 'created': 'Sat, 2 Sep 2023 04:28:42 GMT'}]",2023-09-06,"[['Feng', 'Hao', ''], ['Wang', 'Zijian', ''], ['Tang', 'Jingqun', ''], ['Lu', 'Jinghui', ''], ['Zhou', 'Wengang', ''], ['Li', 'Houqiang', ''], ['Huang', 'Can', '']]",0,0,2023-08-19,2,7,2,0,0,0,3647e7c48fa4888ba584d8b8a5c5813e6ee48366,261065237.0,https://www.semanticscholar.org/paper/3647e7c48fa4888ba584d8b8a5c5813e6ee48366,arXiv.org,2023.0,30.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2113276516', 'name': 'Hao Feng'}, {'authorId': '50219006', 'name': 'Zijian Wang'}, {'authorId': '2112226871', 'name': 'Ji Tang'}, {'authorId': '2115404023', 'name': 'Jinghui Lu'}, {'authorId': '38272296', 'name': 'Wen-gang Zhou'}, {'authorId': '2144406784', 'name': 'Houqiang Li'}, {'authorId': '2179698374', 'name': 'Can Huang'}]","['ByteDance', 'University of Science and Technology of China']",['China'],2023-08
2308.11654,Bingxin Wang,"Bingxin Wang, Xiaowen Fu, Yuan Lan, Luchan Zhang, and Yang Xiang",Large Transformers are Better EEG Learners,,,,,eess.SP cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Pre-trained large transformer models have achieved remarkable performance in the fields of natural language processing and computer vision. Since the magnitude of available labeled electroencephalogram (EEG) data is much lower than that of text and image data, it is difficult for transformer models pre-trained from EEG to be developed as large as GPT-4 100T to fully unleash the potential of this architecture. In this paper, we show that transformers pre-trained from images as well as text can be directly fine-tuned for EEG-based prediction tasks. We design AdaCE, plug-and-play Adapters for Converting EEG data into image as well as text forms, to fine-tune pre-trained vision and language transformers. The proposed AdaCE module is highly effective for fine-tuning pre-trained transformers while achieving state-of-the-art performance on diverse EEG-based prediction tasks. For example, AdaCE on the pre-trained Swin-Transformer achieves 99.6%, an absolute improvement of 9.2%, on the EEG-decoding task of human activity recognition (UCI HAR). Furthermore, we empirically show that applying the proposed AdaCE to fine-tune larger pre-trained models can achieve better performance on EEG-based predicting tasks, indicating the potential of our adapters for even larger transformers. The plug-and-play AdaCE module can be applied to fine-tuning most of the popular pre-trained transformers on many other time-series data with multiple channels, not limited to EEG data and the models we use. Our code will be available at https://github.com/wangbxj1234/AdaCE. ","[{'version': 'v1', 'created': 'Sun, 20 Aug 2023 12:54:17 GMT'}]",2023-08-24,"[['Wang', 'Bingxin', ''], ['Fu', 'Xiaowen', ''], ['Lan', 'Yuan', ''], ['Zhang', 'Luchan', ''], ['Xiang', 'Yang', '']]",0,1,2023-08-20,1,5,3,1,0,1,9678db830c1276ff8e93a425efab0f4e4ed13c84,261075973.0,https://www.semanticscholar.org/paper/9678db830c1276ff8e93a425efab0f4e4ed13c84,arXiv.org,2023.0,31.0,0.0,0.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '119568216', 'name': 'Bingxin Wang'}, {'authorId': '2208045059', 'name': 'Xiao-Ying Fu'}, {'authorId': '2016549200', 'name': 'Yuan Lan'}, {'authorId': '36794873', 'name': 'Luchan Zhang'}, {'authorId': '2068340540', 'name': 'Yang Xiang'}]","['Shenzhen University', 'Hong Kong University of Science and Technology']",['China'],2023-08
2308.11761,Xintao Wang,"Xintao Wang, Qianwen Yang, Yongting Qiu, Jiaqing Liang, Qianyu He,
  Zhouhong Gu, Yanghua Xiao, Wei Wang",KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases,"21 pages, 10 figures",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have demonstrated impressive impact in the field of natural language processing, but they still struggle with several issues regarding, such as completeness, timeliness, faithfulness and adaptability. While recent efforts have focuses on connecting LLMs with external knowledge sources, the integration of knowledge bases (KBs) remains understudied and faces several challenges. In this paper, we introduce KnowledGPT, a comprehensive framework to bridge LLMs with various knowledge bases, facilitating both the retrieval and storage of knowledge. The retrieval process employs the program of thought prompting, which generates search language for KBs in code format with pre-defined functions for KB operations. Besides retrieval, KnowledGPT offers the capability to store knowledge in a personalized KB, catering to individual user demands. With extensive experiments, we show that by integrating LLMs with KBs, KnowledGPT properly answers a broader range of questions requiring world knowledge compared with vanilla LLMs, utilizing both knowledge existing in widely-known KBs and extracted into personalized KBs. ","[{'version': 'v1', 'created': 'Thu, 17 Aug 2023 13:07:00 GMT'}]",2023-08-24,"[['Wang', 'Xintao', ''], ['Yang', 'Qianwen', ''], ['Qiu', 'Yongting', ''], ['Liang', 'Jiaqing', ''], ['He', 'Qianyu', ''], ['Gu', 'Zhouhong', ''], ['Xiao', 'Yanghua', ''], ['Wang', 'Wei', '']]",0,1,2023-08-17,1,8,2,0,0,0,49408c5e1ac75854f1580e561384df2be870d559,261076315.0,https://www.semanticscholar.org/paper/49408c5e1ac75854f1580e561384df2be870d559,arXiv.org,2023.0,59.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108003209', 'name': 'Xintao Wang'}, {'authorId': '2117860658', 'name': 'Qian Yang'}, {'authorId': '40703435', 'name': 'Yongting Qiu'}, {'authorId': '3366523', 'name': 'Jiaqing Liang'}, {'authorId': '2152880833', 'name': 'Qi He'}, {'authorId': '2160631240', 'name': 'Zhouhong Gu'}, {'authorId': '2116642640', 'name': 'Yanghua Xiao'}, {'authorId': '49337119', 'name': 'W. Wang'}]",['Fudan University'],['China'],2023-08
2308.11767,Ahmed Abdeen Hamed Ph.D,Ahmed Abdeen Hamed and Xindong Wu,Improving Detection of ChatGPT-Generated Fake Science Using Real Publication Text: Introducing xFakeBibs a Supervised-Learning Network Algorithm,"14 pages, 6 figures, 4 tables, 2 algorithms",,,,cs.CL cs.IR,http://creativecommons.org/licenses/by/4.0/,"  ChatGPT is becoming a new reality. In this paper, we show how to distinguish ChatGPT-generated publications from counterparts produced by scientists. Using a newly designed supervised Machine Learning algorithm, we demonstrate how to detect machine-generated publications from those produced by scientists. The algorithm was trained using 100 real publication abstracts, followed by a 10-fold calibration approach to establish a lower-upper bound range of acceptance. In the comparison with ChatGPT content, it was evident that ChatGPT contributed merely 23\% of the bigram content, which is less than 50\% of any of the other 10 calibrating folds. This analysis highlights a significant disparity in technical terms where ChatGPT fell short of matching real science. When categorizing the individual articles, the xFakeBibs algorithm accurately identified 98 out of 100 publications as fake, with 2 articles incorrectly classified as real publications. Though this work introduced an algorithmic approach that detected the ChatGPT-generated fake science with a high degree of accuracy, it remains challenging to detect all fake records. This work is indeed a step in the right direction to counter fake science and misinformation. ","[{'version': 'v1', 'created': 'Tue, 15 Aug 2023 23:22:37 GMT'}]",2023-08-24,"[['Hamed', 'Ahmed Abdeen', ''], ['Wu', 'Xindong', '']]",1,1,2023-08-15,1,2,2,1,0,1,dc118b205135647abf20630001f49953c39f9cd7,261076066.0,https://www.semanticscholar.org/paper/dc118b205135647abf20630001f49953c39f9cd7,arXiv.org,2023.0,40.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2851243', 'name': 'A. Hamed'}, {'authorId': '145739804', 'name': 'Xin Wu'}]","['Zhejiang Lab', 'Sano Centre for Computational Medicine, Clinical Data Science, Cracow, 30-072, Poland']","['China', 'Poland']",2023-08
2308.11914,Ziyi Tang,"Ziyi Tang, Ruilin Wang, Weixing Chen, Keze Wang, Yang Liu, Tianshui
  Chen, Liang Lin",Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge Reasoning via Promoting Causal Consistency in LLMs,"8 pages, 3 figures. 4 tables",,,,cs.AI cs.MA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite advancements in LLMs, knowledge-based reasoning remains a longstanding issue due to the fragility of knowledge recall and inference. Existing methods primarily encourage LLMs to autonomously plan and solve problems or to extensively sample reasoning chains without addressing the conceptual and inferential fallacies. Attempting to alleviate inferential fallacies and drawing inspiration from multi-agent collaboration, we present a framework to increase faithfulness and causality for knowledge-based reasoning. Specifically, we propose to employ multiple intelligent agents (i.e., reasoners and an evaluator) to work collaboratively in a reasoning-and-consensus paradigm for elevated reasoning faithfulness. The reasoners focus on providing solutions with human-like causality to solve open-domain problems. On the other hand, the \textit{evaluator} agent scrutinizes if a solution is deducible from a non-causal perspective and if it still holds when challenged by a counterfactual candidate. According to the extensive and comprehensive evaluations on a variety of knowledge reasoning tasks (e.g., science question answering and commonsense reasoning), our framework outperforms all compared state-of-the-art approaches by large margins. ","[{'version': 'v1', 'created': 'Wed, 23 Aug 2023 04:59:21 GMT'}, {'version': 'v2', 'created': 'Mon, 4 Sep 2023 10:15:51 GMT'}]",2023-09-06,"[['Tang', 'Ziyi', ''], ['Wang', 'Ruilin', ''], ['Chen', 'Weixing', ''], ['Wang', 'Keze', ''], ['Liu', 'Yang', ''], ['Chen', 'Tianshui', ''], ['Lin', 'Liang', '']]",0,1,2023-08-23,2,7,2,0,0,0,04dd492b506e48b7dd91ecb1e1fdb80c1ce30e34,261076364.0,https://www.semanticscholar.org/paper/04dd492b506e48b7dd91ecb1e1fdb80c1ce30e34,arXiv.org,2023.0,43.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Philosophy', 'source': 's2-fos-model'}]","[{'authorId': '2112518736', 'name': 'Ziyi Tang'}, {'authorId': '2151036102', 'name': 'Rui Wang'}, {'authorId': '2108946830', 'name': 'Weixing Chen'}, {'authorId': '2124615864', 'name': 'Keze Wang'}, {'authorId': '47909156', 'name': 'Y. Liu'}, {'authorId': '1765674', 'name': 'Tianshui Chen'}, {'authorId': '2110902045', 'name': 'Liang Lin'}]","['Sun Yat-sen University', 'Guangdong University of Technology']",['China'],2023-08
2308.12028,Hao Chen,"Chen hao, Xie Runfeng, Cui Xiangyang, Yan Zhou, Wang Xin, Xuan
  Zhanwei, Zhang Kai",LKPNR: LLM and KG for Personalized News Recommendation Framework,,,,,cs.IR cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Accurately recommending candidate news articles to users is a basic challenge faced by personalized news recommendation systems. Traditional methods are usually difficult to grasp the complex semantic information in news texts, resulting in unsatisfactory recommendation results. Besides, these traditional methods are more friendly to active users with rich historical behaviors. However, they can not effectively solve the ""long tail problem"" of inactive users. To address these issues, this research presents a novel general framework that combines Large Language Models (LLM) and Knowledge Graphs (KG) into semantic representations of traditional methods. In order to improve semantic understanding in complex news texts, we use LLMs' powerful text understanding ability to generate news representations containing rich semantic information. In addition, our method combines the information about news entities and mines high-order structural information through multiple hops in KG, thus alleviating the challenge of long tail distribution. Experimental results demonstrate that compared with various traditional models, the framework significantly improves the recommendation effect. The successful integration of LLM and KG in our framework has established a feasible path for achieving more accurate personalized recommendations in the news field. Our code is available at https://github.com/Xuan-ZW/LKPNR. ","[{'version': 'v1', 'created': 'Wed, 23 Aug 2023 09:39:18 GMT'}]",2023-08-24,"[['hao', 'Chen', ''], ['Runfeng', 'Xie', ''], ['Xiangyang', 'Cui', ''], ['Zhou', 'Yan', ''], ['Xin', 'Wang', ''], ['Zhanwei', 'Xuan', ''], ['Kai', 'Zhang', '']]",0,0,2023-08-23,1,7,2,0,0,0,b034b79a61513439ce3e2bca4cb90c55757af81e,261076481.0,https://www.semanticscholar.org/paper/b034b79a61513439ce3e2bca4cb90c55757af81e,arXiv.org,2023.0,36.0,1.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2257354591', 'name': 'Hao Chen'}, {'authorId': '116067006', 'name': 'Runfeng Xie'}, {'authorId': '2444361', 'name': 'Xia Cui'}, {'authorId': '2110096616', 'name': 'Zhou Yan'}, {'authorId': None, 'name': 'Xin Wang'}, {'authorId': '2233088576', 'name': 'Zhanwei Xuan'}, {'authorId': '2158520757', 'name': 'Kai Zhang'}]","[""State Key Laboratory of Communication Content Cognition, People's Daily Online, Beijing 100733, China""]",['China'],2023-08
2308.12032,Ming Li,"Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng,
  Jianzong Wang, Tianyi Zhou, Jing Xiao",From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the realm of Large Language Models, the balance between instruction data quality and quantity has become a focal point. Recognizing this, we introduce a self-guided methodology for LLMs to autonomously discern and select cherry samples from vast open-source datasets, effectively minimizing manual curation and potential cost for instruction tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD) metric, emerges as a pivotal tool to identify discrepancies between a model's expected responses and its autonomous generation prowess. Through the adept application of IFD, cherry samples are pinpointed, leading to a marked uptick in model training efficiency. Empirical validations on renowned datasets like Alpaca and WizardLM underpin our findings; with a mere 10% of conventional data input, our strategy showcases improved results. This synthesis of self-guided cherry-picking and the IFD metric signifies a transformative leap in the optimization of LLMs, promising both efficiency and resource-conscious advancements. Codes, data, and models are available: https://github.com/MingLiiii/Cherry_LLM ","[{'version': 'v1', 'created': 'Wed, 23 Aug 2023 09:45:29 GMT'}, {'version': 'v2', 'created': 'Fri, 8 Sep 2023 05:11:01 GMT'}, {'version': 'v3', 'created': 'Fri, 15 Sep 2023 20:33:44 GMT'}]",2023-09-19,"[['Li', 'Ming', ''], ['Zhang', 'Yong', ''], ['Li', 'Zhitao', ''], ['Chen', 'Jiuhai', ''], ['Chen', 'Lichang', ''], ['Cheng', 'Ning', ''], ['Wang', 'Jianzong', ''], ['Zhou', 'Tianyi', ''], ['Xiao', 'Jing', '']]",0,0,2023-08-23,3,9,1,1,0,1,e3052ebca5eeae6a8a73e44517903d39746f5f3a,261076515.0,https://www.semanticscholar.org/paper/e3052ebca5eeae6a8a73e44517903d39746f5f3a,arXiv.org,2023.0,37.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Business', 'source': 's2-fos-model'}]","[{'authorId': '2150655891', 'name': 'Ming Li'}, {'authorId': '2144289768', 'name': 'Yong Zhang'}, {'authorId': '2111336489', 'name': 'Zhitao Li'}, {'authorId': '1391200710', 'name': 'Jiuhai Chen'}, {'authorId': '2108451006', 'name': 'Lichang Chen'}, {'authorId': '145292435', 'name': 'Ning Cheng'}, {'authorId': '66063851', 'name': 'Jianzong Wang'}, {'authorId': '2213956781', 'name': 'Tianyi Zhou'}, {'authorId': '91353860', 'name': 'Jing Xiao'}]","['https://github.com/MingLiiii/Cherry LLM', 'Ping An Technology (Shenzhen) Co., Ltd., China', 'University of Maryland, College Park']","['China', 'United States']",2023-08
2308.12033,Chenrui Zhang,"Chenrui Zhang, Lin Liu, Jinpeng Wang, Chuyuan Wang, Xiao Sun, Hongyu
  Wang, Mingchen Cai",PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine,"8 pages, 4 figures",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As an effective tool for eliciting the power of Large Language Models (LLMs), prompting has recently demonstrated unprecedented abilities across a variety of complex tasks. To further improve the performance, prompt ensemble has attracted substantial interest for tackling the hallucination and instability of LLMs. However, existing methods usually adopt a two-stage paradigm, which requires a pre-prepared set of prompts with substantial manual effort, and is unable to perform directed optimization for different weak learners. In this paper, we propose a simple, universal, and automatic method named PREFER (Pompt Ensemble learning via Feedback-Reflect-Refine) to address the stated limitations. Specifically, given the fact that weak learners are supposed to focus on hard examples during boosting, PREFER builds a feedback mechanism for reflecting on the inadequacies of existing weak learners. Based on this, the LLM is required to automatically synthesize new prompts for iterative refinement. Moreover, to enhance stability of the prompt effect evaluation, we propose a novel prompt bagging method involving forward and backward thinking, which is superior to majority voting and is beneficial for both feedback and weight calculation in boosting. Extensive experiments demonstrate that our PREFER achieves state-of-the-art performance in multiple types of tasks by a significant margin. We have made our code publicly available. ","[{'version': 'v1', 'created': 'Wed, 23 Aug 2023 09:46:37 GMT'}]",2023-08-24,"[['Zhang', 'Chenrui', ''], ['Liu', 'Lin', ''], ['Wang', 'Jinpeng', ''], ['Wang', 'Chuyuan', ''], ['Sun', 'Xiao', ''], ['Wang', 'Hongyu', ''], ['Cai', 'Mingchen', '']]",0,0,2023-08-23,1,7,2,0,0,0,f53a4f34757d1f237446b4d887d5323f2a17ed02,261075887.0,https://www.semanticscholar.org/paper/f53a4f34757d1f237446b4d887d5323f2a17ed02,arXiv.org,2023.0,30.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1491625476', 'name': 'Chenrui Zhang'}, {'authorId': '2166025432', 'name': 'Lina Liu'}, {'authorId': '48093158', 'name': 'Jinpeng Wang'}, {'authorId': '2118820179', 'name': 'Chuyuan Wang'}, {'authorId': '2168803085', 'name': 'Xiaodi Sun'}, {'authorId': '2109800642', 'name': 'Hongyu Wang'}, {'authorId': '2149926941', 'name': 'Mingchen Cai'}]","['Beijing Jiaotong University', 'Meituan']",['China'],2023-08
2308.12038,Jinyi Hu,"Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen,
  Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue,
  Dahai Li, Zhiyuan Liu, Maosong Sun",Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages,https://github.com/OpenBMB/VisCPM.git,,,,cs.CL cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Recently there has been a significant surge in multimodal learning in terms of both image-to-text and text-to-image generation. However, the success is typically limited to English, leaving other languages largely behind. Building a competitive counterpart in other languages is highly challenging due to the low-resource nature of non-English multimodal data (i.e., lack of large-scale, high-quality image-text data). In this work, we propose MPM, an effective training paradigm for training large multimodal models in low-resource languages. MPM demonstrates that Multilingual language models can Pivot zero-shot Multimodal learning across languages. Specifically, based on a strong multilingual large language model, multimodal models pretrained on English-only image-text data can well generalize to other languages in a zero-shot manner for both image-to-text and text-to-image generation, even surpassing models trained on image-text data in native languages. Taking Chinese as a practice of MPM, we build large multimodal models VisCPM in image-to-text and text-to-image generation, which achieve state-of-the-art (open-source) performance in Chinese. To facilitate future research, we open-source codes and model weights at https://github.com/OpenBMB/VisCPM.git. ","[{'version': 'v1', 'created': 'Wed, 23 Aug 2023 09:55:41 GMT'}]",2023-08-24,"[['Hu', 'Jinyi', ''], ['Yao', 'Yuan', ''], ['Wang', 'Chongyi', ''], ['Wang', 'Shan', ''], ['Pan', 'Yinxu', ''], ['Chen', 'Qianyu', ''], ['Yu', 'Tianyu', ''], ['Wu', 'Hanghao', ''], ['Zhao', 'Yue', ''], ['Zhang', 'Haoye', ''], ['Han', 'Xu', ''], ['Lin', 'Yankai', ''], ['Xue', 'Jiao', ''], ['Li', 'Dahai', ''], ['Liu', 'Zhiyuan', ''], ['Sun', 'Maosong', '']]",0,0,2023-08-23,1,16,2,0,0,0,1245ef1926416d649b62323975c6fa22dfb885ee,261076491.0,https://www.semanticscholar.org/paper/1245ef1926416d649b62323975c6fa22dfb885ee,arXiv.org,2023.0,78.0,2.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '92837695', 'name': 'Jinyi Hu'}, {'authorId': '1390925224', 'name': 'Yuan Yao'}, {'authorId': '2146309007', 'name': 'Chong Wang'}, {'authorId': '2205366189', 'name': 'Shanonan Wang'}, {'authorId': '47304253', 'name': 'Yinxu Pan'}, {'authorId': '2157954216', 'name': 'Qi-An Chen'}, {'authorId': '2256865435', 'name': 'Tianyu Yu'}, {'authorId': '2180430182', 'name': 'Han Wu'}, {'authorId': None, 'name': 'Yue Zhao'}, {'authorId': '2233320353', 'name': 'Haoye Zhang'}, {'authorId': '48506411', 'name': 'Xu Han'}, {'authorId': '2427350', 'name': 'Yankai Lin'}, {'authorId': '2233089123', 'name': 'Jiao Xue'}, {'authorId': '2144118403', 'name': 'Dahai Li'}, {'authorId': '2141313179', 'name': 'Zhiyuan Liu'}, {'authorId': '1753344', 'name': 'Maosong Sun'}]","['Beijing University of Posts and Telecommunications', 'Tsinghua University', 'Renmin University of China', 'Zhihu Inc.']",['China'],2023-08
2308.12067,Lai Wei,"Lai Wei, Zihao Jiang, Weiran Huang, Lichao Sun",InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4,,,,,cs.LG cs.AI cs.CL cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal large language models acquire their instruction-following capabilities through a two-stage training process: pre-training on image-text pairs and fine-tuning on supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6% of the instruction-following data used in the alignment dataset for MiniGPT-4. We first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present a simple and effective data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations (e.g., visual question answering, GPT-4 preference). Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient to enable multimodal large language models to generate better output. ","[{'version': 'v1', 'created': 'Wed, 23 Aug 2023 11:27:30 GMT'}]",2023-08-24,"[['Wei', 'Lai', ''], ['Jiang', 'Zihao', ''], ['Huang', 'Weiran', ''], ['Sun', 'Lichao', '']]",0,1,2023-08-23,1,4,4,1,0,1,d7e92d03dfa5427c0c5ef2b59de54733e0589606,261075922.0,https://www.semanticscholar.org/paper/d7e92d03dfa5427c0c5ef2b59de54733e0589606,arXiv.org,2023.0,44.0,8.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2110973198', 'name': 'Lai Wei'}, {'authorId': '31089377', 'name': 'Zihao Jiang'}, {'authorId': '8007867', 'name': 'Weiran Huang'}, {'authorId': '2219704055', 'name': 'Lichao Sun'}]","['Shanghai Jiao Tong University', 'Lehigh University']","['China', 'United States']",2023-08
2308.12097,Yijin Liu,"Yijin Liu, Xianfeng Zeng, Fandong Meng, Jie Zhou",Instruction Position Matters in Sequence Generation with Large Language Models,"Codes and results are at
  https://github.com/Adaxry/Post-Instruction/tree/main",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) are capable of performing conditional sequence generation tasks, such as translation or summarization, through instruction fine-tuning. The fine-tuning data is generally sequentially concatenated from a specific task instruction, an input sentence, and the corresponding response. Considering the locality modeled by the self-attention mechanism of LLMs, these models face the risk of instruction forgetting when generating responses for long input sentences. To mitigate this issue, we propose enhancing the instruction-following capability of LLMs by shifting the position of task instructions after the input sentences. Theoretical analysis suggests that our straightforward method can alter the model's learning focus, thereby emphasizing the training of instruction-following capabilities. Concurrently, experimental results demonstrate that our approach consistently outperforms traditional settings across various model scales (1B / 7B / 13B) and different sequence generation tasks (translation and summarization), without any additional data or annotation costs. Notably, our method significantly improves the zero-shot performance on conditional sequence generation, e.g., up to 9.7 BLEU points on WMT zero-shot translation tasks. ","[{'version': 'v1', 'created': 'Wed, 23 Aug 2023 12:36:57 GMT'}]",2023-08-24,"[['Liu', 'Yijin', ''], ['Zeng', 'Xianfeng', ''], ['Meng', 'Fandong', ''], ['Zhou', 'Jie', '']]",0,0,2023-08-23,1,4,1,0,0,0,639928ed560350e2700cd582057a675e86707ddb,261076308.0,https://www.semanticscholar.org/paper/639928ed560350e2700cd582057a675e86707ddb,arXiv.org,2023.0,45.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '46398775', 'name': 'Yanjun Liu'}, {'authorId': '2152293624', 'name': 'Xianfeng Zeng'}, {'authorId': '33427918', 'name': 'Fandong Meng'}, {'authorId': '48128428', 'name': 'Jie Zhou'}]",['Tencent'],['China'],2023-08
2308.12219,Jiasheng Ye,"Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, Quanquan Gu",Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning,added references,,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream language tasks. We further discover that instruction finetuning can elicit zero-shot and few-shot in-context learning abilities that help tackle many unseen tasks by following natural language instructions, and show promise in advanced and challenging abilities such as reasoning. ","[{'version': 'v1', 'created': 'Wed, 23 Aug 2023 16:01:12 GMT'}, {'version': 'v2', 'created': 'Fri, 25 Aug 2023 16:32:31 GMT'}]",2023-08-28,"[['Ye', 'Jiasheng', ''], ['Zheng', 'Zaixiang', ''], ['Bao', 'Yu', ''], ['Qian', 'Lihua', ''], ['Gu', 'Quanquan', '']]",0,0,2023-08-23,2,5,3,0,0,0,9cbbb250a565228ba328038ee7944b89cff53e84,261076029.0,https://www.semanticscholar.org/paper/9cbbb250a565228ba328038ee7944b89cff53e84,arXiv.org,2023.0,105.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2153258452', 'name': 'Jiasheng Ye'}, {'authorId': '24018493', 'name': 'Zaixiang Zheng'}, {'authorId': '145854784', 'name': 'Yu Bao'}, {'authorId': '2072789464', 'name': 'Lihua Qian'}, {'authorId': '144966687', 'name': 'Quanquan Gu'}]",['Fudan University'],['China'],2023-08
2308.12241,Junling Liu,"Junling Liu, Chao Liu, Peilin Zhou, Qichen Ye, Dading Chong, Kang
  Zhou, Yueqi Xie, Yuwei Cao, Shoujin Wang, Chenyu You, Philip S.Yu",LLMRec: Benchmarking Large Language Models on Recommendation Task,,,,,cs.IR cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, the fast development of Large Language Models (LLMs) such as ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. However, the application of LLMs in the recommendation domain has not been thoroughly investigated. To bridge this gap, we propose LLMRec, a LLM-based recommender system designed for benchmarking LLMs on various recommendation tasks. Specifically, we benchmark several popular off-the-shelf LLMs, such as ChatGPT, LLaMA, ChatGLM, on five recommendation tasks, including rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. Furthermore, we investigate the effectiveness of supervised finetuning to improve LLMs' instruction compliance ability. The benchmark results indicate that LLMs displayed only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation. However, they demonstrated comparable performance to state-of-the-art methods in explainability-based tasks. We also conduct qualitative evaluations to further evaluate the quality of contents generated by different models, and the results show that LLMs can truly understand the provided information and generate clearer and more reasonable results. We aspire that this benchmark will serve as an inspiration for researchers to delve deeper into the potential of LLMs in enhancing recommendation performance. Our codes, processed data and benchmark results are available at https://github.com/williamliujl/LLMRec. ","[{'version': 'v1', 'created': 'Wed, 23 Aug 2023 16:32:54 GMT'}]",2023-08-24,"[['Liu', 'Junling', ''], ['Liu', 'Chao', ''], ['Zhou', 'Peilin', ''], ['Ye', 'Qichen', ''], ['Chong', 'Dading', ''], ['Zhou', 'Kang', ''], ['Xie', 'Yueqi', ''], ['Cao', 'Yuwei', ''], ['Wang', 'Shoujin', ''], ['You', 'Chenyu', ''], ['Yu', 'Philip S.', '']]",1,1,2023-08-23,1,11,2,3,2,1,85722b13631d9846866d45ff2bfc2a2fe1026ac8,261076297.0,https://www.semanticscholar.org/paper/85722b13631d9846866d45ff2bfc2a2fe1026ac8,arXiv.org,2023.0,60.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2218869839', 'name': 'Junling Liu'}, {'authorId': '3741691', 'name': 'Chao-Hong Liu'}, {'authorId': '1800462890', 'name': 'Peilin Zhou'}, {'authorId': '2190432576', 'name': 'Qichen Ye'}, {'authorId': '52290752', 'name': 'Dading Chong'}, {'authorId': '2165702320', 'name': 'Kangan Zhou'}, {'authorId': '2154871075', 'name': 'Yueqi Xie'}, {'authorId': '150346771', 'name': 'Yuwei Cao'}, {'authorId': '2116951322', 'name': 'Shoujin Wang'}, {'authorId': '2061592207', 'name': 'Chenyu You'}, {'authorId': '2233087809', 'name': 'Philip S.Yu'}]","['Hong Kong University of Science and Technology', 'Yale University', 'University of Illinois at Chicago', 'University of Technology Sydney', 'Peking University']","['China', 'United States', 'Australia']",2023-08
2308.12247,Chiwun Yang,"Timothy Chu, Zhao Song, Chiwun Yang",How to Protect Copyright Data in Optimization of Large Language Models?,,,,,cs.LG cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Large language models (LLMs) and generative AI have played a transformative role in computer research and applications. Controversy has arisen as to whether these models output copyrighted data, which can occur if the data the models are trained on is copyrighted. LLMs are built on the transformer neural network architecture, which in turn relies on a mathematical computation called Attention that uses the softmax function.   In this paper, we show that large language model training and optimization can be seen as a softmax regression problem. We then establish a method of efficiently performing softmax regression, in a way that prevents the regression function from generating copyright data. This establishes a theoretical method of training large language models in a way that avoids generating copyright data. ","[{'version': 'v1', 'created': 'Wed, 23 Aug 2023 16:48:04 GMT'}]",2023-08-24,"[['Chu', 'Timothy', ''], ['Song', 'Zhao', ''], ['Yang', 'Chiwun', '']]",0,0,2023-08-23,1,3,2,0,0,0,761af20e7966e8a7d899975a332ac7eee3f92116,261076348.0,https://www.semanticscholar.org/paper/761af20e7966e8a7d899975a332ac7eee3f92116,arXiv.org,2023.0,84.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '39145583', 'name': 'T. Chu'}, {'authorId': '2214956470', 'name': 'Zhao Song'}, {'authorId': '2233128774', 'name': 'Chiwun Yang'}]",['Sun Yat-sen University'],['China'],2023-08
2308.12261,Vijay Viswanathan,"Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Wu,
  Graham Neubig",Prompt2Model: Generating Deployable Models from Natural Language Instructions,8 pages,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model. ","[{'version': 'v1', 'created': 'Wed, 23 Aug 2023 17:28:21 GMT'}]",2023-08-24,"[['Viswanathan', 'Vijay', ''], ['Zhao', 'Chenyang', ''], ['Bertsch', 'Amanda', ''], ['Wu', 'Tongshuang', ''], ['Neubig', 'Graham', '']]",0,1,2023-08-23,1,5,1,1,0,1,e69684fb06a7b1fe621d7ef0c97fc2ca0e122c43,261075905.0,https://www.semanticscholar.org/paper/e69684fb06a7b1fe621d7ef0c97fc2ca0e122c43,arXiv.org,2023.0,44.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2061499362', 'name': 'Vijay Viswanathan'}, {'authorId': '2023526', 'name': 'Chenyang Zhao'}, {'authorId': '2138301112', 'name': 'Amanda Bertsch'}, {'authorId': '35232494', 'name': 'Tongshuang Sherry Wu'}, {'authorId': '2075395906', 'name': 'Graham Neubig'}]","['Carnegie Mellon University', 'Tsinghua University']","['China', 'United States']",2023-08
2308.12488,Rui Mao,"Rui Mao, Guanyi Chen, Xulang Zhang, Frank Guerin, Erik Cambria",GPTEval: A Survey on Assessments of ChatGPT and GPT-4,,,,,cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The emergence of ChatGPT has generated much speculation in the press about its potential to disrupt social and economic systems. Its astonishing language ability has aroused strong curiosity among scholars about its performance in different domains. There have been many studies evaluating the ability of ChatGPT and GPT-4 in different tasks and disciplines. However, a comprehensive review summarizing the collective assessment findings is lacking. The objective of this survey is to thoroughly analyze prior assessments of ChatGPT and GPT-4, focusing on its language and reasoning abilities, scientific knowledge, and ethical considerations. Furthermore, an examination of the existing evaluation methods is conducted, offering several recommendations for future research in evaluating large language models. ","[{'version': 'v1', 'created': 'Thu, 24 Aug 2023 01:17:16 GMT'}]",2023-08-25,"[['Mao', 'Rui', ''], ['Chen', 'Guanyi', ''], ['Zhang', 'Xulang', ''], ['Guerin', 'Frank', ''], ['Cambria', 'Erik', '']]",1,1,2023-08-24,1,5,2,2,0,2,916c798b34d55a78c47ae44906fe018a60a30613,261100760.0,https://www.semanticscholar.org/paper/916c798b34d55a78c47ae44906fe018a60a30613,arXiv.org,2023.0,107.0,12.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2106694812', 'name': 'Rui Mao'}, {'authorId': '48390820', 'name': 'Guanyi Chen'}, {'authorId': '2154710134', 'name': 'Xulang Zhang'}, {'authorId': '8139616', 'name': 'Frank Guerin'}, {'authorId': '49943757', 'name': 'E. Cambria'}]","['University of Surrey', 'Central China Normal University', 'Nanyang Technological University', 'Hubei Provincial Key Laboratory of Artificial Intelligence and Smart Learning, National Language Resources Monitoring and Research Center for Network Media,']","['China', 'United Kingdom', 'Singapore']",2023-08
2308.12503,Jinxin Shi,"Shi Jinxin, Zhao Jiabao, Wang Yilei, Wu Xingjiao, Li Jiawen, He Liang",CGMI: Configurable General Multi-Agent Interaction Framework,"11 pages, 15 figures",,,,cs.AI cs.HC cs.MA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Benefiting from the powerful capabilities of large language models (LLMs), agents based on LLMs have shown the potential to address domain-specific tasks and emulate human behaviors. However, the content generated by these agents remains somewhat superficial, owing to their limited domain expertise and the absence of an effective cognitive architecture. To address this, we present the Configurable General Multi-Agent Interaction (CGMI) framework, designed to replicate human interactions in real-world scenarios. Specifically, we propose a tree-structured methodology for the assignment, detection, and maintenance of agent personality. Additionally, we designed a cognitive architecture equipped with a skill library based on the ACT* model, which contains memory, reflection, and planning modules. We have also integrated general agents to augment the virtual environment's realism. Using the CGMI framework, we simulated numerous classroom interactions between teacher and students. The experiments indicate that aspects such as the teaching methodology, curriculum, and student performance closely mirror real classroom settings. We will open source our work. ","[{'version': 'v1', 'created': 'Thu, 24 Aug 2023 02:03:29 GMT'}, {'version': 'v2', 'created': 'Mon, 28 Aug 2023 05:44:39 GMT'}]",2023-08-29,"[['Jinxin', 'Shi', ''], ['Jiabao', 'Zhao', ''], ['Yilei', 'Wang', ''], ['Xingjiao', 'Wu', ''], ['Jiawen', 'Li', ''], ['Liang', 'He', '']]",0,0,2023-08-24,2,6,3,0,0,0,30e010ab295e5973c924088947608bf4a57e9b70,261101102.0,https://www.semanticscholar.org/paper/30e010ab295e5973c924088947608bf4a57e9b70,arXiv.org,2023.0,0.0,4.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2109351705', 'name': 'Jinxin Shi'}, {'authorId': '2210714', 'name': 'Jiabao Zhao'}, {'authorId': '2108103487', 'name': 'Yilei Wang'}, {'authorId': '153028502', 'name': 'Xingjiao Wu'}, {'authorId': '2210254745', 'name': 'Jiawen Li'}, {'authorId': '2112480319', 'name': 'Liangbo He'}]","['East China Normal University', 'Fudan University']",['China'],2023-08
2308.12519,Xin Cong,"Yining Ye, Xin Cong, Yujia Qin, Yankai Lin, Zhiyuan Liu, Maosong Sun",Large Language Model as Autonomous Decision Maker,Work in progess,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While large language models (LLMs) exhibit impressive language understanding and in-context learning abilities, their decision-making ability still heavily relies on the guidance of task-specific expert knowledge when solving real-world tasks. To unleash the potential of LLMs as autonomous decision makers, this paper presents an approach JuDec to endow LLMs with the self-judgment ability, enabling LLMs to achieve autonomous judgment and exploration for decision making. Specifically, in JuDec, Elo-based Self-Judgment Mechanism is designed to assign Elo scores to decision steps to judge their values and utilities via pairwise comparisons between two solutions and then guide the decision-searching process toward the optimal solution accordingly. Experimental results on the ToolBench dataset demonstrate JuDec's superiority over baselines, achieving over 10% improvement in Pass Rate on diverse tasks. It offers higher-quality solutions and reduces costs (ChatGPT API calls), highlighting its effectiveness and efficiency. ","[{'version': 'v1', 'created': 'Thu, 24 Aug 2023 03:11:45 GMT'}]",2023-08-25,"[['Ye', 'Yining', ''], ['Cong', 'Xin', ''], ['Qin', 'Yujia', ''], ['Lin', 'Yankai', ''], ['Liu', 'Zhiyuan', ''], ['Sun', 'Maosong', '']]",1,1,2023-08-24,1,6,1,1,0,1,85c966ca9f13f3ef290c1fd52126ef39a32a3000,261100678.0,https://www.semanticscholar.org/paper/85c966ca9f13f3ef290c1fd52126ef39a32a3000,arXiv.org,2023.0,24.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2114059497', 'name': 'Yining Ye'}, {'authorId': '2214579778', 'name': 'Xin Cong'}, {'authorId': '50625437', 'name': 'Yujia Qin'}, {'authorId': '2149202150', 'name': 'Yankai Lin'}, {'authorId': '2141313179', 'name': 'Zhiyuan Liu'}, {'authorId': '1753344', 'name': 'Maosong Sun'}]","['Tsinghua University', 'Renmin University of China']",['China'],2023-08
2308.12578,Yachao Zhao,"Yachao Zhao, Bo Wang, Dongming Zhao, Kun Huang, Yan Wang, Ruifang He,
  Yuexian Hou",Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large Language Models,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent researches indicate that Pre-trained Large Language Models (LLMs) possess cognitive constructs similar to those observed in humans, prompting researchers to investigate the cognitive aspects of LLMs. This paper focuses on explicit and implicit social bias, a distinctive two-level cognitive construct in psychology. It posits that individuals' explicit social bias, which is their conscious expression of bias in the statements, may differ from their implicit social bias, which represents their unconscious bias. We propose a two-stage approach and discover a parallel phenomenon in LLMs known as ""re-judge inconsistency"" in social bias. In the initial stage, the LLM is tasked with automatically completing statements, potentially incorporating implicit social bias. However, in the subsequent stage, the same LLM re-judges the biased statement generated by itself but contradicts it. We propose that this re-judge inconsistency can be similar to the inconsistency between human's unaware implicit social bias and their aware explicit social bias. Experimental investigations on ChatGPT and GPT-4 concerning common gender biases examined in psychology corroborate the highly stable nature of the re-judge inconsistency. This finding may suggest that diverse cognitive constructs emerge as LLMs' capabilities strengthen. Consequently, leveraging psychological theories can provide enhanced insights into the underlying mechanisms governing the expressions of explicit and implicit constructs in LLMs. ","[{'version': 'v1', 'created': 'Thu, 24 Aug 2023 05:35:58 GMT'}]",2023-08-25,"[['Zhao', 'Yachao', ''], ['Wang', 'Bo', ''], ['Zhao', 'Dongming', ''], ['Huang', 'Kun', ''], ['Wang', 'Yan', ''], ['He', 'Ruifang', ''], ['Hou', 'Yuexian', '']]",1,1,2023-08-24,1,7,2,2,0,2,5a839db23d769bef6fa224feda6bf17dd0688e8a,261101090.0,https://www.semanticscholar.org/paper/5a839db23d769bef6fa224feda6bf17dd0688e8a,arXiv.org,2023.0,23.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Psychology', 'source': 's2-fos-model'}]","[{'authorId': '2233316526', 'name': 'Yachao Zhao'}, {'authorId': '2153213077', 'name': 'Bo Wang'}, {'authorId': '2163295226', 'name': 'Dongming Zhao'}, {'authorId': '2187583931', 'name': 'Kun Huang'}, {'authorId': '2152547812', 'name': 'Yan Wang'}, {'authorId': '1724097', 'name': 'Ruifang He'}, {'authorId': '1785922', 'name': 'Yuexian Hou'}]","['AI Lab, China Mobile Communication Group Tianjin Co., Ltd.', 'Tianjin University']",['China'],2023-08
2308.12674,Yijie Chen,"Yijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, Jie Zhou",Improving Translation Faithfulness of Large Language Models via Augmenting Instructions,"Our code and datasets are released in Github:
  https://github.com/pppa2019/swie_overmiss_llm4mt",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) present strong general capabilities, and a current compelling challenge is stimulating their specialized capabilities, such as machine translation, through low-cost instruction tuning. The standard instruction-following data is sequentially organized as the concatenation of an instruction, an input, and a response. As the attention mechanism of LLMs has limitations on local focus, LLMs tend to focus more on the words or sentences nearby at each position. This leads to a high risk of instruction forgetting during decoding. To alleviate the above issues, We propose SWIE (Segment-Weighted Instruction Embedding) and an instruction-following dataset OVERMISS. SWIE improves the model instruction understanding by adding a global instruction representation on the following input and response representations. OVERMISS improves model faithfulness by comparing over-translation and miss-translation results with the correct translation. We apply our methods to two main-stream open-source LLMs, BLOOM and LLaMA. The experimental results demonstrate significant improvements in translation performance with SWIE based on BLOOMZ-3b, particularly in zero-shot and long text translations due to reduced instruction forgetting risk. Additionally, OVERMISS outperforms the baseline in translation performance (e.g. an increase in BLEU scores from 0.69 to 3.12 and an average improvement of 0.48 percentage comet scores for LLaMA-7b) with further enhancements seen in models combining OVERMISS and SWIE (e.g. the BLUE scores increase up to 0.56 from English to German across three different backbones), and both exhibit improvements in the faithfulness metric based on word alignment. ","[{'version': 'v1', 'created': 'Thu, 24 Aug 2023 09:32:29 GMT'}]",2023-08-25,"[['Chen', 'Yijie', ''], ['Liu', 'Yijin', ''], ['Meng', 'Fandong', ''], ['Chen', 'Yufeng', ''], ['Xu', 'Jinan', ''], ['Zhou', 'Jie', '']]",0,0,2023-08-24,1,6,2,3,3,0,92221598758165d1679c8c9792435bb5a9a547a0,261101210.0,https://www.semanticscholar.org/paper/92221598758165d1679c8c9792435bb5a9a547a0,arXiv.org,2023.0,26.0,3.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2109358868', 'name': 'Yijie Chen'}, {'authorId': '46398775', 'name': 'Yanjun Liu'}, {'authorId': '33427918', 'name': 'Fandong Meng'}, {'authorId': '47559028', 'name': 'Yufeng Chen'}, {'authorId': '2310092', 'name': 'Jinan Xu'}, {'authorId': '48128428', 'name': 'Jie Zhou'}]","['Tencent', 'Beijing Jiaotong University']",['China'],2023-08
2308.12697,Chenyuan Zhang,"Chenyuan Zhang, Hao Liu, Jiutian Zeng, Kejing Yang, Yuhong Li, Hui Li",Prompt-Enhanced Software Vulnerability Detection Using ChatGPT,"13 Pages, 4 figures",,,,cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the increase in software vulnerabilities that cause significant economic and social losses, automatic vulnerability detection has become essential in software development and maintenance. Recently, large language models (LLMs) like GPT have received considerable attention due to their stunning intelligence, and some studies consider using ChatGPT for vulnerability detection. However, they do not fully consider the characteristics of LLMs, since their designed questions to ChatGPT are simple without a specific prompt design tailored for vulnerability detection. This paper launches a study on the performance of software vulnerability detection using ChatGPT with different prompt designs. Firstly, we complement previous work by applying various improvements to the basic prompt. Moreover, we incorporate structural and sequential auxiliary information to improve the prompt design. Besides, we leverage ChatGPT's ability of memorizing multi-round dialogue to design suitable prompts for vulnerability detection. We conduct extensive experiments on two vulnerability datasets to demonstrate the effectiveness of prompt-enhanced vulnerability detection using ChatGPT. We also analyze the merit and demerit of using ChatGPT for vulnerability detection. ","[{'version': 'v1', 'created': 'Thu, 24 Aug 2023 10:30:33 GMT'}]",2023-08-25,"[['Zhang', 'Chenyuan', ''], ['Liu', 'Hao', ''], ['Zeng', 'Jiutian', ''], ['Yang', 'Kejing', ''], ['Li', 'Yuhong', ''], ['Li', 'Hui', '']]",1,1,2023-08-24,1,6,1,1,0,1,28ba105e12eb1cdbf5d9423f105771de07037f8c,261100663.0,https://www.semanticscholar.org/paper/28ba105e12eb1cdbf5d9423f105771de07037f8c,arXiv.org,2023.0,80.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '3303548', 'name': 'Chenyuan Zhang'}, {'authorId': '2143857503', 'name': 'Hao Liu'}, {'authorId': '1568987029', 'name': 'Jiutian Zeng'}, {'authorId': '2233435784', 'name': 'Kejing Yang'}, {'authorId': '2233385976', 'name': 'Yuhong Li'}, {'authorId': '2256782747', 'name': 'Hui Li'}]","['of and , of of , of , Jiutian Zeng Kejing Yang Yuhong Li', 'Xiamen University', 'Ministry of Education']","['China', 'Thailand']",2023-08
2308.12711,Yue Wang,"Yue Wang, Xinrui Wang, Juntao Li, Jinxiong Chang, Qishen Zhang,
  Zhongyi Liu, Guannan Zhang, Min Zhang",Harnessing the Power of David against Goliath: Exploring Instruction Data Generation without Using Closed-Source Models,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Instruction tuning is instrumental in enabling Large Language Models~(LLMs) to follow user instructions to complete various open-domain tasks. The success of instruction tuning depends on the availability of high-quality instruction data. Owing to the exorbitant cost and substandard quality of human annotation, recent works have been deeply engaged in the exploration of the utilization of powerful closed-source models to generate instruction data automatically. However, these methods carry potential risks arising from the usage requirements of powerful closed-source models, which strictly forbid the utilization of their outputs to develop machine learning models. To deal with this problem, in this work, we explore alternative approaches to generate high-quality instruction data that do not rely on closed-source models. Our exploration includes an investigation of various existing instruction generation methods, culminating in the integration of the most efficient variant with two novel strategies to enhance the quality further. Evaluation results from two benchmarks and the GPT-4 model demonstrate the effectiveness of our generated instruction data, which can outperform Alpaca, a method reliant on closed-source models. We hope that more progress can be achieved in generating high-quality instruction data without using closed-source models. ","[{'version': 'v1', 'created': 'Thu, 24 Aug 2023 11:07:47 GMT'}]",2023-08-25,"[['Wang', 'Yue', ''], ['Wang', 'Xinrui', ''], ['Li', 'Juntao', ''], ['Chang', 'Jinxiong', ''], ['Zhang', 'Qishen', ''], ['Liu', 'Zhongyi', ''], ['Zhang', 'Guannan', ''], ['Zhang', 'Min', '']]",0,1,2023-08-24,1,8,1,2,0,2,1ce1738d7f224ebd7ad98e23692404f06697b5f4,261100796.0,https://www.semanticscholar.org/paper/1ce1738d7f224ebd7ad98e23692404f06697b5f4,arXiv.org,2023.0,44.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2118462606', 'name': 'Yue Wang'}, {'authorId': '2108082634', 'name': 'Xinrui Wang'}, {'authorId': '2109013629', 'name': 'Juntao Li'}, {'authorId': '48809227', 'name': 'Jinxiong Chang'}, {'authorId': '2165316621', 'name': 'Qishen Zhang'}, {'authorId': '2145285454', 'name': 'Zhongyi Liu'}, {'authorId': '119557985', 'name': 'Guannan Zhang'}, {'authorId': '2220937823', 'name': 'Min Zhang'}]",['Soochow University'],['China'],2023-08
2308.12714,Bin Wang,"Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong, Pan Zhang,
  Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang, Conghui He",VIGC: Visual Instruction Generation and Correction,"Project Website: https://opendatalab.github.io/VIGC, Code and
  Pretrained Model: https://github.com/opendatalab/VIGC, Dataset:
  https://opendatalab.com/OpenDataLab/VIGC-InstData",,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However, the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models (MLLMs) to generate instruction data for vision-language tasks. However, it's worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to generate instruction-tuning data and progressively enhance its quality on-the-fly. Specifically, Visual Instruction Generation (VIG) guides the vision-language model to generate diverse instruction-tuning data. To ensure generation quality, Visual Instruction Correction (VIC) adopts an iterative update mechanism to correct any inaccuracies in data produced by VIG, effectively reducing the risk of hallucination. Leveraging the diverse, high-quality data generated by VIGC, we finetune mainstream models and validate data quality based on various evaluations. Experimental results demonstrate that VIGC not only compensates for the shortcomings of language-only data generation methods, but also effectively enhances the benchmark performance. The models, datasets, and code are available at https://opendatalab.github.io/VIGC. ","[{'version': 'v1', 'created': 'Thu, 24 Aug 2023 11:21:05 GMT'}, {'version': 'v2', 'created': 'Mon, 11 Sep 2023 08:29:48 GMT'}]",2023-09-12,"[['Wang', 'Bin', ''], ['Wu', 'Fan', ''], ['Han', 'Xiao', ''], ['Peng', 'Jiahui', ''], ['Zhong', 'Huaping', ''], ['Zhang', 'Pan', ''], ['Dong', 'Xiaoyi', ''], ['Li', 'Weijia', ''], ['Li', 'Wei', ''], ['Wang', 'Jiaqi', ''], ['He', 'Conghui', '']]",0,1,2023-08-24,2,11,2,1,0,1,e47d276bad18f441950c8136672ae6864e95323f,261100735.0,https://www.semanticscholar.org/paper/e47d276bad18f441950c8136672ae6864e95323f,arXiv.org,2023.0,38.0,3.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2256857728', 'name': 'Bin Wang'}, {'authorId': '2257430491', 'name': 'Fan Wu'}, {'authorId': '2118232372', 'name': 'Xiao Han'}, {'authorId': '2233445161', 'name': 'Jiahui Peng'}, {'authorId': '2106683752', 'name': 'Huaping Zhong'}, {'authorId': '2213750400', 'name': 'Pan Zhang'}, {'authorId': '2118187561', 'name': 'Xiao-wen Dong'}, {'authorId': '2186959436', 'name': 'Weijia Li'}, {'authorId': '2256598803', 'name': 'Wei Li'}, {'authorId': '2156546701', 'name': 'Jiaqi Wang'}, {'authorId': '3486481', 'name': 'Conghui He'}]","['Shanghai Artificial Intelligence Laboratory', 'Sun Yat-sen University', 'SenseTime Research,']",['China'],2023-08
2308.12792,Siddique Latif,"Siddique Latif, Moazzam Shoukat, Fahad Shamshad, Muhammad Usama, Yi
  Ren, Heriberto Cuay\'ahuitl, Wenwu Wang, Xulong Zhang, Roberto Togneri, Erik
  Cambria, and Bj\""orn W. Schuller",Sparks of Large Audio Models: A Survey and Outlook,"Under review, Repo URL:
  https://github.com/EmulationAI/awesome-large-audio-models",,,,cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This survey paper provides a comprehensive overview of the recent advancements and challenges in applying large language models to the field of audio signal processing. Audio processing, with its diverse signal representations and a wide range of sources--from human voices to musical instruments and environmental sounds--poses challenges distinct from those found in traditional Natural Language Processing scenarios. Nevertheless, \textit{Large Audio Models}, epitomized by transformer-based architectures, have shown marked efficacy in this sphere. By leveraging massive amount of data, these models have demonstrated prowess in a variety of audio tasks, spanning from Automatic Speech Recognition and Text-To-Speech to Music Generation, among others. Notably, recently these Foundational Audio Models, like SeamlessM4T, have started showing abilities to act as universal translators, supporting multiple speech tasks for up to 100 languages without any reliance on separate task-specific systems. This paper presents an in-depth analysis of state-of-the-art methodologies regarding \textit{Foundational Large Audio Models}, their performance benchmarks, and their applicability to real-world scenarios. We also highlight current limitations and provide insights into potential future research directions in the realm of \textit{Large Audio Models} with the intent to spark further discussion, thereby fostering innovation in the next generation of audio-processing systems. Furthermore, to cope with the rapid development in this area, we will consistently update the relevant repository with relevant recent articles and their open-source implementations at https://github.com/EmulationAI/awesome-large-audio-models. ","[{'version': 'v1', 'created': 'Thu, 24 Aug 2023 13:47:16 GMT'}, {'version': 'v2', 'created': 'Sun, 3 Sep 2023 13:47:02 GMT'}, {'version': 'v3', 'created': 'Fri, 22 Sep 2023 01:44:46 GMT'}]",2023-09-25,"[['Latif', 'Siddique', ''], ['Shoukat', 'Moazzam', ''], ['Shamshad', 'Fahad', ''], ['Usama', 'Muhammad', ''], ['Ren', 'Yi', ''], ['Cuayhuitl', 'Heriberto', ''], ['Wang', 'Wenwu', ''], ['Zhang', 'Xulong', ''], ['Togneri', 'Roberto', ''], ['Cambria', 'Erik', ''], ['Schuller', 'Bjrn W.', '']]",0,0,2023-08-24,3,11,2,0,0,0,374ebdc8240a35820cb7ab8bfca37e180e21b605,261100979.0,https://www.semanticscholar.org/paper/374ebdc8240a35820cb7ab8bfca37e180e21b605,arXiv.org,2023.0,337.0,3.0,1.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '24040678', 'name': 'S. Latif'}, {'authorId': '2212367091', 'name': 'Moazzam Shoukat'}, {'authorId': '29434078', 'name': 'Fahad Shamshad'}, {'authorId': '145474282', 'name': 'M. Usama'}, {'authorId': '2052729953', 'name': ""Heriberto Cuay'ahuitl""}, {'authorId': '145411696', 'name': 'Bjrn Schuller'}]","['University of Surrey', 'Imperial College London', 'Lab of Large Audio Models, Ping An Technology, China.', 'ByteDance', 'Mohamed bin Zayed University of Artificial Intelligence', 'Queensland University of Technology', 'University of Western Australia', 'Nanyang Technological University', 'University of Augsburg', 'University of Lincoln']","['Germany', 'Singapore', 'Australia', 'United Kingdom', 'China', 'United Arab Emirates']",2023-08
2308.12915,Zhouyi Li,"Yuqian Sun, Zhouyi Li, Ke Fang, Chang Hee Lee, Ali Asadipour",Language as Reality: A Co-Creative Storytelling Game Experience in 1001 Nights using Generative AI,"The paper was accepted by The 19th AAAI Conference on Artificial
  Intelligence and Interactive Digital Entertainment (AIIDE 23)",,,,cs.HC cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we present ""1001 Nights"", an AI-native game that allows players lead in-game reality through co-created storytelling with the character driven by large language model. The concept is inspired by Wittgenstein's idea of the limits of one's world being determined by the bounds of their language. Using advanced AI tools like GPT-4 and Stable Diffusion, the second iteration of the game enables the protagonist, Shahrzad, to realize words and stories in her world. The player can steer the conversation with the AI King towards specific keywords, which then become battle equipment in the game. This blend of interactive narrative and text-to-image transformation challenges the conventional border between the game world and reality through a dual perspective. We focus on Shahrzad, who seeks to alter her fate compared to the original folklore, and the player, who collaborates with AI to craft narratives and shape the game world. We explore the technical and design elements of implementing such a game with an objective to enhance the narrative game genre with AI-generated content and to delve into AI-native gameplay possibilities. ","[{'version': 'v1', 'created': 'Thu, 24 Aug 2023 16:42:23 GMT'}, {'version': 'v2', 'created': 'Mon, 18 Sep 2023 15:16:04 GMT'}]",2023-09-19,"[['Sun', 'Yuqian', ''], ['Li', 'Zhouyi', ''], ['Fang', 'Ke', ''], ['Lee', 'Chang Hee', ''], ['Asadipour', 'Ali', '']]",0,1,2023-08-24,2,5,2,1,0,1,722d47ddb374c1196a02fe2f3b44575136e5dd6b,261100607.0,https://www.semanticscholar.org/paper/722d47ddb374c1196a02fe2f3b44575136e5dd6b,Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment,2023.0,56.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Art', 'source': 's2-fos-model'}]","[{'authorId': '1666629650', 'name': 'Yuqian Sun'}, {'authorId': '2185686731', 'name': 'Zhouyi Li'}, {'authorId': '2182451457', 'name': 'Ke Fang'}, {'authorId': '2187449929', 'name': 'Chang Hee Lee'}, {'authorId': '2024403', 'name': 'A. Asadipour'}]","['Korea Advanced Institute of Science and Technology', 'Tsinghua University', 'Royal College of Art', 'Radarbolaget (Sweden)']","['China', 'South Korea', 'United Kingdom', 'Sweden']",2023-08
2308.13137,Wenqi Shao,"Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao,
  Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, Ping Luo",OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models,A differentiable quantization method for LLM,,,,cs.LG cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving the computational efficiency of LLM, they hand-craft quantization parameters, which leads to low performance and fails to deal with extremely low-bit quantization. To tackle this issue, we introduce an Omnidirectionally calibrated Quantization (OmniQuant) technique for LLMs, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of PTQ by efficiently optimizing various quantization parameters. OmniQuant comprises two innovative components including Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). LWC modulates the extreme values of weights by optimizing the clipping threshold. Meanwhile, LET tackles activation outliers by shifting the challenge of quantization from activations to weights through a learnable equivalent transformation. Operating within a differentiable framework using block-wise error minimization, OmniQuant can optimize the quantization process efficiently for both weight-only and weight-activation quantization. For instance, the LLaMA-2 model family with the size of 7-70B can be processed with OmniQuant on a single A100-40G GPU within 1-16 hours using 128 samples. Extensive experiments validate OmniQuant's superior performance across diverse quantization configurations such as W4A4, W6A6, W4A16, W3A16, and W2A16. Additionally, OmniQuant demonstrates effectiveness in instruction-tuned models and delivers notable improvements in inference speed and memory reduction on real devices. Codes and models are available at \url{https://github.com/OpenGVLab/OmniQuant}. ","[{'version': 'v1', 'created': 'Fri, 25 Aug 2023 02:28:35 GMT'}]",2023-08-28,"[['Shao', 'Wenqi', ''], ['Chen', 'Mengzhao', ''], ['Zhang', 'Zhaoyang', ''], ['Xu', 'Peng', ''], ['Zhao', 'Lirui', ''], ['Li', 'Zhiqian', ''], ['Zhang', 'Kaipeng', ''], ['Gao', 'Peng', ''], ['Qiao', 'Yu', ''], ['Luo', 'Ping', '']]",0,0,2023-08-25,1,10,2,1,1,0,eb2c2330177f765038a2b17e2ee3498965865797,261214575.0,https://www.semanticscholar.org/paper/eb2c2330177f765038a2b17e2ee3498965865797,arXiv.org,2023.0,43.0,8.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1485702259', 'name': 'Wenqi Shao'}, {'authorId': '2133595441', 'name': 'Mengzhao Chen'}, {'authorId': '2156121714', 'name': 'Zhaoyang Zhang'}, {'authorId': '2153917002', 'name': 'Peng Xu'}, {'authorId': '2164162549', 'name': 'Lirui Zhao'}, {'authorId': '2109766514', 'name': 'Zhiqiang Li'}, {'authorId': '3393556', 'name': 'Kaipeng Zhang'}, {'authorId': '144740494', 'name': 'Peng Gao'}, {'authorId': '2059129841', 'name': 'Y. Qiao'}, {'authorId': '2143481782', 'name': 'Ping Luo'}]","['Chinese University of Hong Kong', 'OpenGVLab, Shanghai AI Laboratory', 'University of Hong Kong']","['China', 'Hong Kong']",2023-08
2308.13149,Liangtai Sun,"Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen,
  Lu Chen and Kai Yu",SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research,"12 pages, 17 figures, 12 tables. Under Review",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Recently, there has been growing interest in using Large Language Models (LLMs) for scientific research. Numerous benchmarks have been proposed to evaluate the ability of LLMs for scientific research. However, current benchmarks are mostly based on pre-collected objective questions. This design suffers from data leakage problem and lacks the evaluation of subjective Q/A ability. In this paper, we propose SciEval, a comprehensive and multi-disciplinary evaluation benchmark to address these issues. Based on Bloom's taxonomy, SciEval covers four dimensions to systematically evaluate scientific research ability. In particular, we design a ""dynamic"" subset based on scientific principles to prevent evaluation from potential data leakage. Both objective and subjective questions are included in SciEval. These characteristics make SciEval a more effective benchmark for scientific research ability evaluation of LLMs. Comprehensive experiments on most advanced LLMs show that, although GPT-4 achieves SOTA performance compared to other LLMs, there is still substantial room for improvement, especially for dynamic questions. The data and codes are now publicly available. ","[{'version': 'v1', 'created': 'Fri, 25 Aug 2023 03:05:33 GMT'}]",2023-08-28,"[['Sun', 'Liangtai', ''], ['Han', 'Yang', ''], ['Zhao', 'Zihan', ''], ['Ma', 'Da', ''], ['Shen', 'Zhennan', ''], ['Chen', 'Baocai', ''], ['Chen', 'Lu', ''], ['Yu', 'Kai', '']]",0,1,2023-08-25,1,8,1,2,1,1,f53a955ea1812fb0481504fdfd8febcb2a553a45,261214653.0,https://www.semanticscholar.org/paper/f53a955ea1812fb0481504fdfd8febcb2a553a45,arXiv.org,2023.0,30.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2218987690', 'name': 'Liangtai Sun'}, {'authorId': '2234135731', 'name': 'Yang Han'}, {'authorId': '1806179720', 'name': 'Zihan Zhao'}, {'authorId': '2087451371', 'name': 'Da Ma'}, {'authorId': '2191055342', 'name': 'Zhe-Wei Shen'}, {'authorId': '2234089084', 'name': 'Baocai Chen'}, {'authorId': '1390833791', 'name': 'Lu Chen'}, {'authorId': '2114076600', 'name': 'Kai Yu'}]","['Shanghai Jiao Tong University', 'https://github.com/OpenDFM/BAI-SciEval']",['China'],2023-08
2308.13259,Keheng Wang,"Keheng Wang, Feiyu Duan, Sirui Wang, Peiguang Li, Yunsen Xian,
  Chuantao Yin, Wenge Rong, Zhang Xiong",Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Equipped with Chain-of-Thought (CoT), Large language models (LLMs) have shown impressive reasoning ability in various downstream tasks. Even so, suffering from hallucinations and the inability to access external knowledge, LLMs often come with incorrect or unfaithful intermediate reasoning steps, especially in the context of answering knowledge-intensive tasks such as KBQA. To alleviate this issue, we propose a framework called Knowledge-Driven Chain-of-Thought (KD-CoT) to verify and modify reasoning traces in CoT via interaction with external knowledge, and thus overcome the hallucinations and error propagation. Concretely, we formulate the CoT rationale process of LLMs into a structured multi-round QA format. In each round, LLMs interact with a QA system that retrieves external knowledge and produce faithful reasoning traces based on retrieved precise answers. The structured CoT reasoning of LLMs is facilitated by our developed KBQA CoT collection, which serves as in-context learning demonstrations and can also be utilized as feedback augmentation to train a robust retriever. Extensive experiments on WebQSP and ComplexWebQuestion datasets demonstrate the effectiveness of proposed KD-CoT in task-solving reasoning generation, which outperforms the vanilla CoT ICL with an absolute success rate of 8.0% and 5.1%. Furthermore, our proposed feedback-augmented retriever outperforms the state-of-the-art baselines for retrieving knowledge, achieving significant improvement in Hit performance. ","[{'version': 'v1', 'created': 'Fri, 25 Aug 2023 09:23:55 GMT'}]",2023-08-28,"[['Wang', 'Keheng', ''], ['Duan', 'Feiyu', ''], ['Wang', 'Sirui', ''], ['Li', 'Peiguang', ''], ['Xian', 'Yunsen', ''], ['Yin', 'Chuantao', ''], ['Rong', 'Wenge', ''], ['Xiong', 'Zhang', '']]",0,0,2023-08-25,1,8,2,0,0,0,10955e63aa49fab146267949f8ebc9ebe8275183,261214582.0,https://www.semanticscholar.org/paper/10955e63aa49fab146267949f8ebc9ebe8275183,arXiv.org,2023.0,40.0,6.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2234024810', 'name': 'Keheng Wang'}, {'authorId': '2141973564', 'name': 'Feiyu Duan'}, {'authorId': '2592528', 'name': 'Sirui Wang'}, {'authorId': '8855602', 'name': 'Peiguang Li'}, {'authorId': '2069503881', 'name': 'Yunsen Xian'}, {'authorId': '40548403', 'name': 'Chuantao Yin'}, {'authorId': '21505283', 'name': 'Wenge Rong'}, {'authorId': '2091444262', 'name': 'Zhang Xiong'}]","['Beihang University', 'Meituan']",['China'],2023-08
2308.13319,Ming Yan,"Ming Yan, Junjie Chen, Jie M. Zhang, Xuejie Cao, Chen Yang, Mark
  Harman",COCO: Testing Code Generation Systems via Concretized Instructions,,,,,cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Code generation systems have been extensively developed in recent years to generate source code based on natural language instructions. However, despite their advancements, these systems still face robustness issues where even slightly different instructions can result in significantly different code semantics. Robustness is critical for code generation systems, as it can have significant impacts on software development, software quality, and trust in the generated code. Although existing testing techniques for general text-to-text software can detect some robustness issues, they are limited in effectiveness due to ignoring the characteristics of code generation systems. In this work, we propose a novel technique COCO to test the robustness of code generation systems. It exploits the usage scenario of code generation systems to make the original programming instruction more concrete by incorporating features known to be contained in the original code. A robust system should maintain code semantics for the concretized instruction, and COCO detects robustness inconsistencies when it does not. We evaluated COCO on eight advanced code generation systems, including commercial tools such as Copilot and ChatGPT, using two widely-used datasets. Our results demonstrate the effectiveness of COCO in testing the robustness of code generation systems, outperforming two techniques adopted from general text-to-text software testing by 466.66% and 104.02%, respectively. Furthermore, concretized instructions generated by COCO can help reduce robustness inconsistencies by 18.35% to 53.91% through fine-tuning. ","[{'version': 'v1', 'created': 'Fri, 25 Aug 2023 11:49:27 GMT'}]",2023-08-28,"[['Yan', 'Ming', ''], ['Chen', 'Junjie', ''], ['Zhang', 'Jie M.', ''], ['Cao', 'Xuejie', ''], ['Yang', 'Chen', ''], ['Harman', 'Mark', '']]",1,1,2023-08-25,1,6,1,1,0,1,1b8d8ad3202572c916cf1bb75a3a348a4ab0a7a2,261214646.0,https://www.semanticscholar.org/paper/1b8d8ad3202572c916cf1bb75a3a348a4ab0a7a2,arXiv.org,2023.0,31.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2114007921', 'name': 'Ming-Yi Yan'}, {'authorId': '123878903', 'name': 'Junjie Chen'}, {'authorId': '47540116', 'name': 'Jie M. Zhang'}, {'authorId': '1996206078', 'name': 'Xuejie Cao'}, {'authorId': '2223406543', 'name': 'Chen Yang'}, {'authorId': '145836176', 'name': 'M. Harman'}]","[""King's College London"", 'University College London', 'Tianjin University']","['China', 'United Kingdom']",2023-08
2308.13416,Ensheng Shi,"Ensheng Shi, Fengji Zhang, Yanlin Wang, Bei Chen, Lun Du, Hongyu
  Zhang, Shi Han, Dongmei Zhang, Hongbin Sun",SoTaNa: The Open-Source Software Development Assistant,,,,,cs.SE cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Software development plays a crucial role in driving innovation and efficiency across modern societies. To meet the demands of this dynamic field, there is a growing need for an effective software development assistant. However, existing large language models represented by ChatGPT suffer from limited accessibility, including training data and model weights. Although other large open-source models like LLaMA have shown promise, they still struggle with understanding human intent. In this paper, we present SoTaNa, an open-source software development assistant. SoTaNa utilizes ChatGPT to generate high-quality instruction-based data for the domain of software engineering and employs a parameter-efficient fine-tuning approach to enhance the open-source foundation model, LLaMA. We evaluate the effectiveness of \our{} in answering Stack Overflow questions and demonstrate its capabilities. Additionally, we discuss its capabilities in code summarization and generation, as well as the impact of varying the volume of generated data on model performance. Notably, SoTaNa can run on a single GPU, making it accessible to a broader range of researchers. Our code, model weights, and data are public at \url{https://github.com/DeepSoftwareAnalytics/SoTaNa}. ","[{'version': 'v1', 'created': 'Fri, 25 Aug 2023 14:56:21 GMT'}]",2023-08-28,"[['Shi', 'Ensheng', ''], ['Zhang', 'Fengji', ''], ['Wang', 'Yanlin', ''], ['Chen', 'Bei', ''], ['Du', 'Lun', ''], ['Zhang', 'Hongyu', ''], ['Han', 'Shi', ''], ['Zhang', 'Dongmei', ''], ['Sun', 'Hongbin', '']]",1,1,2023-08-25,1,9,2,2,1,1,7ee2841cced52dc38d5f334d1939c205c1cad716,261214379.0,https://www.semanticscholar.org/paper/7ee2841cced52dc38d5f334d1939c205c1cad716,arXiv.org,2023.0,58.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2006371687', 'name': 'Ensheng Shi'}, {'authorId': '2158120018', 'name': 'Fengji Zhang'}, {'authorId': '2108975906', 'name': 'Yanlin Wang'}, {'authorId': '143876723', 'name': 'B. Chen'}, {'authorId': '12723949', 'name': 'Lun Du'}, {'authorId': '2108881199', 'name': 'Hongyu Zhang'}, {'authorId': '2109750123', 'name': 'Shi Han'}, {'authorId': '2140415600', 'name': 'Dongmei Zhang'}, {'authorId': '2152992779', 'name': 'Hongbin Sun'}]","['Sun Yat-sen University', ""Xi'an Jiaotong University"", 'Microsoft']","['China', 'United States']",2023-08
2308.13437,Chi Chen,"Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li, Maosong Sun, Yang
  Liu",Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, Multimodal Large Language Models (MLLMs) that enable Large Language Models (LLMs) to interpret images through visual instruction tuning have achieved significant success. However, existing visual instruction tuning methods only utilize image-language instruction data to align the language and image modalities, lacking a more fine-grained cross-modal alignment. In this paper, we propose Position-enhanced Visual Instruction Tuning (PVIT), which extends the functionality of MLLMs by integrating an additional region-level vision encoder. This integration promotes a more detailed comprehension of images for the MLLM. In addition, to efficiently achieve a fine-grained alignment between the vision modules and the LLM, we design multiple data generation strategies to construct an image-region-language instruction dataset. Finally, we present both quantitative experiments and qualitative analysis that demonstrate the superiority of the proposed model. Code and data will be released at https://github.com/PVIT-official/PVIT. ","[{'version': 'v1', 'created': 'Fri, 25 Aug 2023 15:33:47 GMT'}, {'version': 'v2', 'created': 'Thu, 14 Sep 2023 15:00:06 GMT'}]",2023-09-15,"[['Chen', 'Chi', ''], ['Qin', 'Ruoyu', ''], ['Luo', 'Fuwen', ''], ['Mi', 'Xiaoyue', ''], ['Li', 'Peng', ''], ['Sun', 'Maosong', ''], ['Liu', 'Yang', '']]",0,0,2023-08-25,2,7,1,0,0,0,ef1ebdb24ce45fb57e271783a0c9a877fe0e79c3,261214794.0,https://www.semanticscholar.org/paper/ef1ebdb24ce45fb57e271783a0c9a877fe0e79c3,arXiv.org,2023.0,34.0,5.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2116885534', 'name': 'Chi Chen'}, {'authorId': '2151362427', 'name': 'Ruoyu Qin'}, {'authorId': '2160560153', 'name': 'Fuwen Luo'}, {'authorId': '1657382290', 'name': 'Xiaoyue Mi'}, {'authorId': '2152926422', 'name': 'Peng Li'}, {'authorId': '1753344', 'name': 'Maosong Sun'}, {'authorId': '2152797839', 'name': 'Yang Liu'}]","['Tsinghua University', 'Institute of Computing Technology']",['China'],2023-08
2308.13565,Tong Xie,"Tong Xie, Yuwei Wan, Wei Huang, Zhenyu Yin, Yixuan Liu, Shaozhou Wang,
  Qingyuan Linghu, Chunyu Kit, Clara Grazian, Wenjie Zhang, Imran Razzak, Bram
  Hoex",DARWIN Series: Domain Specific Large Language Models for Natural Science,,,,,cs.CL cond-mat.mtrl-sci physics.app-ph,http://creativecommons.org/licenses/by-sa/4.0/,"  Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In natural science, traditional manual, serial, and labour-intensive work is being augmented by automated, parallel, and iterative processes driven by artificial intelligence-based experimental automation and more. To add new capabilities in natural science, enabling the acceleration and enrichment of automation of the discovery process, we present DARWIN, a series of tailored LLMs for natural science, mainly in physics, chemistry, and material science. This series relies on open-source LLM, incorporating structured and unstructured scientific knowledge from public datasets and literature. We fine-tuned the models using over 60,000 instruction data points, emphasizing factual correctness. During the fine-tuning, we introduce the Scientific Instruction Generation (SIG) model, automating instruction generation from scientific texts. This eliminates the need for manual extraction or domain-specific knowledge graphs and efficiently injects scientific knowledge into the model. We also explore multi-task training strategies, revealing interconnections between scientific tasks. DARWIN series not only achieves state-of-the-art results on various scientific tasks but also diminishes reliance on closed-source AI models. Our research showcases the ability of LLM in the scientific domain, with the overarching goal of fostering prosperity within the broader AI for science community. ","[{'version': 'v1', 'created': 'Fri, 25 Aug 2023 01:40:48 GMT'}]",2023-08-29,"[['Xie', 'Tong', ''], ['Wan', 'Yuwei', ''], ['Huang', 'Wei', ''], ['Yin', 'Zhenyu', ''], ['Liu', 'Yixuan', ''], ['Wang', 'Shaozhou', ''], ['Linghu', 'Qingyuan', ''], ['Kit', 'Chunyu', ''], ['Grazian', 'Clara', ''], ['Zhang', 'Wenjie', ''], ['Razzak', 'Imran', ''], ['Hoex', 'Bram', '']]",0,0,2023-08-25,1,12,3,0,0,0,abe19d0bab2fcf494eaaca33d0c6e622384985bf,264960172.0,https://www.semanticscholar.org/paper/abe19d0bab2fcf494eaaca33d0c6e622384985bf,arXiv.org,2023.0,45.0,0.0,0.0,True,"['Computer Science', 'Physics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Physics', 'source': 'external'}, {'category': 'Chemistry', 'source': 's2-fos-model'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Materials Science', 'source': 's2-fos-model'}, {'category': 'Physics', 'source': 's2-fos-model'}]","[{'authorId': '2176813450', 'name': 'Tong Xie'}, {'authorId': '2162059414', 'name': 'Yuwei Wan'}, {'authorId': '2265213246', 'name': 'Wei Huang'}, {'authorId': '2265363699', 'name': 'Zhenyu Yin'}, {'authorId': '2213070637', 'name': 'Yixuan Liu'}, {'authorId': '2265947383', 'name': 'Shaozhou Wang'}, {'authorId': '81030877', 'name': 'Qingyuan Linghu'}, {'authorId': '2263723846', 'name': 'Chunyu Kit'}, {'authorId': '35432008', 'name': 'C. Grazian'}, {'authorId': '2264538576', 'name': 'Wenjie Zhang'}, {'authorId': '2264771423', 'name': 'Imran Razzak'}, {'authorId': '2263693445', 'name': 'Bram Hoex'}]","['GreenDynamics Pty. Ltd, Kensington, NSW, Australia', 'UNSW Sydney', 'University of Sydney', 'City University of Hong Kong', 'University of Melbourne', 'DARE ARC Training Centre in Data Analytics for Resources and Environments, Australia']","['China', 'Australia']",2023-08
2308.13566,Bin Wang,"Zhiyuan Zhao, Linke Ouyang, Bin Wang, Siyuan Huang, Pan Zhang, Xiaoyi
  Dong, Jiaqi Wang, Conghui He",MLLM-DataEngine: An Iterative Refinement Approach for MLLM,"Code and models are available at
  https://github.com/opendatalab/MLLM-DataEngine",,,,cs.LG cs.AI cs.CL cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental dataset based on the benchmarking results. For quality, we resort to GPT-4 to generate high-quality data with each given data type. For correctness, prompt design is critical for the data generation results. Rather than previous hand-crafted prompt, we propose an Interactive Prompt Optimization strategy, which optimizes the prompt with the multi-round interaction between human and GPT, and improve the correctness of generated data greatly. Through extensive experiments, we find our MLLM-DataEngine could boost the MLLM capability in a targeted and automatic manner, with only a few human participation. We hope it could be a general solution for the following MLLMs building. The MLLM-DataEngine has been open-sourced and is now available at https://github.com/opendatalab/MLLM-DataEngine. ","[{'version': 'v1', 'created': 'Fri, 25 Aug 2023 01:41:04 GMT'}, {'version': 'v2', 'created': 'Mon, 11 Sep 2023 08:28:40 GMT'}]",2023-09-12,"[['Zhao', 'Zhiyuan', ''], ['Ouyang', 'Linke', ''], ['Wang', 'Bin', ''], ['Huang', 'Siyuan', ''], ['Zhang', 'Pan', ''], ['Dong', 'Xiaoyi', ''], ['Wang', 'Jiaqi', ''], ['He', 'Conghui', '']]",0,1,2023-08-25,2,8,4,1,0,1,58a282c89864f35bff1741f5ab439222da6bb3ec,261245210.0,https://www.semanticscholar.org/paper/58a282c89864f35bff1741f5ab439222da6bb3ec,arXiv.org,2023.0,33.0,2.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2146631014', 'name': 'Zhiyuan Zhao'}, {'authorId': '2161162356', 'name': 'Linke Ouyang'}, {'authorId': '2256857728', 'name': 'Bin Wang'}, {'authorId': '2110059550', 'name': 'Siyuan Huang'}, {'authorId': '2213750400', 'name': 'Pan Zhang'}, {'authorId': '2118187561', 'name': 'Xiao-wen Dong'}, {'authorId': '2156546701', 'name': 'Jiaqi Wang'}, {'authorId': '3486481', 'name': 'Conghui He'}]",['Shanghai Artificial Intelligence Laboratory'],['China'],2023-08
2308.13782,Fan Zhang,"Fan Zhang, Kebing Jin, and Hankz Hankui Zhuo",Planning with Logical Graph-based Language Model for Instruction Generation,"9 pages, 8 figures",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the superior performance of large language models to generate natural language texts, it is hard to generate texts with correct logic according to a given task, due to the difficulties for neural models to capture implied rules from free-form texts. In this paper, we propose a novel graph-based language model, Logical-GLM, to infuse logic into language models for more valid text generation and interpretability. Specifically, we first capture information from natural language instructions and construct logical bayes graphs that generally describe domains. Next, we generate logical skeletons to guide language model training, infusing domain knowledge into language models. Finally, we alternately optimize the searching policy of graphs and language models until convergence. The experimental results show that Logical-GLM is both effective and efficient compared with traditional language models, despite using smaller-scale training data and fewer parameters. Our approach can generate instructional texts with more correct logic owing to the internalized domain knowledge. Moreover, the usage of logical graphs reflects the inner mechanism of the language models, which improves the interpretability of black-box models. ","[{'version': 'v1', 'created': 'Sat, 26 Aug 2023 06:28:14 GMT'}]",2023-08-29,"[['Zhang', 'Fan', ''], ['Jin', 'Kebing', ''], ['Zhuo', 'Hankz Hankui', '']]",0,0,2023-08-26,1,3,2,1,1,0,ddc8e42be6e35f4ff413d34b2c9e0403540b38a7,261243903.0,https://www.semanticscholar.org/paper/ddc8e42be6e35f4ff413d34b2c9e0403540b38a7,arXiv.org,2023.0,43.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2162659995', 'name': 'Fan Zhang'}, {'authorId': '2118877522', 'name': 'Kebing Jin'}, {'authorId': '74076606', 'name': 'H. Zhuo'}]","['Guizhou University', 'Sun Yat-sen University']",['China'],2023-08
2308.13785,Minheng Ni,"Minheng Ni, Chenfei Wu, Xiaodong Wang, Shengming Yin, Lijuan Wang,
  Zicheng Liu, Nan Duan",ORES: Open-vocabulary Responsible Visual Synthesis,,,,,cs.CV,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Avoiding synthesizing specific visual concepts is an essential challenge in responsible visual synthesis. However, the visual concept that needs to be avoided for responsible visual synthesis tends to be diverse, depending on the region, context, and usage scenarios. In this work, we formalize a new task, Open-vocabulary Responsible Visual Synthesis (ORES), where the synthesis model is able to avoid forbidden visual concepts while allowing users to input any desired content. To address this problem, we present a Two-stage Intervention (TIN) framework. By introducing 1) rewriting with learnable instruction through a large-scale language model (LLM) and 2) synthesizing with prompt intervention on a diffusion synthesis model, it can effectively synthesize images avoiding any concepts but following the user's query as much as possible. To evaluate on ORES, we provide a publicly available dataset, baseline models, and benchmark. Experimental results demonstrate the effectiveness of our method in reducing risks of image generation. Our work highlights the potential of LLMs in responsible visual synthesis. Our code and dataset is public available. ","[{'version': 'v1', 'created': 'Sat, 26 Aug 2023 06:47:34 GMT'}]",2023-08-29,"[['Ni', 'Minheng', ''], ['Wu', 'Chenfei', ''], ['Wang', 'Xiaodong', ''], ['Yin', 'Shengming', ''], ['Wang', 'Lijuan', ''], ['Liu', 'Zicheng', ''], ['Duan', 'Nan', '']]",0,0,2023-08-26,1,7,1,0,0,0,95be6a38f9b3ca3b7a9d81215e52cdcf545d554a,261243073.0,https://www.semanticscholar.org/paper/95be6a38f9b3ca3b7a9d81215e52cdcf545d554a,arXiv.org,2023.0,26.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1576502392', 'name': 'Minheng Ni'}, {'authorId': '2151101534', 'name': 'Chenfei Wu'}, {'authorId': '2211071131', 'name': 'Xiaodong Wang'}, {'authorId': '2008158083', 'name': 'Sheng-Siang Yin'}, {'authorId': '29957038', 'name': 'Lijuan Wang'}, {'authorId': '2145253136', 'name': 'Zicheng Liu'}, {'authorId': '2072609829', 'name': 'Nan Duan'}]",['Microsoft'],"['China', 'United States']",2023-08
2308.13844,Zihao Zhou,"Jie Yao, Zihao Zhou, Qiufeng Wang",Solving Math Word Problem with Problem Type Classification,Accpected by NLPCC2023,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Math word problems (MWPs) require analyzing text descriptions and generating mathematical equations to derive solutions. Existing works focus on solving MWPs with two types of solvers: tree-based solver and large language model (LLM) solver. However, these approaches always solve MWPs by a single solver, which will bring the following problems: (1) Single type of solver is hard to solve all types of MWPs well. (2) A single solver will result in poor performance due to over-fitting. To address these challenges, this paper utilizes multiple ensemble approaches to improve MWP-solving ability. Firstly, We propose a problem type classifier that combines the strengths of the tree-based solver and the LLM solver. This ensemble approach leverages their respective advantages and broadens the range of MWPs that can be solved. Furthermore, we also apply ensemble techniques to both tree-based solver and LLM solver to improve their performance. For the tree-based solver, we propose an ensemble learning framework based on ten-fold cross-validation and voting mechanism. In the LLM solver, we adopt self-consistency (SC) method to improve answer selection. Experimental results demonstrate the effectiveness of these ensemble approaches in enhancing MWP-solving ability. The comprehensive evaluation showcases improved performance, validating the advantages of our proposed approach. Our code is available at this url: https://github.com/zhouzihao501/NLPCC2023-Shared-Task3-ChineseMWP. ","[{'version': 'v1', 'created': 'Sat, 26 Aug 2023 10:35:16 GMT'}]",2023-08-29,"[['Yao', 'Jie', ''], ['Zhou', 'Zihao', ''], ['Wang', 'Qiufeng', '']]",0,0,2023-08-26,1,3,1,0,0,0,d9e88bbfead1a9a5f26f95037d591731d2b2486d,261245555.0,https://www.semanticscholar.org/paper/d9e88bbfead1a9a5f26f95037d591731d2b2486d,Natural Language Processing and Chinese Computing,2023.0,23.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2220808197', 'name': 'Jie Yao'}, {'authorId': '2117793393', 'name': 'Zihao Zhou'}, {'authorId': '1500377117', 'name': 'Qiufeng Wang'}]",['Xian Jiaotong-Liverpool University'],['China'],2023-08
2308.13851,Jinrun Liu,"Jinrun Liu, Xinyu Tang, Linlin Li, Panpan Chen, Yepang Liu",Which is a better programming assistant? A comparative study between chatgpt and stack overflow,,,,,cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Programmers often seek help from Q\&A websites to resolve issues they encounter during programming. Stack Overflow has been a widely used platform for this purpose for over a decade. Recently, revolutionary AI-powered platforms like ChatGPT have quickly gained popularity among programmers for their efficient and personalized programming assistance via natural language interactions. Both platforms can offer valuable assistance to programmers, but it's unclear which is more effective at enhancing programmer productivity. In our paper, we conducted an exploratory user study to compare the performance of Stack Overflow and ChatGPT in enhancing programmer productivity. Two groups of students with similar programming abilities were instructed to use the two platforms to solve three different types of programming tasks: algorithmic challenges, library usage, and debugging. During the experiments, we measured and compared the quality of code produced and the time taken to complete tasks for the two groups. The results show that, concerning code quality, ChatGPT outperforms Stack Overflow significantly in helping complete algorithmic and library-related tasks, while Stack Overflow is better for debugging tasks. Regarding task completion speed, the ChatGPT group is obviously faster than the Stack Overflow group in the algorithmic challenge, but the two groups have a similar performance in the other two tasks. Additionally, we conducted a post-experiment survey with the participants to understand how the platforms have helped them complete the programming tasks. We analyzed the questionnaires to summarize ChatGPT and Stack Overflow's strengths and weaknesses pointed out by the participants. By comparing these, we identified the reasons behind the two platforms' divergent performances in programming assistance. ","[{'version': 'v1', 'created': 'Sat, 26 Aug 2023 11:25:18 GMT'}]",2023-08-29,"[['Liu', 'Jinrun', ''], ['Tang', 'Xinyu', ''], ['Li', 'Linlin', ''], ['Chen', 'Panpan', ''], ['Liu', 'Yepang', '']]",1,1,2023-08-26,1,5,1,1,0,1,3ab8cb56dedafb54c2c4c01155a91001567abb60,261243297.0,https://www.semanticscholar.org/paper/3ab8cb56dedafb54c2c4c01155a91001567abb60,arXiv.org,2023.0,38.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2234381830', 'name': 'Jinrun Liu'}, {'authorId': '2205725406', 'name': 'Xinyu Tang'}, {'authorId': '2185125812', 'name': 'Linlin Li'}, {'authorId': '2234531096', 'name': 'Panpan Chen'}, {'authorId': '39584070', 'name': 'Yepang Liu'}]",['Southern University of Science and Technology'],['China'],2023-08
2308.13894,Dongqi Cai,"Mengwei Xu, Yaozong Wu, Dongqi Cai, Xiang Li, Shangguang Wang",Federated Fine-tuning of Billion-Sized Language Models across Mobile Devices,under review,,,,cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) are transforming the landscape of mobile intelligence. Federated Learning (FL), a method to preserve user data privacy, is often employed in fine-tuning LLMs to downstream mobile tasks, an approach known as FedLLM. Though recent efforts have addressed the network issue induced by the vast model size, they have not practically mitigated vital challenges concerning integration with mobile devices, such as significant memory consumption and sluggish model convergence.   In response to these challenges, this work introduces FwdLLM, an innovative FL protocol designed to enhance the FedLLM efficiency. The key idea of FwdLLM to employ backpropagation (BP)-free training methods, requiring devices only to execute ``perturbed inferences''. Consequently, FwdLLM delivers way better memory efficiency and time efficiency (expedited by mobile NPUs and an expanded array of participant devices). FwdLLM centers around three key designs: (1) it combines BP-free training with parameter-efficient training methods, an essential way to scale the approach to the LLM era; (2) it systematically and adaptively allocates computational loads across devices, striking a careful balance between convergence speed and accuracy; (3) it discriminatively samples perturbed predictions that are more valuable to model convergence. Comprehensive experiments with five LLMs and three NLP tasks illustrate FwdLLM's significant advantages over conventional methods, including up to three orders of magnitude faster convergence and a 14.6x reduction in memory footprint. Uniquely, FwdLLM paves the way for federated learning of billion-parameter LLMs such as LLaMA on COTS mobile devices -- a feat previously unattained. ","[{'version': 'v1', 'created': 'Sat, 26 Aug 2023 14:36:30 GMT'}]",2023-08-29,"[['Xu', 'Mengwei', ''], ['Wu', 'Yaozong', ''], ['Cai', 'Dongqi', ''], ['Li', 'Xiang', ''], ['Wang', 'Shangguang', '']]",0,0,2023-08-26,1,5,2,1,1,0,1c09585099a2fa90b9d7110079d1e6abe9e293f7,261243922.0,https://www.semanticscholar.org/paper/1c09585099a2fa90b9d7110079d1e6abe9e293f7,arXiv.org,2023.0,104.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2529558', 'name': 'Mengwei Xu'}, {'authorId': '2166503951', 'name': 'Yaozong Wu'}, {'authorId': '1751449', 'name': 'Dongqi Cai'}, {'authorId': '51300801', 'name': 'Xiang Li'}, {'authorId': '2166005800', 'name': 'Shangguang Wang'}]",['Beijing University of Posts and Telecommunications'],['China'],2023-08
2308.13916,Liang Yao,"Liang Yao, Jiazhen Peng, Chengsheng Mao, Yuan Luo",Exploring Large Language Models for Knowledge Graph Completion,Work in progress,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge graphs play a vital role in numerous artificial intelligence tasks, yet they frequently face the issue of incompleteness. In this study, we explore utilizing Large Language Models (LLM) for knowledge graph completion. We consider triples in knowledge graphs as text sequences and introduce an innovative framework called Knowledge Graph LLM (KG-LLM) to model these triples. Our technique employs entity and relation descriptions of a triple as prompts and utilizes the response for predictions. Experiments on various benchmark knowledge graphs demonstrate that our method attains state-of-the-art performance in tasks such as triple classification and relation prediction. We also find that fine-tuning relatively smaller models (e.g., LLaMA-7B, ChatGLM-6B) outperforms recent ChatGPT and GPT-4. ","[{'version': 'v1', 'created': 'Sat, 26 Aug 2023 16:51:17 GMT'}, {'version': 'v2', 'created': 'Thu, 31 Aug 2023 08:53:34 GMT'}, {'version': 'v3', 'created': 'Sun, 10 Sep 2023 17:42:37 GMT'}]",2023-09-12,"[['Yao', 'Liang', ''], ['Peng', 'Jiazhen', ''], ['Mao', 'Chengsheng', ''], ['Luo', 'Yuan', '']]",1,1,2023-08-26,3,4,2,4,2,2,450c65dce00b05bdf5d58e48289acd066aca8383,261242758.0,https://www.semanticscholar.org/paper/450c65dce00b05bdf5d58e48289acd066aca8383,arXiv.org,2023.0,41.0,3.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '100680875', 'name': 'Liang Yao'}, {'authorId': '2208655669', 'name': 'Jiazhen Peng'}, {'authorId': '145449667', 'name': 'Chengsheng Mao'}, {'authorId': '1830568527', 'name': 'Yuan Luo'}]","['Tencent', 'Northwestern University']","['China', 'United States']",2023-08
2308.13961,Jiangjie Chen,"Shuang Li, Jiangjie Chen, Siyu Yuan, Xinyi Wu, Hao Yang, Shimin Tao,
  Yanghua Xiao","Translate Meanings, Not Just Words: IdiomKB's Role in Optimizing Idiomatic Translation with Language Models",Work in progress,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  To translate well, machine translation (MT) systems and general-purposed language models (LMs) need a deep understanding of both source and target languages and cultures. Therefore, idioms, with their non-compositional nature, pose particular challenges for Transformer-based systems, as literal translations often miss the intended meaning. Traditional methods, which replace idioms using existing knowledge bases (KBs), often lack scale and context awareness. Addressing these challenges, our approach prioritizes context awareness and scalability, allowing for offline storage of idioms in a manageable KB size. This ensures efficient serving with smaller models and provides a more comprehensive understanding of idiomatic expressions. We introduce a multilingual idiom KB (IdiomKB) developed using large LMs to address this. This KB facilitates better translation by smaller models, such as BLOOMZ (7.1B), Alpaca (7B), and InstructGPT (6.7B), by retrieving idioms' figurative meanings. We present a novel, GPT-4-powered metric for human-aligned evaluation, demonstrating that IdiomKB considerably boosts model performance. Human evaluations further validate our KB's quality. ","[{'version': 'v1', 'created': 'Sat, 26 Aug 2023 21:38:31 GMT'}]",2023-08-29,"[['Li', 'Shuang', ''], ['Chen', 'Jiangjie', ''], ['Yuan', 'Siyu', ''], ['Wu', 'Xinyi', ''], ['Yang', 'Hao', ''], ['Tao', 'Shimin', ''], ['Xiao', 'Yanghua', '']]",0,1,2023-08-26,1,7,1,4,1,3,5de595241ff4c7014a2a7263f074443c5f2cb63d,261242746.0,https://www.semanticscholar.org/paper/5de595241ff4c7014a2a7263f074443c5f2cb63d,arXiv.org,2023.0,67.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2236184114', 'name': 'Shuang Li'}, {'authorId': '5040052', 'name': 'Jiangjie Chen'}, {'authorId': '2145968425', 'name': 'Siyu Yuan'}, {'authorId': '50171962', 'name': 'Xinyi Wu'}, {'authorId': '2257352978', 'name': 'Hao Yang'}, {'authorId': '1978838820', 'name': 'Shimin Tao'}, {'authorId': '2116642640', 'name': 'Yanghua Xiao'}]","['Joint Research Center', 'Fudan University', 'Huawei Technologies (China)']","['China', 'Spain']",2023-08
2308.14034,Shen Gao,"Shen Gao, Zhengliang Shi, Minghang Zhu, Bowen Fang, Xin Xin, Pengjie
  Ren, Zhumin Chen, Jun Ma",Confucius: Iterative Tool Learning from Introspection Feedback by Easy-to-Difficult Curriculum,,,,,cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Augmenting large language models (LLMs) with external tools has emerged as a promising approach to extending the capability of LLMs. Although some works employ open-source LLMs for the tool learning task, most of them are trained in a controlled environment in which LLMs only learn to execute the human-provided tools. However, selecting proper tools from the large toolset is also a crucial ability for the tool learning model to be applied in real-world applications. Existing methods usually directly employ self-instruction methods to train the model, which ignores differences in tool complexity. In this paper, we propose the Confucius, a novel tool learning framework to train LLM to use complicated tools in real-world scenarios, which contains two main phases: (1) We first propose a multi-stage learning method to teach the LLM to use various tools from an easy-to-difficult curriculum; (2) thenceforth, we propose the Iterative Self-instruct from Introspective Feedback (ISIF) to dynamically construct the dataset to improve the ability to use the complicated tool. Extensive experiments conducted on both controlled and real-world settings demonstrate the superiority of our tool learning framework in the real-world application scenarios compared to both tuning-free (e.g. ChatGPT, Claude) and tuning-based baselines (e.g. GPT4Tools). ","[{'version': 'v1', 'created': 'Sun, 27 Aug 2023 07:53:00 GMT'}]",2023-08-29,"[['Gao', 'Shen', ''], ['Shi', 'Zhengliang', ''], ['Zhu', 'Minghang', ''], ['Fang', 'Bowen', ''], ['Xin', 'Xin', ''], ['Ren', 'Pengjie', ''], ['Chen', 'Zhumin', ''], ['Ma', 'Jun', '']]",1,1,2023-08-27,1,8,2,2,0,2,3b36d16985286b03e06e8404a7be49a9713d37b9,261243312.0,https://www.semanticscholar.org/paper/3b36d16985286b03e06e8404a7be49a9713d37b9,arXiv.org,2023.0,42.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2345018', 'name': 'Shen Gao'}, {'authorId': '2195381022', 'name': 'Zhengliang Shi'}, {'authorId': '2203918573', 'name': 'Minghang Zhu'}, {'authorId': '2234357048', 'name': 'Bowen Fang'}, {'authorId': '47911456', 'name': 'Xin Xin'}, {'authorId': '1749477', 'name': 'Pengjie Ren'}, {'authorId': '1721165', 'name': 'Zhumin Chen'}, {'authorId': '2152611495', 'name': 'Jun Ma'}]","['Shandong University', 'Leiden University']","['China', 'Netherlands']",2023-08
2308.14284,Longchao Da,"Longchao Da, Minchiuan Gao, Hao Mei, Hua Wei",LLM Powered Sim-to-real Transfer for Traffic Signal Control,"9 pages, 7 figures",,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Numerous solutions are proposed for the Traffic Signal Control (TSC) tasks aiming to provide efficient transportation and mitigate congestion waste. In recent, promising results have been attained by Reinforcement Learning (RL) methods through trial and error in simulators, bringing confidence in solving cities' congestion headaches. However, there still exist performance gaps when simulator-trained policies are deployed to the real world. This issue is mainly introduced by the system dynamic difference between the training simulator and the real-world environments. The Large Language Models (LLMs) are trained on mass knowledge and proved to be equipped with astonishing inference abilities. In this work, we leverage LLMs to understand and profile the system dynamics by a prompt-based grounded action transformation. Accepting the cloze prompt template, and then filling in the answer based on accessible context, the pre-trained LLM's inference ability is exploited and applied to understand how weather conditions, traffic states, and road types influence traffic dynamics, being aware of this, the policies' action is taken and grounded based on realistic dynamics, thus help the agent learn a more realistic policy. We conduct experiments using DQN to show the effectiveness of the proposed PromptGAT's ability in mitigating the performance gap from simulation to reality (sim-to-real). ","[{'version': 'v1', 'created': 'Mon, 28 Aug 2023 03:49:13 GMT'}, {'version': 'v2', 'created': 'Mon, 4 Sep 2023 22:31:44 GMT'}]",2023-09-06,"[['Da', 'Longchao', ''], ['Gao', 'Minchiuan', ''], ['Mei', 'Hao', ''], ['Wei', 'Hua', '']]",0,0,2023-08-28,2,4,1,0,0,0,d40430275383ef8a453eefb693c44cbc686008e0,261245212.0,https://www.semanticscholar.org/paper/d40430275383ef8a453eefb693c44cbc686008e0,arXiv.org,2023.0,43.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1387492282', 'name': 'Longchao Da'}, {'authorId': '39703662', 'name': 'Mingchen Gao'}, {'authorId': '2176403267', 'name': 'Hao Mei'}, {'authorId': '1474226770', 'name': 'Hua Wei'}]","['Arizona State University', 'Zhejiang University']","['China', 'United States']",2023-08
2308.14346,Zhijie Bao,"Zhijie Bao, Wei Chen, Shengze Xiao, Kuang Ren, Jiaao Wu, Cheng Zhong,
  Jiajie Peng, Xuanjing Huang, Zhongyu Wei",DISC-MedLLM: Bridging General Large Language Models and Real-World Medical Consultation,Work in progress,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose DISC-MedLLM, a comprehensive solution that leverages Large Language Models (LLMs) to provide accurate and truthful medical response in end-to-end conversational healthcare services. To construct high-quality Supervised Fine-Tuning (SFT) datasets, we employ three strategies: utilizing medical knowledge-graphs, reconstructing real-world dialogues, and incorporating human-guided preference rephrasing. These datasets are instrumental in training DISC-MedLLM, surpassing existing medical LLMs in both single-turn and multi-turn consultation scenarios. Extensive experimental results demonstrate the effectiveness of the proposed model in bridging the gap between general language models and real-world medical consultation. Additionally, we release the constructed dataset and model weights to further contribute to research and development. Further details and resources can be found at https://github.com/FudanDISC/DISC-MedLLM ","[{'version': 'v1', 'created': 'Mon, 28 Aug 2023 06:41:49 GMT'}]",2023-08-29,"[['Bao', 'Zhijie', ''], ['Chen', 'Wei', ''], ['Xiao', 'Shengze', ''], ['Ren', 'Kuang', ''], ['Wu', 'Jiaao', ''], ['Zhong', 'Cheng', ''], ['Peng', 'Jiajie', ''], ['Huang', 'Xuanjing', ''], ['Wei', 'Zhongyu', '']]",0,0,2023-08-28,1,9,2,0,0,0,4c5b4a8e31d3119c1e3b5753693ff283c9717218,261243110.0,https://www.semanticscholar.org/paper/4c5b4a8e31d3119c1e3b5753693ff283c9717218,arXiv.org,2023.0,50.0,4.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2234360519', 'name': 'Zhijie Bao'}, {'authorId': '2256716476', 'name': 'Wei Chen'}, {'authorId': '2234363077', 'name': 'Shengze Xiao'}, {'authorId': '2234351785', 'name': 'Kuang Ren'}, {'authorId': '2234503209', 'name': 'Jiaao Wu'}, {'authorId': '2046752978', 'name': 'Cheng Zhong'}, {'authorId': '2122807664', 'name': 'J. Peng'}, {'authorId': '1790227', 'name': 'Xuanjing Huang'}, {'authorId': '2118602528', 'name': 'Zhongyu Wei'}]","['University of Toronto', 'Fudan University', 'Northwestern Polytechnical University']","['China', 'Canada']",2023-08
2308.14352,Rongjie Yi,"Rongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shangguang Wang, Mengwei
  Xu",EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models,,,,,cs.LG cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) such as GPTs and LLaMa have ushered in a revolution in machine intelligence, owing to their exceptional capabilities in a wide range of machine learning tasks. However, the transition of LLMs from data centers to edge devices presents a set of challenges and opportunities. While this shift can enhance privacy and availability, it is hampered by the enormous parameter sizes of these models, leading to impractical runtime costs. In light of these considerations, we introduce EdgeMoE, the first on-device inference engine tailored for mixture-of-expert (MoE) LLMs, a popular variant of sparse LLMs that exhibit nearly constant computational complexity as their parameter size scales. EdgeMoE achieves both memory and computational efficiency by strategically partitioning the model across the storage hierarchy. Specifically, non-expert weights are stored in the device's memory, while expert weights are kept in external storage and are fetched into memory only when they are activated. This design is underpinned by a crucial insight that expert weights, though voluminous, are infrequently accessed due to sparse activation patterns. To further mitigate the overhead associated with expert I/O swapping, EdgeMoE incorporates two innovative techniques: (1) Expert-wise bitwidth adaptation: This method reduces the size of expert weights with an acceptable level of accuracy loss. (2) Expert management: It predicts the experts that will be activated in advance and preloads them into the compute-I/O pipeline, thus further optimizing the process. In empirical evaluations conducted on well-established MoE LLMs and various edge devices, EdgeMoE demonstrates substantial memory savings and performance improvements when compared to competitive baseline solutions. ","[{'version': 'v1', 'created': 'Mon, 28 Aug 2023 06:56:08 GMT'}]",2023-08-29,"[['Yi', 'Rongjie', ''], ['Guo', 'Liwei', ''], ['Wei', 'Shiyun', ''], ['Zhou', 'Ao', ''], ['Wang', 'Shangguang', ''], ['Xu', 'Mengwei', '']]",0,1,2023-08-28,1,6,3,1,1,0,dc7b27b0a1d891e16f80dd9b8eec0aa8baf95b36,261243273.0,https://www.semanticscholar.org/paper/dc7b27b0a1d891e16f80dd9b8eec0aa8baf95b36,arXiv.org,2023.0,85.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2170360770', 'name': 'Rongjie Yi'}, {'authorId': '2110519994', 'name': 'Liwei Guo'}, {'authorId': '1384679988', 'name': 'Shiyun Wei'}, {'authorId': '2536849', 'name': 'Ao Zhou'}, {'authorId': '2166005800', 'name': 'Shangguang Wang'}, {'authorId': '2529558', 'name': 'Mengwei Xu'}]","['Beijing University of Posts and Telecommunications', 'ZGC Laboratory', 'Huawei Technologies (China)']",['China'],2023-08
2308.14353,Baoli Zhang,"Baoli Zhang, Haining Xie, Pengfan Du, Junhao Chen, Pengfei Cao, Yubo
  Chen, Shengping Liu, Kang Liu, Jun Zhao","ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models",,,,,cs.CL,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  The unprecedented performance of large language models (LLMs) requires comprehensive and accurate evaluation. We argue that for LLMs evaluation, benchmarks need to be comprehensive and systematic. To this end, we propose the ZhuJiu benchmark, which has the following strengths: (1) Multi-dimensional ability coverage: We comprehensively evaluate LLMs across 7 ability dimensions covering 51 tasks. Especially, we also propose a new benchmark that focuses on knowledge ability of LLMs. (2) Multi-faceted evaluation methods collaboration: We use 3 different yet complementary evaluation methods to comprehensively evaluate LLMs, which can ensure the authority and accuracy of the evaluation results. (3) Comprehensive Chinese benchmark: ZhuJiu is the pioneering benchmark that fully assesses LLMs in Chinese, while also providing equally robust evaluation abilities in English. (4) Avoiding potential data leakage: To avoid data leakage, we construct evaluation data specifically for 37 tasks. We evaluate 10 current mainstream LLMs and conduct an in-depth discussion and analysis of their results. The ZhuJiu benchmark and open-participation leaderboard are publicly released at http://www.zhujiu-benchmark.com/ and we also provide a demo video at https://youtu.be/qypkJ89L1Ic. ","[{'version': 'v1', 'created': 'Mon, 28 Aug 2023 06:56:44 GMT'}]",2023-08-29,"[['Zhang', 'Baoli', ''], ['Xie', 'Haining', ''], ['Du', 'Pengfan', ''], ['Chen', 'Junhao', ''], ['Cao', 'Pengfei', ''], ['Chen', 'Yubo', ''], ['Liu', 'Shengping', ''], ['Liu', 'Kang', ''], ['Zhao', 'Jun', '']]",0,0,2023-08-28,1,9,1,0,0,0,425bf938731c41de398aa3179e28bab98409b650,261242536.0,https://www.semanticscholar.org/paper/425bf938731c41de398aa3179e28bab98409b650,arXiv.org,2023.0,41.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2118875035', 'name': 'Baolin Zhang'}, {'authorId': '2117713865', 'name': 'Hai-Yong Xie'}, {'authorId': '2234347190', 'name': 'Pengfan Du'}, {'authorId': '2155463899', 'name': 'Junhao Chen'}, {'authorId': '49776272', 'name': 'Pengfei Cao'}, {'authorId': '1763402', 'name': 'Yubo Chen'}, {'authorId': '2035396', 'name': 'Shengping Liu'}, {'authorId': '77397868', 'name': 'Kang Liu'}, {'authorId': '11447228', 'name': 'Jun Zhao'}]","['University of Chinese Academy of Sciences', 'Harbin Engineering University', 'Beijing Unisound Information Technology Co., Ltd', 'Chinese Academy of Sciences']",['China'],2023-08
2308.14363,Jinliang Yuan,"Jinliang Yuan, Chen Yang, Dongqi Cai, Shihe Wang, Xin Yuan, Zeling
  Zhang, Xiang Li, Dingge Zhang, Hanzi Mei, Xianqing Jia, Shangguang Wang,
  Mengwei Xu",Rethinking Mobile AI Ecosystem in the LLM Era,,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In today's landscape, smartphones have evolved into hubs for hosting a multitude of deep learning models aimed at local execution. A key realization driving this work is the notable fragmentation among these models, characterized by varied architectures, operators, and implementations. This fragmentation imposes a significant burden on the comprehensive optimization of hardware, system settings, and algorithms.   Buoyed by the recent strides in large foundation models, this work introduces a pioneering paradigm for mobile AI: a collaborative management approach between the mobile OS and hardware, overseeing a foundational model capable of serving a broad spectrum of mobile AI tasks, if not all. This foundational model resides within the NPU and remains impervious to app or OS revisions, akin to firmware. Concurrently, each app contributes a concise, offline fine-tuned ""adapter"" tailored to distinct downstream tasks. From this concept emerges a concrete instantiation known as \sys. It amalgamates a curated selection of publicly available Large Language Models (LLMs) and facilitates dynamic data flow. This concept's viability is substantiated through the creation of an exhaustive benchmark encompassing 38 mobile AI tasks spanning 50 datasets, including domains such as Computer Vision (CV), Natural Language Processing (NLP), audio, sensing, and multimodal inputs. Spanning this benchmark, \sys unveils its impressive performance. It attains accuracy parity in 85\% of tasks, demonstrates improved scalability in terms of storage and memory, and offers satisfactory inference speed on Commercial Off-The-Shelf (COTS) mobile devices fortified with NPU support. This stands in stark contrast to task-specific models tailored for individual applications. ","[{'version': 'v1', 'created': 'Mon, 28 Aug 2023 07:21:26 GMT'}]",2023-08-29,"[['Yuan', 'Jinliang', ''], ['Yang', 'Chen', ''], ['Cai', 'Dongqi', ''], ['Wang', 'Shihe', ''], ['Yuan', 'Xin', ''], ['Zhang', 'Zeling', ''], ['Li', 'Xiang', ''], ['Zhang', 'Dingge', ''], ['Mei', 'Hanzi', ''], ['Jia', 'Xianqing', ''], ['Wang', 'Shangguang', ''], ['Xu', 'Mengwei', '']]",0,0,2023-08-28,1,12,1,0,0,0,0b0ba1e858573e25245c06e2ec54b72882e1d076,261242681.0,https://www.semanticscholar.org/paper/0b0ba1e858573e25245c06e2ec54b72882e1d076,arXiv.org,2023.0,109.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2237130158', 'name': 'Jinliang Yuan'}, {'authorId': '2154173187', 'name': 'Chenchen Yang'}, {'authorId': '1751449', 'name': 'Dongqi Cai'}, {'authorId': '2109511224', 'name': 'Shihe Wang'}, {'authorId': '2225872016', 'name': 'Xin Yuan'}, {'authorId': '2134911086', 'name': 'Zeling Zhang'}, {'authorId': '51300801', 'name': 'Xiang Li'}, {'authorId': '2141082943', 'name': 'Di Zhang'}, {'authorId': '2234353663', 'name': 'Hanzi Mei'}, {'authorId': '2234504694', 'name': 'Xianqing Jia'}, {'authorId': '2166005800', 'name': 'Shangguang Wang'}, {'authorId': '2529558', 'name': 'Mengwei Xu'}]",['Beijing University of Posts and Telecommunications'],['China'],2023-08
2308.14448,Yicheng Zhong,"Yicheng Zhong, Huawei Wei, Peiji Yang, Zhisheng Wang",ExpCLIP: Bridging Text and Facial Expressions via Semantic Alignment,,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The objective of stylized speech-driven facial animation is to create animations that encapsulate specific emotional expressions. Existing methods often depend on pre-established emotional labels or facial expression templates, which may limit the necessary flexibility for accurately conveying user intent. In this research, we introduce a technique that enables the control of arbitrary styles by leveraging natural language as emotion prompts. This technique presents benefits in terms of both flexibility and user-friendliness. To realize this objective, we initially construct a Text-Expression Alignment Dataset (TEAD), wherein each facial expression is paired with several prompt-like descriptions.We propose an innovative automatic annotation method, supported by Large Language Models (LLMs), to expedite the dataset construction, thereby eliminating the substantial expense of manual annotation. Following this, we utilize TEAD to train a CLIP-based model, termed ExpCLIP, which encodes text and facial expressions into semantically aligned style embeddings. The embeddings are subsequently integrated into the facial animation generator to yield expressive and controllable facial animations. Given the limited diversity of facial emotions in existing speech-driven facial animation training data, we further introduce an effective Expression Prompt Augmentation (EPA) mechanism to enable the animation generator to support unprecedented richness in style control. Comprehensive experiments illustrate that our method accomplishes expressive facial animation generation and offers enhanced flexibility in effectively conveying the desired style. ","[{'version': 'v1', 'created': 'Mon, 28 Aug 2023 09:35:13 GMT'}, {'version': 'v2', 'created': 'Mon, 11 Sep 2023 08:56:32 GMT'}]",2023-09-12,"[['Zhong', 'Yicheng', ''], ['Wei', 'Huawei', ''], ['Yang', 'Peiji', ''], ['Wang', 'Zhisheng', '']]",0,0,2023-08-28,2,4,2,0,0,0,68159fc73e3b9d289d0b72137b1e3357d50ce2fe,261243936.0,https://www.semanticscholar.org/paper/68159fc73e3b9d289d0b72137b1e3357d50ce2fe,arXiv.org,2023.0,42.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2088676959', 'name': 'Yicheng Zhong'}, {'authorId': '48056150', 'name': 'Huawei Wei'}, {'authorId': '2119185269', 'name': 'Pei-Yin Yang'}, {'authorId': '2108086267', 'name': 'Zhisheng Wang'}]",['Tencent'],['China'],2023-08
2308.14508,Yushi Bai,"Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian
  Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,
  Juanzi Li","LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding","18 pages, 6 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs' long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability. The code and datasets are available at https://github.com/THUDM/LongBench. ","[{'version': 'v1', 'created': 'Mon, 28 Aug 2023 11:53:40 GMT'}]",2023-08-29,"[['Bai', 'Yushi', ''], ['Lv', 'Xin', ''], ['Zhang', 'Jiajie', ''], ['Lyu', 'Hongchang', ''], ['Tang', 'Jiankai', ''], ['Huang', 'Zhidian', ''], ['Du', 'Zhengxiao', ''], ['Liu', 'Xiao', ''], ['Zeng', 'Aohan', ''], ['Hou', 'Lei', ''], ['Dong', 'Yuxiao', ''], ['Tang', 'Jie', ''], ['Li', 'Juanzi', '']]",0,1,2023-08-28,1,13,1,1,0,1,b31a5884a8ebe96b6300839b28608b97f8f8ef76,261245264.0,https://www.semanticscholar.org/paper/b31a5884a8ebe96b6300839b28608b97f8f8ef76,arXiv.org,2023.0,67.0,15.0,6.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2141377570', 'name': 'Yushi Bai'}, {'authorId': '48574888', 'name': 'Xin Lv'}, {'authorId': '2107983722', 'name': 'Jiajie Zhang'}, {'authorId': '2220304036', 'name': 'Hong Lyu'}, {'authorId': '2204826271', 'name': 'Jiankai Tang'}, {'authorId': '2234629967', 'name': 'Zhidian Huang'}, {'authorId': '66395694', 'name': 'Zhengxiao Du'}, {'authorId': '2111312892', 'name': 'Xiao Liu'}, {'authorId': '2051712753', 'name': 'Aohan Zeng'}, {'authorId': '2055765060', 'name': 'Lei Hou'}, {'authorId': '2047998', 'name': 'Yuxiao Dong'}, {'authorId': '2148911975', 'name': 'Jie Tang'}, {'authorId': '2133353675', 'name': 'Juanzi Li'}]","['Tsinghua University', 'Chinese Academy of Sciences']",['China'],2023-08
2308.14536,Baorian Nuchged,"Linkai Peng, Baorian Nuchged and Yingming Gao",Spoken Language Intelligence of Large Language Models for Language Learning,"28 pages, 7 figures, Preprint",,,,cs.CL cs.AI cs.LG cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  People have long hoped for a conversational system that can assist in real-life situations, and recent progress on large language models (LLMs) is bringing this idea closer to reality. While LLMs are often impressive in performance, their efficacy in real-world scenarios that demand expert knowledge remains unclear. LLMs are believed to hold the most potential and value in education, especially in the development of Artificial intelligence (AI) based virtual teachers capable of facilitating language learning. Our focus is centered on evaluating the efficacy of LLMs in the realm of education, specifically in the areas of spoken language learning which encompass phonetics, phonology, and second language acquisition. We introduce a new multiple-choice question dataset to evaluate the effectiveness of LLMs in the aforementioned scenarios, including understanding and application of spoken language knowledge. In addition, we investigate the influence of various prompting techniques such as zero- and few-shot method (prepending the question with question-answer exemplars), chain-of-thought (CoT, think step-by-step), in-domain exampler and external tools (Google, Wikipedia). We conducted large-scale evaluation on popular LLMs (20 distinct models) using these methods. We achieved significant performance improvements compared to the zero-shot baseline in the practical questions reasoning (GPT-3.5, 49.1% -> 63.1%; LLaMA2-70B-Chat, 42.2% -> 48.6%). We found that models of different sizes have good understanding of concepts in phonetics, phonology, and second language acquisition, but show limitations in reasoning for real-world problems. Additionally, we also explore preliminary findings on conversational communication. ","[{'version': 'v1', 'created': 'Mon, 28 Aug 2023 12:47:41 GMT'}]",2023-08-29,"[['Peng', 'Linkai', ''], ['Nuchged', 'Baorian', ''], ['Gao', 'Yingming', '']]",0,1,2023-08-28,1,3,5,1,0,1,19b43ff57e5d8f8a99da4110fbc30b4ecc39a527,261243763.0,https://www.semanticscholar.org/paper/19b43ff57e5d8f8a99da4110fbc30b4ecc39a527,arXiv.org,2023.0,59.0,1.0,0.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2117699319', 'name': 'Linkai Peng'}, {'authorId': '2234373508', 'name': 'Baorian Nuchged'}, {'authorId': '2118543175', 'name': 'Yingming Gao'}]","['Beijing University of Posts and Telecommunications', 'NetEase', 'The University of Texas at Austin']","['China', 'United States']",2023-08
2308.15022,Qingyue Wang,"Qingyue Wang, Liang Ding, Yanan Cao, Zhiliang Tian, Shi Wang, Dacheng
  Tao, Li Guo",Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Most open-domain dialogue systems suffer from forgetting important information, especially in a long-term conversation. Existing works usually train the specific retriever or summarizer to obtain key information from the past, which is time-consuming and highly depends on the quality of labeled data. To alleviate this problem, we propose to recursively generate summaries/ memory using large language models (LLMs) to enhance long-term memory ability. Specifically, our method first stimulates LLMs to memorize small dialogue contexts and then recursively produce new memory using previous memory and following contexts. Finally, the LLM can easily generate a highly consistent response with the help of the latest memory. We evaluate our method using ChatGPT and text-davinci-003, and the experiments on the widely-used public dataset show that our method can generate more consistent responses in a long-context conversation. Notably, our method is a potential solution to enable the LLM to model the extremely long context. Code and scripts will be released later. ","[{'version': 'v1', 'created': 'Tue, 29 Aug 2023 04:59:53 GMT'}]",2023-08-30,"[['Wang', 'Qingyue', ''], ['Ding', 'Liang', ''], ['Cao', 'Yanan', ''], ['Tian', 'Zhiliang', ''], ['Wang', 'Shi', ''], ['Tao', 'Dacheng', ''], ['Guo', 'Li', '']]",1,1,2023-08-29,1,7,2,1,0,1,d56c1e49fcc6a8effe90786af5c7d1f0a295a7a6,261276822.0,https://www.semanticscholar.org/paper/d56c1e49fcc6a8effe90786af5c7d1f0a295a7a6,arXiv.org,2023.0,37.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2115979374', 'name': 'Qingyue Wang'}, {'authorId': '46573238', 'name': 'Liang Ding'}, {'authorId': '9310727', 'name': 'Yanan Cao'}, {'authorId': '33992653', 'name': 'Zhiliang Tian'}, {'authorId': '2108668664', 'name': 'Shi Wang'}, {'authorId': '2140448089', 'name': 'Dacheng Tao'}, {'authorId': '2148933499', 'name': 'Li Guo'}]","['Institute of Information Engineering', 'National University of Defense Technology', 'University of Chinese Academy of Sciences', 'University of Sydney', 'Chinese Academy of Sciences']","['China', 'Australia']",2023-08
2308.15078,Kezhi Wang,"Li Dong, Feibo Jiang, Yubo Peng, Kezhi Wang, Kun Yang, Cunhua Pan,
  Robert Schober",LAMBO: Large Language Model Empowered Edge Intelligence,To be submitted for possible journal publication,,,,cs.AI cs.NI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Next-generation edge intelligence is anticipated to bring huge benefits to various applications, e.g., offloading systems. However, traditional deep offloading architectures face several issues, including heterogeneous constraints, partial perception, uncertain generalization, and lack of tractability. In this context, the integration of offloading with large language models (LLMs) presents numerous advantages. Therefore, we propose an LLM-Based Offloading (LAMBO) framework for mobile edge computing (MEC), which comprises four components: (i) Input embedding (IE), which is used to represent the information of the offloading system with constraints and prompts through learnable vectors with high quality; (ii) Asymmetric encoderdecoder (AED) model, which is a decision-making module with a deep encoder and a shallow decoder. It can achieve high performance based on multi-head self-attention schemes; (iii) Actor-critic reinforcement learning (ACRL) module, which is employed to pre-train the whole AED for different optimization tasks under corresponding prompts; and (iv) Active learning from expert feedback (ALEF), which can be used to finetune the decoder part of the AED while adapting to dynamic environmental changes. Our simulation results corroborate the advantages of the proposed LAMBO framework. ","[{'version': 'v1', 'created': 'Tue, 29 Aug 2023 07:25:42 GMT'}]",2023-08-30,"[['Dong', 'Li', ''], ['Jiang', 'Feibo', ''], ['Peng', 'Yubo', ''], ['Wang', 'Kezhi', ''], ['Yang', 'Kun', ''], ['Pan', 'Cunhua', ''], ['Schober', 'Robert', '']]",0,0,2023-08-29,1,7,2,0,0,0,03ec12e0fdc22785dfc7491c7bb3fac9c3e005eb,261276447.0,https://www.semanticscholar.org/paper/03ec12e0fdc22785dfc7491c7bb3fac9c3e005eb,arXiv.org,2023.0,21.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2152288497', 'name': 'Li Dong'}, {'authorId': '40540111', 'name': 'Feibo Jiang'}, {'authorId': '2111035161', 'name': 'Yubo Peng'}, {'authorId': '1878751', 'name': 'Kezhi Wang'}, {'authorId': '2214014346', 'name': 'Kun Yang'}, {'authorId': '144442022', 'name': 'Cunhua Pan'}, {'authorId': '143677566', 'name': 'R. Schober'}]","['Brunel University London', 'Hunan University of Technology', 'Changchun Institute of Technology', 'University of Essex', 'Hunan Normal University', 'Friedrich-Alexander-Universitt Erlangen-Nrnberg', 'Southeast University']","['China', 'Germany', 'United Kingdom']",2023-08
2308.15126,Yiyang Zhou,"Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao,
  Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, Jitao Sang, Haoyu Tang",Evaluation and Analysis of Hallucination in Large Vision-Language Models,"11 pages, 5 figures",,,,cs.LG cs.AI cs.CL cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Vision-Language Models (LVLMs) have recently achieved remarkable success. However, LVLMs are still plagued by the hallucination problem, which limits the practicality in many scenarios. Hallucination refers to the information of LVLMs' responses that does not exist in the visual input, which poses potential risks of substantial consequences. There has been limited work studying hallucination evaluation in LVLMs. In this paper, we propose Hallucination Evaluation based on Large Language Models (HaELM), an LLM-based hallucination evaluation framework. HaELM achieves an approximate 95% performance comparable to ChatGPT and has additional advantages including low cost, reproducibility, privacy preservation and local deployment. Leveraging the HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem. Our training data and human annotation hallucination data will be made public soon. ","[{'version': 'v1', 'created': 'Tue, 29 Aug 2023 08:51:24 GMT'}, {'version': 'v2', 'created': 'Tue, 19 Sep 2023 02:39:04 GMT'}]",2023-09-20,"[['Wang', 'Junyang', ''], ['Zhou', 'Yiyang', ''], ['Xu', 'Guohai', ''], ['Shi', 'Pengcheng', ''], ['Zhao', 'Chenlin', ''], ['Xu', 'Haiyang', ''], ['Ye', 'Qinghao', ''], ['Yan', 'Ming', ''], ['Zhang', 'Ji', ''], ['Zhu', 'Jihua', ''], ['Sang', 'Jitao', ''], ['Tang', 'Haoyu', '']]",1,1,2023-08-29,2,12,4,1,0,1,bb1083425517bdac8d9a6438fcf5032543acb20e,261276646.0,https://www.semanticscholar.org/paper/bb1083425517bdac8d9a6438fcf5032543acb20e,arXiv.org,2023.0,35.0,5.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2110125710', 'name': 'Junyan Wang'}, {'authorId': '2118764703', 'name': 'Yi Zhou'}, {'authorId': '2115723816', 'name': 'Guohai Xu'}, {'authorId': '2055357477', 'name': 'Pengcheng Shi'}, {'authorId': '2232751254', 'name': 'Chenlin Zhao'}, {'authorId': '153194420', 'name': 'Haiyang Xu'}, {'authorId': '2199011713', 'name': 'Qinghao Ye'}, {'authorId': '2114009661', 'name': 'Mingshi Yan'}, {'authorId': '2116921824', 'name': 'Ji Zhang'}, {'authorId': '2237811', 'name': 'Jihua Zhu'}, {'authorId': '1798398', 'name': 'J. Sang'}, {'authorId': '49687714', 'name': 'Haoyu Tang'}]","['Alibaba', ""Xi'an Jiaotong University"", 'Chinese Academy of Sciences', 'Beijing Jiaotong University', 'Shandong University']",['China'],2023-08
2308.15143,Lei Han,"Lei Han, Qingxu Zhu, Jiapeng Sheng, Chong Zhang, Tingguang Li, Yizheng
  Zhang, He Zhang, Yuzhen Liu, Cheng Zhou, Rui Zhao, Jie Li, Yufeng Zhang, Rui
  Wang, Wanchao Chi, Xiong Li, Yonghui Zhu, Lingzhu Xiang, Xiao Teng, Zhengyou
  Zhang",Lifelike Agility and Play on Quadrupedal Robots using Reinforcement Learning and Generative Pre-trained Models,,,,,cs.RO cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Summarizing knowledge from animals and human beings inspires robotic innovations. In this work, we propose a framework for driving legged robots act like real animals with lifelike agility and strategy in complex environments. Inspired by large pre-trained models witnessed with impressive performance in language and image understanding, we introduce the power of advanced deep generative models to produce motor control signals stimulating legged robots to act like real animals. Unlike conventional controllers and end-to-end RL methods that are task-specific, we propose to pre-train generative models over animal motion datasets to preserve expressive knowledge of animal behavior. The pre-trained model holds sufficient primitive-level knowledge yet is environment-agnostic. It is then reused for a successive stage of learning to align with the environments by traversing a number of challenging obstacles that are rarely considered in previous approaches, including creeping through narrow spaces, jumping over hurdles, freerunning over scattered blocks, etc. Finally, a task-specific controller is trained to solve complex downstream tasks by reusing the knowledge from previous stages. Enriching the knowledge regarding each stage does not affect the usage of other levels of knowledge. This flexible framework offers the possibility of continual knowledge accumulation at different levels. We successfully apply the trained multi-level controllers to the MAX robot, a quadrupedal robot developed in-house, to mimic animals, traverse complex obstacles, and play in a designed challenging multi-agent Chase Tag Game, where lifelike agility and strategy emerge on the robots. The present research pushes the frontier of robot control with new insights on reusing multi-level pre-trained knowledge and solving highly complex downstream tasks in the real world. ","[{'version': 'v1', 'created': 'Tue, 29 Aug 2023 09:22:12 GMT'}]",2023-08-30,"[['Han', 'Lei', ''], ['Zhu', 'Qingxu', ''], ['Sheng', 'Jiapeng', ''], ['Zhang', 'Chong', ''], ['Li', 'Tingguang', ''], ['Zhang', 'Yizheng', ''], ['Zhang', 'He', ''], ['Liu', 'Yuzhen', ''], ['Zhou', 'Cheng', ''], ['Zhao', 'Rui', ''], ['Li', 'Jie', ''], ['Zhang', 'Yufeng', ''], ['Wang', 'Rui', ''], ['Chi', 'Wanchao', ''], ['Li', 'Xiong', ''], ['Zhu', 'Yonghui', ''], ['Xiang', 'Lingzhu', ''], ['Teng', 'Xiao', ''], ['Zhang', 'Zhengyou', '']]",0,1,2023-08-29,1,19,2,0,0,0,15669c70376e6d4734881fe501b3b12306ff55e0,261276656.0,https://www.semanticscholar.org/paper/15669c70376e6d4734881fe501b3b12306ff55e0,arXiv.org,2023.0,46.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2112661118', 'name': 'Lei Han'}, {'authorId': '2152208973', 'name': 'Qing Zhu'}, {'authorId': '2166150288', 'name': 'Jiapeng Sheng'}, {'authorId': '2144016094', 'name': 'Chong Zhang'}, {'authorId': '30720137', 'name': 'Tingguang Li'}, {'authorId': '1591122785', 'name': 'Yizheng Zhang'}, {'authorId': '2153528235', 'name': 'He Zhang'}, {'authorId': '2224381214', 'name': 'Yuzhen Liu'}, {'authorId': '2111168222', 'name': 'Cheng Zhou'}, {'authorId': '2198455188', 'name': 'Rui Zhao'}, {'authorId': '49298718', 'name': 'Jie Li'}, {'authorId': '2108298468', 'name': 'Yufeng Zhang'}, {'authorId': '2151037873', 'name': 'Rui Wang'}, {'authorId': '2066297735', 'name': 'Wanchao Chi'}, {'authorId': '2235248190', 'name': 'Xiong Li'}, {'authorId': '2159072214', 'name': 'Y. Zhu'}, {'authorId': '2977581', 'name': 'Lingzhu Xiang'}, {'authorId': '2224133142', 'name': 'Xiao Teng'}, {'authorId': '2148905709', 'name': 'Zhengyou Zhang'}]","['Tencent', 'Equal contribution.']",['China'],2023-08
2308.15192,Guanghui Fu,"Guanghui Fu, Qing Zhao, Jianqiang Li, Dan Luo, Changwei Song, Wei
  Zhai, Shuo Liu, Fan Wang, Yan Wang, Lijuan Cheng, Juan Zhang, Bing Xiang Yang",Enhancing Psychological Counseling with Large Language Model: A Multifaceted Decision-Support System for Non-Professionals,,,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the contemporary landscape of social media, an alarming number of users express negative emotions, some of which manifest as strong suicidal intentions. This situation underscores a profound need for trained psychological counselors who can enact effective mental interventions. However, the development of these professionals is often an imperative but time-consuming task. Consequently, the mobilization of non-professionals or volunteers in this capacity emerges as a pressing concern. Leveraging the capabilities of artificial intelligence, and in particular, the recent advances in large language models, offers a viable solution to this challenge. This paper introduces a novel model constructed on the foundation of large language models to fully assist non-professionals in providing psychological interventions on online user discourses. This framework makes it plausible to harness the power of non-professional counselors in a meaningful way. A comprehensive study was conducted involving ten professional psychological counselors of varying expertise, evaluating the system across five critical dimensions. The findings affirm that our system is capable of analyzing patients' issues with relative accuracy and proffering professional-level strategies recommendations, thereby enhancing support for non-professionals. This research serves as a compelling validation of the application of large language models in the field of psychology and lays the groundwork for a new paradigm of community-based mental health support. ","[{'version': 'v1', 'created': 'Tue, 29 Aug 2023 10:20:53 GMT'}]",2023-08-30,"[['Fu', 'Guanghui', ''], ['Zhao', 'Qing', ''], ['Li', 'Jianqiang', ''], ['Luo', 'Dan', ''], ['Song', 'Changwei', ''], ['Zhai', 'Wei', ''], ['Liu', 'Shuo', ''], ['Wang', 'Fan', ''], ['Wang', 'Yan', ''], ['Cheng', 'Lijuan', ''], ['Zhang', 'Juan', ''], ['Yang', 'Bing Xiang', '']]",0,0,2023-08-29,1,12,2,0,0,0,d240186d5d401cef977f21ba1db4617924c4d225,261276995.0,https://www.semanticscholar.org/paper/d240186d5d401cef977f21ba1db4617924c4d225,arXiv.org,2023.0,44.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Psychology', 'source': 's2-fos-model'}]","[{'authorId': '2150452138', 'name': 'Guanghui Fu'}, {'authorId': '2118377381', 'name': 'Qing Zhao'}, {'authorId': '2144521397', 'name': 'Jianqiang Li'}, {'authorId': '2051844095', 'name': 'Dan Luo'}, {'authorId': '2152601172', 'name': 'Changwei Song'}, {'authorId': '2235074081', 'name': 'Wei Zhai'}, {'authorId': '50152261', 'name': 'Shuo Liu'}, {'authorId': '2145903611', 'name': 'Fan Wang'}, {'authorId': '2170544494', 'name': 'Yan Wang'}, {'authorId': '2236245200', 'name': 'Lijuan Cheng'}, {'authorId': '2108137641', 'name': 'Juan Zhang'}, {'authorId': '50699238', 'name': 'B. Yang'}]","['Wuhan University', 'Wuhan Hospital for Psychotherapy, Wuhan 430012, Hubei province, China', 'Beijing University of Technology', 'Piti-Salptrire Hospital', 'Shanghai Mental Health Center']","['China', 'France']",2023-08
2308.15272,Hao Wen,"Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun
  Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, Yunxin Liu",Empowering LLM to use Smartphone for Intelligent Task Automation,,,,,cs.AI cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Mobile task automation is an attractive technique that aims to enable voice-based hands-free user interaction with smartphones. However, existing approaches suffer from poor scalability due to the limited language understanding ability and the non-trivial manual efforts required from developers or end-users. The recent advance of large language models (LLMs) in language understanding and reasoning inspires us to rethink the problem from a model-centric perspective, where task preparation, comprehension, and execution are handled by a unified language model. In this work, we introduce AutoDroid, a mobile task automation system that can handle arbitrary tasks on any Android application without manual efforts. The key insight is to combine the commonsense knowledge of LLMs and domain-specific knowledge of apps through automated dynamic analysis. The main components include a functionality-aware UI representation method that bridges the UI with the LLM, exploration-based memory injection techniques that augment the app-specific domain knowledge of LLM, and a multi-granularity query optimization module that reduces the cost of model inference. We integrate AutoDroid with off-the-shelf LLMs including online GPT-4/GPT-3.5 and on-device Vicuna, and evaluate its performance on a new benchmark for memory-augmented Android task automation with 158 common tasks. The results demonstrated that AutoDroid is able to precisely generate actions with an accuracy of 90.9%, and complete tasks with a success rate of 71.3%, outperforming the GPT-4-powered baselines by 36.4% and 39.7%. The demo, benchmark suites, and source code of AutoDroid will be released at url{https://autodroid-sys.github.io/}. ","[{'version': 'v1', 'created': 'Tue, 29 Aug 2023 13:02:30 GMT'}, {'version': 'v2', 'created': 'Wed, 6 Sep 2023 14:14:04 GMT'}, {'version': 'v3', 'created': 'Sat, 9 Sep 2023 14:08:32 GMT'}]",2023-09-12,"[['Wen', 'Hao', ''], ['Li', 'Yuanchun', ''], ['Liu', 'Guohong', ''], ['Zhao', 'Shanhui', ''], ['Yu', 'Tao', ''], ['Li', 'Toby Jia-Jun', ''], ['Jiang', 'Shiqi', ''], ['Liu', 'Yunhao', ''], ['Zhang', 'Yaqin', ''], ['Liu', 'Yunxin', '']]",0,1,2023-08-29,3,10,2,3,1,2,cba18e04e8474b21838b076626486a6b02a68406,261277501.0,https://www.semanticscholar.org/paper/cba18e04e8474b21838b076626486a6b02a68406,arXiv.org,2023.0,69.0,6.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Engineering', 'source': 's2-fos-model'}]","[{'authorId': '2188861830', 'name': 'Hao Wen'}, {'authorId': '2144416765', 'name': 'Yuanchun Li'}, {'authorId': '2235520970', 'name': 'Guohong Liu'}, {'authorId': '92736469', 'name': 'Shanhui Zhao'}, {'authorId': '2256865742', 'name': 'Tao Yu'}, {'authorId': '34997918', 'name': 'Toby Jia-Jun Li'}, {'authorId': '2170664295', 'name': 'Shiqi Jiang'}, {'authorId': '2235192744', 'name': 'Yunhao Liu'}, {'authorId': '2108201285', 'name': 'Yaqin Zhang'}, {'authorId': '2161440197', 'name': 'Yunxin Liu'}]","['Tsinghua University', 'Microsoft', 'University of Notre Dame']","['China', 'United States']",2023-08
2308.15276,YongHao Wu,"Yonghao Wu, Zheng Li, Jie M. Zhang, Mike Papadakis, Mark Harman, and
  Yong Liu",Large Language Models in Fault Localisation,,,,,cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) have shown promise in multiple software engineering tasks including code generation, program repair, code summarisation, and test generation. Fault localisation is instrumental in enabling automated debugging and repair of programs and was prominently featured as a highlight during the launch event of ChatGPT-4. Nevertheless, the performance of LLMs compared to state-of-the-art methods, as well as the impact of prompt design and context length on their efficacy, remains unclear. To fill this gap, this paper presents an in-depth investigation into the capability of ChatGPT-3.5 and ChatGPT-4, the two state-of-the-art LLMs, on fault localisation. Using the widely-adopted large-scale Defects4J dataset, we compare the two LLMs with the existing fault localisation techniques. We also investigate the consistency of LLMs in fault localisation, as well as how prompt engineering and the length of code context affect the fault localisation effectiveness.   Our findings demonstrate that within function-level context, ChatGPT-4 outperforms all the existing fault localisation methods. Additional error logs can further improve ChatGPT models' localisation accuracy and consistency, with an average 46.9% higher accuracy over the state-of-the-art baseline SmartFL on the Defects4J dataset in terms of TOP-1 metric. However, when the code context of the Defects4J dataset expands to the class-level, ChatGPT-4's performance suffers a significant drop, with 49.9% lower accuracy than SmartFL under TOP-1 metric. These observations indicate that although ChatGPT can effectively localise faults under specific conditions, limitations are evident. Further research is needed to fully harness the potential of LLMs like ChatGPT for practical fault localisation applications. ","[{'version': 'v1', 'created': 'Tue, 29 Aug 2023 13:07:27 GMT'}, {'version': 'v2', 'created': 'Sat, 2 Sep 2023 14:54:39 GMT'}, {'version': 'v3', 'created': 'Mon, 2 Oct 2023 16:09:14 GMT'}]",2023-10-03,"[['Wu', 'Yonghao', ''], ['Li', 'Zheng', ''], ['Zhang', 'Jie M.', ''], ['Papadakis', 'Mike', ''], ['Harman', 'Mark', ''], ['Liu', 'Yong', '']]",1,1,2023-08-29,3,6,1,1,0,1,7d3d98707182e0733d8cf5ee763314c60d638f4a,261276149.0,https://www.semanticscholar.org/paper/7d3d98707182e0733d8cf5ee763314c60d638f4a,arXiv.org,2023.0,70.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2115859696', 'name': 'Yonghao Wu'}, {'authorId': '2146247928', 'name': 'Zheng Li'}, {'authorId': '51250527', 'name': 'J Zhang'}, {'authorId': '37766916', 'name': 'Mike Papadakis'}, {'authorId': '145836176', 'name': 'M. Harman'}, {'authorId': '2144385436', 'name': 'Yong Liu'}]","['National metrology institute VTT MIKES', 'University College London', 'University of Luxembourg', 'Beijing University of Chemical Technology']","['China', 'Luxembourg', 'United Kingdom', 'Finland']",2023-08
2308.15363,Dawei Gao,"Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin
  Ding, Jingren Zhou",Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation,We have released code on https://github.com/BeachWang/DAIL-SQL,,,,cs.DB cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL task. However, the absence of a systematical benchmark inhibits the development of designing effective, efficient and economic LLM-based Text-to-SQL solutions. To address this challenge, in this paper, we first conduct a systematical and extensive comparison over existing prompt engineering methods, including question representation, example selection and example organization, and with these experimental results, we elaborate their pros and cons. Based on these findings, we propose a new integrated solution, named DAIL-SQL, which refreshes the Spider leaderboard with 86.6% execution accuracy and sets a new bar. To explore the potential of open-source LLM, we investigate them in various scenarios, and further enhance their performance with supervised fine-tuning. Our explorations highlight open-source LLMs' potential in Text-to-SQL, as well as the advantages and disadvantages of the supervised fine-tuning. Additionally, towards an efficient and economic LLM-based Text-to-SQL solution, we emphasize the token efficiency in prompt engineering and compare the prior studies under this metric. We hope that our work provides a deeper understanding of Text-to-SQL with LLMs, and inspires further investigations and broad applications. ","[{'version': 'v1', 'created': 'Tue, 29 Aug 2023 14:59:54 GMT'}, {'version': 'v2', 'created': 'Fri, 8 Sep 2023 10:13:16 GMT'}, {'version': 'v3', 'created': 'Wed, 20 Sep 2023 04:16:31 GMT'}]",2023-09-21,"[['Gao', 'Dawei', ''], ['Wang', 'Haibin', ''], ['Li', 'Yaliang', ''], ['Sun', 'Xiuyu', ''], ['Qian', 'Yichen', ''], ['Ding', 'Bolin', ''], ['Zhou', 'Jingren', '']]",0,0,2023-08-29,3,7,3,0,0,0,1fc89ce338b94f6a46e41b9a13aa99366a762eea,261276437.0,https://www.semanticscholar.org/paper/1fc89ce338b94f6a46e41b9a13aa99366a762eea,arXiv.org,2023.0,53.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2162036220', 'name': 'Dawei Gao'}, {'authorId': '2159088201', 'name': 'Haibin Wang'}, {'authorId': '2110479359', 'name': 'Yaliang Li'}, {'authorId': '2109166067', 'name': 'Xiuyu Sun'}, {'authorId': '2087074114', 'name': 'Yichen Qian'}, {'authorId': '1696332', 'name': 'Bolin Ding'}, {'authorId': '1709595', 'name': 'Jingren Zhou'}]",['Alibaba'],['China'],2023-08
2308.15366,Zhaopeng Gu,"Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang, Jinqiao
  Wang",AnomalyGPT: Detecting Industrial Anomalies using Large Vision-Language Models,Project page: https://anomalygpt.github.io,,,,cs.CV,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Large Vision-Language Models (LVLMs) such as MiniGPT-4 and LLaVA have demonstrated the capability of understanding images and achieved remarkable performance in various visual tasks. Despite their strong abilities in recognizing common objects due to extensive training datasets, they lack specific domain knowledge and have a weaker understanding of localized details within objects, which hinders their effectiveness in the Industrial Anomaly Detection (IAD) task. On the other hand, most existing IAD methods only provide anomaly scores and necessitate the manual setting of thresholds to distinguish between normal and abnormal samples, which restricts their practical implementation. In this paper, we explore the utilization of LVLM to address the IAD problem and propose AnomalyGPT, a novel IAD approach based on LVLM. We generate training data by simulating anomalous images and producing corresponding textual descriptions for each image. We also employ an image decoder to provide fine-grained semantic and design a prompt learner to fine-tune the LVLM using prompt embeddings. Our AnomalyGPT eliminates the need for manual threshold adjustments, thus directly assesses the presence and locations of anomalies. Additionally, AnomalyGPT supports multi-turn dialogues and exhibits impressive few-shot in-context learning capabilities. With only one normal shot, AnomalyGPT achieves the state-of-the-art performance with an accuracy of 86.1%, an image-level AUC of 94.1%, and a pixel-level AUC of 95.3% on the MVTec-AD dataset. Code is available at https://github.com/CASIA-IVA-Lab/AnomalyGPT. ","[{'version': 'v1', 'created': 'Tue, 29 Aug 2023 15:02:53 GMT'}, {'version': 'v2', 'created': 'Mon, 4 Sep 2023 11:44:48 GMT'}, {'version': 'v3', 'created': 'Wed, 13 Sep 2023 14:58:14 GMT'}]",2023-09-14,"[['Gu', 'Zhaopeng', ''], ['Zhu', 'Bingke', ''], ['Zhu', 'Guibo', ''], ['Chen', 'Yingying', ''], ['Tang', 'Ming', ''], ['Wang', 'Jinqiao', '']]",0,1,2023-08-29,3,6,1,0,0,0,f2ec0182c6646d3128afa5100f37d9de7b533463,261276492.0,https://www.semanticscholar.org/paper/f2ec0182c6646d3128afa5100f37d9de7b533463,arXiv.org,2023.0,41.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2130167', 'name': 'Zhaopeng Gu'}, {'authorId': '14898414', 'name': 'Bingke Zhu'}, {'authorId': '2894321', 'name': 'Guibo Zhu'}, {'authorId': '50580380', 'name': 'Yingying Chen'}, {'authorId': '2113727378', 'name': 'Ming Tang'}, {'authorId': '1519293616', 'name': 'Jinqiao Wang'}]","['Wuhan AI Research, Wuhan, China', 'University of Chinese Academy of Sciences', 'Chinese Academy of Sciences', 'Objecteye Inc., Beijing, China']",['China'],2023-08
2308.15452,Ningyu Zhang,"Zhen Bi, Ningyu Zhang, Yinuo Jiang, Shumin Deng, Guozhou Zheng, Huajun
  Chen",When Do Program-of-Thoughts Work for Reasoning?,Work in progress,,,,cs.CL cs.AI cs.LG cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing and stratifying algorithm, and apply it to instruction generation for mathematical reasoning and code data filtering for code generation tasks. Extensive results demonstrates the effectiveness of our proposed approach. Code will be integrated into the EasyInstruct framework at https://github.com/zjunlp/EasyInstruct. ","[{'version': 'v1', 'created': 'Tue, 29 Aug 2023 17:22:39 GMT'}, {'version': 'v2', 'created': 'Fri, 8 Sep 2023 02:31:35 GMT'}]",2023-09-11,"[['Bi', 'Zhen', ''], ['Zhang', 'Ningyu', ''], ['Jiang', 'Yinuo', ''], ['Deng', 'Shumin', ''], ['Zheng', 'Guozhou', ''], ['Chen', 'Huajun', '']]",0,0,2023-08-29,2,6,4,0,0,0,023f0045686f86332a26856f8d8c3203566925ad,261277160.0,https://www.semanticscholar.org/paper/023f0045686f86332a26856f8d8c3203566925ad,arXiv.org,2023.0,54.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2059276046', 'name': 'Zhen Bi'}, {'authorId': '2153010067', 'name': 'Ningyu Zhang'}, {'authorId': '2157838086', 'name': 'Yinuo Jiang'}, {'authorId': '152931849', 'name': 'Shumin Deng'}, {'authorId': '1706307', 'name': 'Guozhou Zheng'}, {'authorId': '2144200945', 'name': 'Huajun Chen'}]","['Donghai Laboratory', 'Alibaba', 'National University of Singapore', 'https://github.com/zjunlp/EasyInstruct', 'Zhejiang University']","['China', 'Singapore']",2023-08
2308.15727,Jiuyang Xiang,"Zhenhong Zhou, Jiuyang Xiang, Chaomeng Chen, Sen Su",Quantifying and Analyzing Entity-level Memorization in Large Language Models,"9 pages, 7 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have been proven capable of memorizing their training data, which can be extracted through specifically designed prompts. As the scale of datasets continues to grow, privacy risks arising from memorization have attracted increasing attention. Quantifying language model memorization helps evaluate potential privacy risks. However, prior works on quantifying memorization require access to the precise original data or incur substantial computational overhead, making it difficult for applications in real-world language models. To this end, we propose a fine-grained, entity-level definition to quantify memorization with conditions and metrics closer to real-world scenarios. In addition, we also present an approach for efficiently extracting sensitive entities from autoregressive language models. We conduct extensive experiments based on the proposed, probing language models' ability to reconstruct sensitive entities under different settings. We find that language models have strong memorization at the entity level and are able to reproduce the training data even with partial leakages. The results demonstrate that LLMs not only memorize their training data but also understand associations between entities. These findings necessitate that trainers of LLMs exercise greater prudence regarding model memorization, adopting memorization mitigation techniques to preclude privacy violations. ","[{'version': 'v1', 'created': 'Wed, 30 Aug 2023 03:06:47 GMT'}]",2023-08-31,"[['Zhou', 'Zhenhong', ''], ['Xiang', 'Jiuyang', ''], ['Chen', 'Chaomeng', ''], ['Su', 'Sen', '']]",0,0,2023-08-30,1,4,1,0,0,0,99c8724fd43e9ed6c7d759e420021950e984aa95,261339641.0,https://www.semanticscholar.org/paper/99c8724fd43e9ed6c7d759e420021950e984aa95,arXiv.org,2023.0,32.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2236011027', 'name': 'Zhenhong Zhou'}, {'authorId': '2235836076', 'name': 'Jiuyang Xiang'}, {'authorId': '2145765162', 'name': 'Chao-Yi Chen'}, {'authorId': '2150496984', 'name': 'Sen Su'}]","['Beijing University of Posts and Telecommunications', 'University of MichiganAnn Arbor']","['China', 'United States']",2023-08
2308.15851,Yang Zhou,"Yang Zhou, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao",Prompting Vision Language Model with Knowledge from Large Language Model for Knowledge-Based VQA,,,,,cs.MM,http://creativecommons.org/licenses/by/4.0/,"  Knowledge-based visual question answering is a very challenging and widely concerned task. Previous methods adopts the implicit knowledge in large language models (LLM) to achieve excellent results, but we argue that existing methods may suffer from biasing understanding of the image and insufficient knowledge to solve the problem. In this paper, we propose PROOFREAD -PROmpting vision language model with knOwledge From laRgE lAnguage moDel, a novel, lightweight and efficient kowledge-based VQA framework, which make the vision language model and the large language model cooperate to give full play to their respective strengths and bootstrap each other. In detail, our proposed method uses LLM to obtain knowledge explicitly, uses the vision language model which can see the image to get the knowledge answer, and introduces knowledge perceiver to filter out knowledge that is harmful for getting the correct final answer. Experimental results on two datasets prove the effectiveness of our approach. Our method outperforms all state-of-the-art methods on the A-OKVQA dataset in two settings and also achieves relatively good performance on the OKVQA dataset. ","[{'version': 'v1', 'created': 'Wed, 30 Aug 2023 08:35:31 GMT'}]",2023-08-31,"[['Zhou', 'Yang', ''], ['Cao', 'Pengfei', ''], ['Chen', 'Yubo', ''], ['Liu', 'Kang', ''], ['Zhao', 'Jun', '']]",0,0,2023-08-30,1,5,1,0,0,0,6660a20e26e9e8c9916ffdc488e925e313605d8d,261339124.0,https://www.semanticscholar.org/paper/6660a20e26e9e8c9916ffdc488e925e313605d8d,arXiv.org,2023.0,39.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2145499382', 'name': 'Yang Zhou'}, {'authorId': '49776272', 'name': 'Pengfei Cao'}, {'authorId': '1763402', 'name': 'Yubo Chen'}, {'authorId': '2200096', 'name': 'Kang Liu'}, {'authorId': '11447228', 'name': 'Jun Zhao'}]","['University of Chinese Academy of Sciences', 'Chinese Academy of Sciences']",['China'],2023-08
2308.15962,Tianyu Wang,"Tianyu Wang, Yifan Li, Haitao Lin, Xiangyang Xue, Yanwei Fu",WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model,"14 pages, 8 figures. See https://star-uu-wang.github.io/WALL-E/",,,,cs.RO cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Enabling robots to understand language instructions and react accordingly to visual perception has been a long-standing goal in the robotics research community. Achieving this goal requires cutting-edge advances in natural language processing, computer vision, and robotics engineering. Thus, this paper mainly investigates the potential of integrating the most recent Large Language Models (LLMs) and existing visual grounding and robotic grasping system to enhance the effectiveness of the human-robot interaction. We introduce the WALL-E (Embodied Robotic WAiter load lifting with Large Language model) as an example of this integration. The system utilizes the LLM of ChatGPT to summarize the preference object of the users as a target instruction via the multi-round interactive dialogue. The target instruction is then forwarded to a visual grounding system for object pose and size estimation, following which the robot grasps the object accordingly. We deploy this LLM-empowered system on the physical robot to provide a more user-friendly interface for the instruction-guided grasping task. The further experimental results on various real-world scenarios demonstrated the feasibility and efficacy of our proposed framework. See the project website at: https://star-uu-wang.github.io/WALL-E/ ","[{'version': 'v1', 'created': 'Wed, 30 Aug 2023 11:35:21 GMT'}, {'version': 'v2', 'created': 'Thu, 31 Aug 2023 13:51:56 GMT'}]",2023-09-01,"[['Wang', 'Tianyu', ''], ['Li', 'Yifan', ''], ['Lin', 'Haitao', ''], ['Xue', 'Xiangyang', ''], ['Fu', 'Yanwei', '']]",1,1,2023-08-30,2,5,2,1,0,1,b24488203f61e413fe2497957a33780e149b6feb,261339015.0,https://www.semanticscholar.org/paper/b24488203f61e413fe2497957a33780e149b6feb,arXiv.org,2023.0,45.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2236603796', 'name': 'Tianyu Wang'}, {'authorId': '2235795703', 'name': 'Yifan Li'}, {'authorId': '48444926', 'name': 'Haitao Lin'}, {'authorId': '2151740597', 'name': 'Xiangyang Xue'}, {'authorId': '35782003', 'name': 'Yanwei Fu'}]",['Fudan University'],['China'],2023-08
2308.15987,Yifan Zhang,"Qingyuan Li, Yifan Zhang, Liang Li, Peng Yao, Bo Zhang, Xiangxiang
  Chu, Yerui Sun, Li Du, Yuchen Xie",FPTQ: Fine-grained Post-Training Quantization for Large Language Models,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the era of large-scale language models, the substantial parameter size poses significant challenges for deployment. Being a prevalent compression technique, quantization has emerged as the mainstream practice to tackle this issue, which is mainly centered on two recipes W8A8 and W4A16 (i.e. weights and activations in such bit widths). In this study, we propose a novel W4A8 post-training quantization method for the available open-sourced LLMs, which combines the advantages of both two recipes. Therefore, we can leverage the benefit in the I/O utilization of 4-bit weight quantization and the acceleration due to 8-bit matrix computation. Nevertheless, the W4A8 faces notorious performance degradation. As a remedy, we involve layerwise activation quantization strategies which feature a novel logarithmic equalization for most intractable layers, and we combine them with fine-grained weight quantization. Without whistles and bells, we eliminate the necessity for further fine-tuning and obtain the state-of-the-art W4A8 quantized performance on BLOOM, LLaMA, and LLaMA-2 on standard benchmarks. We confirm that the W4A8 quantization is achievable for the deployment of large language models, fostering their wide-spreading real-world applications. ","[{'version': 'v1', 'created': 'Wed, 30 Aug 2023 12:18:18 GMT'}]",2023-08-31,"[['Li', 'Qingyuan', ''], ['Zhang', 'Yifan', ''], ['Li', 'Liang', ''], ['Yao', 'Peng', ''], ['Zhang', 'Bo', ''], ['Chu', 'Xiangxiang', ''], ['Sun', 'Yerui', ''], ['Du', 'Li', ''], ['Xie', 'Yuchen', '']]",0,0,2023-08-30,1,9,3,2,2,0,9b200baa28a8c30b320c61b167fce0bdd829e8ad,261339179.0,https://www.semanticscholar.org/paper/9b200baa28a8c30b320c61b167fce0bdd829e8ad,arXiv.org,2023.0,40.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '30750669', 'name': 'Qingyuan Li'}, {'authorId': '2174541666', 'name': 'Yifan Zhang'}, {'authorId': '2154883488', 'name': 'Liang Li'}, {'authorId': '2099667842', 'name': 'Peng Yao'}, {'authorId': '49846372', 'name': 'Bo Zhang'}, {'authorId': '27628828', 'name': 'Xiangxiang Chu'}, {'authorId': '2235992666', 'name': 'Yerui Sun'}, {'authorId': '8386604', 'name': 'Li-Qiang Du'}, {'authorId': '2236015688', 'name': 'Yuchen Xie'}]",['Nanjing University'],['China'],2023-08
2308.16463,Yupan Huang,"Yupan Huang and Zaiqiao Meng and Fangyu Liu and Yixuan Su and Nigel
  Collier and Yutong Lu",Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models,Reduced main content to 9 pages; typos corrected,,,,cs.CV cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large language models exhibit enhanced zero-shot performance on various tasks when fine-tuned with instruction-following data. Multimodal instruction-following models extend these capabilities by integrating both text and images. However, existing models such as MiniGPT-4 face challenges in maintaining dialogue coherence in scenarios involving multiple images. A primary reason is the lack of a specialized dataset for this critical application. To bridge these gaps, we present SparklesChat, a multimodal instruction-following model for open-ended dialogues across multiple images. To support the training, we introduce SparklesDialogue, the first machine-generated dialogue dataset tailored for word-level interleaved multi-image and text interactions. Furthermore, we construct SparklesEval, a GPT-assisted benchmark for quantitatively assessing a model's conversational competence across multiple images and dialogue turns. Our experiments validate the effectiveness of SparklesChat in understanding and reasoning across multiple images and dialogue turns. Specifically, SparklesChat outperformed MiniGPT-4 on established vision-and-language benchmarks, including the BISON binary image selection task and the NLVR2 visual reasoning task. Moreover, SparklesChat scored 8.56 out of 10 on SparklesEval, substantially exceeding MiniGPT-4's score of 3.91 and nearing GPT-4's score of 9.26. Qualitative evaluations further demonstrate SparklesChat's generality in handling real-world applications. All resources are available at https://github.com/HYPJUDY/Sparkles. ","[{'version': 'v1', 'created': 'Thu, 31 Aug 2023 05:15:27 GMT'}, {'version': 'v2', 'created': 'Mon, 2 Oct 2023 03:31:17 GMT'}]",2023-10-03,"[['Huang', 'Yupan', ''], ['Meng', 'Zaiqiao', ''], ['Liu', 'Fangyu', ''], ['Su', 'Yixuan', ''], ['Collier', 'Nigel', ''], ['Lu', 'Yutong', '']]",0,1,2023-08-31,2,6,2,1,0,1,41d3970660540476ade80828d40e7c6b68924571,261395170.0,https://www.semanticscholar.org/paper/41d3970660540476ade80828d40e7c6b68924571,arXiv.org,2023.0,69.0,5.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '102665943', 'name': 'Yupan Huang'}, {'authorId': '3451645', 'name': 'Zaiqiao Meng'}, {'authorId': '144097210', 'name': 'Fangyu Liu'}, {'authorId': '50087162', 'name': 'Yixuan Su'}, {'authorId': '50638196', 'name': 'Nigel Collier'}, {'authorId': '2140026613', 'name': 'Yutong Lu'}]","['University of Cambridge', 'Sun Yat-sen University', 'University of Glasgow']","['China', 'United Kingdom']",2023-08
2308.16474,Yongqiang Zhao,"Yongqiang Zhao, Zhenyu Li, Feng Zhang, Xinhai Xu, Donghong Liu",Enhancing Subtask Performance of Multi-modal Large Language Model,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multi-modal Large Language Model (MLLM) refers to a model expanded from a Large Language Model (LLM) that possesses the capability to handle and infer multi-modal data. Current MLLMs typically begin by using LLMs to decompose tasks into multiple subtasks, then employing individual pre-trained models to complete specific subtasks, and ultimately utilizing LLMs to integrate the results of each subtasks to obtain the results of the task. In real-world scenarios, when dealing with large projects, it is common practice to break down the project into smaller sub-projects, with different teams providing corresponding solutions or results. The project owner then decides which solution or result to use, ensuring the best possible outcome for each subtask and, consequently, for the entire project. Inspired by this, this study considers selecting multiple pre-trained models to complete the same subtask. By combining the results from multiple pre-trained models, the optimal subtask result is obtained, enhancing the performance of the MLLM. Specifically, this study first selects multiple pre-trained models focused on the same subtask based on distinct evaluation approaches, and then invokes these models in parallel to process input data and generate corresponding subtask results. Finally, the results from multiple pre-trained models for the same subtask are compared using the LLM, and the best result is chosen as the outcome for that subtask. Extensive experiments are conducted in this study using GPT-4 annotated datasets and human-annotated datasets. The results of various evaluation metrics adequately demonstrate the effectiveness of the proposed approach in this paper. ","[{'version': 'v1', 'created': 'Thu, 31 Aug 2023 05:37:21 GMT'}]",2023-09-01,"[['Zhao', 'Yongqiang', ''], ['Li', 'Zhenyu', ''], ['Zhang', 'Feng', ''], ['Xu', 'Xinhai', ''], ['Liu', 'Donghong', '']]",0,1,2023-08-31,1,5,2,1,0,1,1e7a2f9f9441462e92ee349f00414aff49617caa,261395211.0,https://www.semanticscholar.org/paper/1e7a2f9f9441462e92ee349f00414aff49617caa,arXiv.org,2023.0,22.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2151271026', 'name': 'Yongqiang Zhao'}, {'authorId': '2236684767', 'name': 'Zhenyu Li'}, {'authorId': '2182282767', 'name': 'Feng Zhang'}, {'authorId': '2152776128', 'name': 'Xinhai Xu'}, {'authorId': '40639829', 'name': 'Donghong Liu'}]","['Peking University', 'PLA Academy of Military Science']",['China'],2023-08
2308.16505,Xu Huang,"Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, Xing Xie",Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations,"16 pages, 15 figures, 4 tables",,,,cs.IR cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recommender models excel at providing domain-specific item recommendations by leveraging extensive user behavior data. Despite their ability to act as lightweight domain experts, they struggle to perform versatile tasks such as providing explanations and engaging in conversations. On the other hand, large language models (LLMs) represent a significant step towards artificial general intelligence, showcasing remarkable capabilities in instruction comprehension, commonsense reasoning, and human interaction. However, LLMs lack the knowledge of domain-specific item catalogs and behavioral patterns, particularly in areas that diverge from general world knowledge, such as online e-commerce. Finetuning LLMs for each domain is neither economic nor efficient.   In this paper, we bridge the gap between recommender models and LLMs, combining their respective strengths to create a versatile and interactive recommender system. We introduce an efficient framework called InteRecAgent, which employs LLMs as the brain and recommender models as tools. We first outline a minimal set of essential tools required to transform LLMs into InteRecAgent. We then propose an efficient workflow within InteRecAgent for task execution, incorporating key components such as a memory bus, dynamic demonstration-augmented task planning, and reflection. InteRecAgent enables traditional recommender systems, such as those ID-based matrix factorization models, to become interactive systems with a natural language interface through the integration of LLMs. Experimental results on several public datasets show that InteRecAgent achieves satisfying performance as a conversational recommender system, outperforming general-purpose LLMs. ","[{'version': 'v1', 'created': 'Thu, 31 Aug 2023 07:36:44 GMT'}, {'version': 'v2', 'created': 'Fri, 1 Sep 2023 15:40:16 GMT'}]",2023-09-04,"[['Huang', 'Xu', ''], ['Lian', 'Jianxun', ''], ['Lei', 'Yuxuan', ''], ['Yao', 'Jing', ''], ['Lian', 'Defu', ''], ['Xie', 'Xing', '']]",0,0,2023-08-31,2,6,2,0,0,0,c237a22698223e4060d83027f399f4fb2aa24291,261395685.0,https://www.semanticscholar.org/paper/c237a22698223e4060d83027f399f4fb2aa24291,arXiv.org,2023.0,51.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2236672137', 'name': 'Xu Huang'}, {'authorId': '2813328', 'name': 'Jianxun Lian'}, {'authorId': '2226475380', 'name': 'Yuxuan Lei'}, {'authorId': '2237129499', 'name': 'Jing Yao'}, {'authorId': '1862782', 'name': 'Defu Lian'}, {'authorId': '2110972323', 'name': 'Xing Xie'}]","['University of Science and Technology of China', 'Microsoft']",['China'],2023-08
2308.16692,Dong Zhang Zhang,"Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, Xipeng Qiu",SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models,"SpeechTokenizer project page is
  https://0nutation.github.io/SpeechTokenizer.github.io/",,,,cs.CL cs.SD eess.AS,http://creativecommons.org/licenses/by-sa/4.0/,"  Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/. ","[{'version': 'v1', 'created': 'Thu, 31 Aug 2023 12:53:09 GMT'}]",2023-09-01,"[['Zhang', 'Xin', ''], ['Zhang', 'Dong', ''], ['Li', 'Shimin', ''], ['Zhou', 'Yaqian', ''], ['Qiu', 'Xipeng', '']]",0,0,2023-08-31,1,5,3,0,0,0,5fc1a1f79ea4d1a1c6e8da1a40ae08022a6d7308,261394297.0,https://www.semanticscholar.org/paper/5fc1a1f79ea4d1a1c6e8da1a40ae08022a6d7308,arXiv.org,2023.0,35.0,2.0,0.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2149175310', 'name': 'Xin Zhang'}, {'authorId': '2109797247', 'name': 'Dong Zhang'}, {'authorId': '50341765', 'name': 'Shimin Li'}, {'authorId': '2110347068', 'name': 'Yaqian Zhou'}, {'authorId': '2188058565', 'name': 'Xipeng Qiu'}]",['Fudan University'],['China'],2023-08
2308.16824,Daoguang Zan,"Daoguang Zan, Ailun Yu, Bo Shen, Jiaxin Zhang, Taihong Chen, Bing
  Geng, Bei Chen, Jichuan Ji, Yafen Yao, Yongji Wang, Qianxiang Wang",Can Programming Languages Boost Each Other via Instruction Tuning?,Work in progress,,,,cs.CL cs.AI cs.PL cs.SE,http://creativecommons.org/licenses/by/4.0/,"  When human programmers have mastered a programming language, it would be easier when they learn a new programming language. In this report, we focus on exploring whether programming languages can boost each other during the instruction fine-tuning phase of code large language models. We conduct extensive experiments of 8 popular programming languages (Python, JavaScript, TypeScript, C, C++, Java, Go, HTML) on StarCoder. Results demonstrate that programming languages can significantly improve each other. For example, CodeM-Python 15B trained on Python is able to increase Java by an absolute 17.95% pass@1 on HumanEval-X. More surprisingly, we found that CodeM-HTML 7B trained on the HTML corpus can improve Java by an absolute 15.24% pass@1. Our training data is released at https://github.com/NL2Code/CodeM. ","[{'version': 'v1', 'created': 'Thu, 31 Aug 2023 15:53:51 GMT'}, {'version': 'v2', 'created': 'Sun, 3 Sep 2023 08:30:29 GMT'}]",2023-09-06,"[['Zan', 'Daoguang', ''], ['Yu', 'Ailun', ''], ['Shen', 'Bo', ''], ['Zhang', 'Jiaxin', ''], ['Chen', 'Taihong', ''], ['Geng', 'Bing', ''], ['Chen', 'Bei', ''], ['Ji', 'Jichuan', ''], ['Yao', 'Yafen', ''], ['Wang', 'Yongji', ''], ['Wang', 'Qianxiang', '']]",0,0,2023-08-31,2,11,4,0,0,0,b0823d1194c817b25d8b0144fa5a5c0979f0f08e,261397124.0,https://www.semanticscholar.org/paper/b0823d1194c817b25d8b0144fa5a5c0979f0f08e,arXiv.org,2023.0,17.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2134434187', 'name': 'Daoguang Zan'}, {'authorId': '2151121986', 'name': 'Ailun Yu'}, {'authorId': '2089966950', 'name': 'Bo Shen'}, {'authorId': None, 'name': 'Jiaxin Zhang'}, {'authorId': '2225208156', 'name': 'Taihong Chen'}, {'authorId': '2225236858', 'name': 'Bing Geng'}, {'authorId': '143876723', 'name': 'B. Chen'}, {'authorId': '2225077064', 'name': 'Jichuan Ji'}, {'authorId': '2236731239', 'name': 'Yafen Yao'}, {'authorId': '2108097250', 'name': 'Yongji Wang'}, {'authorId': '7417844', 'name': 'Qianxiang Wang'}]","['Peking University', 'Chinese Academy of Sciences', 'Huawei Technologies (China)']",['China'],2023-08
2308.16911,Runsen Xu,"Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, Dahua
  Lin",PointLLM: Empowering Large Language Models to Understand Point Clouds,"19 pages. Empowering large language models with 3D point cloud
  understanding, accompanied by a novel dataset and carefully designed
  benchmarks. Project page: https://runsenxu.com/projects/PointLLM",,,,cs.CV cs.AI cs.CL,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  The unprecedented advancements in Large Language Models (LLMs) have created a profound impact on natural language processing but are yet to fully embrace the realm of 3D understanding. This paper introduces PointLLM, a preliminary effort to fill this gap, thereby enabling LLMs to understand point clouds and offering a new avenue beyond 2D visual data. PointLLM processes colored object point clouds with human instructions and generates contextually appropriate responses, illustrating its grasp of point clouds and common sense. Specifically, it leverages a point cloud encoder with a powerful LLM to effectively fuse geometric, appearance, and linguistic information. We collect a novel dataset comprising 660K simple and 70K complex point-text instruction pairs to enable a two-stage training strategy: initially aligning latent spaces and subsequently instruction-tuning the unified model. To rigorously evaluate our model's perceptual abilities and its generalization capabilities, we establish two benchmarks: Generative 3D Object Classification and 3D Object Captioning, assessed through three different methods, including human evaluation, GPT-4/ChatGPT evaluation, and traditional metrics. Experiment results show that PointLLM demonstrates superior performance over existing 2D baselines. Remarkably, in human-evaluated object captioning tasks, PointLLM outperforms human annotators in over 50% of the samples. Codes, datasets, and benchmarks are available at https://github.com/OpenRobotLab/PointLLM . ","[{'version': 'v1', 'created': 'Thu, 31 Aug 2023 17:59:46 GMT'}]",2023-09-01,"[['Xu', 'Runsen', ''], ['Wang', 'Xiaolong', ''], ['Wang', 'Tai', ''], ['Chen', 'Yilun', ''], ['Pang', 'Jiangmiao', ''], ['Lin', 'Dahua', '']]",1,1,2023-08-31,1,6,3,2,0,2,25f7401d87f9b3cf8a2511a76b181a3559a1833f,261397321.0,https://www.semanticscholar.org/paper/25f7401d87f9b3cf8a2511a76b181a3559a1833f,arXiv.org,2023.0,56.0,6.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2087110249', 'name': 'Runsen Xu'}, {'authorId': '2237120220', 'name': 'Xiaolong Wang'}, {'authorId': '2257008641', 'name': 'Tai Wang'}, {'authorId': '2236662733', 'name': 'Yilun Chen'}, {'authorId': '49968574', 'name': 'Jiangmiao Pang'}, {'authorId': '2237091231', 'name': 'Dahua Lin'}]","['Shanghai Artificial Intelligence Laboratory', 'Chinese University of Hong Kong', 'Zhejiang University']",['China'],2023-08
2309.00096,Chaofan Ma,"Chaofan Ma, Yuhuan Yang, Chen Ju, Fei Zhang, Ya Zhang, Yanfeng Wang",Open-Vocabulary Semantic Segmentation via Attribute Decomposition-Aggregation,,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Open-vocabulary semantic segmentation is a challenging task that requires segmenting novel object categories at inference time. Recent works explore vision-language pre-training to handle this task, but suffer from unrealistic assumptions in practical scenarios, i.e., low-quality textual category names. For example, this paradigm assumes that new textual categories will be accurately and completely provided, and exist in lexicons during pre-training. However, exceptions often happen when meet with ambiguity for brief or incomplete names, new words that are not present in the pre-trained lexicons, and difficult-to-describe categories for users. To address these issues, this work proposes a novel decomposition-aggregation framework, inspired by human cognition in understanding new concepts. Specifically, in the decomposition stage, we decouple class names into diverse attribute descriptions to enrich semantic contexts. Two attribute construction strategies are designed: using large language models for common categories, and involving manually labelling for human-invented categories. In the aggregation stage, we group diverse attributes into an integrated global description, to form a discriminative classifier that distinguishes the target object from others. One hierarchical aggregation is further designed to achieve multi-level alignment and deep fusion between vision and text. The final result is obtained by computing the embedding similarity between aggregated attributes and images. To evaluate the effectiveness, we annotate three datasets with attribute descriptions, and conduct extensive experiments and ablation studies. The results show the superior performance of attribute decomposition-aggregation. ","[{'version': 'v1', 'created': 'Thu, 31 Aug 2023 19:34:09 GMT'}]",2023-09-04,"[['Ma', 'Chaofan', ''], ['Yang', 'Yuhuan', ''], ['Ju', 'Chen', ''], ['Zhang', 'Fei', ''], ['Zhang', 'Ya', ''], ['Wang', 'Yanfeng', '']]",0,0,2023-08-31,1,6,2,0,0,0,201bf774bbbad9ea9cae9394751c81c294fca971,261494117.0,https://www.semanticscholar.org/paper/201bf774bbbad9ea9cae9394751c81c294fca971,arXiv.org,2023.0,52.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2237518679', 'name': 'Chaofan Ma'}, {'authorId': '2108773876', 'name': 'Yu-Hao Yang'}, {'authorId': '2059175825', 'name': 'Chen Ju'}, {'authorId': '2237603027', 'name': 'Fei Zhang'}, {'authorId': '2910574', 'name': 'Ya Zhang'}, {'authorId': '2108846176', 'name': 'Yanfeng Wang'}]","['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University']",['China'],2023-08
2309.00255,Mojtaba Valipour,"Mojtaba Valipour, Mehdi Rezagholizadeh, Hossein Rajabzadeh, Marzieh
  Tahaei, Boxing Chen, and Ali Ghodsi","SortedNet, a Place for Every Network and Every Network in its Place: Towards a Generalized Solution for Training Many-in-One Neural Networks",,,,,cs.LG,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  As the size of deep learning models continues to grow, finding optimal models under memory and computation constraints becomes increasingly more important. Although usually the architecture and constituent building blocks of neural networks allow them to be used in a modular way, their training process is not aware of this modularity. Consequently, conventional neural network training lacks the flexibility to adapt the computational load of the model during inference. This paper proposes SortedNet, a generalized and scalable solution to harness the inherent modularity of deep neural networks across various dimensions for efficient dynamic inference. Our training considers a nested architecture for the sub-models with shared parameters and trains them together with the main model in a sorted and probabilistic manner. This sorted training of sub-networks enables us to scale the number of sub-networks to hundreds using a single round of training. We utilize a novel updating scheme during training that combines random sampling of sub-networks with gradient accumulation to improve training efficiency. Furthermore, the sorted nature of our training leads to a search-free sub-network selection at inference time; and the nested architecture of the resulting sub-networks leads to minimal storage requirement and efficient switching between sub-networks at inference. Our general dynamic training approach is demonstrated across various architectures and tasks, including large language models and pre-trained vision models. Experimental results show the efficacy of the proposed approach in achieving efficient sub-networks while outperforming state-of-the-art dynamic training approaches. Our findings demonstrate the feasibility of training up to 160 different sub-models simultaneously, showcasing the extensive scalability of our proposed method while maintaining 96% of the model performance. ","[{'version': 'v1', 'created': 'Fri, 1 Sep 2023 05:12:25 GMT'}]",2023-09-04,"[['Valipour', 'Mojtaba', ''], ['Rezagholizadeh', 'Mehdi', ''], ['Rajabzadeh', 'Hossein', ''], ['Tahaei', 'Marzieh', ''], ['Chen', 'Boxing', ''], ['Ghodsi', 'Ali', '']]",0,0,2023-09-01,1,6,1,0,0,0,b7ff22b71109b36c370a6b6fe9a01910897aa172,261494134.0,https://www.semanticscholar.org/paper/b7ff22b71109b36c370a6b6fe9a01910897aa172,arXiv.org,2023.0,19.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '9200111', 'name': 'Mojtaba Valipour'}, {'authorId': '2066076226', 'name': 'Mehdi Rezagholizadeh'}, {'authorId': '49214038', 'name': 'Hossein Rajabzadeh'}, {'authorId': '1996315', 'name': 'Marzieh S. Tahaei'}, {'authorId': '2237517964', 'name': 'Boxing Chen'}, {'authorId': '2237425782', 'name': 'Ali Ghodsi'}]","['University of Waterloo', 'Huawei Technologies (China)']","['China', 'Canada']",2023-09
2309.00363,Yaliang Li,"Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei Gao, Xuchen
  Pan, Yuexiang Xie, Yaliang Li, Bolin Ding, Jingren Zhou",FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning,Source code: https://github.com/alibaba/FederatedScope/tree/llm,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  LLMs have demonstrated great capabilities in various NLP tasks. Different entities can further improve the performance of those LLMs on their specific downstream tasks by fine-tuning LLMs. When several entities have similar interested tasks, but their data cannot be shared because of privacy concerns regulations, federated learning (FL) is a mainstream solution to leverage the data of different entities. However, fine-tuning LLMs in federated learning settings still lacks adequate support from existing FL frameworks because it has to deal with optimizing the consumption of significant communication and computational resources, data preparation for different tasks, and distinct information protection demands. This paper first discusses these challenges of federated fine-tuning LLMs, and introduces our package FS-LLM as a main contribution, which consists of the following components: (1) we build an end-to-end benchmarking pipeline, automizing the processes of dataset preprocessing, federated fine-tuning execution, and performance evaluation on federated LLM fine-tuning; (2) we provide comprehensive federated parameter-efficient fine-tuning algorithm implementations and versatile programming interfaces for future extension in FL scenarios with low communication and computation costs, even without accessing the full model; (3) we adopt several accelerating and resource-efficient operators for fine-tuning LLMs with limited resources and the flexible pluggable sub-routines for interdisciplinary study. We conduct extensive experiments to validate the effectiveness of FS-LLM and benchmark advanced LLMs with state-of-the-art parameter-efficient fine-tuning algorithms in FL settings, which also yields valuable insights into federated fine-tuning LLMs for the research community. To facilitate further research and adoption, we release FS-LLM at https://github.com/alibaba/FederatedScope/tree/llm. ","[{'version': 'v1', 'created': 'Fri, 1 Sep 2023 09:40:36 GMT'}]",2023-09-04,"[['Kuang', 'Weirui', ''], ['Qian', 'Bingchen', ''], ['Li', 'Zitao', ''], ['Chen', 'Daoyuan', ''], ['Gao', 'Dawei', ''], ['Pan', 'Xuchen', ''], ['Xie', 'Yuexiang', ''], ['Li', 'Yaliang', ''], ['Ding', 'Bolin', ''], ['Zhou', 'Jingren', '']]",0,0,2023-09-01,1,10,1,0,0,0,529ff7d6441d244212cf2becafd12a7e67ac56d9,261494317.0,https://www.semanticscholar.org/paper/529ff7d6441d244212cf2becafd12a7e67ac56d9,arXiv.org,2023.0,93.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2162042348', 'name': 'Weirui Kuang'}, {'authorId': '2237427279', 'name': 'Bingchen Qian'}, {'authorId': '2007553615', 'name': 'Zitao Li'}, {'authorId': '49025612', 'name': 'Daoyuan Chen'}, {'authorId': '2162036220', 'name': 'Dawei Gao'}, {'authorId': '2211993531', 'name': 'Xuchen Pan'}, {'authorId': '52133762', 'name': 'Yuexiang Xie'}, {'authorId': '2237607166', 'name': 'Yaliang Li'}, {'authorId': '1696332', 'name': 'Bolin Ding'}, {'authorId': '2237499232', 'name': 'Jingren Zhou'}]",['Alibaba'],['China'],2023-09
2309.00615,Ziyu Guo,"Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma,
  Jiaming Han, Kexin Chen, Peng Gao, Xianzhi Li, Hongsheng Li, Pheng-Ann Heng","Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following","Work in progress. Code is available at
  https://github.com/ZiyuGuo99/Point-Bind_Point-LLM",,,,cs.CV cs.AI cs.CL cs.LG cs.MM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce Point-Bind, a 3D multi-modality model aligning point clouds with 2D image, language, audio, and video. Guided by ImageBind, we construct a joint embedding space between 3D and multi-modalities, enabling many promising applications, e.g., any-to-3D generation, 3D embedding arithmetic, and 3D open-world understanding. On top of this, we further present Point-LLM, the first 3D large language model (LLM) following 3D multi-modal instructions. By parameter-efficient fine-tuning techniques, Point-LLM injects the semantics of Point-Bind into pre-trained LLMs, e.g., LLaMA, which requires no 3D instruction data, but exhibits superior 3D and multi-modal question-answering capacity. We hope our work may cast a light on the community for extending 3D point clouds to multi-modality applications. Code is available at https://github.com/ZiyuGuo99/Point-Bind_Point-LLM. ","[{'version': 'v1', 'created': 'Fri, 1 Sep 2023 17:59:47 GMT'}]",2023-09-04,"[['Guo', 'Ziyu', ''], ['Zhang', 'Renrui', ''], ['Zhu', 'Xiangyang', ''], ['Tang', 'Yiwen', ''], ['Ma', 'Xianzheng', ''], ['Han', 'Jiaming', ''], ['Chen', 'Kexin', ''], ['Gao', 'Peng', ''], ['Li', 'Xianzhi', ''], ['Li', 'Hongsheng', ''], ['Heng', 'Pheng-Ann', '']]",0,0,2023-09-01,1,11,5,1,1,0,22ebfc211d184ed615729378a43fde175bf14478,261493787.0,https://www.semanticscholar.org/paper/22ebfc211d184ed615729378a43fde175bf14478,arXiv.org,2023.0,99.0,7.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2237599228', 'name': 'Ziyu Guo'}, {'authorId': '2115713503', 'name': 'Renrui Zhang'}, {'authorId': '2237590429', 'name': 'Xiangyang Zhu'}, {'authorId': '2193267071', 'name': 'Yiwen Tang'}, {'authorId': '1387903470', 'name': 'Xianzheng Ma'}, {'authorId': '150147382', 'name': 'Jiaming Han'}, {'authorId': '32811782', 'name': 'Ke Chen'}, {'authorId': '144740494', 'name': 'Peng Gao'}, {'authorId': '2237583550', 'name': 'Xianzhi Li'}, {'authorId': '49404547', 'name': 'Hongsheng Li'}, {'authorId': '1714602', 'name': 'P. Heng'}]","['Shanghai Artificial Intelligence Laboratory', 'Huazhong University of Science and Technology', 'Chinese University of Hong Kong']",['China'],2023-09
2309.00916,Chen Wang,"Chen Wang, Minpeng Liao, Zhongqiang Huang, Jinliang Lu, Junhong Wu,
  Yuchen Liu, Chengqing Zong, Jiajun Zhang",BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment of Continuation Writing,,,,,cs.CL cs.SD eess.AS,http://creativecommons.org/licenses/by/4.0/,"  The emergence of large language models (LLMs) has sparked significant interest in extending their remarkable language capabilities to speech. However, modality alignment between speech and text still remains an open problem. Current solutions can be categorized into two strategies. One is a cascaded approach where outputs (tokens or states) of a separately trained speech recognition system are used as inputs for LLMs, which limits their potential in modeling alignment between speech and text. The other is an end-to-end approach that relies on speech instruction data, which is very difficult to collect in large quantities. In this paper, we address these issues and propose the BLSP approach that Bootstraps Language-Speech Pre-training via behavior alignment of continuation writing. We achieve this by learning a lightweight modality adapter between a frozen speech encoder and an LLM, ensuring that the LLM exhibits the same generation behavior regardless of the modality of input: a speech segment or its transcript. The training process can be divided into two steps. The first step prompts an LLM to generate texts with speech transcripts as prefixes, obtaining text continuations. In the second step, these continuations are used as supervised signals to train the modality adapter in an end-to-end manner. We demonstrate that this straightforward process can extend the capabilities of LLMs to speech, enabling speech recognition, speech translation, spoken language understanding, and speech conversation, even in zero-shot cross-lingual scenarios. ","[{'version': 'v1', 'created': 'Sat, 2 Sep 2023 11:46:05 GMT'}]",2023-09-06,"[['Wang', 'Chen', ''], ['Liao', 'Minpeng', ''], ['Huang', 'Zhongqiang', ''], ['Lu', 'Jinliang', ''], ['Wu', 'Junhong', ''], ['Liu', 'Yuchen', ''], ['Zong', 'Chengqing', ''], ['Zhang', 'Jiajun', '']]",0,0,2023-09-02,1,8,3,0,0,0,204fd6c5e247c477d607f507ee01d94a8dbd408f,261530415.0,https://www.semanticscholar.org/paper/204fd6c5e247c477d607f507ee01d94a8dbd408f,arXiv.org,2023.0,37.0,1.0,1.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2188130997', 'name': 'Chen Wang'}, {'authorId': '2211476336', 'name': 'Minpeng Liao'}, {'authorId': '2237956897', 'name': 'Zhongqiang Huang'}, {'authorId': '81758928', 'name': 'Jinliang Lu'}, {'authorId': '2237788942', 'name': 'Junhong Wu'}, {'authorId': '2237952656', 'name': 'Yuchen Liu'}, {'authorId': '2064100826', 'name': 'Chengqing Zong'}, {'authorId': '38358352', 'name': 'Jiajun Zhang'}]","['Alibaba', 'University of Chinese Academy of Sciences', 'Wuhan AI Research', 'Chinese Academy of Sciences']",['China'],2023-09
2309.00986,Chenliang Li,"Chenliang Li, Hehong Chen, Ming Yan, Weizhou Shen, Haiyang Xu, Zhikai
  Wu, Zhicheng Zhang, Wenmeng Zhou, Yingda Chen, Chen Cheng, Hongzhu Shi, Ji
  Zhang, Fei Huang, Jingren Zhou",ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have recently demonstrated remarkable capabilities to comprehend human intentions, engage in reasoning, and design planning-like behavior. To further unleash the power of LLMs to accomplish complex tasks, there is a growing trend to build agent framework that equips LLMs, such as ChatGPT, with tool-use abilities to connect with massive external APIs. In this work, we introduce ModelScope-Agent, a general and customizable agent framework for real-world applications, based on open-source LLMs as controllers. It provides a user-friendly system library, with customizable engine design to support model training on multiple open-source LLMs, while also enabling seamless integration with both model APIs and common APIs in a unified way. To equip the LLMs with tool-use abilities, a comprehensive framework has been proposed spanning over tool-use data collection, tool retrieval, tool registration, memory control, customized model training, and evaluation for practical real-world applications. Finally, we showcase ModelScopeGPT, a real-world intelligent assistant of ModelScope Community based on the ModelScope-Agent framework, which is able to connect open-source LLMs with more than 1000 public AI models and localized community knowledge in ModelScope. The ModelScope-Agent library\footnote{https://github.com/modelscope/modelscope-agent} and online demo\footnote{https://modelscope.cn/studios/damo/ModelScopeGPT/summary} are now publicly available. ","[{'version': 'v1', 'created': 'Sat, 2 Sep 2023 16:50:30 GMT'}]",2023-09-06,"[['Li', 'Chenliang', ''], ['Chen', 'Hehong', ''], ['Yan', 'Ming', ''], ['Shen', 'Weizhou', ''], ['Xu', 'Haiyang', ''], ['Wu', 'Zhikai', ''], ['Zhang', 'Zhicheng', ''], ['Zhou', 'Wenmeng', ''], ['Chen', 'Yingda', ''], ['Cheng', 'Chen', ''], ['Shi', 'Hongzhu', ''], ['Zhang', 'Ji', ''], ['Huang', 'Fei', ''], ['Zhou', 'Jingren', '']]",1,1,2023-09-02,1,14,1,1,0,1,e2f1f04f648a8863d11439aa4c80ee65d6caccda,261531214.0,https://www.semanticscholar.org/paper/e2f1f04f648a8863d11439aa4c80ee65d6caccda,arXiv.org,2023.0,25.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2829009', 'name': 'Chenliang Li'}, {'authorId': '123655156', 'name': 'Hehong Chen'}, {'authorId': '2114009661', 'name': 'Mingshi Yan'}, {'authorId': '2237809880', 'name': 'Weizhou Shen'}, {'authorId': '153194420', 'name': 'Haiyang Xu'}, {'authorId': '2238047773', 'name': 'Zhikai Wu'}, {'authorId': '2237946970', 'name': 'Zhicheng Zhang'}, {'authorId': '2237956023', 'name': 'Wenmeng Zhou'}, {'authorId': '2237827934', 'name': 'Yingda Chen'}, {'authorId': '2237996616', 'name': 'Chen Cheng'}, {'authorId': '2238549612', 'name': 'Hongzhu Shi'}, {'authorId': '2116921824', 'name': 'Ji Zhang'}, {'authorId': '143857288', 'name': 'Fei Huang'}, {'authorId': '2237981776', 'name': 'Jingren Zhou'}]",['Alibaba'],['China'],2023-09
2309.01029,Haiyan Zhao,"Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi
  Cai, Shuaiqiang Wang, Dawei Yin, Mengnan Du",Explainability for Large Language Models: A Survey,,,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional machine learning models. ","[{'version': 'v1', 'created': 'Sat, 2 Sep 2023 22:14:26 GMT'}, {'version': 'v2', 'created': 'Sun, 17 Sep 2023 02:24:30 GMT'}]",2023-09-19,"[['Zhao', 'Haiyan', ''], ['Chen', 'Hanjie', ''], ['Yang', 'Fan', ''], ['Liu', 'Ninghao', ''], ['Deng', 'Huiqi', ''], ['Cai', 'Hengyi', ''], ['Wang', 'Shuaiqiang', ''], ['Yin', 'Dawei', ''], ['Du', 'Mengnan', '']]",0,0,2023-09-02,2,9,3,0,0,0,26089bdfdbca1e6eaaceca71e3116b715bec6d47,261530292.0,https://www.semanticscholar.org/paper/26089bdfdbca1e6eaaceca71e3116b715bec6d47,arXiv.org,2023.0,169.0,7.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2237987232', 'name': 'Haiyan Zhao'}, {'authorId': '2237948828', 'name': 'Hanjie Chen'}, {'authorId': '145338224', 'name': 'F. Yang'}, {'authorId': '47717322', 'name': 'Ninghao Liu'}, {'authorId': '13689700', 'name': 'Huiqi Deng'}, {'authorId': '22561596', 'name': 'Hengyi Cai'}, {'authorId': '2237948548', 'name': 'Shuaiqiang Wang'}, {'authorId': '2136400100', 'name': 'Dawei Yin'}, {'authorId': '2237804196', 'name': 'Mengnan Du'}]","['Rice University', 'New Jersey Institute of Technology', 'Baidu', 'Wake Forest University', 'Shanghai Jiao Tong University', 'Institute of Computing Technology', 'Johns Hopkins University', 'University of Georgia']","['China', 'United States']",2023-09
2309.01071,Quanzhou Hu,"Rui Zhu, Quanzhou Hu, Wenxin Li, Honghao Xiao, Chaogang Wang, Zixin
  Zhou",Business Process Text Sketch Automation Generation Using Large Language Model,"10 pages, 7 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Business Process Management (BPM) is gaining increasing attention as it has the potential to cut costs while boosting output and quality. Business process document generation is a crucial stage in BPM. However, due to a shortage of datasets, data-driven deep learning techniques struggle to deliver the expected results. We propose an approach to transform Conditional Process Trees (CPTs) into Business Process Text Sketches (BPTSs) using Large Language Models (LLMs). The traditional prompting approach (Few-shot In-Context Learning) tries to get the correct answer in one go, and it can find the pattern of transforming simple CPTs into BPTSs, but for close-domain and CPTs with complex hierarchy, the traditional prompts perform weakly and with low correctness. We suggest using this technique to break down a difficult CPT into a number of basic CPTs and then solve each one in turn, drawing inspiration from the divide-and-conquer strategy. We chose 100 process trees with depths ranging from 2 to 5 at random, as well as CPTs with many nodes, many degrees of selection, and cyclic nesting. Experiments show that our method can achieve a correct rate of 93.42%, which is 45.17% better than traditional prompting methods. Our proposed method provides a solution for business process document generation in the absence of datasets, and secondly, it becomes potentially possible to provide a large number of datasets for the process model extraction (PME) domain. ","[{'version': 'v1', 'created': 'Sun, 3 Sep 2023 04:19:02 GMT'}]",2023-09-06,"[['Zhu', 'Rui', ''], ['Hu', 'Quanzhou', ''], ['Li', 'Wenxin', ''], ['Xiao', 'Honghao', ''], ['Wang', 'Chaogang', ''], ['Zhou', 'Zixin', '']]",0,0,2023-09-03,1,6,1,0,0,0,f6a752ec416df387895db0f9ae8bbdbca2eb27d3,261530971.0,https://www.semanticscholar.org/paper/f6a752ec416df387895db0f9ae8bbdbca2eb27d3,arXiv.org,2023.0,12.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2237796618', 'name': 'Rui Zhu'}, {'authorId': '2237944172', 'name': 'Quanzhou Hu'}, {'authorId': '2237958908', 'name': 'Wenxin Li'}, {'authorId': '2237953402', 'name': 'Honghao Xiao'}, {'authorId': '2238091332', 'name': 'Chaogang Wang'}, {'authorId': '2237952589', 'name': 'Zixin Zhou'}]",['Yunnan University'],['China'],2023-09
2309.01093,Jiajin Tang,"Jiajin Tang, Ge Zheng, Jingyi Yu, Sibei Yang",CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection,Accepted by ICCV 2023,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Task driven object detection aims to detect object instances suitable for affording a task in an image. Its challenge lies in object categories available for the task being too diverse to be limited to a closed set of object vocabulary for traditional object detection. Simply mapping categories and visual features of common objects to the task cannot address the challenge. In this paper, we propose to explore fundamental affordances rather than object categories, i.e., common attributes that enable different objects to accomplish the same task. Moreover, we propose a novel multi-level chain-of-thought prompting (MLCoT) to extract the affordance knowledge from large language models, which contains multi-level reasoning steps from task to object examples to essential visual attributes with rationales. Furthermore, to fully exploit knowledge to benefit object recognition and localization, we propose a knowledge-conditional detection framework, namely CoTDet. It conditions the detector from the knowledge to generate object queries and regress boxes. Experimental results demonstrate that our CoTDet outperforms state-of-the-art methods consistently and significantly (+15.6 box AP and +14.8 mask AP) and can generate rationales for why objects are detected to afford the task. ","[{'version': 'v1', 'created': 'Sun, 3 Sep 2023 06:18:39 GMT'}]",2023-09-06,"[['Tang', 'Jiajin', ''], ['Zheng', 'Ge', ''], ['Yu', 'Jingyi', ''], ['Yang', 'Sibei', '']]",0,0,2023-09-03,1,4,1,0,0,0,4c2ed9907a3e966b3caadec5dddfde0e4c83f9da,261531326.0,https://www.semanticscholar.org/paper/4c2ed9907a3e966b3caadec5dddfde0e4c83f9da,arXiv.org,2023.0,68.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2224559104', 'name': 'Jiajin Tang'}, {'authorId': '2224503429', 'name': 'Ge Zheng'}, {'authorId': '2238055478', 'name': 'Jingyi Yu'}, {'authorId': '3144952', 'name': 'Sibei Yang'}]",['ShanghaiTech University'],['China'],2023-09
2309.01114,Yang Tan,"Yang Tan, Mingchen Li, Zijie Huang, Huiqun Yu and Guisheng Fan",MedChatZH: a Better Medical Adviser Learns from Better Instructions,"7 pages, 3 figures",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generative large language models (LLMs) have shown great success in various applications, including question-answering (QA) and dialogue systems. However, in specialized domains like traditional Chinese medical QA, these models may perform unsatisfactorily without fine-tuning on domain-specific datasets. To address this, we introduce MedChatZH, a dialogue model designed specifically for traditional Chinese medical QA. Our model is pre-trained on Chinese traditional medical books and fine-tuned with a carefully curated medical instruction dataset. It outperforms several solid baselines on a real-world medical dialogue dataset. We release our model, code, and dataset on https://github.com/tyang816/MedChatZH to facilitate further research in the domain of traditional Chinese medicine and LLMs. ","[{'version': 'v1', 'created': 'Sun, 3 Sep 2023 08:08:15 GMT'}]",2023-09-06,"[['Tan', 'Yang', ''], ['Li', 'Mingchen', ''], ['Huang', 'Zijie', ''], ['Yu', 'Huiqun', ''], ['Fan', 'Guisheng', '']]",0,0,2023-09-03,1,5,2,0,0,0,0a10b5af8d6024b1888ae8faed08d5b4e62b09a6,261530957.0,https://www.semanticscholar.org/paper/0a10b5af8d6024b1888ae8faed08d5b4e62b09a6,arXiv.org,2023.0,23.0,1.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2237936667', 'name': 'Yang Tan'}, {'authorId': '2237955385', 'name': 'Mingchen Li'}, {'authorId': '2142592610', 'name': 'Zijie Huang'}, {'authorId': '2237960031', 'name': 'Huiqun Yu'}, {'authorId': '13795313', 'name': 'Guisheng Fan'}]",['East China University of Science and Technology'],['China'],2023-09
2309.01157,Lei Li,"Lei Li, Yongfeng Zhang, Dugang Liu, Li Chen",Large Language Models for Generative Recommendation: A Survey and Visionary Discussions,,,,,cs.IR cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Recent years have witnessed the wide adoption of large language models (LLM) in different fields, especially natural language processing and computer vision. Such a trend can also be observed in recommender systems (RS). However, most of related work treat LLM as a component of the conventional recommendation pipeline (e.g., as a feature extractor) which may not be able to fully leverage the generative power of LLM. Instead of separating the recommendation process into multiple stages such as score computation and re-ranking, this process can be simplified to one stage with LLM: directly generating recommendations from the complete pool of items. This survey reviews the progress, methods and future directions of LLM-based generative recommendation by examining three questions: 1) What generative recommendation is, 2) Why RS should advance to generative recommendation, and 3) How to implement LLM-based generative recommendation for various RS tasks. We hope that the survey can provide the context and guidance needed to explore this interesting and emerging topic. ","[{'version': 'v1', 'created': 'Sun, 3 Sep 2023 12:33:47 GMT'}]",2023-09-06,"[['Li', 'Lei', ''], ['Zhang', 'Yongfeng', ''], ['Liu', 'Dugang', ''], ['Chen', 'Li', '']]",0,0,2023-09-03,1,4,3,0,0,0,a1081c6fc6921d6b76d9ebda4d712333fd7bbbf5,261531422.0,https://www.semanticscholar.org/paper/a1081c6fc6921d6b76d9ebda4d712333fd7bbbf5,arXiv.org,2023.0,85.0,6.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2151529879', 'name': 'Lei Li'}, {'authorId': '1739818', 'name': 'Yongfeng Zhang'}, {'authorId': '2237956346', 'name': 'Dugang Liu'}, {'authorId': '152875291', 'name': 'L. Chen'}]","['Rutgers, The State University of New Jersey', 'Hong Kong Baptist University', 'Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), Shenzhen, China']","['China', 'United States']",2023-09
2309.01172,Zhenheng Tang,"Zhenheng Tang, Yuxin Wang, Xin He, Longteng Zhang, Xinglin Pan, Qiang
  Wang, Rongfei Zeng, Kaiyong Zhao, Shaohuai Shi, Bingsheng He, Xiaowen Chu",FusionAI: Decentralized Training and Deploying LLMs with Massive Consumer-Level GPUs,,,,,cs.DC cs.AI cs.LG cs.NI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The rapid growth of memory and computation requirements of large language models (LLMs) has outpaced the development of hardware, hindering people who lack large-scale high-end GPUs from training or deploying LLMs. However, consumer-level GPUs, which constitute a larger market share, are typically overlooked in LLM due to their weaker computing performance, smaller storage capacity, and lower communication bandwidth. Additionally, users may have privacy concerns when interacting with remote LLMs. In this paper, we envision a decentralized system unlocking the potential vast untapped consumer-level GPUs in pre-training, inference and fine-tuning of LLMs with privacy protection. However, this system faces critical challenges, including limited CPU and GPU memory, low network bandwidth, the variability of peer and device heterogeneity. To address these challenges, our system design incorporates: 1) a broker with backup pool to implement dynamic join and quit of computing providers; 2) task scheduling with hardware performance to improve system efficiency; 3) abstracting ML procedures into directed acyclic graphs (DAGs) to achieve model and task universality; 4) abstracting intermediate represention and execution planes to ensure compatibility of various devices and deep learning (DL) frameworks. Our performance analysis demonstrates that 50 RTX 3080 GPUs can achieve throughputs comparable to those of 4 H100 GPUs, which are significantly more expensive. ","[{'version': 'v1', 'created': 'Sun, 3 Sep 2023 13:27:56 GMT'}]",2023-09-06,"[['Tang', 'Zhenheng', ''], ['Wang', 'Yuxin', ''], ['He', 'Xin', ''], ['Zhang', 'Longteng', ''], ['Pan', 'Xinglin', ''], ['Wang', 'Qiang', ''], ['Zeng', 'Rongfei', ''], ['Zhao', 'Kaiyong', ''], ['Shi', 'Shaohuai', ''], ['He', 'Bingsheng', ''], ['Chu', 'Xiaowen', '']]",0,0,2023-09-03,1,11,4,0,0,0,660380b17d3a37d8132f2e6dcb5cb47092e5b7d1,261530813.0,https://www.semanticscholar.org/paper/660380b17d3a37d8132f2e6dcb5cb47092e5b7d1,arXiv.org,2023.0,86.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Engineering', 'source': 's2-fos-model'}]","[{'authorId': '66873962', 'name': 'Zhenheng Tang'}, {'authorId': '2115828967', 'name': 'Yuxin Wang'}, {'authorId': '2237786665', 'name': 'Xin He'}, {'authorId': '2220130622', 'name': 'Longteng Zhang'}, {'authorId': '2237187329', 'name': 'Xinglin Pan'}, {'authorId': '2183630484', 'name': 'Qiang Wang'}, {'authorId': '2237804841', 'name': 'Rongfei Zeng'}, {'authorId': '2047645', 'name': 'Kaiyong Zhao'}, {'authorId': '2268704', 'name': 'S. Shi'}, {'authorId': '2237810067', 'name': 'Bingsheng He'}, {'authorId': '1680596', 'name': 'Xiaowen Chu'}]","['Data Science and Analytics Thrust, HKUST(GZ)', 'National University of Singapore', 'Harbin Institute of Technology', 'Northeastern University', 'Hong Kong Baptist University']","['China', 'Singapore']",2023-09
2309.01189,Jiaxing Qi,"Jiaxing Qi, Shaohan Huang, Zhongzhi Luan, Carol Fung, Hailong Yang,
  Depei Qian",LogGPT: Exploring ChatGPT for Log-Based Anomaly Detection,,,,,cs.LG cs.AI cs.SE,http://creativecommons.org/licenses/by/4.0/,"  The increasing volume of log data produced by software-intensive systems makes it impractical to analyze them manually. Many deep learning-based methods have been proposed for log-based anomaly detection. These methods face several challenges such as high-dimensional and noisy log data, class imbalance, generalization, and model interpretability. Recently, ChatGPT has shown promising results in various domains. However, there is still a lack of study on the application of ChatGPT for log-based anomaly detection. In this work, we proposed LogGPT, a log-based anomaly detection framework based on ChatGPT. By leveraging the ChatGPT's language interpretation capabilities, LogGPT aims to explore the transferability of knowledge from large-scale corpora to log-based anomaly detection. We conduct experiments to evaluate the performance of LogGPT and compare it with three deep learning-based methods on BGL and Spirit datasets. LogGPT shows promising results and has good interpretability. This study provides preliminary insights into prompt-based models, such as ChatGPT, for the log-based anomaly detection task. ","[{'version': 'v1', 'created': 'Sun, 3 Sep 2023 14:22:57 GMT'}]",2023-09-06,"[['Qi', 'Jiaxing', ''], ['Huang', 'Shaohan', ''], ['Luan', 'Zhongzhi', ''], ['Fung', 'Carol', ''], ['Yang', 'Hailong', ''], ['Qian', 'Depei', '']]",1,1,2023-09-03,1,6,3,1,0,1,d308562bc7eac8bb5c6705af1c41d8074e3a6882,261530676.0,https://www.semanticscholar.org/paper/d308562bc7eac8bb5c6705af1c41d8074e3a6882,arXiv.org,2023.0,37.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2237806343', 'name': 'Jiaxing Qi'}, {'authorId': '3110003', 'name': 'Shaohan Huang'}, {'authorId': '1698860', 'name': 'Zhongzhi Luan'}, {'authorId': '1746786', 'name': 'Carol J. Fung'}, {'authorId': '46402263', 'name': 'Hailong Yang'}, {'authorId': '145047216', 'name': 'D. Qian'}]","['Concordia University', 'Beihang University']","['Canada', 'China']",2023-09
2309.01219,Yue Zhang,"Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu,
  Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu,
  Wei Bi, Freda Shi, Shuming Shi",Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models,work in progress; 32 pages,,,,cs.CL cs.AI cs.CY cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research. ","[{'version': 'v1', 'created': 'Sun, 3 Sep 2023 16:56:48 GMT'}, {'version': 'v2', 'created': 'Sun, 24 Sep 2023 16:03:24 GMT'}]",2023-09-26,"[['Zhang', 'Yue', ''], ['Li', 'Yafu', ''], ['Cui', 'Leyang', ''], ['Cai', 'Deng', ''], ['Liu', 'Lemao', ''], ['Fu', 'Tingchen', ''], ['Huang', 'Xinting', ''], ['Zhao', 'Enbo', ''], ['Zhang', 'Yu', ''], ['Chen', 'Yulong', ''], ['Wang', 'Longyue', ''], ['Luu', 'Anh Tuan', ''], ['Bi', 'Wei', ''], ['Shi', 'Freda', ''], ['Shi', 'Shuming', '']]",0,0,2023-09-03,2,15,4,0,0,0,669441cb46666036f663f19def44bec2a838a518,261530162.0,https://www.semanticscholar.org/paper/669441cb46666036f663f19def44bec2a838a518,arXiv.org,2023.0,215.0,37.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1895977079', 'name': 'Yue Zhang'}, {'authorId': '2110450452', 'name': 'Yafu Li'}, {'authorId': '152496687', 'name': 'Leyang Cui'}, {'authorId': '1724421', 'name': 'Deng Cai'}, {'authorId': '2978364', 'name': 'Lemao Liu'}, {'authorId': '2156525869', 'name': 'Tingchen Fu'}, {'authorId': '14799547', 'name': 'Xinting Huang'}, {'authorId': '2065703096', 'name': 'Enbo Zhao'}, {'authorId': '2257439415', 'name': 'Yu Zhang'}, {'authorId': '2109404730', 'name': 'Yulong Chen'}, {'authorId': '1800190', 'name': 'Longyue Wang'}, {'authorId': '1755919', 'name': 'A. Luu'}, {'authorId': '2237804371', 'name': 'Wei Bi'}, {'authorId': '2237805147', 'name': 'Freda Shi'}, {'authorId': '34720053', 'name': 'Shuming Shi'}]","['Zhejiang University', 'Soochow University', 'Tencent', 'Nanyang Technological University', 'Toyota Technological Institute at Chicago', 'Renmin University of China']","['China', 'United States', 'Singapore']",2023-09
2309.01249,Kezhi Wang,"Feibo Jiang, Yubo Peng, Li Dong, Kezhi Wang, Kun Yang, Cunhua Pan,
  Xiaohu You",Large AI Model Empowered Multimodal Semantic Communications,To be submitted for journal publication,,,,cs.AI cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal signals, including text, audio, image and video, can be integrated into Semantic Communication (SC) for providing an immersive experience with low latency and high quality at the semantic level. However, the multimodal SC has several challenges, including data heterogeneity, semantic ambiguity, and signal fading. Recent advancements in large AI models, particularly in Multimodal Language Model (MLM) and Large Language Model (LLM), offer potential solutions for these issues. To this end, we propose a Large AI Model-based Multimodal SC (LAM-MSC) framework, in which we first present the MLM-based Multimodal Alignment (MMA) that utilizes the MLM to enable the transformation between multimodal and unimodal data while preserving semantic consistency. Then, a personalized LLM-based Knowledge Base (LKB) is proposed, which allows users to perform personalized semantic extraction or recovery through the LLM. This effectively addresses the semantic ambiguity. Finally, we apply the Conditional Generative adversarial networks-based channel Estimation (CGE) to obtain Channel State Information (CSI). This approach effectively mitigates the impact of fading channels in SC. Finally, we conduct simulations that demonstrate the superior performance of the LAM-MSC framework. ","[{'version': 'v1', 'created': 'Sun, 3 Sep 2023 19:24:34 GMT'}]",2023-09-06,"[['Jiang', 'Feibo', ''], ['Peng', 'Yubo', ''], ['Dong', 'Li', ''], ['Wang', 'Kezhi', ''], ['Yang', 'Kun', ''], ['Pan', 'Cunhua', ''], ['You', 'Xiaohu', '']]",0,0,2023-09-03,1,7,3,0,0,0,cab457fb2a9db431173bf744b273bcd7df543365,261530761.0,https://www.semanticscholar.org/paper/cab457fb2a9db431173bf744b273bcd7df543365,arXiv.org,2023.0,20.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '40540111', 'name': 'Feibo Jiang'}, {'authorId': '2238127953', 'name': 'Yubo Peng'}, {'authorId': '2152288497', 'name': 'Li Dong'}, {'authorId': '1878751', 'name': 'Kezhi Wang'}, {'authorId': '2214014346', 'name': 'Kun Yang'}, {'authorId': '2238046378', 'name': 'Cunhua Pan'}, {'authorId': '2237802990', 'name': 'Xiaohu You'}]","['Brunel University London', 'Hunan University of Technology', 'Changchun Institute of Technology', 'University of Essex', 'Hunan Normal University', 'Southeast University', 'Purple Mountain Laboratories', 'Large AI Model Empowered Multimodal Semantic Communications']","['China', 'United Kingdom']",2023-09
2309.01352,Shaohui Peng,"Shaohui Peng, Xing Hu, Qi Yi, Rui Zhang, Jiaming Guo, Di Huang, Zikang
  Tian, Ruizhi Chen, Zidong Du, Qi Guo, Yunji Chen, Ling Li",Self-driven Grounding: Large Language Model Agents with Automatical Language-aligned Skill Learning,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) show their powerful automatic reasoning and planning capability with a wealth of semantic knowledge about the human world. However, the grounding problem still hinders the applications of LLMs in the real-world environment. Existing studies try to fine-tune the LLM or utilize pre-defined behavior APIs to bridge the LLMs and the environment, which not only costs huge human efforts to customize for every single task but also weakens the generality strengths of LLMs. To autonomously ground the LLM onto the environment, we proposed the Self-Driven Grounding (SDG) framework to automatically and progressively ground the LLM with self-driven skill learning. SDG first employs the LLM to propose the hypothesis of sub-goals to achieve tasks and then verify the feasibility of the hypothesis via interacting with the underlying environment. Once verified, SDG can then learn generalized skills with the guidance of these successfully grounded subgoals. These skills can be further utilized to accomplish more complex tasks which fail to pass the verification phase. Verified in the famous instruction following task set-BabyAI, SDG achieves comparable performance in the most challenging tasks compared with imitation learning methods that cost millions of demonstrations, proving the effectiveness of learned skills and showing the feasibility and efficiency of our framework. ","[{'version': 'v1', 'created': 'Mon, 4 Sep 2023 04:31:24 GMT'}]",2023-09-06,"[['Peng', 'Shaohui', ''], ['Hu', 'Xing', ''], ['Yi', 'Qi', ''], ['Zhang', 'Rui', ''], ['Guo', 'Jiaming', ''], ['Huang', 'Di', ''], ['Tian', 'Zikang', ''], ['Chen', 'Ruizhi', ''], ['Du', 'Zidong', ''], ['Guo', 'Qi', ''], ['Chen', 'Yunji', ''], ['Li', 'Ling', '']]",0,0,2023-09-04,1,12,2,0,0,0,cc92496398fbb78646341f95d79e25080ff58b53,261530737.0,https://www.semanticscholar.org/paper/cc92496398fbb78646341f95d79e25080ff58b53,arXiv.org,2023.0,27.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2072713784', 'name': 'Shaohui Peng'}, {'authorId': '2109635961', 'name': 'Xingui Hu'}, {'authorId': '2187686017', 'name': 'Qi Yi'}, {'authorId': '2118404461', 'name': 'Rui Zhang'}, {'authorId': '47093626', 'name': 'Jiaming Guo'}, {'authorId': '2110180755', 'name': 'Di Huang'}, {'authorId': '2237949836', 'name': 'Zikang Tian'}, {'authorId': '2118229954', 'name': 'Rui Chen'}, {'authorId': '1678776', 'name': 'Zidong Du'}, {'authorId': '145461472', 'name': 'Qi Guo'}, {'authorId': '7377735', 'name': 'Yunji Chen'}, {'authorId': '3353457', 'name': 'Ling Li'}]","['University of Chinese Academy of Sciences', 'Cambricon Technologies', 'Chinese Academy of Sciences', 'Institute of Computing Technology', 'University of Science and Technology of China']",['China'],2023-09
2309.01398,Danqing Hu,"Danqing Hu, Bing Liu, Xiaofeng Zhu, Xudong Lu, Nan Wu",Zero-shot information extraction from radiological reports using ChatGPT,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Electronic health records contain an enormous amount of valuable information, but many are recorded in free text. Information extraction is the strategy to transform the sequence of characters into structured data, which can be employed for secondary analysis. However, the traditional information extraction components, such as named entity recognition and relation extraction, require annotated data to optimize the model parameters, which has become one of the major bottlenecks in building information extraction systems. With the large language models achieving good performances on various downstream NLP tasks without parameter tuning, it becomes possible to use large language models for zero-shot information extraction. In this study, we aim to explore whether the most popular large language model, ChatGPT, can extract useful information from the radiological reports. We first design the prompt template for the interested information in the CT reports. Then, we generate the prompts by combining the prompt template with the CT reports as the inputs of ChatGPT to obtain the responses. A post-processing module is developed to transform the responses into structured extraction results. We conducted the experiments with 847 CT reports collected from Peking University Cancer Hospital. The experimental results indicate that ChatGPT can achieve competitive performances for some extraction tasks compared with the baseline information extraction system, but some limitations need to be further improved. ","[{'version': 'v1', 'created': 'Mon, 4 Sep 2023 07:00:26 GMT'}, {'version': 'v2', 'created': 'Thu, 7 Sep 2023 01:36:08 GMT'}]",2023-09-08,"[['Hu', 'Danqing', ''], ['Liu', 'Bing', ''], ['Zhu', 'Xiaofeng', ''], ['Lu', 'Xudong', ''], ['Wu', 'Nan', '']]",1,1,2023-09-04,2,5,1,1,0,1,0386711d1f9c4240ded4de56026ca18e475b507a,261530161.0,https://www.semanticscholar.org/paper/0386711d1f9c4240ded4de56026ca18e475b507a,arXiv.org,2023.0,32.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '49592130', 'name': 'D. Hu'}, {'authorId': '2237875396', 'name': 'Bing Liu'}, {'authorId': '2237953806', 'name': 'Xiaofeng Zhu'}, {'authorId': '2157481275', 'name': 'Xudong Lu'}, {'authorId': '2237804955', 'name': 'Nan Wu'}]","['Zhejiang Lab', 'Peking University Cancer Hospital', 'Zhejiang University']",['China'],2023-09
2309.01431,Jiawei Chen,"Jiawei Chen, Hongyu Lin, Xianpei Han, Le Sun",Benchmarking Large Language Models in Retrieval-Augmented Generation,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs. ","[{'version': 'v1', 'created': 'Mon, 4 Sep 2023 08:28:44 GMT'}]",2023-09-06,"[['Chen', 'Jiawei', ''], ['Lin', 'Hongyu', ''], ['Han', 'Xianpei', ''], ['Sun', 'Le', '']]",0,0,2023-09-04,1,4,1,0,0,0,28e2ecb4183ebc0eec504b12dddc677f8aef8745,261530434.0,https://www.semanticscholar.org/paper/28e2ecb4183ebc0eec504b12dddc677f8aef8745,arXiv.org,2023.0,48.0,4.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2115448879', 'name': 'Jiawei Chen'}, {'authorId': '2116455765', 'name': 'Hongyu Lin'}, {'authorId': '2118233348', 'name': 'Xianpei Han'}, {'authorId': '2110832778', 'name': 'Le Sun'}]","['University of Chinese Academy of Sciences', 'Chinese Academy of Sciences', 'Chinese Information Processing Laboratory']",['China'],2023-09
2309.01522,Rui Wang,"Rui Wang, Xing Liu, Yanan Wang and Haiping Huang",What are Public Concerns about ChatGPT? A Novel Self-Supervised Neural Topic Model Tells You,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The recently released artificial intelligence conversational agent, ChatGPT, has gained significant attention in academia and real life. A multitude of early ChatGPT users eagerly explore its capabilities and share their opinions on it via social media. Both user queries and social media posts express public concerns regarding this advanced dialogue system. To mine public concerns about ChatGPT, a novel Self-Supervised neural Topic Model (SSTM), which formalizes topic modeling as a representation learning procedure, is proposed in this paper. Extensive experiments have been conducted on Twitter posts about ChatGPT and queries asked by ChatGPT users. And experimental results demonstrate that the proposed approach could extract higher quality public concerns with improved interpretability and diversity, surpassing the performance of state-of-the-art approaches. ","[{'version': 'v1', 'created': 'Mon, 4 Sep 2023 11:05:10 GMT'}]",2023-09-06,"[['Wang', 'Rui', ''], ['Liu', 'Xing', ''], ['Wang', 'Yanan', ''], ['Huang', 'Haiping', '']]",1,1,2023-09-04,1,4,1,1,0,1,8419583904fefc39a09e4c13d0c737fd4f55cf1b,261531344.0,https://www.semanticscholar.org/paper/8419583904fefc39a09e4c13d0c737fd4f55cf1b,arXiv.org,2023.0,27.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2237887553', 'name': 'Rui Wang'}, {'authorId': '2146035633', 'name': 'Xing Liu'}, {'authorId': '2238125114', 'name': 'Yanan Wang'}, {'authorId': '2146285809', 'name': 'Haiping Huang'}]",['Nanjing University of Posts and Telecommunications'],['China'],2023-09
2309.01645,Linping Zhong,"Siyi Cao, Linping Zhong",Exploring the effectiveness of ChatGPT-based feedback compared with teacher feedback and self-feedback: Evidence from Chinese to English translation,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  ChatGPT,a cutting-edge AI-powered Chatbot,can quickly generate responses on given commands. While it was reported that ChatGPT had the capacity to deliver useful feedback, it is still unclear about its effectiveness compared with conventional feedback approaches,such as teacher feedback (TF) and self-feedback (SF). To address this issue, this study compared the revised Chinese to English translation texts produced by Chinese Master of Translation and Interpretation (MTI) students,who learned English as a Second/Foreign Language (ESL/EFL), based on three feedback types (i.e., ChatGPT-based feedback, TF and SF). The data was analyzed using BLEU score to gauge the overall translation quality as well as Coh-Metrix to examine linguistic features across three dimensions: lexicon, syntax, and cohesion.The findings revealed that TF- and SF-guided translation texts surpassed those with ChatGPT-based feedback, as indicated by the BLEU score. In terms of linguistic features,ChatGPT-based feedback demonstrated superiority, particularly in enhancing lexical capability and referential cohesion in the translation texts. However, TF and SF proved more effective in developing syntax-related skills,as it addressed instances of incorrect usage of the passive voice. These diverse outcomes indicate ChatGPT's potential as a supplementary resource, complementing traditional teacher-led methods in translation practice. ","[{'version': 'v1', 'created': 'Mon, 4 Sep 2023 14:54:39 GMT'}]",2023-09-06,"[['Cao', 'Siyi', ''], ['Zhong', 'Linping', '']]",1,1,2023-09-04,1,2,1,1,0,1,2bd0865829ed7fa36364331471933735fbcdbecb,261531294.0,https://www.semanticscholar.org/paper/2bd0865829ed7fa36364331471933735fbcdbecb,arXiv.org,2023.0,114.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '2237958193', 'name': 'Siyi Cao'}, {'authorId': '2237817209', 'name': 'Linping Zhong'}]","['Monash University', 'Southeast University', 'Hong Kong Polytechnic University']","['China', 'Australia', 'Hong Kong']",2023-09
2309.01686,Zihao Zhou,"Zihao Zhou and Qiufeng Wang and Mingyu Jin and Jie Yao and Jianan Ye
  and Wei Liu and Wei Wang and Xiaowei Huang and Kaizhu Huang",MathAttack: Attacking Large Language Models Towards Math Solving Ability,"11 pages, 6 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the boom of Large Language Models (LLMs), the research of solving Math Word Problem (MWP) has recently made great progress. However, there are few studies to examine the security of LLMs in math solving ability. Instead of attacking prompts in the use of LLMs, we propose a MathAttack model to attack MWP samples which are closer to the essence of security in solving math problems. Compared to traditional text adversarial attack, it is essential to preserve the mathematical logic of original MWPs during the attacking. To this end, we propose logical entity recognition to identify logical entries which are then frozen. Subsequently, the remaining text are attacked by adopting a word-level attacker. Furthermore, we propose a new dataset RobustMath to evaluate the robustness of LLMs in math solving ability. Extensive experiments on our RobustMath and two another math benchmark datasets GSM8K and MultiAirth show that MathAttack could effectively attack the math solving ability of LLMs. In the experiments, we observe that (1) Our adversarial samples from higher-accuracy LLMs are also effective for attacking LLMs with lower accuracy (e.g., transfer from larger to smaller-size LLMs, or from few-shot to zero-shot prompts); (2) Complex MWPs (such as more solving steps, longer text, more numbers) are more vulnerable to attack; (3) We can improve the robustness of LLMs by using our adversarial samples in few-shot prompts. Finally, we hope our practice and observation can serve as an important attempt towards enhancing the robustness of LLMs in math solving ability. We will release our code and dataset. ","[{'version': 'v1', 'created': 'Mon, 4 Sep 2023 16:02:23 GMT'}]",2023-09-06,"[['Zhou', 'Zihao', ''], ['Wang', 'Qiufeng', ''], ['Jin', 'Mingyu', ''], ['Yao', 'Jie', ''], ['Ye', 'Jianan', ''], ['Liu', 'Wei', ''], ['Wang', 'Wei', ''], ['Huang', 'Xiaowei', ''], ['Huang', 'Kaizhu', '']]",0,0,2023-09-04,1,9,1,0,0,0,3886f3bd2a0af9e75bf9fa5b7db4224969dbf346,261531153.0,https://www.semanticscholar.org/paper/3886f3bd2a0af9e75bf9fa5b7db4224969dbf346,arXiv.org,2023.0,32.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2117793393', 'name': 'Zihao Zhou'}, {'authorId': '1500377117', 'name': 'Qiufeng Wang'}, {'authorId': '2237809092', 'name': 'Mingyu Jin'}, {'authorId': '2220808197', 'name': 'Jie Yao'}, {'authorId': '2153258359', 'name': 'Jianan Ye'}, {'authorId': '2157222998', 'name': 'Wei Liu'}, {'authorId': '145200778', 'name': 'Wei Wang'}, {'authorId': '2145052820', 'name': 'Xiaowei Huang'}, {'authorId': '2238398725', 'name': 'Kaizhu Huang'}]","['Xian Jiaotong-Liverpool University', 'Northwestern University', 'ShanghaiTech University', 'University of Liverpool', 'Duke Kunshan University']","['China', 'United States', 'United Kingdom']",2023-09
2309.01957,Ankita Sharma,"Ankita Sharma, Xuanmao Li, Hong Guan, Guoxin Sun, Liang Zhang, Lanjun
  Wang, Kesheng Wu, Lei Cao, Erkang Zhu, Alexander Sim, Teresa Wu, Jia Zou",Automatic Data Transformation Using Large Language Model: An Experimental Study on Building Energy Data,"10 pages, 7 figures",,,,cs.DB,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing approaches to automatic data transformation are insufficient to meet the requirements in many real-world scenarios, such as the building sector. First, there is no convenient interface for domain experts to provide domain knowledge easily. Second, they require significant training data collection overheads. Third, the accuracy suffers from complicated schema changes. To bridge this gap, we present a novel approach that leverages the unique capabilities of large language models (LLMs) in coding, complex reasoning, and zero-shot learning to generate SQL code that transforms the source datasets into the target datasets. We demonstrate the viability of this approach by designing an LLM-based framework, termed SQLMorpher, which comprises a prompt generator that integrates the initial prompt with optional domain knowledge and historical patterns in external databases. It also implements an iterative prompt optimization mechanism that automatically improves the prompt based on flaw detection. The key contributions of this work include (1) pioneering an end-to-end LLM-based solution for data transformation, (2) developing a benchmark dataset of 105 real-world building energy data transformation problems, and (3) conducting an extensive empirical evaluation where our approach achieved 96% accuracy in all 105 problems. SQLMorpher demonstrates the effectiveness of utilizing LLMs in complex, domain-specific challenges, highlighting the potential of their potential to drive sustainable solutions. ","[{'version': 'v1', 'created': 'Tue, 5 Sep 2023 05:19:35 GMT'}, {'version': 'v2', 'created': 'Wed, 6 Sep 2023 04:25:02 GMT'}]",2023-09-07,"[['Sharma', 'Ankita', ''], ['Li', 'Xuanmao', ''], ['Guan', 'Hong', ''], ['Sun', 'Guoxin', ''], ['Zhang', 'Liang', ''], ['Wang', 'Lanjun', ''], ['Wu', 'Kesheng', ''], ['Cao', 'Lei', ''], ['Zhu', 'Erkang', ''], ['Sim', 'Alexander', ''], ['Wu', 'Teresa', ''], ['Zou', 'Jia', '']]",0,0,2023-09-05,2,12,1,0,0,0,3120c2763edab339b937ddbe76991ebdfe0e01e6,261530167.0,https://www.semanticscholar.org/paper/3120c2763edab339b937ddbe76991ebdfe0e01e6,arXiv.org,2023.0,32.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2237952927', 'name': 'Ankita Sharma'}, {'authorId': '2237949801', 'name': 'Xuanmao Li'}, {'authorId': '2237800042', 'name': 'Hong Guan'}, {'authorId': '2237955180', 'name': 'Guoxin Sun'}, {'authorId': '2237796000', 'name': 'Liang Zhang'}, {'authorId': '2237944344', 'name': 'Lanjun Wang'}, {'authorId': '2238407326', 'name': 'Kesheng Wu'}, {'authorId': '2237951882', 'name': 'Lei Cao'}, {'authorId': '2237798544', 'name': 'Erkang Zhu'}, {'authorId': '2165305224', 'name': 'Alexander Sim'}, {'authorId': '2237957088', 'name': 'Teresa Wu'}, {'authorId': '2237786723', 'name': 'Jia Zou'}]","['Lawrence Berkeley National Laboratory', 'Huazhong University of Science and Technology', 'National Renewable Energy Laboratory', 'Microsoft']","['China', 'United States']",2023-09
2309.02045,Yajing Wang,Yajing Wang and Zongwei Luo,Enhance Multi-domain Sentiment Analysis of Review Texts through Prompting Strategies,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have made significant strides in both scientific research and practical applications. Existing studies have demonstrated the state-of-the-art (SOTA) performance of LLMs in various natural language processing tasks. However, the question of how to further enhance LLMs' performance in specific task using prompting strategies remains a pivotal concern. This paper explores the enhancement of LLMs' performance in sentiment analysis through the application of prompting strategies. We formulate the process of prompting for sentiment analysis tasks and introduce two novel strategies tailored for sentiment analysis: RolePlaying (RP) prompting and Chain-of-thought (CoT) prompting. Specifically, we also propose the RP-CoT prompting strategy which is a combination of RP prompting and CoT prompting. We conduct comparative experiments on three distinct domain datasets to evaluate the effectiveness of the proposed sentiment analysis strategies. The results demonstrate that the adoption of the proposed prompting strategies leads to a increasing enhancement in sentiment analysis accuracy. Further, the CoT prompting strategy exhibits a notable impact on implicit sentiment analysis, with the RP-CoT prompting strategy delivering the most superior performance among all strategies. ","[{'version': 'v1', 'created': 'Tue, 5 Sep 2023 08:44:23 GMT'}]",2023-09-06,"[['Wang', 'Yajing', ''], ['Luo', 'Zongwei', '']]",0,0,2023-09-05,1,2,3,0,0,0,fb7f6be7feac7c9d04da18caf0930bec7b1ac09b,261557149.0,https://www.semanticscholar.org/paper/fb7f6be7feac7c9d04da18caf0930bec7b1ac09b,arXiv.org,2023.0,33.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2238120483', 'name': 'Yajing Wang'}, {'authorId': '2238213850', 'name': 'Zongwei Luo'}]","['Hong Kong Baptist University', 'Beijing Normal University', 'United International College', 'Department of computer science Hongkong, Zhuhai, China', 'BNU-UIC Institute of AI and Future Networks, Artificial Intelligence and Data Science Research Hub']",['China'],2023-09
2309.02077,Yusheng Liao,"Yusheng Liao, Yutong Meng, Hongcheng Liu, Yanfeng Wang, Yu Wang",An Automatic Evaluation Framework for Multi-turn Medical Consultations Capabilities of Large Language Models,"10 pages, 9figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have achieved significant success in interacting with human. However, recent studies have revealed that these models often suffer from hallucinations, leading to overly confident but incorrect judgments. This limits their application in the medical domain, where tasks require the utmost accuracy. This paper introduces an automated evaluation framework that assesses the practical capabilities of LLMs as virtual doctors during multi-turn consultations. Consultation tasks are designed to require LLMs to be aware of what they do not know, to inquire about missing medical information from patients, and to ultimately make diagnoses. To evaluate the performance of LLMs for these tasks, a benchmark is proposed by reformulating medical multiple-choice questions from the United States Medical Licensing Examinations (USMLE), and comprehensive evaluation metrics are developed and evaluated on three constructed test sets. A medical consultation training set is further constructed to improve the consultation ability of LLMs. The results of the experiments show that fine-tuning with the training set can alleviate hallucinations and improve LLMs' performance on the proposed benchmark. Extensive experiments and ablation studies are conducted to validate the effectiveness and robustness of the proposed framework. ","[{'version': 'v1', 'created': 'Tue, 5 Sep 2023 09:24:48 GMT'}]",2023-09-06,"[['Liao', 'Yusheng', ''], ['Meng', 'Yutong', ''], ['Liu', 'Hongcheng', ''], ['Wang', 'Yanfeng', ''], ['Wang', 'Yu', '']]",0,0,2023-09-05,1,5,1,0,0,0,fd2545958ca8c0954d84ba6458ad6abb9b8b834a,261557089.0,https://www.semanticscholar.org/paper/fd2545958ca8c0954d84ba6458ad6abb9b8b834a,arXiv.org,2023.0,18.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2114121719', 'name': 'Yusheng Liao'}, {'authorId': '2238189600', 'name': 'Yutong Meng'}, {'authorId': '2220071082', 'name': 'Hongcheng Liu'}, {'authorId': '2238120662', 'name': 'Yanfeng Wang'}, {'authorId': '2220688186', 'name': 'Yu Wang'}]","['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University']",['China'],2023-09
2309.02144,Peiyi Wang,"Peiyi Wang and Lei Li and Liang Chen and Feifan Song and Binghuai Lin
  and Yunbo Cao and Tianyu Liu and Zhifang Sui",Making Large Language Models Better Reasoners with Alignment,Large Language Models; Reasoning; Alignment,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Reasoning is a cognitive process of using evidence to reach a sound conclusion. The reasoning capability is essential for large language models (LLMs) to serve as the brain of the artificial general intelligence agent. Recent studies reveal that fine-tuning LLMs on data with the chain of thought (COT) reasoning process can significantly enhance their reasoning capabilities. However, we find that the fine-tuned LLMs suffer from an \textit{Assessment Misalignment} problem, i.e., they frequently assign higher scores to subpar COTs, leading to potential limitations in their reasoning abilities. To address this problem, we introduce an \textit{Alignment Fine-Tuning (AFT)} paradigm, which involves three steps: 1) fine-tuning LLMs with COT training data; 2) generating multiple COT responses for each question, and categorizing them into positive and negative ones based on whether they achieve the correct answer; 3) calibrating the scores of positive and negative responses given by LLMs with a novel constraint alignment loss. Specifically, the constraint alignment loss has two objectives: a) Alignment, which guarantees that positive scores surpass negative scores to encourage answers with high-quality COTs; b) Constraint, which keeps the negative scores confined to a reasonable range to prevent the model degradation. Beyond just the binary positive and negative feedback, the constraint alignment loss can be seamlessly adapted to the ranking situations when ranking feedback is accessible. Furthermore, we also delve deeply into recent ranking-based alignment methods, such as DPO, RRHF, and PRO, and discover that the constraint, which has been overlooked by these approaches, is also crucial for their performance. Extensive experiments on four reasoning benchmarks with both binary and ranking feedback demonstrate the effectiveness of AFT. ","[{'version': 'v1', 'created': 'Tue, 5 Sep 2023 11:32:48 GMT'}]",2023-09-06,"[['Wang', 'Peiyi', ''], ['Li', 'Lei', ''], ['Chen', 'Liang', ''], ['Song', 'Feifan', ''], ['Lin', 'Binghuai', ''], ['Cao', 'Yunbo', ''], ['Liu', 'Tianyu', ''], ['Sui', 'Zhifang', '']]",0,0,2023-09-05,1,8,3,0,0,0,74b4b993babe99bc5f5c589c27fef0f1baba606b,261558535.0,https://www.semanticscholar.org/paper/74b4b993babe99bc5f5c589c27fef0f1baba606b,arXiv.org,2023.0,34.0,7.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '144202874', 'name': 'Peiyi Wang'}, {'authorId': '46255707', 'name': 'Lei Li'}, {'authorId': '2146034504', 'name': 'Liang Chen'}, {'authorId': '66947198', 'name': 'Feifan Song'}, {'authorId': '3186130', 'name': 'Binghuai Lin'}, {'authorId': '2238148953', 'name': 'Yunbo Cao'}, {'authorId': '1701889', 'name': 'Tianyu Liu'}, {'authorId': '3335836', 'name': 'Zhifang Sui'}]","['Peking University', 'Tencent', 'University of Hong Kong']","['China', 'Hong Kong']",2023-09
2309.02301,Hongyu Hu,"Hongyu Hu, Jiyuan Zhang, Minyi Zhao, Zhenbang Sun",CIEM: Contrastive Instruction Evaluation Method for Better Instruction Tuning,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Nowadays, the research on Large Vision-Language Models (LVLMs) has been significantly promoted thanks to the success of Large Language Models (LLM). Nevertheless, these Vision-Language Models (VLMs) are suffering from the drawback of hallucination -- due to insufficient understanding of vision and language modalities, VLMs may generate incorrect perception information when doing downstream applications, for example, captioning a non-existent entity. To address the hallucination phenomenon, on the one hand, we introduce a Contrastive Instruction Evaluation Method (CIEM), which is an automatic pipeline that leverages an annotated image-text dataset coupled with an LLM to generate factual/contrastive question-answer pairs for the evaluation of the hallucination of VLMs. On the other hand, based on CIEM, we further propose a new instruction tuning method called CIT (the abbreviation of Contrastive Instruction Tuning) to alleviate the hallucination of VLMs by automatically producing high-quality factual/contrastive question-answer pairs and corresponding justifications for model tuning. Through extensive experiments on CIEM and CIT, we pinpoint the hallucination issues commonly present in existing VLMs, the disability of the current instruction-tuning dataset to handle the hallucination phenomenon and the superiority of CIT-tuned VLMs over both CIEM and public datasets. ","[{'version': 'v1', 'created': 'Tue, 5 Sep 2023 15:06:37 GMT'}]",2023-09-06,"[['Hu', 'Hongyu', ''], ['Zhang', 'Jiyuan', ''], ['Zhao', 'Minyi', ''], ['Sun', 'Zhenbang', '']]",0,0,2023-09-05,1,4,1,0,0,0,73814a52609a9ee4c8f1b115e376b6a300ab6a57,261557047.0,https://www.semanticscholar.org/paper/73814a52609a9ee4c8f1b115e376b6a300ab6a57,arXiv.org,2023.0,28.0,1.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2238158720', 'name': 'Hongyu Hu'}, {'authorId': '2238121882', 'name': 'Jiyuan Zhang'}, {'authorId': '2238258246', 'name': 'Minyi Zhao'}, {'authorId': '2238143153', 'name': 'Zhenbang Sun'}]",['ByteDance'],['China'],2023-09
2309.02317,Yuxiang Guo,"Yuxiang Guo, Xiaopeng Gao, Zhenyu Zhang, W.K.Chan and Bo Jiang",A study on the impact of pre-trained model on Just-In-Time defect prediction,,,,,cs.SE cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Previous researchers conducting Just-In-Time (JIT) defect prediction tasks have primarily focused on the performance of individual pre-trained models, without exploring the relationship between different pre-trained models as backbones. In this study, we build six models: RoBERTaJIT, CodeBERTJIT, BARTJIT, PLBARTJIT, GPT2JIT, and CodeGPTJIT, each with a distinct pre-trained model as its backbone. We systematically explore the differences and connections between these models. Specifically, we investigate the performance of the models when using Commit code and Commit message as inputs, as well as the relationship between training efficiency and model distribution among these six models. Additionally, we conduct an ablation experiment to explore the sensitivity of each model to inputs. Furthermore, we investigate how the models perform in zero-shot and few-shot scenarios. Our findings indicate that each model based on different backbones shows improvements, and when the backbone's pre-training model is similar, the training resources that need to be consumed are much more closer. We also observe that Commit code plays a significant role in defect detection, and different pre-trained models demonstrate better defect detection ability with a balanced dataset under few-shot scenarios. These results provide new insights for optimizing JIT defect prediction tasks using pre-trained models and highlight the factors that require more attention when constructing such models. Additionally, CodeGPTJIT and GPT2JIT achieved better performance than DeepJIT and CC2Vec on the two datasets respectively under 2000 training samples. These findings emphasize the effectiveness of transformer-based pre-trained models in JIT defect prediction tasks, especially in scenarios with limited training data. ","[{'version': 'v1', 'created': 'Tue, 5 Sep 2023 15:34:22 GMT'}]",2023-09-06,"[['Guo', 'Yuxiang', ''], ['Gao', 'Xiaopeng', ''], ['Zhang', 'Zhenyu', ''], ['Chan', 'W. K.', ''], ['Jiang', 'Bo', '']]",0,1,2023-09-05,1,5,2,0,0,0,daa7b3f8d5610cd2dd1badde8c3a3e6f58726aa3,261557143.0,https://www.semanticscholar.org/paper/daa7b3f8d5610cd2dd1badde8c3a3e6f58726aa3,arXiv.org,2023.0,46.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2238018618', 'name': 'Yuxiang Guo'}, {'authorId': '2238341101', 'name': 'Xiaopeng Gao'}, {'authorId': '48805937', 'name': 'Zhenyu Zhang'}, {'authorId': '2220002980', 'name': 'Wing-Kwong Chan'}, {'authorId': '2237992825', 'name': 'Bo Jiang'}]","['City University of Hong Kong', 'Beihang University', 'Chinese Academy of Sciences']",['China'],2023-09
2309.02411,Bojia Zi,"Bojia Zi, Xianbiao Qi, Lingzhi Wang, Jianan Wang, Kam-Fai Wong, Lei
  Zhang",Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices,,,,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we present Delta-LoRA, which is a novel parameter-efficient approach to fine-tune large language models (LLMs). In contrast to LoRA and other low-rank adaptation methods such as AdaLoRA, Delta-LoRA not only updates the low-rank matrices $\bA$ and $\bB$, but also propagate the learning to the pre-trained weights $\bW$ via updates utilizing the delta of the product of two low-rank matrices ($\bA^{(t+1)}\bB^{(t+1)} - \bA^{(t)}\bB^{(t)}$). Such a strategy effectively addresses the limitation that the incremental update of low-rank matrices is inadequate for learning representations capable for downstream tasks. Moreover, as the update of $\bW$ does not need to compute the gradients of $\bW$ and store their momentums, Delta-LoRA shares comparable memory requirements and computational costs with LoRA. Extensive experiments show that Delta-LoRA significantly outperforms existing low-rank adaptation methods. We further support these results with comprehensive analyses that underscore the effectiveness of Delta-LoRA. ","[{'version': 'v1', 'created': 'Tue, 5 Sep 2023 17:40:34 GMT'}]",2023-09-06,"[['Zi', 'Bojia', ''], ['Qi', 'Xianbiao', ''], ['Wang', 'Lingzhi', ''], ['Wang', 'Jianan', ''], ['Wong', 'Kam-Fai', ''], ['Zhang', 'Lei', '']]",0,0,2023-09-05,1,6,1,0,0,0,587d0627031c165985c69036f62d5d21fc38e3f7,261556652.0,https://www.semanticscholar.org/paper/587d0627031c165985c69036f62d5d21fc38e3f7,arXiv.org,2023.0,38.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2237982760', 'name': 'Bojia Zi'}, {'authorId': '2689287', 'name': 'Xianbiao Qi'}, {'authorId': '2210097732', 'name': 'Lingzhi Wang'}, {'authorId': '2109495782', 'name': 'Jianan Wang'}, {'authorId': '2238149136', 'name': 'Kam-Fai Wong'}, {'authorId': '2256831731', 'name': 'Lei Zhang'}]","['International Digital Economy Academy (IDEA), Shenzhen, Guangdong, China', 'Chinese University of Hong Kong', 'MoE Key Laboratory of High Confidence Software Technologies, China']",['China'],2023-09
2309.02726,Zonglin Yang,"Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, Erik
  Cambria",Large Language Models for Automated Open-domain Scientific Hypotheses Discovery,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Hypothetical induction is recognized as the main reasoning type when scientists make observations about the world and try to propose hypotheses to explain those observations. Past research on hypothetical induction has a limited setting that (1) the observation annotations of the dataset are not raw web corpus but are manually selected sentences (resulting in a close-domain setting); and (2) the ground truth hypotheses annotations are mostly commonsense knowledge, making the task less challenging. In this work, we propose the first NLP dataset for social science academic hypotheses discovery, consisting of 50 recent papers published in top social science journals. Raw web corpora that are necessary for developing hypotheses in the published papers are also collected in the dataset, with the final goal of creating a system that automatically generates valid, novel, and helpful (to human researchers) hypotheses, given only a pile of raw web corpora. The new dataset can tackle the previous problems because it requires to (1) use raw web corpora as observations; and (2) propose hypotheses even new to humanity. A multi-module framework is developed for the task, as well as three different feedback mechanisms that empirically show performance gain over the base framework. Finally, our framework exhibits high performance in terms of both GPT-4 based evaluation and social science expert evaluation. ","[{'version': 'v1', 'created': 'Wed, 6 Sep 2023 05:19:41 GMT'}]",2023-09-07,"[['Yang', 'Zonglin', ''], ['Du', 'Xinya', ''], ['Li', 'Junxian', ''], ['Zheng', 'Jie', ''], ['Poria', 'Soujanya', ''], ['Cambria', 'Erik', '']]",0,1,2023-09-06,1,6,2,1,0,1,5aea5c8b536380c5ad1d42108c2c6767622318ee,261557055.0,https://www.semanticscholar.org/paper/5aea5c8b536380c5ad1d42108c2c6767622318ee,arXiv.org,2023.0,29.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2124477940', 'name': 'Zonglin Yang'}, {'authorId': '13728923', 'name': 'Xinya Du'}, {'authorId': '2238138967', 'name': 'Junxian Li'}, {'authorId': '2237998834', 'name': 'Jie Zheng'}, {'authorId': '1746416', 'name': 'Soujanya Poria'}, {'authorId': '49943757', 'name': 'E. Cambria'}]","['Huazhong University of Science and Technology', 'Singapore University of Technology and Design', 'The University of Texas at Dallas', 'Nanyang Technological University']","['China', 'United States', 'Singapore']",2023-09
2309.02731,Zhenpeng Su,"Zhenpeng Su, Xing Wu, Wei Zhou, Guangyuan Ma, Songlin Hu",HC3 Plus: A Semantic-Invariant Human ChatGPT Comparison Corpus,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  ChatGPT has gained significant interest due to its impressive performance, but people are increasingly concerned about its potential risks, particularly around the detection of AI-generated content (AIGC), which is often difficult for untrained humans to identify. Current datasets utilized for detecting ChatGPT-generated text primarily center around question-answering, yet they tend to disregard tasks that possess semantic-invariant properties, such as summarization, translation, and paraphrasing. Our primary studies demonstrate that detecting model-generated text on semantic-invariant tasks is more difficult. To fill this gap, we introduce a more extensive and comprehensive dataset that considers more types of tasks than previous work, including semantic-invariant tasks. In addition, the model after a large number of task instruction fine-tuning shows a strong powerful performance. Owing to its previous success, we further instruct fine-tuning Tk-instruct and built a more powerful detection system. Experimental results show that our proposed detector outperforms the previous state-of-the-art RoBERTa-based detector. ","[{'version': 'v1', 'created': 'Wed, 6 Sep 2023 05:33:57 GMT'}]",2023-09-07,"[['Su', 'Zhenpeng', ''], ['Wu', 'Xing', ''], ['Zhou', 'Wei', ''], ['Ma', 'Guangyuan', ''], ['Hu', 'Songlin', '']]",1,1,2023-09-06,1,5,2,2,1,1,ff7ee54876220ccf425050a14f14bb9893849f05,261556768.0,https://www.semanticscholar.org/paper/ff7ee54876220ccf425050a14f14bb9893849f05,arXiv.org,2023.0,20.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2216413613', 'name': 'Zhenpeng Su'}, {'authorId': '2155226596', 'name': 'Xing Wu'}, {'authorId': '2256662678', 'name': 'Wei Zhou'}, {'authorId': '2068996632', 'name': 'Guangyuan Ma'}, {'authorId': '2115190434', 'name': 'Song Hu'}]","['Institute of Information Engineering', 'University of Chinese Academy of Sciences']",['China'],2023-09
2309.02772,Yuqi Zhu,"Yuqi Zhu, Jia Allen Li, Ge Li, YunFei Zhao, Jia Li, Zhi Jin, Hong Mei",Improving Code Generation by Dynamic Temperature Sampling,,,,,cs.SE cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, Large Language Models (LLMs) have shown impressive results in code generation. However, existing decoding strategies are designed for Natural Language (NL) generation, overlooking the differences between NL and programming languages (PL). Due to this oversight, a better decoding strategy for code generation remains an open question. In this paper, we conduct the first systematic study to explore a decoding strategy specialized in code generation. With an analysis of loss distributions of code tokens, we find that code tokens can be divided into two categories: challenging tokens that are difficult to predict and confident tokens that can be easily inferred. Among them, the challenging tokens mainly appear at the beginning of a code block. Inspired by the above findings, we propose a simple yet effective method: Adaptive Temperature (AdapT) sampling, which dynamically adjusts the temperature coefficient when decoding different tokens. We apply a larger temperature when sampling for challenging tokens, allowing LLMs to explore diverse choices. We employ a smaller temperature for confident tokens avoiding the influence of tail randomness noises. We apply AdapT sampling to LLMs with different sizes and conduct evaluations on two popular datasets. Results show that AdapT sampling significantly outperforms state-of-the-art decoding strategy. ","[{'version': 'v1', 'created': 'Wed, 6 Sep 2023 06:27:33 GMT'}]",2023-09-07,"[['Zhu', 'Yuqi', ''], ['Li', 'Jia Allen', ''], ['Li', 'Ge', ''], ['Zhao', 'YunFei', ''], ['Li', 'Jia', ''], ['Jin', 'Zhi', ''], ['Mei', 'Hong', '']]",0,0,2023-09-06,1,7,2,0,0,0,04daa674f42b224b23d8813945e8f9cb646cf9e0,261557691.0,https://www.semanticscholar.org/paper/04daa674f42b224b23d8813945e8f9cb646cf9e0,arXiv.org,2023.0,22.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2238318585', 'name': 'Yuqi Zhu'}, {'authorId': '49299163', 'name': 'Jia Li'}, {'authorId': '2154591375', 'name': 'Ge Li'}, {'authorId': '2155133281', 'name': 'Yunfei Zhao'}, {'authorId': '2118373749', 'name': 'Jia Li'}, {'authorId': '2152843753', 'name': 'Zhi Jin'}, {'authorId': '2072391113', 'name': 'Hong Mei'}]","['Peking University', 'Academy of Military Medical Sciences', 'Beijing Institute of Big Data Research']",['China'],2023-09
2309.02823,Chenyang Liu,"Mengjuan Liu, Chenyang Liu, Yunfan Yang, Jiang Liu, Mohan Jing",Promoting Open-domain Dialogue Generation through Learning Pattern Information between Contexts and Responses,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, utilizing deep neural networks to build the opendomain dialogue models has become a hot topic. However, the responses generated by these models suffer from many problems such as responses not being contextualized and tend to generate generic responses that lack information content, damaging the user's experience seriously. Therefore, many studies try introducing more information into the dialogue models to make the generated responses more vivid and informative. Unlike them, this paper improves the quality of generated responses by learning the implicit pattern information between contexts and responses in the training samples. In this paper, we first build an open-domain dialogue model based on the pre-trained language model (i.e., GPT-2). And then, an improved scheduled sampling method is proposed for pre-trained models, by which the responses can be used to guide the response generation in the training phase while avoiding the exposure bias problem. More importantly, we design a response-aware mechanism for mining the implicit pattern information between contexts and responses so that the generated replies are more diverse and approximate to human replies. Finally, we evaluate the proposed model (RAD) on the Persona-Chat and DailyDialog datasets; and the experimental results show that our model outperforms the baselines on most automatic and manual metrics. ","[{'version': 'v1', 'created': 'Wed, 6 Sep 2023 08:11:39 GMT'}]",2023-09-07,"[['Liu', 'Mengjuan', ''], ['Liu', 'Chenyang', ''], ['Yang', 'Yunfan', ''], ['Liu', 'Jiang', ''], ['Jing', 'Mohan', '']]",0,1,2023-09-06,1,5,2,1,1,0,1d5341fb857212950b73e21e15c644cfd5aaa0a3,261557110.0,https://www.semanticscholar.org/paper/1d5341fb857212950b73e21e15c644cfd5aaa0a3,Natural Language Processing and Chinese Computing,2023.0,17.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2152972994', 'name': 'Mengjuan Liu'}, {'authorId': '2238178165', 'name': 'Chenyang Liu'}, {'authorId': '2116590607', 'name': 'Yun-Hee Yang'}, {'authorId': '2142783806', 'name': 'Jiang Liu'}, {'authorId': '2237988495', 'name': 'Mohan Jing'}]",['University of Electronic Science and Technology of China'],['China'],2023-09
2309.02926,Tong Liu,"Tong Liu, Zizhuang Deng, Guozhu Meng, Yuekang Li, Kai Chen",Demystifying RCE Vulnerabilities in LLM-Integrated Apps,,,,,cs.CR,http://creativecommons.org/licenses/by/4.0/,"  In recent years, Large Language Models (LLMs) have demonstrated remarkable potential across various downstream tasks. LLM-integrated frameworks, which serve as the essential infrastructure, have given rise to many LLM-integrated web apps. However, some of these frameworks suffer from Remote Code Execution (RCE) vulnerabilities, allowing attackers to execute arbitrary code on apps' servers remotely via prompt injections. Despite the severity of these vulnerabilities, no existing work has been conducted for a systematic investigation of them. This leaves a great challenge on how to detect vulnerabilities in frameworks as well as LLM-integrated apps in real-world scenarios.   To fill this gap, we present two novel strategies, including 1) a static analysis-based tool called LLMSmith to scan the source code of the framework to detect potential RCE vulnerabilities and 2) a prompt-based automated testing approach to verify the vulnerability in LLM-integrated web apps. We discovered 13 vulnerabilities in 6 frameworks, including 12 RCE vulnerabilities and 1 arbitrary file read/write vulnerability. 11 of them are confirmed by the framework developers, resulting in the assignment of 7 CVE IDs. After testing 51 apps, we found vulnerabilities in 17 apps, 16 of which are vulnerable to RCE and 1 to SQL injection. We responsibly reported all 17 issues to the corresponding developers and received acknowledgments. Furthermore, we amplify the attack impact beyond achieving RCE by allowing attackers to exploit other app users (e.g. app responses hijacking, user API key leakage) without direct interaction between the attacker and the victim. Lastly, we propose some mitigating strategies for improving the security awareness of both framework and app developers, helping them to mitigate these risks effectively. ","[{'version': 'v1', 'created': 'Wed, 6 Sep 2023 11:39:37 GMT'}]",2023-09-07,"[['Liu', 'Tong', ''], ['Deng', 'Zizhuang', ''], ['Meng', 'Guozhu', ''], ['Li', 'Yuekang', ''], ['Chen', 'Kai', '']]",0,0,2023-09-06,1,5,1,0,0,0,9be0dea0d6b892a2162490fb02712efaf10c0c87,261557096.0,https://www.semanticscholar.org/paper/9be0dea0d6b892a2162490fb02712efaf10c0c87,arXiv.org,2023.0,36.0,1.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2230082066', 'name': 'Tong Liu'}, {'authorId': '1663456658', 'name': 'Zizhuang Deng'}, {'authorId': '2018895', 'name': 'Guozhu Meng'}, {'authorId': '2238298604', 'name': 'Yuekang Li'}, {'authorId': '2159406392', 'name': 'Kai Chen'}]","['Institute of Information Engineering', 'University of Chinese Academy of Sciences', 'UNSW Sydney']","['China', 'Australia']",2023-09
2309.03126,Pengyu Cheng,"Pengyu Cheng, Jiawen Xie, Ke Bai, Yong Dai, Nan Du",Everyone Deserves A Reward: Learning Customized Human Preferences,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Reward models (RMs) are essential for aligning large language models (LLMs) with human preferences to improve interaction quality. However, the real world is pluralistic, which leads to diversified human preferences with respect to different religions, politics, cultures, etc. Moreover, each individual can have their unique preferences on various topics. Neglecting the diversity of human preferences, current human feedback aligning methods only consider a general reward model, which is below satisfaction for customized or personalized application scenarios. To explore customized preference learning, we collect a domain-specific preference (DSP) dataset, which includes preferred responses for each given query from four practical domains. Besides, from the perspective of data efficiency, we propose a three-stage customized RM learning scheme, then empirically verify its effectiveness on both general preference datasets and our DSP set. Furthermore, we test multiple training and data strategies on the three learning stages. We find several ways to better preserve the general preferring ability while training the customized RMs, especially general preference enrichment, and customized preference imitation learning. The DSP dataset and code are available at https://github.com/Linear95/DSP. ","[{'version': 'v1', 'created': 'Wed, 6 Sep 2023 16:03:59 GMT'}, {'version': 'v2', 'created': 'Fri, 15 Sep 2023 09:24:30 GMT'}]",2023-09-18,"[['Cheng', 'Pengyu', ''], ['Xie', 'Jiawen', ''], ['Bai', 'Ke', ''], ['Dai', 'Yong', ''], ['Du', 'Nan', '']]",0,0,2023-09-06,2,5,1,0,0,0,e0e106fc72458cd46b42d6341f809b4474b6226b,261557043.0,https://www.semanticscholar.org/paper/e0e106fc72458cd46b42d6341f809b4474b6226b,arXiv.org,2023.0,33.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '144533942', 'name': 'Pengyu Cheng'}, {'authorId': '2238052559', 'name': 'Jiawen Xie'}, {'authorId': '2146069687', 'name': 'Ke Bai'}, {'authorId': '2238187314', 'name': 'Yong Dai'}, {'authorId': '2237986798', 'name': 'Nan Du'}]","['Duke University', 'Shanghai Jiao Tong University', 'Tencent']","['China', 'United States']",2023-09
2309.03241,Zhen Yang,"Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo,
  Jinfeng Bai, Jie Tang",GPT Can Solve Mathematical Problems Without a Calculator,"26pages,14figures",,,,cs.LG cs.AI cs.CL,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Previous studies have typically assumed that large language models are unable to accurately perform arithmetic operations, particularly multiplication of >8 digits, and operations involving decimals and fractions, without the use of calculator tools. This paper aims to challenge this misconception. With sufficient training data, a 2 billion-parameter language model can accurately perform multi-digit arithmetic operations with almost 100% accuracy without data leakage, significantly surpassing GPT-4 (whose multi-digit multiplication accuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from GLM-10B on a dataset with additional multi-step arithmetic operations and math problems described in text, achieves similar performance to GPT-4 on a 5,000-samples Chinese math problem test set. Our code and data are public at https://github.com/THUDM/MathGLM. ","[{'version': 'v1', 'created': 'Wed, 6 Sep 2023 06:18:16 GMT'}, {'version': 'v2', 'created': 'Tue, 12 Sep 2023 11:01:25 GMT'}]",2023-09-13,"[['Yang', 'Zhen', ''], ['Ding', 'Ming', ''], ['Lv', 'Qingsong', ''], ['Jiang', 'Zhihuan', ''], ['He', 'Zehai', ''], ['Guo', 'Yuyi', ''], ['Bai', 'Jinfeng', ''], ['Tang', 'Jie', '']]",0,1,2023-09-06,2,8,3,2,1,1,ee7b871213e1deafadea4b7752467b5c5ab1b9fb,261582750.0,https://www.semanticscholar.org/paper/ee7b871213e1deafadea4b7752467b5c5ab1b9fb,arXiv.org,2023.0,45.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Mathematics', 'source': 's2-fos-model'}]","[{'authorId': '102849462', 'name': 'Z. Yang'}, {'authorId': '145573466', 'name': 'Ming Ding'}, {'authorId': '2053104889', 'name': 'Qingsong Lv'}, {'authorId': '2238395560', 'name': 'Zhihuan Jiang'}, {'authorId': '2238398774', 'name': 'Zehai He'}, {'authorId': '2238348261', 'name': 'Yuyi Guo'}, {'authorId': '2238396257', 'name': 'Jinfeng Bai'}, {'authorId': '2238207092', 'name': 'Jie Tang'}]",['Tsinghua University'],['China'],2023-09
2309.03852,Yequan Wang,"Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng, Siqi Fan,
  Peng Han, Jing Li, Li Du, Bowen Qin, Zheng Zhang, Aixin Sun, Yequan Wang",FLM-101B: An Open LLM and How to Train It with $100K Budget,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have achieved remarkable success in NLP and multimodal tasks, among others. Despite these successes, two main challenges remain in developing LLMs: (i) high computational cost, and (ii) fair and objective evaluations. In this paper, we report a solution to significantly reduce LLM training cost through a growth strategy. We demonstrate that a 101B-parameter LLM with 0.31T tokens can be trained with a budget of 100K US dollars. Inspired by IQ tests, we also consolidate an additional range of evaluations on top of existing evaluations that focus on knowledge-oriented abilities. These IQ evaluations include symbolic mapping, rule understanding, pattern mining, and anti-interference. Such evaluations minimize the potential impact of memorization. Experimental results show that our model, named FLM-101B, trained with a budget of 100K US dollars, achieves performance comparable to powerful and well-known models, e.g., GPT-3 and GLM-130B, especially on the additional range of IQ evaluations. The checkpoint of FLM-101B is released at https://huggingface.co/CofeAI/FLM-101B. ","[{'version': 'v1', 'created': 'Thu, 7 Sep 2023 17:07:36 GMT'}, {'version': 'v2', 'created': 'Sun, 17 Sep 2023 07:38:10 GMT'}]",2023-09-19,"[['Li', 'Xiang', ''], ['Yao', 'Yiqun', ''], ['Jiang', 'Xin', ''], ['Fang', 'Xuezhi', ''], ['Meng', 'Xuying', ''], ['Fan', 'Siqi', ''], ['Han', 'Peng', ''], ['Li', 'Jing', ''], ['Du', 'Li', ''], ['Qin', 'Bowen', ''], ['Zhang', 'Zheng', ''], ['Sun', 'Aixin', ''], ['Wang', 'Yequan', '']]",0,1,2023-09-07,2,13,2,2,1,1,f8afa4bd5b05f52ffa304f56aed7a5792a42ef1f,261582615.0,https://www.semanticscholar.org/paper/f8afa4bd5b05f52ffa304f56aed7a5792a42ef1f,arXiv.org,2023.0,86.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '48868160', 'name': 'Xiang Li'}, {'authorId': '2238398612', 'name': 'Yiqun Yao'}, {'authorId': '145820291', 'name': 'Xin Jiang'}, {'authorId': '2238406105', 'name': 'Xuezhi Fang'}, {'authorId': '3134123', 'name': 'Xuying Meng'}, {'authorId': '2238292255', 'name': 'Siqi Fan'}, {'authorId': '144433445', 'name': 'Peng Han'}, {'authorId': '2256795790', 'name': 'Jing Li'}, {'authorId': '2238210094', 'name': 'Li Du'}, {'authorId': '2238208493', 'name': 'Bowen Qin'}, {'authorId': '143644345', 'name': 'Zheng Zhang'}, {'authorId': '1735962', 'name': 'Aixin Sun'}, {'authorId': '6285226', 'name': 'Yequan Wang'}]","['University of Electronic Science and Technology of China', 'Beijing Academy of Artificial Intelligence', 'Indicates equal contribution.', 'Chinese Academy of Sciences', 'Harbin Institute of Technology', 'Nanyang Technological University']","['China', 'Singapore']",2023-09
2309.03905,Renrui Zhang,"Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao,
  Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, Xudong Lu, Shuai Ren, Yafei
  Wen, Xiaoxin Chen, Xiangyu Yue, Hongsheng Li, Yu Qiao",ImageBind-LLM: Multi-modality Instruction Tuning,Code is available at https://github.com/OpenGVLab/LLaMA-Adapter,,,,cs.MM cs.CL cs.CV cs.LG cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present ImageBind-LLM, a multi-modality instruction tuning method of large language models (LLMs) via ImageBind. Existing works mainly focus on language and image instruction tuning, different from which, our ImageBind-LLM can respond to multi-modality conditions, including audio, 3D point clouds, video, and their embedding-space arithmetic by only image-text alignment training. During training, we adopt a learnable bind network to align the embedding space between LLaMA and ImageBind's image encoder. Then, the image features transformed by the bind network are added to word tokens of all layers in LLaMA, which progressively injects visual instructions via an attention-free and zero-initialized gating mechanism. Aided by the joint embedding of ImageBind, the simple image-text training enables our model to exhibit superior multi-modality instruction-following capabilities. During inference, the multi-modality inputs are fed into the corresponding ImageBind encoders, and processed by a proposed visual cache model for further cross-modal embedding enhancement. The training-free cache model retrieves from three million image features extracted by ImageBind, which effectively mitigates the training-inference modality discrepancy. Notably, with our approach, ImageBind-LLM can respond to instructions of diverse modalities and demonstrate significant language generation quality. Code is released at https://github.com/OpenGVLab/LLaMA-Adapter. ","[{'version': 'v1', 'created': 'Thu, 7 Sep 2023 17:59:45 GMT'}, {'version': 'v2', 'created': 'Mon, 11 Sep 2023 20:25:16 GMT'}]",2023-09-13,"[['Han', 'Jiaming', ''], ['Zhang', 'Renrui', ''], ['Shao', 'Wenqi', ''], ['Gao', 'Peng', ''], ['Xu', 'Peng', ''], ['Xiao', 'Han', ''], ['Zhang', 'Kaipeng', ''], ['Liu', 'Chris', ''], ['Wen', 'Song', ''], ['Guo', 'Ziyu', ''], ['Lu', 'Xudong', ''], ['Ren', 'Shuai', ''], ['Wen', 'Yafei', ''], ['Chen', 'Xiaoxin', ''], ['Yue', 'Xiangyu', ''], ['Li', 'Hongsheng', ''], ['Qiao', 'Yu', '']]",0,0,2023-09-07,2,17,6,1,1,0,54c68b8623505dc6bf7a0b08aaa77ca9165f2d7f,261582620.0,https://www.semanticscholar.org/paper/54c68b8623505dc6bf7a0b08aaa77ca9165f2d7f,arXiv.org,2023.0,80.0,8.0,3.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '150147382', 'name': 'Jiaming Han'}, {'authorId': '2115713503', 'name': 'Renrui Zhang'}, {'authorId': '1485702259', 'name': 'Wenqi Shao'}, {'authorId': '144740494', 'name': 'Peng Gao'}, {'authorId': '2238406311', 'name': 'Peng Xu'}, {'authorId': '2238398546', 'name': 'Han Xiao'}, {'authorId': '3393556', 'name': 'Kaipeng Zhang'}, {'authorId': '2238219059', 'name': 'Chris Liu'}, {'authorId': '2238210433', 'name': 'Song Wen'}, {'authorId': '2237599228', 'name': 'Ziyu Guo'}, {'authorId': '2238395310', 'name': 'Xudong Lu'}, {'authorId': '2204576896', 'name': 'Shuai Ren'}, {'authorId': '2238363251', 'name': 'Yafei Wen'}, {'authorId': '2238219937', 'name': 'Xiaoxin Chen'}, {'authorId': '27577617', 'name': 'Xiangyu Yue'}, {'authorId': '49404547', 'name': 'Hongsheng Li'}, {'authorId': '2059129841', 'name': 'Y. Qiao'}]","['vivo AI Lab, Shenzhen, 518000, China.', 'Shanghai Artificial Intelligence Laboratory', 'Chinese University of Hong Kong']",['China'],2023-09
2309.04158,Hongyu Hu,"Hongyu Hu, Tiancheng Lin, Jie Wang, Zhenbang Sun, Yi Xu",Context-Aware Prompt Tuning for Vision-Language Model with Dual-Alignment,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large-scale vision-language models (VLMs), e.g., CLIP, learn broad visual concepts from tedious training data, showing superb generalization ability. Amount of prompt learning methods have been proposed to efficiently adapt the VLMs to downstream tasks with only a few training samples. We introduce a novel method to improve the prompt learning of vision-language models by incorporating pre-trained large language models (LLMs), called Dual-Aligned Prompt Tuning (DuAl-PT). Learnable prompts, like CoOp, implicitly model the context through end-to-end training, which are difficult to control and interpret. While explicit context descriptions generated by LLMs, like GPT-3, can be directly used for zero-shot classification, such prompts are overly relying on LLMs and still underexplored in few-shot domains. With DuAl-PT, we propose to learn more context-aware prompts, benefiting from both explicit and implicit context modeling. To achieve this, we introduce a pre-trained LLM to generate context descriptions, and we encourage the prompts to learn from the LLM's knowledge by alignment, as well as the alignment between prompts and local image features. Empirically, DuAl-PT achieves superior performance on 11 downstream datasets on few-shot recognition and base-to-new generalization. Hopefully, DuAl-PT can serve as a strong baseline. Code will be available. ","[{'version': 'v1', 'created': 'Fri, 8 Sep 2023 06:51:15 GMT'}]",2023-09-11,"[['Hu', 'Hongyu', ''], ['Lin', 'Tiancheng', ''], ['Wang', 'Jie', ''], ['Sun', 'Zhenbang', ''], ['Xu', 'Yi', '']]",0,1,2023-09-08,1,5,1,1,0,1,3b2bd913a80a2013cd90ee0768be57bdf717006e,261660574.0,https://www.semanticscholar.org/paper/3b2bd913a80a2013cd90ee0768be57bdf717006e,arXiv.org,2023.0,34.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2238158720', 'name': 'Hongyu Hu'}, {'authorId': '2115348695', 'name': 'Tiancheng Lin'}, {'authorId': '2238809194', 'name': 'Jie Wang'}, {'authorId': '2238143153', 'name': 'Zhenbang Sun'}, {'authorId': '2238945205', 'name': 'Yi Xu'}]","['ByteDance', 'Shanghai Jiao Tong University']",['China'],2023-09
2309.04174,Haochun Wang,"Haochun Wang, Sendong Zhao, Chi Liu, Nuwa Xi, Muzhen Cai, Bing Qin,
  Ting Liu",Manifold-based Verbalizer Space Re-embedding for Tuning-free Prompt-based Classification,"11 pages, 3 figures",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Prompt-based classification adapts tasks to a cloze question format utilizing the [MASK] token and the filled tokens are then mapped to labels through pre-defined verbalizers. Recent studies have explored the use of verbalizer embeddings to reduce labor in this process. However, all existing studies require a tuning process for either the pre-trained models or additional trainable embeddings. Meanwhile, the distance between high-dimensional verbalizer embeddings should not be measured by Euclidean distance due to the potential for non-linear manifolds in the representation space. In this study, we propose a tuning-free manifold-based space re-embedding method called Locally Linear Embedding with Intra-class Neighborhood Constraint (LLE-INC) for verbalizer embeddings, which preserves local properties within the same class as guidance for classification. Experimental results indicate that even without tuning any parameters, our LLE-INC is on par with automated verbalizers with parameter tuning. And with the parameter updating, our approach further enhances prompt-based tuning by up to 3.2%. Furthermore, experiments with the LLaMA-7B&13B indicate that LLE-INC is an efficient tuning-free classification approach for the hyper-scale language models. ","[{'version': 'v1', 'created': 'Fri, 8 Sep 2023 07:42:29 GMT'}]",2023-09-11,"[['Wang', 'Haochun', ''], ['Zhao', 'Sendong', ''], ['Liu', 'Chi', ''], ['Xi', 'Nuwa', ''], ['Cai', 'Muzhen', ''], ['Qin', 'Bing', ''], ['Liu', 'Ting', '']]",0,0,2023-09-08,1,7,2,1,1,0,2fd79f43164e215cd1de881b8ad151d9417fec61,261660677.0,https://www.semanticscholar.org/paper/2fd79f43164e215cd1de881b8ad151d9417fec61,arXiv.org,2023.0,42.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2164011959', 'name': 'Hao Wang'}, {'authorId': '40326824', 'name': 'Sendong Zhao'}, {'authorId': '117660020', 'name': 'Chi-Liang Liu'}, {'authorId': '2184773728', 'name': 'Nuwa Xi'}, {'authorId': '2238639086', 'name': 'Muzhen Cai'}, {'authorId': '2166387643', 'name': 'Bing Qin'}, {'authorId': '2238862997', 'name': 'Ting Liu'}]",['Harbin Institute of Technology'],['China'],2023-09
2309.04175,Haochun Wang,"Haochun Wang, Sendong Zhao, Zewen Qiang, Zijian Li, Nuwa Xi, Yanrui
  Du, MuZhen Cai, Haoqiang Guo, Yuhan Chen, Haoming Xu, Bing Qin, Ting Liu",Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese,"11 pages, 5 figures",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have demonstrated remarkable success in diverse natural language processing (NLP) tasks in general domains. However, LLMs sometimes generate responses with the hallucination about medical facts due to limited domain knowledge. Such shortcomings pose potential risks in the utilization of LLMs within medical contexts. To address this challenge, we propose knowledge-tuning, which leverages structured medical knowledge bases for the LLMs to grasp domain knowledge efficiently and facilitate reliable response generation. We also release cMedKnowQA, a Chinese medical knowledge question-answering dataset constructed from medical knowledge bases to assess the medical knowledge proficiency of LLMs. Experimental results show that the LLMs which are knowledge-tuned with cMedKnowQA, can exhibit higher levels of accuracy in response generation compared with vanilla instruction-tuning and offer a new reliable way for the domain adaptation of LLMs. ","[{'version': 'v1', 'created': 'Fri, 8 Sep 2023 07:42:57 GMT'}]",2023-09-11,"[['Wang', 'Haochun', ''], ['Zhao', 'Sendong', ''], ['Qiang', 'Zewen', ''], ['Li', 'Zijian', ''], ['Xi', 'Nuwa', ''], ['Du', 'Yanrui', ''], ['Cai', 'MuZhen', ''], ['Guo', 'Haoqiang', ''], ['Chen', 'Yuhan', ''], ['Xu', 'Haoming', ''], ['Qin', 'Bing', ''], ['Liu', 'Ting', '']]",0,0,2023-09-08,1,12,2,0,0,0,1fb2bde5c2f3a3c4d7b810b29ec3f21f60e75d35,261660541.0,https://www.semanticscholar.org/paper/1fb2bde5c2f3a3c4d7b810b29ec3f21f60e75d35,arXiv.org,2023.0,41.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2164011959', 'name': 'Hao Wang'}, {'authorId': '40326824', 'name': 'Sendong Zhao'}, {'authorId': '2214541370', 'name': 'Zewen Qiang'}, {'authorId': '2239036328', 'name': 'Zijian Li'}, {'authorId': '2184773728', 'name': 'Nuwa Xi'}, {'authorId': '2166447772', 'name': 'Yanrui Du'}, {'authorId': '2238639086', 'name': 'Muzhen Cai'}, {'authorId': '2238907077', 'name': 'Haoqiang Guo'}, {'authorId': '2238896941', 'name': 'Yuhan Chen'}, {'authorId': '2238996555', 'name': 'Haoming Xu'}, {'authorId': '2166387643', 'name': 'Bing Qin'}, {'authorId': '2238862997', 'name': 'Ting Liu'}]",['Harbin Institute of Technology'],['China'],2023-09
2309.04198,Du Yanrui,"Yanrui Du, Sendong Zhao, Muzhen Cai, Jianyu Chen, Haochun Wang, Yuhan
  Chen, Haoqiang Guo, Bing Qin",The CALLA Dataset: Probing LLMs' Interactive Knowledge Acquisition from Chinese Medical Literature,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The application of Large Language Models (LLMs) to the medical domain has stimulated the interest of researchers. Recent studies have focused on constructing Instruction Fine-Tuning (IFT) data through medical knowledge graphs to enrich the interactive medical knowledge of LLMs. However, the medical literature serving as a rich source of medical knowledge remains unexplored. Our work introduces the CALLA dataset to probe LLMs' interactive knowledge acquisition from Chinese medical literature. It assesses the proficiency of LLMs in mastering medical knowledge through a free-dialogue fact-checking task. We identify a phenomenon called the ``fact-following response``, where LLMs tend to affirm facts mentioned in questions and display a reluctance to challenge them. To eliminate the inaccurate evaluation caused by this phenomenon, for the golden fact, we artificially construct test data from two perspectives: one consistent with the fact and one inconsistent with the fact. Drawing from the probing experiment on the CALLA dataset, we conclude that IFT data highly correlated with the medical literature corpus serves as a potent catalyst for LLMs, enabling themselves to skillfully employ the medical knowledge acquired during the pre-training phase within interactive scenarios, enhancing accuracy. Furthermore, we design a framework for automatically constructing IFT data based on medical literature and discuss some real-world applications. ","[{'version': 'v1', 'created': 'Fri, 8 Sep 2023 08:20:46 GMT'}, {'version': 'v2', 'created': 'Tue, 12 Sep 2023 13:51:14 GMT'}]",2023-09-13,"[['Du', 'Yanrui', ''], ['Zhao', 'Sendong', ''], ['Cai', 'Muzhen', ''], ['Chen', 'Jianyu', ''], ['Wang', 'Haochun', ''], ['Chen', 'Yuhan', ''], ['Guo', 'Haoqiang', ''], ['Qin', 'Bing', '']]",0,0,2023-09-08,2,8,1,0,0,0,b094bcc6df78fdfa0a428e1bcc00c7a7688b6e80,261660693.0,https://www.semanticscholar.org/paper/b094bcc6df78fdfa0a428e1bcc00c7a7688b6e80,arXiv.org,2023.0,25.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2166447772', 'name': 'Yanrui Du'}, {'authorId': '1571183602', 'name': 'Sendong Zhao'}, {'authorId': '2238896941', 'name': 'Yuhan Chen'}, {'authorId': '2238622434', 'name': 'Rai Bai'}, {'authorId': '46700619', 'name': 'Jing Liu'}, {'authorId': '46477059', 'name': 'Huaqin Wu'}, {'authorId': '2238917361', 'name': 'Haifeng Wang'}, {'authorId': '2166387643', 'name': 'Bing Qin'}]",['Harbin Institute of Technology'],['China'],2023-09
2309.04255,Daliang Xu,"Daliang Xu, Wangsong Yin, Xin Jin, Ying Zhang, Shiyun Wei, Mengwei Xu,
  and Xuanzhe Liu",LLMCad: Fast and Scalable On-device Large Language Model Inference,,,,,cs.NI cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Generative tasks, such as text generation and question answering, hold a crucial position in the realm of mobile applications. Due to their sensitivity to privacy concerns, there is a growing demand for their execution directly on mobile devices. Currently, the execution of these generative tasks heavily depends on Large Language Models (LLMs). Nevertheless, the limited memory capacity of these devices presents a formidable challenge to the scalability of such models.   In our research, we introduce LLMCad, an innovative on-device inference engine specifically designed for efficient generative Natural Language Processing (NLP) tasks. The core idea behind LLMCad revolves around model collaboration: a compact LLM, residing in memory, takes charge of generating the most straightforward tokens, while a high-precision LLM steps in to validate these tokens and rectify any identified errors. LLMCad incorporates three novel techniques: (1) Instead of generating candidate tokens in a sequential manner, LLMCad employs the smaller LLM to construct a token tree, encompassing a wider range of plausible token pathways. Subsequently, the larger LLM can efficiently validate all of these pathways simultaneously. (2) It employs a self-adjusting fallback strategy, swiftly initiating the verification process whenever the smaller LLM generates an erroneous token. (3) To ensure a continuous flow of token generation, LLMCad speculatively generates tokens during the verification process by implementing a compute-IO pipeline. Through an extensive series of experiments, LLMCad showcases an impressive token generation speed, achieving rates up to 9.3x faster than existing inference engines. ","[{'version': 'v1', 'created': 'Fri, 8 Sep 2023 10:44:19 GMT'}]",2023-09-11,"[['Xu', 'Daliang', ''], ['Yin', 'Wangsong', ''], ['Jin', 'Xin', ''], ['Zhang', 'Ying', ''], ['Wei', 'Shiyun', ''], ['Xu', 'Mengwei', ''], ['Liu', 'Xuanzhe', '']]",0,0,2023-09-08,1,7,2,0,0,0,00e889fcfaf4396a20f37f681cf8b14f3e878879,261660737.0,https://www.semanticscholar.org/paper/00e889fcfaf4396a20f37f681cf8b14f3e878879,arXiv.org,2023.0,76.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2238890503', 'name': 'Daliang Xu'}, {'authorId': '2238575108', 'name': 'Wangsong Yin'}, {'authorId': '2239060901', 'name': 'Xin Jin'}, {'authorId': '48379623', 'name': 'Y. Zhang'}, {'authorId': '2238910539', 'name': 'Shiyun Wei'}, {'authorId': '2529558', 'name': 'Mengwei Xu'}, {'authorId': '2237080638', 'name': 'Xuanzhe Liu'}]","['Peking University', 'Zhongguancun Laboratory, Beijing, China.', 'State Key Laboratory of Networking and Switching Technology (BUPT), Beijing, China']",['China'],2023-09
2309.04295,Chengwu Liu,"Chengwu Liu, Jianhao Shen, Huajian Xin, Zhengying Liu, Ye Yuan,
  Haiming Wang, Wei Ju, Chuanyang Zheng, Yichun Yin, Lin Li, Ming Zhang, Qun
  Liu",FIMO: A Challenge Formal Dataset for Automated Theorem Proving,,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present FIMO, an innovative dataset comprising formal mathematical problem statements sourced from the International Mathematical Olympiad (IMO) Shortlisted Problems. Designed to facilitate advanced automated theorem proving at the IMO level, FIMO is currently tailored for the Lean formal language. It comprises 149 formal problem statements, accompanied by both informal problem descriptions and their corresponding LaTeX-based informal proofs. Through initial experiments involving GPT-4, our findings underscore the existing limitations in current methodologies, indicating a substantial journey ahead before achieving satisfactory IMO-level automated theorem proving outcomes. ","[{'version': 'v1', 'created': 'Fri, 8 Sep 2023 12:34:28 GMT'}]",2023-09-11,"[['Liu', 'Chengwu', ''], ['Shen', 'Jianhao', ''], ['Xin', 'Huajian', ''], ['Liu', 'Zhengying', ''], ['Yuan', 'Ye', ''], ['Wang', 'Haiming', ''], ['Ju', 'Wei', ''], ['Zheng', 'Chuanyang', ''], ['Yin', 'Yichun', ''], ['Li', 'Lin', ''], ['Zhang', 'Ming', ''], ['Liu', 'Qun', '']]",0,1,2023-09-08,1,12,1,1,0,1,217ea09e54a80043ec06d7f5ec1e7f2f7eef5b43,261660403.0,https://www.semanticscholar.org/paper/217ea09e54a80043ec06d7f5ec1e7f2f7eef5b43,arXiv.org,2023.0,16.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2238605593', 'name': 'Chengwu Liu'}, {'authorId': '2115733983', 'name': 'Jianhao Shen'}, {'authorId': '2238628841', 'name': 'Huajian Xin'}, {'authorId': '2239065052', 'name': 'Zhengying Liu'}, {'authorId': '2221302624', 'name': 'Ye Yuan'}, {'authorId': '2109589929', 'name': 'Haiming Wang'}, {'authorId': '2154453854', 'name': 'Wei Ju'}, {'authorId': '2238892973', 'name': 'Chuanyang Zheng'}, {'authorId': '1384668226', 'name': 'Yichun Yin'}, {'authorId': '2155689998', 'name': 'Lin Li'}, {'authorId': '2145178175', 'name': 'Ming Zhang'}, {'authorId': '2238911873', 'name': 'Qun Liu'}]","['Peking University', 'Sun Yat-sen University', 'Chinese University of Hong Kong', 'Huawei Technologies (China)']",['China'],2023-09
2309.04369,Jiatong Li,"Jiatong Li, Rui Li, Qi Liu",Beyond Static Datasets: A Deep Interaction Approach to LLM Evaluation,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have made progress in various real-world tasks, which stimulates requirements for the evaluation of LLMs. Existing LLM evaluation methods are mainly supervised signal-based which depends on static datasets and cannot evaluate the ability of LLMs in dynamic real-world scenarios where deep interaction widely exists. Other LLM evaluation methods are human-based which are costly and time-consuming and are incapable of large-scale evaluation of LLMs. To address the issues above, we propose a novel Deep Interaction-based LLM-evaluation framework. In our proposed framework, LLMs' performances in real-world domains can be evaluated from their deep interaction with other LLMs in elaborately designed evaluation tasks. Furthermore, our proposed framework is a general evaluation method that can be applied to a host of real-world tasks such as machine translation and code generation. We demonstrate the effectiveness of our proposed method through extensive experiments on four elaborately designed evaluation tasks. ","[{'version': 'v1', 'created': 'Fri, 8 Sep 2023 15:00:41 GMT'}]",2023-09-11,"[['Li', 'Jiatong', ''], ['Li', 'Rui', ''], ['Liu', 'Qi', '']]",0,0,2023-09-08,1,3,2,0,0,0,627bd1dbbf8c237c9819c0c593091aadd604f8c0,261660216.0,https://www.semanticscholar.org/paper/627bd1dbbf8c237c9819c0c593091aadd604f8c0,arXiv.org,2023.0,35.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2237588755', 'name': 'Jiatong Li'}, {'authorId': '2238695374', 'name': 'Rui Li'}, {'authorId': '2238907998', 'name': 'Qi Liu'}]","['University of Science and Technology of China', 'Linkping University']","['China', 'Sweden']",2023-09
2309.04372,Sijia Li,"Sijia Li, Chen Chen, Haonan Lu",MoEController: Instruction-based Arbitrary Image Manipulation with Mixture-of-Expert Controllers,"5 pages,6 figures",,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Diffusion-model-based text-guided image generation has recently made astounding progress, producing fascinating results in open-domain image manipulation tasks. Few models, however, currently have complete zero-shot capabilities for both global and local image editing due to the complexity and diversity of image manipulation tasks. In this work, we propose a method with a mixture-of-expert (MOE) controllers to align the text-guided capacity of diffusion models with different kinds of human instructions, enabling our model to handle various open-domain image manipulation tasks with natural language instructions. First, we use large language models (ChatGPT) and conditional image synthesis models (ControlNet) to generate a large number of global image transfer dataset in addition to the instruction-based local image editing dataset. Then, using an MOE technique and task-specific adaptation training on a large-scale dataset, our conditional diffusion model can edit images globally and locally. Extensive experiments demonstrate that our approach performs surprisingly well on various image manipulation tasks when dealing with open-domain images and arbitrary human instructions. Please refer to our project page: [https://oppo-mente-lab.github.io/moe_controller/] ","[{'version': 'v1', 'created': 'Fri, 8 Sep 2023 15:06:05 GMT'}]",2023-09-11,"[['Li', 'Sijia', ''], ['Chen', 'Chen', ''], ['Lu', 'Haonan', '']]",1,1,2023-09-08,1,3,2,1,0,1,6deb76b8ac8d35dded7dd76d768a245bb69ffe91,261660692.0,https://www.semanticscholar.org/paper/6deb76b8ac8d35dded7dd76d768a245bb69ffe91,arXiv.org,2023.0,26.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2238948430', 'name': 'Sijia Li'}, {'authorId': '2141809694', 'name': 'Chen Chen'}, {'authorId': '2130373', 'name': 'H. Lu'}]",['OPPO'],['China'],2023-09
2309.04565,Lanning Wei,"Lanning Wei, Zhiqiang He, Huan Zhao, Quanming Yao",Unleashing the Power of Graph Learning through LLM-based Autonomous Agents,,,,,cs.LG cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Graph structured data are widely existed and applied in the real-world applications, while it is a challenge to handling these diverse data and learning tasks on graph in an efficient manner. When facing the complicated graph learning tasks, experts have designed diverse Graph Neural Networks (GNNs) in recent years. They have also implemented AutoML in Graph, also known as AutoGraph, to automatically generate data-specific solutions. Despite their success, they encounter limitations in (1) managing diverse learning tasks at various levels, (2) dealing with different procedures in graph learning beyond architecture design, and (3) the huge requirements on the prior knowledge when using AutoGraph. In this paper, we propose to use Large Language Models (LLMs) as autonomous agents to simplify the learning process on diverse real-world graphs. Specifically, in response to a user request which may contain varying data and learning targets at the node, edge, or graph levels, the complex graph learning task is decomposed into three components following the agent planning, namely, detecting the learning intent, configuring solutions based on AutoGraph, and generating a response. The AutoGraph agents manage crucial procedures in automated graph learning, including data-processing, AutoML configuration, searching architectures, and hyper-parameter fine-tuning. With these agents, those components are processed by decomposing and completing step by step, thereby generating a solution for the given data automatically, regardless of the learning task on node or graph. The proposed method is dubbed Auto$^2$Graph, and the comparable performance on different datasets and learning tasks. Its effectiveness is demonstrated by its comparable performance on different datasets and learning tasks, as well as the human-like decisions made by the agents. ","[{'version': 'v1', 'created': 'Fri, 8 Sep 2023 19:34:29 GMT'}]",2023-09-12,"[['Wei', 'Lanning', ''], ['He', 'Zhiqiang', ''], ['Zhao', 'Huan', ''], ['Yao', 'Quanming', '']]",0,0,2023-09-08,1,4,2,0,0,0,443089da371f96f9d96d34f3f99179f3b251b050,261682230.0,https://www.semanticscholar.org/paper/443089da371f96f9d96d34f3f99179f3b251b050,arXiv.org,2023.0,66.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2110974142', 'name': 'Lanning Wei'}, {'authorId': '2152629091', 'name': 'Zhiqiang He'}, {'authorId': '46430770', 'name': 'Huan Zhao'}, {'authorId': '3259992', 'name': 'Quanming Yao'}]","['University of Chinese Academy of Sciences', 'Lenovo (China)', 'Tsinghua University', 'Institute of Computing Technology', '4Paradigm. Inc.,']",['China'],2023-09
2309.04658,Yuzhuang Xu,"Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong
  Liu, Yang Liu",Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf,"23 pages, 5 figures and 4 tables",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Communication games, which we refer to as incomplete information games that heavily depend on natural language communication, hold significant research value in fields such as economics, social science, and artificial intelligence. In this work, we explore the problem of how to engage large language models (LLMs) in communication games, and in response, propose a tuning-free framework. Our approach keeps LLMs frozen, and relies on the retrieval and reflection on past communications and experiences for improvement. An empirical study on the representative and widely-studied communication game, ``Werewolf'', demonstrates that our framework can effectively play Werewolf game without tuning the parameters of the LLMs. More importantly, strategic behaviors begin to emerge in our experiments, suggesting that it will be a fruitful journey to engage LLMs in communication games and associated domains. ","[{'version': 'v1', 'created': 'Sat, 9 Sep 2023 01:56:40 GMT'}]",2023-09-12,"[['Xu', 'Yuzhuang', ''], ['Wang', 'Shuo', ''], ['Li', 'Peng', ''], ['Luo', 'Fuwen', ''], ['Wang', 'Xiaolong', ''], ['Liu', 'Weidong', ''], ['Liu', 'Yang', '']]",0,0,2023-09-09,1,7,1,0,0,0,24d52678c887331b9da0368e8a2f58bec07f7203,261681932.0,https://www.semanticscholar.org/paper/24d52678c887331b9da0368e8a2f58bec07f7203,arXiv.org,2023.0,49.0,13.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2239055610', 'name': 'Yuzhuang Xu'}, {'authorId': '12782489', 'name': 'Shuo Wang'}, {'authorId': '2239330639', 'name': 'Peng Li'}, {'authorId': '2238952295', 'name': 'Fuwen Luo'}, {'authorId': '1709719', 'name': 'Xiaolong Wang'}, {'authorId': '2109585836', 'name': 'Weidong Liu'}, {'authorId': '40457423', 'name': 'Yang Liu'}]","['Tsinghua University', 'Zhongguancun Laboratory, Beijing, China']",['China'],2023-09
2309.04669,Yang Jin,"Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Quzhe
  Huang, Bin Chen, Chenyi Lei, An Liu, Chengru Song, Xiaoqiang Lei, Di Zhang,
  Wenwu Ou, Kun Gai, Yadong Mu",Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, the remarkable advance of the Large Language Model (LLM) has inspired researchers to transfer its extraordinary reasoning capability to both vision and language data. However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen LLM. Such an inequitable treatment of vision and language heavily constrains the model's potential. In this paper, we break through this limitation by representing both vision and language in a unified form. Specifically, we introduce a well-designed visual tokenizer to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read. The resulting visual tokens encompass high-level semantics worthy of a word and also support dynamic sequence length varying from the image. Coped with this tokenizer, the presented foundation model called LaVIT can handle both image and text indiscriminately under the same generative learning paradigm. This unification empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously. Extensive experiments further showcase that it outperforms the existing models by a large margin on massive vision-language tasks. Our code and models will be available at https://github.com/jy0205/LaVIT. ","[{'version': 'v1', 'created': 'Sat, 9 Sep 2023 03:01:38 GMT'}, {'version': 'v2', 'created': 'Fri, 29 Sep 2023 16:41:15 GMT'}]",2023-10-02,"[['Jin', 'Yang', ''], ['Xu', 'Kun', ''], ['Xu', 'Kun', ''], ['Chen', 'Liwei', ''], ['Liao', 'Chao', ''], ['Tan', 'Jianchao', ''], ['Huang', 'Quzhe', ''], ['Chen', 'Bin', ''], ['Lei', 'Chenyi', ''], ['Liu', 'An', ''], ['Song', 'Chengru', ''], ['Lei', 'Xiaoqiang', ''], ['Zhang', 'Di', ''], ['Ou', 'Wenwu', ''], ['Gai', 'Kun', ''], ['Mu', 'Yadong', '']]",0,0,2023-09-09,2,16,1,0,0,0,6862ab6977cb9be9af942bdb132fda2245414eeb,263889455.0,https://www.semanticscholar.org/paper/6862ab6977cb9be9af942bdb132fda2245414eeb,arXiv.org,2023.0,58.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2239056878', 'name': 'Yang Jin'}, {'authorId': '2266735757', 'name': 'Kun Xu'}, {'authorId': '2266419021', 'name': 'Liwei Chen'}, {'authorId': '2239059653', 'name': 'Chao Liao'}, {'authorId': '2239091862', 'name': 'Jianchao Tan'}, {'authorId': '2258794930', 'name': 'Quzhe Huang'}, {'authorId': '2230906921', 'name': 'Bin Chen'}, {'authorId': '2238954318', 'name': 'Chenyi Lei'}, {'authorId': '2239069665', 'name': 'An Liu'}, {'authorId': '2241686105', 'name': 'Chengru Song'}, {'authorId': '2238955477', 'name': 'Xiaoqiang Lei'}, {'authorId': '2228125963', 'name': 'Di Zhang'}, {'authorId': '2238953778', 'name': 'Wenwu Ou'}, {'authorId': '2238953242', 'name': 'Kun Gai'}, {'authorId': '2238953689', 'name': 'Yadong Mu'}]","['Peking University', 'Kuaishou Technology']",['China'],2023-09
2309.04695,Zhongyuan Wang,"Zhijie Nie, Richong Zhang, Zhongyuan Wang, Xudong Liu",Code-Style In-Context Learning for Knowledge-Based Question Answering,work in progress,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Current methods for Knowledge-Based Question Answering (KBQA) usually rely on complex training techniques and model frameworks, leading to many limitations in practical applications. Recently, the emergence of In-Context Learning (ICL) capabilities in Large Language Models (LLMs) provides a simple and training-free semantic parsing paradigm for KBQA: Given a small number of questions and their labeled logical forms as demo examples, LLMs can understand the task intent and generate the logic form for a new question. However, current powerful LLMs have little exposure to logic forms during pre-training, resulting in a high format error rate. To solve this problem, we propose a code-style in-context learning method for KBQA, which converts the generation process of unfamiliar logical form into the more familiar code generation process for LLMs. Experimental results on three mainstream datasets show that our method dramatically mitigated the formatting error problem in generating logic forms while realizing a new SOTA on WebQSP, GrailQA, and GraphQ under the few-shot setting. ","[{'version': 'v1', 'created': 'Sat, 9 Sep 2023 06:27:00 GMT'}]",2023-09-12,"[['Nie', 'Zhijie', ''], ['Zhang', 'Richong', ''], ['Wang', 'Zhongyuan', ''], ['Liu', 'Xudong', '']]",0,0,2023-09-09,1,4,2,0,0,0,36ad72488276cdf4d308cdd433a293445d2b3913,261682610.0,https://www.semanticscholar.org/paper/36ad72488276cdf4d308cdd433a293445d2b3913,arXiv.org,2023.0,33.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2220895313', 'name': 'Zhijie Nie'}, {'authorId': '1747018', 'name': 'Richong Zhang'}, {'authorId': '2239130409', 'name': 'Zhongyuan Wang'}, {'authorId': '2239059349', 'name': 'Xudong Liu'}]","['Beihang University', 'Zhongguancun Laboratory, Beijing, China']",['China'],2023-09
2309.04716,Qiao Xiang,"Qiao Xiang, Yuling Lin, Mingjun Fang, Bang Huang, Siyong Huang, Ridi
  Wen, Franck Le, Linghe Kong, Jiwu Shu",Toward Reproducing Network Research Results Using Large Language Models,,,,,cs.LG cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Reproducing research results in the networking community is important for both academia and industry. The current best practice typically resorts to three approaches: (1) looking for publicly available prototypes; (2) contacting the authors to get a private prototype; and (3) manually implementing a prototype following the description of the publication. However, most published network research does not have public prototypes and private prototypes are hard to get. As such, most reproducing efforts are spent on manual implementation based on the publications, which is both time and labor consuming and error-prone. In this paper, we boldly propose reproducing network research results using the emerging large language models (LLMs). In particular, we first prove its feasibility with a small-scale experiment, in which four students with essential networking knowledge each reproduces a different networking system published in prominent conferences and journals by prompt engineering ChatGPT. We report the experiment's observations and lessons and discuss future open research questions of this proposal. This work raises no ethical issue. ","[{'version': 'v1', 'created': 'Sat, 9 Sep 2023 08:07:54 GMT'}]",2023-09-12,"[['Xiang', 'Qiao', ''], ['Lin', 'Yuling', ''], ['Fang', 'Mingjun', ''], ['Huang', 'Bang', ''], ['Huang', 'Siyong', ''], ['Wen', 'Ridi', ''], ['Le', 'Franck', ''], ['Kong', 'Linghe', ''], ['Shu', 'Jiwu', '']]",1,1,2023-09-09,1,9,3,1,0,1,279c798fd53c8dc84044273d08b6a060dbe9f702,261681766.0,https://www.semanticscholar.org/paper/279c798fd53c8dc84044273d08b6a060dbe9f702,ACM Workshop on Hot Topics in Networks,2023.0,38.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2238950148', 'name': 'Qiao Xiang'}, {'authorId': '2225781874', 'name': 'Yuling Lin'}, {'authorId': '2238950694', 'name': 'Mingjun Fang'}, {'authorId': '2239067290', 'name': 'Bang Huang'}, {'authorId': '2239060850', 'name': 'Siyong Huang'}, {'authorId': '2165381977', 'name': 'Ridi Wen'}, {'authorId': '2238949428', 'name': 'Franck Le'}, {'authorId': '3254296', 'name': 'L. Kong'}, {'authorId': '2238950279', 'name': 'Jiwu Shu'}]","['Shanghai Jiao Tong University', 'Minjiang University']",['China'],2023-09
2309.04725,Hongyuan Lu,"Hongyuan Lu, Wai Lam",EPA: Easy Prompt Augmentation on Large Language Models via Multiple Sources and Multiple Targets,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) have shown promising performance on various NLP tasks via task prompting. And their performance can be further improved by appending task demonstrations to the head of the prompt. And usually, a better performance can be achieved with more demonstrations. However, asking the users to write the demonstrations can be cumbersome. As a simple yet cost-effective workaround, this paper proposes a novel method called EPA (\textbf{E}asy \textbf{P}rompt \textbf{A}ugmentation)\footnote{While this paper considers augmenting prompts via demonstrations, we name it EPA as the name EDA is already taken by a well-known NLP method \citep{wei-zou-2019-eda}.} that effectively minimizes user efforts in writing demonstrations while improving the model performance at the same time. EPA achieves these goals by automatically augmenting the demonstrations with multiple sources/targets, where each of them paraphrases each other. This is well motivated as augmenting data via paraphrasing effectively improves neural language models. EPA thus employs paraphrasing as an augmentation method for in-context learning. Extensive experiments indicate that EPA effectively improves both NLU and NLG tasks, covering from natural language inference to machine translation in translating tens of languages.\footnote{Code and data will be released upon publication.} ","[{'version': 'v1', 'created': 'Sat, 9 Sep 2023 09:03:50 GMT'}]",2023-09-12,"[['Lu', 'Hongyuan', ''], ['Lam', 'Wai', '']]",0,0,2023-09-09,1,2,1,0,0,0,2b1738d75e1c10ad3bfb8d9aa041c1c495ec7e2d,261682193.0,https://www.semanticscholar.org/paper/2b1738d75e1c10ad3bfb8d9aa041c1c495ec7e2d,arXiv.org,2023.0,25.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2156273800', 'name': 'Hongyuan Lu'}, {'authorId': '144594306', 'name': 'Wai Lam'}]",['Chinese University of Hong Kong'],['China'],2023-09
2309.05021,Yaonai Wei Dr.,"Yaonai Wei, Tuo Zhang, Han Zhang, Tianyang Zhong, Lin Zhao, Zhengliang
  Liu, Chong Ma, Songyao Zhang, Muheng Shang, Lei Du, Xiao Li, Tianming Liu and
  Junwei Han",Chat2Brain: A Method for Mapping Open-Ended Semantic Queries to Brain Activation Maps,"8 pages, 4 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Over decades, neuroscience has accumulated a wealth of research results in the text modality that can be used to explore cognitive processes. Meta-analysis is a typical method that successfully establishes a link from text queries to brain activation maps using these research results, but it still relies on an ideal query environment. In practical applications, text queries used for meta-analyses may encounter issues such as semantic redundancy and ambiguity, resulting in an inaccurate mapping to brain images. On the other hand, large language models (LLMs) like ChatGPT have shown great potential in tasks such as context understanding and reasoning, displaying a high degree of consistency with human natural language. Hence, LLMs could improve the connection between text modality and neuroscience, resolving existing challenges of meta-analyses. In this study, we propose a method called Chat2Brain that combines LLMs to basic text-2-image model, known as Text2Brain, to map open-ended semantic queries to brain activation maps in data-scarce and complex query environments. By utilizing the understanding and reasoning capabilities of LLMs, the performance of the mapping model is optimized by transferring text queries to semantic queries. We demonstrate that Chat2Brain can synthesize anatomically plausible neural activation patterns for more complex tasks of text queries. ","[{'version': 'v1', 'created': 'Sun, 10 Sep 2023 13:06:45 GMT'}]",2023-09-12,"[['Wei', 'Yaonai', ''], ['Zhang', 'Tuo', ''], ['Zhang', 'Han', ''], ['Zhong', 'Tianyang', ''], ['Zhao', 'Lin', ''], ['Liu', 'Zhengliang', ''], ['Ma', 'Chong', ''], ['Zhang', 'Songyao', ''], ['Shang', 'Muheng', ''], ['Du', 'Lei', ''], ['Li', 'Xiao', ''], ['Liu', 'Tianming', ''], ['Han', 'Junwei', '']]",1,1,2023-09-10,1,13,1,1,0,1,dd61de46eb2d5681004f8219ec39a0c65eb9170c,261682053.0,https://www.semanticscholar.org/paper/dd61de46eb2d5681004f8219ec39a0c65eb9170c,arXiv.org,2023.0,31.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Psychology', 'source': 's2-fos-model'}]","[{'authorId': '2163675860', 'name': 'Yaonai Wei'}, {'authorId': '2239052067', 'name': 'Tuo Zhang'}, {'authorId': '2239125771', 'name': 'Han Zhang'}, {'authorId': '2215167446', 'name': 'Tianyang Zhong'}, {'authorId': '46586837', 'name': 'Lin Zhao'}, {'authorId': '2145977326', 'name': 'Zheng Liu'}, {'authorId': '120688117', 'name': 'Chong-Yi Ma'}, {'authorId': '2108965026', 'name': 'Songyao Zhang'}, {'authorId': '2144460985', 'name': 'Muheng Shang'}, {'authorId': '2238954278', 'name': 'Lei Du'}, {'authorId': '2108786618', 'name': 'Xiao Li'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}, {'authorId': '2161945158', 'name': 'Jun-Feng Han'}]","['University of Georgia', 'Northwest University', 'Northwestern Polytechnical University']","['China', 'United States']",2023-09
2309.05077,Jingwen Fu,Jingwen Fu and Nanning Zheng,Generalization error bounds for iterative learning algorithms with bounded updates,,,,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper explores the generalization characteristics of iterative learning algorithms with bounded updates for non-convex loss functions, employing information-theoretic techniques. Our key contribution is a novel bound for the generalization error of these algorithms with bounded updates, extending beyond the scope of previous works that only focused on Stochastic Gradient Descent (SGD). Our approach introduces two main novelties: 1) we reformulate the mutual information as the uncertainty of updates, providing a new perspective, and 2) instead of using the chaining rule of mutual information, we employ a variance decomposition technique to decompose information across iterations, allowing for a simpler surrogate process. We analyze our generalization bound under various settings and demonstrate improved bounds when the model dimension increases at the same rate as the number of training data samples. To bridge the gap between theory and practice, we also examine the previously observed scaling behavior in large language models. Ultimately, our work takes a further step for developing practical generalization theories. ","[{'version': 'v1', 'created': 'Sun, 10 Sep 2023 16:55:59 GMT'}, {'version': 'v2', 'created': 'Wed, 13 Sep 2023 12:12:50 GMT'}]",2023-09-14,"[['Fu', 'Jingwen', ''], ['Zheng', 'Nanning', '']]",0,0,2023-09-10,2,2,2,0,0,0,4d6dff386a6fabfed3c4cc00c969ce4112d8084e,261682625.0,https://www.semanticscholar.org/paper/4d6dff386a6fabfed3c4cc00c969ce4112d8084e,arXiv.org,2023.0,50.0,0.0,0.0,True,"['Computer Science', 'Mathematics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Mathematics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '152315096', 'name': 'Jingwen Fu'}, {'authorId': '2238955286', 'name': 'Nanning Zheng'}]","[""Xi'an Jiaotong University""]",['China'],2023-09
2309.05186,Xinpeng Ding,"Xinpeng Ding, Jianhua Han, Hang Xu, Wei Zhang, Xiaomeng Li",HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Autonomous driving systems generally employ separate models for different tasks resulting in intricate designs. For the first time, we leverage singular multimodal large language models (MLLMs) to consolidate multiple autonomous driving tasks from videos, i.e., the Risk Object Localization and Intention and Suggestion Prediction (ROLISP) task. ROLISP uses natural language to simultaneously identify and interpret risk objects, understand ego-vehicle intentions, and provide motion suggestions, eliminating the necessity for task-specific architectures. However, lacking high-resolution (HR) information, existing MLLMs often miss small objects (e.g., traffic cones) and overly focus on salient ones (e.g., large trucks) when applied to ROLISP. We propose HiLM-D (Towards High-Resolution Understanding in MLLMs for Autonomous Driving), an efficient method to incorporate HR information into MLLMs for the ROLISP task. Especially, HiLM-D integrates two branches: (i) the low-resolution reasoning branch, can be any MLLMs, processes low-resolution videos to caption risk objects and discern ego-vehicle intentions/suggestions; (ii) the high-resolution perception branch (HR-PB), prominent to HiLM-D,, ingests HR images to enhance detection by capturing vision-specific HR feature maps and prioritizing all potential risks over merely salient objects. Our HR-PB serves as a plug-and-play module, seamlessly fitting into current MLLMs. Experiments on the ROLISP benchmark reveal HiLM-D's notable advantage over leading MLLMs, with improvements of 4.8% in BLEU-4 for captioning and 17.2% in mIoU for detection. ","[{'version': 'v1', 'created': 'Mon, 11 Sep 2023 01:24:13 GMT'}]",2023-09-12,"[['Ding', 'Xinpeng', ''], ['Han', 'Jianhua', ''], ['Xu', 'Hang', ''], ['Zhang', 'Wei', ''], ['Li', 'Xiaomeng', '']]",0,0,2023-09-11,1,5,1,0,0,0,16ed5f612b66cb7d91e534dd7126b69756f45c34,261682424.0,https://www.semanticscholar.org/paper/16ed5f612b66cb7d91e534dd7126b69756f45c34,arXiv.org,2023.0,36.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '151504088', 'name': 'Xinpeng Ding'}, {'authorId': '47180442', 'name': 'Jianhua Han'}, {'authorId': '2237071248', 'name': 'Hang Xu'}, {'authorId': '2256596407', 'name': 'Wei Zhang'}, {'authorId': '48569608', 'name': 'X. Li'}]","['Hong Kong University of Science and Technology', 'Huawei Technologies (China)']",['China'],2023-09
2309.05203,Yuhan Chen,"Yuhan Chen, Nuwa Xi, Yanrui Du, Haochun Wang, Chen Jianyu, Sendong
  Zhao, Bing Qin",From Artificially Real to Real: Leveraging Pseudo Data from Large Language Models for Low-Resource Molecule Discovery,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Molecule discovery serves as a cornerstone in numerous scientific domains, fueling the development of new materials and innovative drug designs. Recent developments of in-silico molecule discovery have highlighted the promising results of cross-modal techniques, which bridge molecular structures with their descriptive annotations. However, these cross-modal methods frequently encounter the issue of data scarcity, hampering their performance and application. In this paper, we address the low-resource challenge by utilizing artificially-real data generated by Large Language Models (LLMs). We first introduce a retrieval-based prompting strategy to construct high-quality pseudo data, then explore the optimal method to effectively leverage this pseudo data. Experiments show that using pseudo data for domain adaptation outperforms all existing methods, while also requiring a smaller model scale, reduced data size and lower training cost, highlighting its efficiency. Furthermore, our method shows a sustained improvement as the volume of pseudo data increases, revealing the great potential of pseudo data in advancing low-resource cross-modal molecule discovery. ","[{'version': 'v1', 'created': 'Mon, 11 Sep 2023 02:35:36 GMT'}]",2023-09-12,"[['Chen', 'Yuhan', ''], ['Xi', 'Nuwa', ''], ['Du', 'Yanrui', ''], ['Wang', 'Haochun', ''], ['Jianyu', 'Chen', ''], ['Zhao', 'Sendong', ''], ['Qin', 'Bing', '']]",0,0,2023-09-11,1,7,1,0,0,0,c6251c3566d90caa162832eb5e5fb93a9f42a78d,261681771.0,https://www.semanticscholar.org/paper/c6251c3566d90caa162832eb5e5fb93a9f42a78d,arXiv.org,2023.0,51.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2238896941', 'name': 'Yuhan Chen'}, {'authorId': '2184773728', 'name': 'Nuwa Xi'}, {'authorId': '2166447772', 'name': 'Yanrui Du'}, {'authorId': '2164011959', 'name': 'Hao Wang'}, {'authorId': '2239066035', 'name': 'Jianyu Chen'}, {'authorId': '1571183602', 'name': 'Sendong Zhao'}, {'authorId': '2166387643', 'name': 'Bing Qin'}]",['Harbin Institute of Technology'],['China'],2023-09
2309.05217,Li Du,"Li Du, Yequan Wang, Xingrun Xing, Yiqun Ya, Xiang Li, Xin Jiang,
  Xuezhi Fang",Quantifying and Attributing the Hallucination of Large Language Models via Association Analysis,,,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although demonstrating superb performance on various NLP tasks, large language models (LLMs) still suffer from the hallucination problem, which threatens the reliability of LLMs. To measure the level of hallucination of LLMs, previous works first categorize the hallucination according to the phenomenon similarity, then quantify the proportion that model outputs contain hallucinatory contents. However, such hallucination rates could easily be distorted by confounders. Moreover, such hallucination rates could not reflect the reasons for the hallucination, as similar hallucinatory phenomena may originate from different sources. To address these issues, we propose to combine the hallucination level quantification and hallucination reason investigation through an association analysis, which builds the relationship between the hallucination rate of LLMs with a set of risk factors. In this way, we are able to observe the hallucination level under each value of each risk factor, examining the contribution and statistical significance of each risk factor, meanwhile excluding the confounding effect of other factors. Additionally, by recognizing the risk factors according to a taxonomy of model capability, we reveal a set of potential deficiencies in commonsense memorization, relational reasoning, and instruction following, which may further provide guidance for the pretraining and supervised fine-tuning process of LLMs to mitigate the hallucination. ","[{'version': 'v1', 'created': 'Mon, 11 Sep 2023 03:35:00 GMT'}]",2023-09-12,"[['Du', 'Li', ''], ['Wang', 'Yequan', ''], ['Xing', 'Xingrun', ''], ['Ya', 'Yiqun', ''], ['Li', 'Xiang', ''], ['Jiang', 'Xin', ''], ['Fang', 'Xuezhi', '']]",0,0,2023-09-11,1,7,2,0,0,0,5e060f23914aff74d8c7b6973df44e5af8d97db5,261682256.0,https://www.semanticscholar.org/paper/5e060f23914aff74d8c7b6973df44e5af8d97db5,arXiv.org,2023.0,41.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '2238210094', 'name': 'Li Du'}, {'authorId': '6285226', 'name': 'Yequan Wang'}, {'authorId': '2238953879', 'name': 'Xingrun Xing'}, {'authorId': '2238952996', 'name': 'Yiqun Ya'}, {'authorId': '32551341', 'name': 'Xiang Lisa Li'}, {'authorId': '145820291', 'name': 'Xin Jiang'}, {'authorId': '2238406105', 'name': 'Xuezhi Fang'}]",['Beijing Academy of Artificial Intelligence'],['China'],2023-09
2309.05274,Dongyu Yao,"Dongyu Yao, Jianshu Zhang, Ian G. Harris, Marcel Carlsson",FuzzLLM: A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabilities in Large Language Models,"In submission, a preprint version",,,,cs.CR,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Jailbreak vulnerabilities in Large Language Models (LLMs), which exploit meticulously crafted prompts to elicit content that violates service guidelines, have captured the attention of research communities. While model owners can defend against individual jailbreak prompts through safety training strategies, this relatively passive approach struggles to handle the broader category of similar jailbreaks. To tackle this issue, we introduce FuzzLLM, an automated fuzzing framework designed to proactively test and discover jailbreak vulnerabilities in LLMs. We utilize templates to capture the structural integrity of a prompt and isolate key features of a jailbreak class as constraints. By integrating different base classes into powerful combo attacks and varying the elements of constraints and prohibited questions, FuzzLLM enables efficient testing with reduced manual effort. Extensive experiments demonstrate FuzzLLM's effectiveness and comprehensiveness in vulnerability discovery across various LLMs. ","[{'version': 'v1', 'created': 'Mon, 11 Sep 2023 07:15:02 GMT'}]",2023-09-12,"[['Yao', 'Dongyu', ''], ['Zhang', 'Jianshu', ''], ['Harris', 'Ian G.', ''], ['Carlsson', 'Marcel', '']]",0,0,2023-09-11,1,4,1,0,0,0,3c784cd3150a359e269c70cfbadd18774d66055d,261681918.0,https://www.semanticscholar.org/paper/3c784cd3150a359e269c70cfbadd18774d66055d,arXiv.org,2023.0,23.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2238951719', 'name': 'Dongyu Yao'}, {'authorId': '2239054292', 'name': 'Jianshu Zhang'}, {'authorId': '2238951230', 'name': 'Ian G. Harris'}, {'authorId': '145488218', 'name': 'Marcel Carlsson'}]","['Wuhan University', 'University of California, Riverside']","['China', 'United States']",2023-09
2309.05557,Yukai Miao,"Yukai Miao, Yu Bai, Li Chen, Dan Li, Haifeng Sun, Xizheng Wang, Ziqiu
  Luo, Yanyu Ren, Dapeng Sun, Xiuting Xu, Qi Zhang, Chao Xiang, Xinchi Li",An Empirical Study of NetOps Capability of Pre-Trained Large Language Models,,,,,cs.CL cs.AI cs.NI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Nowadays, the versatile capabilities of Pre-trained Large Language Models (LLMs) have attracted much attention from the industry. However, some vertical domains are more interested in the in-domain capabilities of LLMs. For the Networks domain, we present NetEval, an evaluation set for measuring the comprehensive capabilities of LLMs in Network Operations (NetOps). NetEval is designed for evaluating the commonsense knowledge and inference ability in NetOps in a multi-lingual context. NetEval consists of 5,732 questions about NetOps, covering five different sub-domains of NetOps. With NetEval, we systematically evaluate the NetOps capability of 26 publicly available LLMs. The results show that only GPT-4 can achieve a performance competitive to humans. However, some open models like LLaMA 2 demonstrate significant potential. ","[{'version': 'v1', 'created': 'Mon, 11 Sep 2023 15:45:40 GMT'}, {'version': 'v2', 'created': 'Tue, 12 Sep 2023 12:15:38 GMT'}, {'version': 'v3', 'created': 'Tue, 19 Sep 2023 16:04:25 GMT'}]",2023-09-20,"[['Miao', 'Yukai', ''], ['Bai', 'Yu', ''], ['Chen', 'Li', ''], ['Li', 'Dan', ''], ['Sun', 'Haifeng', ''], ['Wang', 'Xizheng', ''], ['Luo', 'Ziqiu', ''], ['Ren', 'Yanyu', ''], ['Sun', 'Dapeng', ''], ['Xu', 'Xiuting', ''], ['Zhang', 'Qi', ''], ['Xiang', 'Chao', ''], ['Li', 'Xinchi', '']]",0,1,2023-09-11,3,13,3,2,1,1,297bc0816ec5039d5f746fb0c876303ff0c1e367,261696561.0,https://www.semanticscholar.org/paper/297bc0816ec5039d5f746fb0c876303ff0c1e367,arXiv.org,2023.0,19.0,1.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2239095099', 'name': 'Yukai Miao'}, {'authorId': '2239392496', 'name': 'Yu Bai'}, {'authorId': '2239195243', 'name': 'Li Chen'}, {'authorId': '2239099410', 'name': 'Dan Li'}, {'authorId': '2239286113', 'name': 'Haifeng Sun'}, {'authorId': '2108390380', 'name': 'Xizheng Wang'}, {'authorId': '2239246135', 'name': 'Ziqiu Luo'}, {'authorId': '2239128949', 'name': 'Dapeng Sun'}, {'authorId': '2239161409', 'name': 'Xiuting Xu'}, {'authorId': '2256972811', 'name': 'Qi Zhang'}, {'authorId': '2239094381', 'name': 'Chao Xiang'}, {'authorId': '2239170049', 'name': 'Xinchi Li'}]","['Beijing University of Posts and Telecommunications', 'Tsinghua University', 'China Telecom']",['China'],2023-09
2309.05689,Li Dong,"Qingxiu Dong, Li Dong, Ke Xu, Guangyan Zhou, Yaru Hao, Zhifang Sui,
  Furu Wei",Large Language Model for Science: A Study on P vs. NP,73 pages,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work, we use large language models (LLMs) to augment and accelerate research on the P versus NP problem, one of the most important open problems in theoretical computer science and mathematics. Specifically, we propose Socratic reasoning, a general framework that promotes in-depth thinking with LLMs for complex problem-solving. Socratic reasoning encourages LLMs to recursively discover, solve, and integrate problems while facilitating self-evaluation and refinement. Our pilot study on the P vs. NP problem shows that GPT-4 successfully produces a proof schema and engages in rigorous reasoning throughout 97 dialogue turns, concluding ""P $\neq$ NP"", which is in alignment with (Xu and Zhou, 2023). The investigation uncovers novel insights within the extensive solution space of LLMs, shedding light on LLM for Science. ","[{'version': 'v1', 'created': 'Mon, 11 Sep 2023 17:49:27 GMT'}]",2023-09-13,"[['Dong', 'Qingxiu', ''], ['Dong', 'Li', ''], ['Xu', 'Ke', ''], ['Zhou', 'Guangyan', ''], ['Hao', 'Yaru', ''], ['Sui', 'Zhifang', ''], ['Wei', 'Furu', '']]",0,1,2023-09-11,1,7,2,1,0,1,40b28fddd19fcbd34f7c69b934259cd436aaeac9,261696523.0,https://www.semanticscholar.org/paper/40b28fddd19fcbd34f7c69b934259cd436aaeac9,arXiv.org,2023.0,13.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2047143813', 'name': 'Qingxiu Dong'}, {'authorId': '145307652', 'name': 'Li Dong'}, {'authorId': '2239148093', 'name': 'Ke Xu'}, {'authorId': '2239174241', 'name': 'Guangyan Zhou'}, {'authorId': '34128716', 'name': 'Y. Hao'}, {'authorId': '3335836', 'name': 'Zhifang Sui'}, {'authorId': '49807919', 'name': 'Furu Wei'}]","['Beihang University', 'Beijing Technology and Business University', 'Microsoft', 'https://aka.ms/GeneralAI', 'Peking University']","['China', 'India']",2023-09
2309.06105,Tinghui Zhu,"Tinghui Zhu, Jingping Liu, Jiaqing Liang, Haiyun Jiang, Yanghua Xiao,
  Zongyu Wang, Rui Xie, Yunsen Xian",Towards Visual Taxonomy Expansion,ACMMM accepted paper,,10.1145/3581783.3613845,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Taxonomy expansion task is essential in organizing the ever-increasing volume of new concepts into existing taxonomies. Most existing methods focus exclusively on using textual semantics, leading to an inability to generalize to unseen terms and the ""Prototypical Hypernym Problem."" In this paper, we propose Visual Taxonomy Expansion (VTE), introducing visual features into the taxonomy expansion task. We propose a textual hypernymy learning task and a visual prototype learning task to cluster textual and visual semantics. In addition to the tasks on respective modalities, we introduce a hyper-proto constraint that integrates textual and visual semantics to produce fine-grained visual semantics. Our method is evaluated on two datasets, where we obtain compelling results. Specifically, on the Chinese taxonomy dataset, our method significantly improves accuracy by 8.75 %. Additionally, our approach performs better than ChatGPT on the Chinese taxonomy dataset. ","[{'version': 'v1', 'created': 'Tue, 12 Sep 2023 10:17:28 GMT'}]",2023-09-13,"[['Zhu', 'Tinghui', ''], ['Liu', 'Jingping', ''], ['Liang', 'Jiaqing', ''], ['Jiang', 'Haiyun', ''], ['Xiao', 'Yanghua', ''], ['Wang', 'Zongyu', ''], ['Xie', 'Rui', ''], ['Xian', 'Yunsen', '']]",1,1,2023-09-12,1,8,2,1,0,1,b40f3d0678a928768ecd42140c259f39c7e0794b,261697387.0,https://www.semanticscholar.org/paper/b40f3d0678a928768ecd42140c259f39c7e0794b,ACM Multimedia,2023.0,35.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2222400339', 'name': 'Tinghui Zhu'}, {'authorId': '4290651', 'name': 'Jingping Liu'}, {'authorId': '3366523', 'name': 'Jiaqing Liang'}, {'authorId': '48579460', 'name': 'Haiyun Jiang'}, {'authorId': '2116642640', 'name': 'Yanghua Xiao'}, {'authorId': '51198136', 'name': 'Zongyu Wang'}, {'authorId': '2053425736', 'name': 'Rui Xie'}, {'authorId': '2239101015', 'name': 'Yunsen Xian'}]","['Jiaqing Liang', 'East China University of Science and Technology', 'Shanghai Construction Group (China)', 'Shanghai, China Yunsen Xian', 'Shanghai, China Rui Xie', 'Fudan University']",['China'],2023-09
2309.06126,Yuan-Sen Ting Assoc. Prof.,"Tuan Dung Nguyen, Yuan-Sen Ting, Ioana Ciuc\u{a}, Charlie O'Neill,
  Ze-Chang Sun, Maja Jab{\l}o\'nska, Sandor Kruk, Ernest Perkowski, Jack
  Miller, Jason Li, Josh Peek, Kartheik Iyer, Tomasz R\'o\.za\'nski, Pranav
  Khetarpal, Sharaf Zaman, David Brodrick, Sergio J. Rodr\'iguez M\'endez,
  Thang Bui, Alyssa Goodman, Alberto Accomazzi, Jill Naiman, Jesse Cranney,
  Kevin Schawinski, UniverseTBD",AstroLLaMA: Towards Specialized Foundation Models in Astronomy,"6 pages, 3 figures, submitted to IJCNLP-AACL 2023. Comments are
  welcome. The model can be found on Hugging Face -
  https://huggingface.co/universeTBD/astrollama",,,,astro-ph.IM astro-ph.CO astro-ph.GA astro-ph.HE cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models excel in many human-language tasks but often falter in highly specialized domains like scholarly astronomy. To bridge this gap, we introduce AstroLLaMA, a 7-billion-parameter model fine-tuned from LLaMA-2 using over 300,000 astronomy abstracts from arXiv. Optimized for traditional causal language modeling, AstroLLaMA achieves a 30% lower perplexity than Llama-2, showing marked domain adaptation. Our model generates more insightful and scientifically relevant text completions and embedding extraction than state-of-the-arts foundation models despite having significantly fewer parameters. AstroLLaMA serves as a robust, domain-specific model with broad fine-tuning potential. Its public release aims to spur astronomy-focused research, including automatic paper summarization and conversational agent development. ","[{'version': 'v1', 'created': 'Tue, 12 Sep 2023 11:02:27 GMT'}]",2023-09-13,"[['Nguyen', 'Tuan Dung', ''], ['Ting', 'Yuan-Sen', ''], ['Ciuc', 'Ioana', ''], [""O'Neill"", 'Charlie', ''], ['Sun', 'Ze-Chang', ''], ['Jaboska', 'Maja', ''], ['Kruk', 'Sandor', ''], ['Perkowski', 'Ernest', ''], ['Miller', 'Jack', ''], ['Li', 'Jason', ''], ['Peek', 'Josh', ''], ['Iyer', 'Kartheik', ''], ['Raski', 'Tomasz', ''], ['Khetarpal', 'Pranav', ''], ['Zaman', 'Sharaf', ''], ['Brodrick', 'David', ''], ['Mndez', 'Sergio J. Rodrguez', ''], ['Bui', 'Thang', ''], ['Goodman', 'Alyssa', ''], ['Accomazzi', 'Alberto', ''], ['Naiman', 'Jill', ''], ['Cranney', 'Jesse', ''], ['Schawinski', 'Kevin', ''], ['UniverseTBD', '', '']]",0,0,2023-09-12,1,24,6,1,1,0,e33a538e80d1877782df26e1493f5adc661ceec4,261696577.0,https://www.semanticscholar.org/paper/e33a538e80d1877782df26e1493f5adc661ceec4,arXiv.org,2023.0,17.0,1.0,0.0,True,"['Computer Science', 'Physics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Physics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Physics', 'source': 's2-fos-model'}]","[{'authorId': '2116225572', 'name': 'Tuan Dung Nguyen'}, {'authorId': '2239095445', 'name': 'Yuan-Sen Ting'}, {'authorId': '50062876', 'name': 'I. Ciuc'}, {'authorId': '2239094893', 'name': ""Charlie O'Neill""}, {'authorId': '2239167308', 'name': 'Ze-Chang Sun'}, {'authorId': '2239094677', 'name': ""Maja Jablo'nska""}, {'authorId': '143919054', 'name': 'S. Kruk'}, {'authorId': '2239094950', 'name': 'Ernest Perkowski'}, {'authorId': '2229023715', 'name': 'Jack W. Miller'}, {'authorId': '2239158695', 'name': 'Jason Li'}, {'authorId': '2239094501', 'name': 'Josh Peek'}, {'authorId': '102239912', 'name': 'K. Iyer'}, {'authorId': '2142546267', 'name': ""Tomasz R'o.za'nski""}, {'authorId': '2239095448', 'name': 'P. Khetarpal'}, {'authorId': '2239094542', 'name': 'Sharaf Zaman'}, {'authorId': '101726491', 'name': 'D. Brodrick'}, {'authorId': '2125183970', 'name': ""Sergio J. Rodr'iguez M'endez""}, {'authorId': '2229238231', 'name': 'Thang Bui'}, {'authorId': '2239094775', 'name': 'Alyssa Goodman'}, {'authorId': '2751757', 'name': 'A. Accomazzi'}, {'authorId': '2239094393', 'name': 'Jill P. Naiman'}, {'authorId': '2401845', 'name': 'Jesse Cranney'}, {'authorId': '6552704', 'name': 'K. Schawinski'}, {'authorId': '2239094383', 'name': 'UniverseTBD'}]","['University of Pennsylvania', 'Learning Machines, Australia', 'Harvard University', 'The Ohio State University', 'Indian Institute of Technology Delhi', 'Columbia University', 'Australian National University', 'Tsinghua University', 'Center for Astrophysics Harvard & Smithsonian', 'University of Illinois Urbana-Champaign', 'University of Wrocaw', 'European Space Astronomy Centre', 'Space Telescope Science Institute']","['Poland', 'Spain', 'United States', 'India', 'China', 'Australia']",2023-09
2309.06256,Yong Lin,"Yong Lin, Lu Tan, Hangyu Lin, Zeming Zheng, Renjie Pi, Jipeng Zhang,
  Shizhe Diao, Haoxiang Wang, Han Zhao, Yuan Yao, and Tong Zhang",Speciality vs Generality: An Empirical Study on Catastrophic Forgetting in Fine-tuning Foundation Models,30 Pages,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Foundation models, including Vision Language Models (VLMs) and Large Language Models (LLMs), possess the $generality$ to handle diverse distributions and tasks, which stems from their extensive pre-training datasets. The fine-tuning of foundation models is a common practice to enhance task performance or align the model's behavior with human expectations, allowing them to gain $speciality$. However, the small datasets used for fine-tuning may not adequately cover the diverse distributions and tasks encountered during pre-training. Consequently, the pursuit of speciality during fine-tuning can lead to a loss of {generality} in the model, which is related to catastrophic forgetting (CF) in deep learning. In this study, we demonstrate this phenomenon in both VLMs and LLMs. For instance, fine-tuning VLMs like CLIP on ImageNet results in a loss of generality in handling diverse distributions, and fine-tuning LLMs like Galactica in the medical domain leads to a loss in following instructions and common sense.   To address the trade-off between the speciality and generality, we investigate multiple regularization methods from continual learning, the weight averaging method (Wise-FT) from out-of-distributional (OOD) generalization, which interpolates parameters between pre-trained and fine-tuned models, and parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA). Our findings show that both continual learning and Wise-ft methods effectively mitigate the loss of generality, with Wise-FT exhibiting the strongest performance in balancing speciality and generality. ","[{'version': 'v1', 'created': 'Tue, 12 Sep 2023 14:16:54 GMT'}, {'version': 'v2', 'created': 'Wed, 4 Oct 2023 13:26:06 GMT'}]",2023-10-05,"[['Lin', 'Yong', ''], ['Tan', 'Lu', ''], ['Lin', 'Hangyu', ''], ['Zheng', 'Zeming', ''], ['Pi', 'Renjie', ''], ['Zhang', 'Jipeng', ''], ['Diao', 'Shizhe', ''], ['Wang', 'Haoxiang', ''], ['Zhao', 'Han', ''], ['Yao', 'Yuan', ''], ['Zhang', 'Tong', '']]",0,0,2023-09-12,2,11,1,0,0,0,3789c115f71886a47feb421f45d4903082a29b22,261697277.0,https://www.semanticscholar.org/paper/3789c115f71886a47feb421f45d4903082a29b22,arXiv.org,2023.0,90.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2238123947', 'name': 'Yong Lin'}, {'authorId': '2239156696', 'name': 'Lu Tan'}, {'authorId': '2239167492', 'name': 'Hangyu Lin'}, {'authorId': '2239197570', 'name': 'Zeming Zheng'}, {'authorId': '2066420772', 'name': 'Renjie Pi'}, {'authorId': '50561049', 'name': 'Jipeng Zhang'}, {'authorId': '50826757', 'name': 'Shizhe Diao'}, {'authorId': '2266194421', 'name': 'Haoxiang Wang'}, {'authorId': '2188204871', 'name': 'Han Zhao'}, {'authorId': '2238126734', 'name': 'Yuan Yao'}, {'authorId': '38144094', 'name': 'T. Zhang'}]","['University of Illinois Urbana-Champaign', 'Tsinghua University', 'Hong Kong University of Science and Technology']","['China', 'United States']",2023-09
2309.06275,Chongyang Tao,"Xiaohan Xu, Chongyang Tao, Tao Shen, Can Xu, Hongbo Xu, Guodong Long,
  Jian-guang Lou",Re-Reading Improves Reasoning in Language Models,25 pages,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Reasoning presents a significant and challenging issue for Large Language Models (LLMs). The predominant focus of research has revolved around developing diverse prompting strategies to guide and structure the reasoning processes of LLMs. However, these approaches based on decoder-only causal language models often operate the input question in a single forward pass, potentially missing the rich, back-and-forth interactions inherent in human reasoning. Scant attention has been paid to a critical dimension, i.e., the input question itself embedded within the prompts. In response, we introduce a deceptively simple yet highly effective prompting strategy, termed question ""re-reading"". Drawing inspiration from human learning and problem-solving, re-reading entails revisiting the question information embedded within input prompts. This approach aligns seamlessly with the cognitive principle of reinforcement, enabling LLMs to extract deeper insights, identify intricate patterns, establish more nuanced connections, and ultimately enhance their reasoning capabilities across various tasks. Experiments conducted on a series of reasoning benchmarks serve to underscore the effectiveness and generality of our method. Moreover, our findings demonstrate that our approach seamlessly integrates with various language models, though-eliciting prompting methods, and ensemble techniques, further underscoring its versatility and compatibility in the realm of LLMs. ","[{'version': 'v1', 'created': 'Tue, 12 Sep 2023 14:36:23 GMT'}]",2023-09-13,"[['Xu', 'Xiaohan', ''], ['Tao', 'Chongyang', ''], ['Shen', 'Tao', ''], ['Xu', 'Can', ''], ['Xu', 'Hongbo', ''], ['Long', 'Guodong', ''], ['Lou', 'Jian-guang', '']]",0,0,2023-09-12,1,7,1,0,0,0,e7c85d7d58d4b1fde4be8a8f166e46c995dc0f1b,261696483.0,https://www.semanticscholar.org/paper/e7c85d7d58d4b1fde4be8a8f166e46c995dc0f1b,arXiv.org,2023.0,72.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2188774115', 'name': 'Xiaohan Xu'}, {'authorId': '8801869', 'name': 'Chongyang Tao'}, {'authorId': '143681703', 'name': 'Tao Shen'}, {'authorId': '46747953', 'name': 'Can Xu'}, {'authorId': '2239160918', 'name': 'Hongbo Xu'}, {'authorId': '2062835', 'name': 'Guodong Long'}, {'authorId': '4648762', 'name': 'Jian-Guang Lou'}]",['Institute of Information Engineering'],['China'],2023-09
2309.06419,Zhengliang Liu,"Zhengliang Liu, Yiwei Li, Peng Shu, Aoxiao Zhong, Longtao Yang, Chao
  Ju, Zihao Wu, Chong Ma, Jie Luo, Cheng Chen, Sekeun Kim, Jiang Hu, Haixing
  Dai, Lin Zhao, Dajiang Zhu, Jun Liu, Wei Liu, Dinggang Shen, Tianming Liu,
  Quanzheng Li, and Xiang Li",Radiology-Llama2: Best-in-Class Large Language Model for Radiology,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  This paper introduces Radiology-Llama2, a large language model specialized for radiology through a process known as instruction tuning. Radiology-Llama2 is based on the Llama2 architecture and further trained on a large dataset of radiology reports to generate coherent and clinically useful impressions from radiological findings. Quantitative evaluations using ROUGE metrics on the MIMIC-CXR and OpenI datasets demonstrate that Radiology-Llama2 achieves state-of-the-art performance compared to other generative language models, with a Rouge-1 score of 0.4834 on MIMIC-CXR and 0.4185 on OpenI. Additional assessments by radiology experts highlight the model's strengths in understandability, coherence, relevance, conciseness, and clinical utility. The work illustrates the potential of localized language models designed and tuned for specialized domains like radiology. When properly evaluated and deployed, such models can transform fields like radiology by automating rote tasks and enhancing human expertise. ","[{'version': 'v1', 'created': 'Tue, 29 Aug 2023 17:44:28 GMT'}]",2023-09-13,"[['Liu', 'Zhengliang', ''], ['Li', 'Yiwei', ''], ['Shu', 'Peng', ''], ['Zhong', 'Aoxiao', ''], ['Yang', 'Longtao', ''], ['Ju', 'Chao', ''], ['Wu', 'Zihao', ''], ['Ma', 'Chong', ''], ['Luo', 'Jie', ''], ['Chen', 'Cheng', ''], ['Kim', 'Sekeun', ''], ['Hu', 'Jiang', ''], ['Dai', 'Haixing', ''], ['Zhao', 'Lin', ''], ['Zhu', 'Dajiang', ''], ['Liu', 'Jun', ''], ['Liu', 'Wei', ''], ['Shen', 'Dinggang', ''], ['Liu', 'Tianming', ''], ['Li', 'Quanzheng', ''], ['Li', 'Xiang', '']]",0,0,2023-08-29,1,21,1,0,0,0,420d6754315ac5db8a040386245cd15b9fe5b459,261696494.0,https://www.semanticscholar.org/paper/420d6754315ac5db8a040386245cd15b9fe5b459,arXiv.org,2023.0,66.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2145977326', 'name': 'Zheng Liu'}, {'authorId': '2111161355', 'name': 'Yiwei Li'}, {'authorId': '2220096705', 'name': 'Peng Shu'}, {'authorId': '40153228', 'name': 'Aoxiao Zhong'}, {'authorId': '2237381357', 'name': 'Longtao Yang'}, {'authorId': '2220096793', 'name': 'Chao Ju'}, {'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '120688117', 'name': 'Chong-Yi Ma'}, {'authorId': '2239186794', 'name': 'Jie Luo'}, {'authorId': '1390805683', 'name': 'Cheng Chen'}, {'authorId': '2239159354', 'name': 'Sekeun Kim'}, {'authorId': '2239161307', 'name': 'Jiang Hu'}, {'authorId': '29944950', 'name': 'Haixing Dai'}, {'authorId': '2111641126', 'name': 'Lin Zhao'}, {'authorId': '2181182', 'name': 'Dajiang Zhu'}, {'authorId': '2157176152', 'name': 'Jun Liu'}, {'authorId': '46641573', 'name': 'W. Liu'}, {'authorId': '2150038187', 'name': 'Dinggang Shen'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}, {'authorId': '2210054417', 'name': 'Quanzheng Li'}, {'authorId': '113075935', 'name': 'Xiang Li'}]","['Northwestern Polytechnical University', 'Second Xiangya Hospital of Central South University', 'Mayo Clinic', 'Harvard University', 'Shanghai United Imaging Intelligence Co., Ltd.', 'ShanghaiTech University', 'Shanghai Clinical Research Center', 'Massachusetts General Hospital', 'University of Georgia', 'The University of Texas at Arlington']","['China', 'United States']",2023-08
2309.06453,Mingxin Li,"Mingxin Li, Richong Zhang, Zhijie Nie, Yongyi Mao",Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model,work in progress,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Sentence Representation Learning (SRL) is a fundamental task in Natural Language Processing (NLP), with Contrastive learning of Sentence Embeddings (CSE) as the mainstream technique due to its superior performance. An intriguing phenomenon in CSE is the significant performance gap between supervised and unsupervised methods, even when their sentence encoder and loss function are the same. Previous works attribute this performance gap to differences in two representation properties (alignment and uniformity). However, alignment and uniformity only measure the results, which means they cannot answer ""What happens during the training process that leads to the performance gap?"" and ""How can the performance gap be narrowed?"". In this paper, we conduct empirical experiments to answer these ""What"" and ""How"" questions. We first answer the ""What"" question by thoroughly comparing the behavior of supervised and unsupervised CSE during their respective training processes. From the comparison, We observe a significant difference in fitting difficulty. Thus, we introduce a metric, called Fitting Difficulty Increment (FDI), to measure the fitting difficulty gap between the evaluation dataset and the held-out training dataset, and use the metric to answer the ""What"" question. Then, based on the insights gained from the ""What"" question, we tackle the ""How"" question by increasing the fitting difficulty of the training dataset. We achieve this by leveraging the In-Context Learning (ICL) capability of the Large Language Model (LLM) to generate data that simulates complex patterns. By utilizing the hierarchical patterns in the LLM-generated data, we effectively narrow the gap between supervised and unsupervised CSE. ","[{'version': 'v1', 'created': 'Tue, 12 Sep 2023 08:16:58 GMT'}]",2023-09-14,"[['Li', 'Mingxin', ''], ['Zhang', 'Richong', ''], ['Nie', 'Zhijie', ''], ['Mao', 'Yongyi', '']]",0,0,2023-09-12,1,4,2,0,0,0,5798d2efb64925117bd5bfe6f328240d3158c590,261705735.0,https://www.semanticscholar.org/paper/5798d2efb64925117bd5bfe6f328240d3158c590,arXiv.org,2023.0,45.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2239393068', 'name': 'Mingxin Li'}, {'authorId': '1747018', 'name': 'Richong Zhang'}, {'authorId': '2220895313', 'name': 'Zhijie Nie'}, {'authorId': '2047889', 'name': 'Yongyi Mao'}]","['Beihang University', 'University of Ottawa', 'Zhongguancun Laboratory, Beijing, China']","['China', 'Canada']",2023-09
2309.06495,Wanling Gao,"Fei Tang, Wanling Gao, Luzhou Peng, Jianfeng Zhan","AGIBench: A Multi-granularity, Multimodal, Human-referenced, Auto-scoring Benchmark for Large Language Models",14 pages,,,,cs.CL cs.AI cs.PF,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Large language models (LLMs) like ChatGPT have revealed amazing intelligence. How to evaluate the question-solving abilities of LLMs and their degrees of intelligence is a hot-spot but challenging issue. First, the question-solving abilities are interlaced with different ability branches like understanding and massive knowledge categories like mathematics. Second, the inputs of questions are multimodal that may involve text and images. Third, the response format of LLMs is diverse and thus poses great challenges for result extraction and evaluation. In this paper, we propose AGIBench -- a multi-granularity, multimodal, human-referenced, and auto-scoring benchmarking methodology for LLMs. Instead of a collection of blended questions, AGIBench focuses on three typical ability branches and adopts a four-tuple <ability branch, knowledge, difficulty, modal> to label the attributes of each question. First, it supports multi-granularity benchmarking, e.g., per-question, per-ability branch, per-knowledge, per-modal, per-dataset, and per-difficulty level granularities. Second, it contains multimodal input, including text and images. Third, it classifies all the questions into five degrees of difficulty according to the average accuracy rate of abundant educated humans (human-referenced). Fourth, it adopts zero-shot learning to avoid introducing additional unpredictability and provides an auto-scoring method to extract and judge the result. Finally, it defines multi-dimensional metrics, including accuracy under the average, worst, best, and majority voting cases, and repeatability. AGIBench is publically available from \url{https://www.benchcouncil.org/agibench}. ","[{'version': 'v1', 'created': 'Tue, 5 Sep 2023 13:43:37 GMT'}]",2023-09-14,"[['Tang', 'Fei', ''], ['Gao', 'Wanling', ''], ['Peng', 'Luzhou', ''], ['Zhan', 'Jianfeng', '']]",1,1,2023-09-05,1,4,3,1,0,1,040fcb3a7da3fb570b25e909f7ba4bc31593579f,261705816.0,https://www.semanticscholar.org/paper/040fcb3a7da3fb570b25e909f7ba4bc31593579f,arXiv.org,2023.0,21.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1820835743', 'name': 'Fei Tang'}, {'authorId': '46874006', 'name': 'Wanling Gao'}, {'authorId': '2239444431', 'name': 'Luzhou Peng'}, {'authorId': '2062319', 'name': 'Jianfeng Zhan'}]","['University of Copenhagen', 'University of Chinese Academy of Sciences', 'Shanghai Lixin University of Accounting and Finance', 'Institute of Computing Technology']","['China', 'Denmark']",2023-09
2309.06687,Zhehua Zhou,"Jiayang Song, Zhehua Zhou, Jiawei Liu, Chunrong Fang, Zhan Shu, Lei Ma",Self-Refined Large Language Model as Automated Reward Function Designer for Deep Reinforcement Learning in Robotics,,,,,cs.RO cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Although Deep Reinforcement Learning (DRL) has achieved notable success in numerous robotic applications, designing a high-performing reward function remains a challenging task that often requires substantial manual input. Recently, Large Language Models (LLMs) have been extensively adopted to address tasks demanding in-depth common-sense knowledge, such as reasoning and planning. Recognizing that reward function design is also inherently linked to such knowledge, LLM offers a promising potential in this context. Motivated by this, we propose in this work a novel LLM framework with a self-refinement mechanism for automated reward function design. The framework commences with the LLM formulating an initial reward function based on natural language inputs. Then, the performance of the reward function is assessed, and the results are presented back to the LLM for guiding its self-refinement process. We examine the performance of our proposed framework through a variety of continuous robotic control tasks across three diverse robotic systems. The results indicate that our LLM-designed reward functions are able to rival or even surpass manually designed reward functions, highlighting the efficacy and applicability of our approach. ","[{'version': 'v1', 'created': 'Wed, 13 Sep 2023 02:56:56 GMT'}, {'version': 'v2', 'created': 'Mon, 2 Oct 2023 17:20:21 GMT'}]",2023-10-03,"[['Song', 'Jiayang', ''], ['Zhou', 'Zhehua', ''], ['Liu', 'Jiawei', ''], ['Fang', 'Chunrong', ''], ['Shu', 'Zhan', ''], ['Ma', 'Lei', '']]",0,0,2023-09-13,2,6,2,0,0,0,6b4da2023c3a11aea6e3ccb9ab13e594833c47eb,261705660.0,https://www.semanticscholar.org/paper/6b4da2023c3a11aea6e3ccb9ab13e594833c47eb,arXiv.org,2023.0,40.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '3427937', 'name': 'Jiayang Song'}, {'authorId': '2239501205', 'name': 'Zhehua Zhou'}, {'authorId': '2239555012', 'name': 'Jiawei Liu'}, {'authorId': '2239197945', 'name': 'Chunrong Fang'}, {'authorId': '2234398874', 'name': 'Zhan Shu'}, {'authorId': '49198778', 'name': 'Lei Ma'}]","['Nanjing University', 'University of Alberta', 'The University of Tokyo']","['China', 'Canada', 'Japan']",2023-09
2309.07045,Zhexin Zhang,"Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong
  Long, Xiao Liu, Xuanyu Lei, Jie Tang, Minlie Huang",SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,15 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the rapid development of Large Language Models (LLMs), increasing attention has been paid to their safety concerns. Consequently, evaluating the safety of LLMs has become an essential task for facilitating the broad applications of LLMs. Nevertheless, the absence of comprehensive safety evaluation benchmarks poses a significant impediment to effectively assess and enhance the safety of LLMs. In this work, we present SafetyBench, a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. Notably, SafetyBench also incorporates both Chinese and English data, facilitating the evaluation in both languages. Our extensive tests over 25 popular Chinese and English LLMs in both zero-shot and few-shot settings reveal a substantial performance advantage for GPT-4 over its counterparts, and there is still significant room for improving the safety of current LLMs. We believe SafetyBench will enable fast and comprehensive evaluation of LLMs' safety, and foster the development of safer LLMs. Data and evaluation guidelines are available at https://github.com/thu-coai/SafetyBench. Submission entrance and leaderboard are available at https://llmbench.ai/safety. ","[{'version': 'v1', 'created': 'Wed, 13 Sep 2023 15:56:50 GMT'}]",2023-09-14,"[['Zhang', 'Zhexin', ''], ['Lei', 'Leqi', ''], ['Wu', 'Lindong', ''], ['Sun', 'Rui', ''], ['Huang', 'Yongkang', ''], ['Long', 'Chong', ''], ['Liu', 'Xiao', ''], ['Lei', 'Xuanyu', ''], ['Tang', 'Jie', ''], ['Huang', 'Minlie', '']]",0,1,2023-09-13,1,10,1,1,0,1,9b9a4fa3ed510fc6eb1bf831979235f3d9f8b556,261706197.0,https://www.semanticscholar.org/paper/9b9a4fa3ed510fc6eb1bf831979235f3d9f8b556,arXiv.org,2023.0,29.0,5.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '101371510', 'name': 'Zhexin Zhang'}, {'authorId': '2239889447', 'name': 'Leqi Lei'}, {'authorId': '2239424207', 'name': 'Lindong Wu'}, {'authorId': '2239238861', 'name': 'Rui Sun'}, {'authorId': '2239394827', 'name': 'Yongkang Huang'}, {'authorId': '2239299334', 'name': 'Chong Long'}, {'authorId': '2111312892', 'name': 'Xiao Liu'}, {'authorId': '2181283109', 'name': 'Xuanyu Lei'}, {'authorId': '2260595820', 'name': 'Jie Tang'}, {'authorId': '1730108', 'name': 'Minlie Huang'}]","['Peking University', 'Tsinghua University', 'Northwest Minzu University', 'China Mobile (China)']",['China'],2023-09
2309.07081,Siyin Wang,"Siyin Wang, Chao-Han Huck Yang, Ji Wu, Chao Zhang",Can Whisper perform speech-based in-context learning,Submitted to ICASSP 2024,,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper investigates the in-context learning abilities of the Whisper automatic speech recognition (ASR) models released by OpenAI. A novel speech-based in-context learning (SICL) approach is proposed for test-time adaptation, which can reduce the word error rates (WERs) with only a small number of labelled speech samples without gradient descent. Language-level adaptation experiments using Chinese dialects showed that when applying SICL to isolated word ASR, consistent and considerable relative WER reductions can be achieved using Whisper models of any size on two dialects, which is on average 32.3%. A k-nearest-neighbours-based in-context example selection technique can be applied to further improve the efficiency of SICL, which can increase the average relative WER reduction to 36.4%. The findings are verified using speaker adaptation or continuous speech recognition tasks, and both achieved considerable relative WER reductions. Detailed quantitative analyses are also provided to shed light on SICL's adaptability to phonological variances and dialect-specific lexical nuances. ","[{'version': 'v1', 'created': 'Wed, 13 Sep 2023 16:46:27 GMT'}]",2023-09-14,"[['Wang', 'Siyin', ''], ['Yang', 'Chao-Han Huck', ''], ['Wu', 'Ji', ''], ['Zhang', 'Chao', '']]",0,0,2023-09-13,1,4,3,0,0,0,3a944ddba8b6fbaaac36126fc955f181f8b8b06a,261706064.0,https://www.semanticscholar.org/paper/3a944ddba8b6fbaaac36126fc955f181f8b8b06a,arXiv.org,2023.0,37.0,3.0,0.0,True,"['Engineering', 'Computer Science']","[{'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2239283908', 'name': 'Siyin Wang'}, {'authorId': '2240094771', 'name': 'Chao-Han Huck Yang'}, {'authorId': '2239514842', 'name': 'Ji Wu'}, {'authorId': '2256776223', 'name': 'Chao Zhang'}]","['Tsinghua University', 'Georgia Institute of Technology']","['China', 'United States']",2023-09
2309.07124,Yuhui Li,"Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, Hongyang Zhang",RAIN: Your Language Models Can Align Themselves without Finetuning,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) often demonstrate inconsistencies with human preferences. Previous research gathered human preference data and then aligned the pre-trained models using reinforcement learning or instruction tuning, the so-called finetuning step. In contrast, aligning frozen LLMs without any extra data is more appealing. This work explores the potential of the latter setting. We discover that by integrating self-evaluation and rewind mechanisms, unaligned LLMs can directly produce responses consistent with human preferences via self-boosting. We introduce a novel inference method, Rewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to evaluate their own generation and use the evaluation results to guide backward rewind and forward generation for AI safety. Notably, RAIN operates without the need of extra data for model alignment and abstains from any training, gradient computation, or parameter updates; during the self-evaluation phase, the model receives guidance on which human preference to align with through a fixed-template prompt, eliminating the need to modify the initial prompt. Experimental results evaluated by GPT-4 and humans demonstrate the effectiveness of RAIN: on the HH dataset, RAIN improves the harmlessness rate of LLaMA 30B over vanilla inference from 82% to 97%, while maintaining the helpfulness rate. Under the leading adversarial attack llm-attacks on Vicuna 33B, RAIN establishes a new defense baseline by reducing the attack success rate from 94% to 19%. ","[{'version': 'v1', 'created': 'Wed, 13 Sep 2023 17:59:09 GMT'}]",2023-09-14,"[['Li', 'Yuhui', ''], ['Wei', 'Fangyun', ''], ['Zhao', 'Jinjing', ''], ['Zhang', 'Chao', ''], ['Zhang', 'Hongyang', '']]",0,1,2023-09-13,1,5,1,3,2,1,b574245f3db22b5eb7fe64bd8b0a147dab467b60,261705563.0,https://www.semanticscholar.org/paper/b574245f3db22b5eb7fe64bd8b0a147dab467b60,arXiv.org,2023.0,47.0,9.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2192674200', 'name': 'Yuhui Li'}, {'authorId': '2239197291', 'name': 'Fangyun Wei'}, {'authorId': '2256929424', 'name': 'Jinjing Zhao'}, {'authorId': '2256776221', 'name': 'Chao Zhang'}, {'authorId': '40975176', 'name': 'Hongyang Zhang'}]","['Peking University', 'University of Sydney', 'Microsoft']","['China', 'Australia']",2023-09
2309.07413,Lei Zhang,"Lei Zhang, Zhengkun Tian, Xiang Chen, Jiaming Sun, Hongyu Xiang, Ke
  Ding, Guanglu Wan",CPPF: A contextual and post-processing-free model for automatic speech recognition,Submitted to ICASSP2024,,,,cs.CL cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  ASR systems have become increasingly widespread in recent years. However, their textual outputs often require post-processing tasks before they can be practically utilized. To address this issue, we draw inspiration from the multifaceted capabilities of LLMs and Whisper, and focus on integrating multiple ASR text processing tasks related to speech recognition into the ASR model. This integration not only shortens the multi-stage pipeline, but also prevents the propagation of cascading errors, resulting in direct generation of post-processed text. In this study, we focus on ASR-related processing tasks, including Contextual ASR and multiple ASR post processing tasks. To achieve this objective, we introduce the CPPF model, which offers a versatile and highly effective alternative to ASR processing. CPPF seamlessly integrates these tasks without any significant loss in recognition performance. ","[{'version': 'v1', 'created': 'Thu, 14 Sep 2023 03:40:14 GMT'}, {'version': 'v2', 'created': 'Thu, 21 Sep 2023 03:02:27 GMT'}]",2023-09-22,"[['Zhang', 'Lei', ''], ['Tian', 'Zhengkun', ''], ['Chen', 'Xiang', ''], ['Sun', 'Jiaming', ''], ['Xiang', 'Hongyu', ''], ['Ding', 'Ke', ''], ['Wan', 'Guanglu', '']]",0,0,2023-09-14,2,7,3,0,0,0,1446dd7b605721adc120fa2fde696282e389efaa,261822820.0,https://www.semanticscholar.org/paper/1446dd7b605721adc120fa2fde696282e389efaa,arXiv.org,2023.0,27.0,0.0,0.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2256831143', 'name': 'Lei Zhang'}, {'authorId': '2240695318', 'name': 'Zhengkun Tian'}, {'authorId': '2240702274', 'name': 'Xiang Chen'}, {'authorId': '2240632141', 'name': 'Jiaming Sun'}, {'authorId': '2240527644', 'name': 'Hongyu Xiang'}, {'authorId': '2240524520', 'name': 'Ke Ding'}, {'authorId': '2240526585', 'name': 'Guanglu Wan'}]",['Soochow University'],['China'],2023-09
2309.07414,Xiaoyu Yang,"Xiaoyu Yang, Wei Kang, Zengwei Yao, Yifan Yang, Liyong Guo, Fangjun
  Kuang, Long Lin, Daniel Povey",PromptASR for contextualized ASR with controllable style,Submitted to ICASSP2024,,,,eess.AS cs.CL cs.SD,http://creativecommons.org/licenses/by/4.0/,"  Prompts are crucial to large language models as they provide context information such as topic or logical relationships. Inspired by this, we propose PromptASR, a framework that integrates prompts in end-to-end automatic speech recognition (E2E ASR) systems to achieve contextualized ASR with controllable style of transcriptions. Specifically, a dedicated text encoder encodes the text prompts and the encodings are injected into the speech encoder by cross-attending the features from two modalities. When using the ground truth text from preceding utterances as content prompt, the proposed system achieves 21.9% and 6.8% relative word error rate reductions on a book reading dataset and an in-house dataset compared to a baseline ASR system. The system can also take word-level biasing lists as prompt to improve recognition accuracy on rare words. An additional style prompt can be given to the text encoder and guide the ASR system to output different styles of transcriptions. The code is available at icefall. ","[{'version': 'v1', 'created': 'Thu, 14 Sep 2023 03:43:07 GMT'}, {'version': 'v2', 'created': 'Wed, 20 Sep 2023 09:13:11 GMT'}]",2023-09-21,"[['Yang', 'Xiaoyu', ''], ['Kang', 'Wei', ''], ['Yao', 'Zengwei', ''], ['Yang', 'Yifan', ''], ['Guo', 'Liyong', ''], ['Kuang', 'Fangjun', ''], ['Lin', 'Long', ''], ['Povey', 'Daniel', '']]",0,0,2023-09-14,2,8,3,0,0,0,17e172a2f4072bd79642ec2b2a57213d6dad17b6,261816860.0,https://www.semanticscholar.org/paper/17e172a2f4072bd79642ec2b2a57213d6dad17b6,arXiv.org,2023.0,23.0,0.0,0.0,True,"['Engineering', 'Computer Science']","[{'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2240481720', 'name': 'Xiaoyu Yang'}, {'authorId': '2173705674', 'name': 'Wei Kang'}, {'authorId': '134909283', 'name': 'Zengwei Yao'}, {'authorId': '2240498753', 'name': 'Yifan Yang'}, {'authorId': '2110520397', 'name': 'Liyong Guo'}, {'authorId': '2173755887', 'name': 'Fangjun Kuang'}, {'authorId': '2173759681', 'name': 'Long Lin'}, {'authorId': '2240534673', 'name': 'Daniel Povey'}]","['Xiaomi Corp. Beijing, China']",['China'],2023-09
2309.07694,Shentong Mo,"Shentong Mo, Miao Xin",Tree of Uncertain Thoughts Reasoning for Large Language Models,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While the recently introduced Tree of Thoughts (ToT) has heralded advancements in allowing Large Language Models (LLMs) to reason through foresight and backtracking for global decision-making, it has overlooked the inherent local uncertainties in intermediate decision points or ""thoughts"". These local uncertainties, intrinsic to LLMs given their potential for diverse responses, remain a significant concern in the reasoning process. Addressing this pivotal gap, we introduce the Tree of Uncertain Thoughts (TouT) - a reasoning framework tailored for LLMs. Our TouT effectively leverages Monte Carlo Dropout to quantify uncertainty scores associated with LLMs' diverse local responses at these intermediate steps. By marrying this local uncertainty quantification with global search algorithms, TouT enhances the model's precision in response generation. We substantiate our approach with rigorous experiments on two demanding planning tasks: Game of 24 and Mini Crosswords. The empirical evidence underscores TouT's superiority over both ToT and chain-of-thought prompting methods. ","[{'version': 'v1', 'created': 'Thu, 14 Sep 2023 13:14:51 GMT'}]",2023-09-15,"[['Mo', 'Shentong', ''], ['Xin', 'Miao', '']]",0,0,2023-09-14,1,2,3,0,0,0,875d71bae61a66f7e65a2b6d363b7a0a27a6ed25,261822536.0,https://www.semanticscholar.org/paper/875d71bae61a66f7e65a2b6d363b7a0a27a6ed25,arXiv.org,2023.0,16.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2066123456', 'name': 'Shentong Mo'}, {'authorId': '1888678', 'name': 'Miao Xin'}]","['Carnegie Mellon University', 'Chinese Academy of Sciences', 'Mohamed bin Zayed University of Artificial Intelligence']","['China', 'United States', 'United Arab Emirates']",2023-09
2309.07915,HaoZhe Zhao,"Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang
  Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, Baobao Chang",MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning,"Code, dataset, checkpoints, and demos are available at
  https://github.com/PKUnlp-icler/MIC",,,,cs.CL cs.AI cs.CV,http://creativecommons.org/licenses/by-sa/4.0/,"  Since the resurgence of deep learning, vision-language models (VLMs) enhanced by large language models (LLMs) have grown exponentially in popularity. However, while LLMs can utilize extensive background knowledge and task information with in-context learning, most VLMs still struggle with understanding complex multi-modal prompts with multiple images, making VLMs less effective in downstream vision-language tasks. In this paper, we address the limitation above by 1) introducing MMICL, a new approach to allow the VLM to deal with multi-modal inputs efficiently; 2) proposing a novel context scheme to augment the in-context learning ability of the VLM; 3) constructing the Multi-modal In-Context Learning (MIC) dataset, designed to enhance the VLM's ability to understand complex multi-modal prompts. Our experiments confirm that MMICL achieves new state-of-the-art zero-shot performance on a wide range of general vision-language tasks, especially for complex benchmarks, including MME and MMBench. Our analysis demonstrates that MMICL effectively tackles the challenge of complex multi-modal prompt understanding and emerges the impressive ICL ability. Furthermore, we observe that MMICL successfully alleviates language bias in VLMs, a common issue for VLMs that often leads to hallucination when faced with extensive textual context. ","[{'version': 'v1', 'created': 'Thu, 14 Sep 2023 17:59:17 GMT'}, {'version': 'v2', 'created': 'Mon, 2 Oct 2023 14:46:01 GMT'}]",2023-10-03,"[['Zhao', 'Haozhe', ''], ['Cai', 'Zefan', ''], ['Si', 'Shuzheng', ''], ['Ma', 'Xiaojian', ''], ['An', 'Kaikai', ''], ['Chen', 'Liang', ''], ['Liu', 'Zixuan', ''], ['Wang', 'Sheng', ''], ['Han', 'Wenjuan', ''], ['Chang', 'Baobao', '']]",0,0,2023-09-14,2,10,3,0,0,0,3803d1f291e162bdaa4678a2c5a2bbcf63c050f4,261823391.0,https://www.semanticscholar.org/paper/3803d1f291e162bdaa4678a2c5a2bbcf63c050f4,arXiv.org,2023.0,120.0,10.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2112675144', 'name': 'Haozhe Zhao'}, {'authorId': '2117632647', 'name': 'Zefan Cai'}, {'authorId': '2053739525', 'name': 'Shuzheng Si'}, {'authorId': '2241105586', 'name': 'Xiaojian Ma'}, {'authorId': '2240549884', 'name': 'Kaikai An'}, {'authorId': '2240946204', 'name': 'Liang Chen'}, {'authorId': '46271003', 'name': 'Zixuan Liu'}, {'authorId': '47673176', 'name': 'Sheng Wang'}, {'authorId': '2240787926', 'name': 'Wenjuan Han'}, {'authorId': '39488576', 'name': 'Baobao Chang'}]","['Peking University', 'University of Washington', 'Beijing Jiaotong University', 'The University of Tokyo']","['China', 'United States', 'Japan']",2023-09
2309.07918,Zeqi Xiao,"Zeqi Xiao, Tai Wang, Jingbo Wang, Jinkun Cao, Wenwei Zhang, Bo Dai,
  Dahua Lin, Jiangmiao Pang",Unified Human-Scene Interaction via Prompted Chain-of-Contacts,"A unified Human-Scene Interaction framework that supports versatile
  interactions through language commands.Project URL:
  https://xizaoqu.github.io/unihsi/ . Code:
  https://github.com/OpenRobotLab/UniHSI",,,,cs.CV,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Human-Scene Interaction (HSI) is a vital component of fields like embodied AI and virtual reality. Despite advancements in motion quality and physical plausibility, two pivotal factors, versatile interaction control and the development of a user-friendly interface, require further exploration before the practical application of HSI. This paper presents a unified HSI framework, UniHSI, which supports unified control of diverse interactions through language commands. This framework is built upon the definition of interaction as Chain of Contacts (CoC): steps of human joint-object part pairs, which is inspired by the strong correlation between interaction types and human-object contact regions. Based on the definition, UniHSI constitutes a Large Language Model (LLM) Planner to translate language prompts into task plans in the form of CoC, and a Unified Controller that turns CoC into uniform task execution. To facilitate training and evaluation, we collect a new dataset named ScenePlan that encompasses thousands of task plans generated by LLMs based on diverse scenarios. Comprehensive experiments demonstrate the effectiveness of our framework in versatile task execution and generalizability to real scanned scenes. The project page is at https://github.com/OpenRobotLab/UniHSI . ","[{'version': 'v1', 'created': 'Thu, 14 Sep 2023 17:59:49 GMT'}, {'version': 'v2', 'created': 'Sun, 17 Sep 2023 14:43:09 GMT'}]",2023-09-19,"[['Xiao', 'Zeqi', ''], ['Wang', 'Tai', ''], ['Wang', 'Jingbo', ''], ['Cao', 'Jinkun', ''], ['Zhang', 'Wenwei', ''], ['Dai', 'Bo', ''], ['Lin', 'Dahua', ''], ['Pang', 'Jiangmiao', '']]",0,0,2023-09-14,2,8,1,0,0,0,dca4ed6d4db18216796336d647f8d4bdf197f039,261822943.0,https://www.semanticscholar.org/paper/dca4ed6d4db18216796336d647f8d4bdf197f039,arXiv.org,2023.0,43.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2241095971', 'name': 'Zeqi Xiao'}, {'authorId': '2257008641', 'name': 'Tai Wang'}, {'authorId': '2240784290', 'name': 'Jingbo Wang'}, {'authorId': '23590685', 'name': 'Jinkun Cao'}, {'authorId': '2240771481', 'name': 'Wenwei Zhang'}, {'authorId': '144445937', 'name': 'Bo Dai'}, {'authorId': '2237091231', 'name': 'Dahua Lin'}, {'authorId': '49968574', 'name': 'Jiangmiao Pang'}]","['Shanghai Artificial Intelligence Laboratory', 'Carnegie Mellon University', 'Nanyang Technological University']","['China', 'United States', 'Singapore']",2023-09
2309.08112,Yulin Chen,"Yulin Chen, Ning Ding, Hai-Tao Zheng, Zhiyuan Liu, Maosong Sun, Bowen
  Zhou",Empowering Private Tutoring by Chaining Large Language Models,,,,,cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Artificial intelligence has been applied in various aspects of online education to facilitate teaching and learning. However, few approaches has been made toward a complete AI-powered tutoring system. In this work, we explore the development of a full-fledged intelligent tutoring system powered by state-of-the-art large language models (LLMs), covering automatic course planning and adjusting, tailored instruction, and flexible quiz evaluation. To make the system robust to prolonged interaction and cater to individualized education, the system is decomposed into three inter-connected core processes-interaction, reflection, and reaction. Each process is implemented by chaining LLM-powered tools along with dynamically updated memory modules. Tools are LLMs prompted to execute one specific task at a time, while memories are data storage that gets updated during education process. Statistical results from learning logs demonstrate the effectiveness and mechanism of each tool usage. Subjective feedback from human users reveal the usability of each function, and comparison with ablation systems further testify the benefits of the designed processes in long-term interaction. ","[{'version': 'v1', 'created': 'Fri, 15 Sep 2023 02:42:03 GMT'}]",2023-09-18,"[['Chen', 'Yulin', ''], ['Ding', 'Ning', ''], ['Zheng', 'Hai-Tao', ''], ['Liu', 'Zhiyuan', ''], ['Sun', 'Maosong', ''], ['Zhou', 'Bowen', '']]",0,0,2023-09-15,1,6,1,0,0,0,f7842099bbde74dc5aec70bb6af85b88de08ed13,262012980.0,https://www.semanticscholar.org/paper/f7842099bbde74dc5aec70bb6af85b88de08ed13,arXiv.org,2023.0,46.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2135835258', 'name': 'Yulin Chen'}, {'authorId': '46649145', 'name': 'Ning Ding'}, {'authorId': '2242734235', 'name': 'Hai-Tao Zheng'}, {'authorId': '1390625267', 'name': 'Zhiyuan Liu'}, {'authorId': '1753344', 'name': 'Maosong Sun'}, {'authorId': '2218723159', 'name': 'Bowen Zhou'}]",['Tsinghua University'],['China'],2023-09
2309.08138,Hongcheng Wang,"Hongcheng Wang, Andy Guan Hong Chen, Xiaoqi Li, Mingdong Wu, Hao Dong",Find What You Want: Learning Demand-conditioned Object Attribute Space for Demand-driven Navigation,,,,,cs.RO cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The task of Visual Object Navigation (VON) involves an agent's ability to locate a particular object within a given scene. In order to successfully accomplish the VON task, two essential conditions must be fulfilled:1) the user must know the name of the desired object; and 2) the user-specified object must actually be present within the scene. To meet these conditions, a simulator can incorporate pre-defined object names and positions into the metadata of the scene. However, in real-world scenarios, it is often challenging to ensure that these conditions are always met. Human in an unfamiliar environment may not know which objects are present in the scene, or they may mistakenly specify an object that is not actually present. Nevertheless, despite these challenges, human may still have a demand for an object, which could potentially be fulfilled by other objects present within the scene in an equivalent manner. Hence, we propose Demand-driven Navigation (DDN), which leverages the user's demand as the task instruction and prompts the agent to find the object matches the specified demand. DDN aims to relax the stringent conditions of VON by focusing on fulfilling the user's demand rather than relying solely on predefined object categories or names. We propose a method first acquire textual attribute features of objects by extracting common knowledge from a large language model. These textual attribute features are subsequently aligned with visual attribute features using Contrastive Language-Image Pre-training (CLIP). By incorporating the visual attribute features as prior knowledge, we enhance the navigation process. Experiments on AI2Thor with the ProcThor dataset demonstrate the visual attribute features improve the agent's navigation performance and outperform the baseline methods commonly used in VON. ","[{'version': 'v1', 'created': 'Fri, 15 Sep 2023 04:07:57 GMT'}]",2023-09-18,"[['Wang', 'Hongcheng', ''], ['Chen', 'Andy Guan Hong', ''], ['Li', 'Xiaoqi', ''], ['Wu', 'Mingdong', ''], ['Dong', 'Hao', '']]",0,0,2023-09-15,1,5,2,0,0,0,18e73620979ac99d775251da435733661e549041,262013087.0,https://www.semanticscholar.org/paper/18e73620979ac99d775251da435733661e549041,arXiv.org,2023.0,71.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2157834018', 'name': 'Hongchen Wang'}, {'authorId': '2243179346', 'name': 'Andy Guan Hong Chen'}, {'authorId': '2108761355', 'name': 'Xiaoqi Li'}, {'authorId': '2238948679', 'name': 'Mingdong Wu'}, {'authorId': '2113411989', 'name': 'Hao Dong'}]",['Peking University'],['China'],2023-09
2309.08168,Jun Zhang,"Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, Sharad
  Mehrotra",Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  We present a novel inference scheme, self-speculative decoding, for accelerating Large Language Models (LLMs) without the need for an auxiliary model. This approach is characterized by a two-stage process: drafting and verification. The drafting stage generates draft tokens at a slightly lower quality but more quickly, which is achieved by selectively skipping certain intermediate layers during drafting Subsequently, the verification stage employs the original LLM to validate those draft output tokens in one forward pass. This process ensures the final output remains identical to that produced by the unaltered LLM, thereby maintaining output quality. The proposed method requires no additional neural network training and no extra memory footprint, making it a plug-and-play and cost-effective solution for inference acceleration. Benchmarks with LLaMA-2 and its fine-tuned models demonstrated a speedup up to 1.73$\times$. ","[{'version': 'v1', 'created': 'Fri, 15 Sep 2023 05:34:32 GMT'}]",2023-09-18,"[['Zhang', 'Jun', ''], ['Wang', 'Jue', ''], ['Li', 'Huan', ''], ['Shou', 'Lidan', ''], ['Chen', 'Ke', ''], ['Chen', 'Gang', ''], ['Mehrotra', 'Sharad', '']]",0,0,2023-09-15,1,7,1,1,1,0,8df524e0c50903d0b2c4be338081906d13ea42af,262013673.0,https://www.semanticscholar.org/paper/8df524e0c50903d0b2c4be338081906d13ea42af,arXiv.org,2023.0,45.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '27672597', 'name': 'Jinchao Zhang'}, {'authorId': '39597242', 'name': 'Jue Wang'}, {'authorId': '2242812507', 'name': 'Huan Li'}, {'authorId': '144128216', 'name': 'L. Shou'}, {'authorId': '32811782', 'name': 'Ke Chen'}, {'authorId': '2196572147', 'name': 'Gang Chen'}, {'authorId': '2242716456', 'name': 'Sharad Mehrotra'}]","['University of California, Irvine', 'Zhejiang University']","['China', 'United States']",2023-09
2309.08173,Linan Yue,"Linan Yue, Qi Liu, Yichao Du, Weibo Gao, Ye Liu, Fangzhou Yao",FedJudge: Federated Legal Large Language Model,Submitted to ICASSP 2024,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) have gained prominence in the field of Legal Intelligence, offering potential applications in assisting legal professionals and laymen. However, the centralized training of these Legal LLMs raises data privacy concerns, as legal data is distributed among various institutions containing sensitive individual information. This paper addresses this challenge by exploring the integration of Legal LLMs with Federated Learning (FL) methodologies. By employing FL, Legal LLMs can be fine-tuned locally on devices or clients, and their parameters are aggregated and distributed on a central server, ensuring data privacy without directly sharing raw data. However, computation and communication overheads hinder the full fine-tuning of LLMs under the FL setting. Moreover, the distribution shift of legal data reduces the effectiveness of FL methods. To this end, in this paper, we propose the first Federated Legal Large Language Model (FedJudge) framework, which fine-tunes Legal LLMs efficiently and effectively. Specifically, FedJudge utilizes parameter-efficient fine-tuning methods to update only a few additional parameters during the FL training. Besides, we explore the continual learning methods to preserve the global model's important parameters when training local clients to mitigate the problem of data shifts. Extensive experimental results on three real-world datasets clearly validate the effectiveness of FedJudge. Code is released at https://github.com/yuelinan/FedJudge. ","[{'version': 'v1', 'created': 'Fri, 15 Sep 2023 05:45:44 GMT'}]",2023-09-18,"[['Yue', 'Linan', ''], ['Liu', 'Qi', ''], ['Du', 'Yichao', ''], ['Gao', 'Weibo', ''], ['Liu', 'Ye', ''], ['Yao', 'Fangzhou', '']]",0,0,2023-09-15,1,6,1,0,0,0,55c481b17cfad6d1a8ce750e7f0b01b70f65d629,262012489.0,https://www.semanticscholar.org/paper/55c481b17cfad6d1a8ce750e7f0b01b70f65d629,arXiv.org,2023.0,22.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2118940856', 'name': 'Linan Yue'}, {'authorId': '2243408560', 'name': 'Qi Liu'}, {'authorId': '121526469', 'name': 'Yichao Du'}, {'authorId': '8361099', 'name': 'Weibo Gao'}, {'authorId': '2108335211', 'name': 'Ye Liu'}, {'authorId': '2242684882', 'name': 'Fangzhou Yao'}]","['University of Science and Technology of China', 'State Key Laboratory of Cognitive Intelligence']",['China'],2023-09
2309.08182,Jingzhe Ding,"Jingzhe Ding, Yan Cen, Xinyuan Wei",Using Large Language Model to Solve and Explain Physics Word Problems Approaching Human Level,"9 pages, 6 figures",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Our work demonstrates that large language model (LLM) pre-trained on texts can not only solve pure math word problems, but also physics word problems, whose solution requires calculation and inference based on prior physical knowledge. We collect and annotate the first physics word problem dataset-PhysQA, which contains over 1000 junior high school physics word problems (covering Kinematics, Mass&Density, Mechanics, Heat, Electricity). Then we use OpenAI' s GPT3.5 to generate the answer of these problems and found that GPT3.5 could automatically solve 49.3% of the problems through zero-shot learning and 73.2% through few-shot learning. This result demonstrates that by using similar problems and their answers as prompt, LLM could solve elementary physics word problems approaching human level performance. In addition to solving problems, GPT3.5 can also summarize the knowledge or topics covered by the problems, provide relevant explanations, and generate new physics word problems based on the input. Our work is the first research to focus on the automatic solving, explanation, and generation of physics word problems across various types and scenarios, and we achieve an acceptable and state-of-the-art accuracy. This underscores the potential of LLMs for further applications in secondary education. ","[{'version': 'v1', 'created': 'Fri, 15 Sep 2023 06:13:06 GMT'}, {'version': 'v2', 'created': 'Wed, 20 Sep 2023 07:08:53 GMT'}]",2023-09-21,"[['Ding', 'Jingzhe', ''], ['Cen', 'Yan', ''], ['Wei', 'Xinyuan', '']]",0,1,2023-09-15,2,3,2,1,0,1,1f8d74abcb89ee21bf01e7133cea503d8c99fef7,261960080.0,https://www.semanticscholar.org/paper/1f8d74abcb89ee21bf01e7133cea503d8c99fef7,arXiv.org,2023.0,24.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Physics', 'source': 's2-fos-model'}, {'category': 'Education', 'source': 's2-fos-model'}]","[{'authorId': '2244168236', 'name': 'Jingzhe Ding'}, {'authorId': '2242254524', 'name': 'Yan Cen'}, {'authorId': '2243749651', 'name': 'Xinyuan Wei'}]","['Fudan University', 'Columbia University']","['China', 'United States']",2023-09
2309.08221,Qi Guo,"Qi Guo (Tianjin University), Junming Cao (Fudan University), Xiaofei
  Xie (Singapore Management University), Shangqing Liu (Nanyang Technological
  University), Xiaohong Li (Tianjin University), Bihuan Chen (Fudan
  University), Xin Peng (Fudan University)",Exploring the Potential of ChatGPT in Automated Code Refinement: An Empirical Study,,,,,cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Code review is an essential activity for ensuring the quality and maintainability of software projects. However, it is a time-consuming and often error-prone task that can significantly impact the development process. Recently, ChatGPT, a cutting-edge language model, has demonstrated impressive performance in various natural language processing tasks, suggesting its potential to automate code review processes. However, it is still unclear how well ChatGPT performs in code review tasks. To fill this gap, in this paper, we conduct the first empirical study to understand the capabilities of ChatGPT in code review tasks, specifically focusing on automated code refinement based on given code reviews. To conduct the study, we select the existing benchmark CodeReview and construct a new code review dataset with high quality. We use CodeReviewer, a state-of-the-art code review tool, as a baseline for comparison with ChatGPT. Our results show that ChatGPT outperforms CodeReviewer in code refinement tasks. Specifically, our results show that ChatGPT achieves higher EM and BLEU scores of 22.78 and 76.44 respectively, while the state-of-the-art method achieves only 15.50 and 62.88 on a high-quality code review dataset. We further identify the root causes for ChatGPT's underperformance and propose several strategies to mitigate these challenges. Our study provides insights into the potential of ChatGPT in automating the code review process, and highlights the potential research directions. ","[{'version': 'v1', 'created': 'Fri, 15 Sep 2023 07:41:33 GMT'}]",2023-09-18,"[['Guo', 'Qi', '', 'Tianjin University'], ['Cao', 'Junming', '', 'Fudan University'], ['Xie', 'Xiaofei', '', 'Singapore Management University'], ['Liu', 'Shangqing', '', 'Nanyang Technological\n  University'], ['Li', 'Xiaohong', '', 'Tianjin University'], ['Chen', 'Bihuan', '', 'Fudan\n  University'], ['Peng', 'Xin', '', 'Fudan University']]",1,1,2023-09-15,1,7,1,1,0,1,d1b11a8f4e06eb377febc01ec3f4e4eb3345becf,261875437.0,https://www.semanticscholar.org/paper/d1b11a8f4e06eb377febc01ec3f4e4eb3345becf,arXiv.org,2023.0,45.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '51037101', 'name': 'Qianyu Guo'}, {'authorId': '2241242920', 'name': 'Junming Cao'}, {'authorId': '2241235435', 'name': 'Xiaofei Xie'}, {'authorId': '13877308', 'name': 'Shangqing Liu'}, {'authorId': '2118890600', 'name': 'Xiaohong Li'}, {'authorId': '144943089', 'name': 'Bihuan Chen'}, {'authorId': '2152488627', 'name': 'Xin Peng'}]","['Tianjin University', 'Fudan University', 'Nanyang Technological University', 'Singapore Management University']","['China', 'Singapore']",2023-09
2309.08448,Chan-Jan Hsu,"Chan-Jan Hsu, Chang-Le Liu, Feng-Ting Liao, Po-Chun Hsu, Yi-Chang
  Chen, Da-shan Shiu",Advancing the Evaluation of Traditional Chinese Language Models: Towards a Comprehensive Benchmark Suite,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The evaluation of large language models is an essential task in the field of language understanding and generation. As language models continue to advance, the need for effective benchmarks to assess their performance has become imperative. In the context of Traditional Chinese, there is a scarcity of comprehensive and diverse benchmarks to evaluate the capabilities of language models, despite the existence of certain benchmarks such as DRCD, TTQA, CMDQA, and FGC dataset. To address this gap, we propose a novel set of benchmarks that leverage existing English datasets and are tailored to evaluate language models in Traditional Chinese. These benchmarks encompass a wide range of tasks, including contextual question-answering, summarization, classification, and table understanding. The proposed benchmarks offer a comprehensive evaluation framework, enabling the assessment of language models' capabilities across different tasks. In this paper, we evaluate the performance of GPT-3.5, Taiwan-LLaMa-v1.0, and Model 7-C, our proprietary model, on these benchmarks. The evaluation results highlight that our model, Model 7-C, achieves performance comparable to GPT-3.5 with respect to a part of the evaluated capabilities. In an effort to advance the evaluation of language models in Traditional Chinese and stimulate further research in this field, we have open-sourced our benchmark and opened the model for trial. ","[{'version': 'v1', 'created': 'Fri, 15 Sep 2023 14:52:23 GMT'}, {'version': 'v2', 'created': 'Mon, 2 Oct 2023 15:22:42 GMT'}]",2023-10-03,"[['Hsu', 'Chan-Jan', ''], ['Liu', 'Chang-Le', ''], ['Liao', 'Feng-Ting', ''], ['Hsu', 'Po-Chun', ''], ['Chen', 'Yi-Chang', ''], ['Shiu', 'Da-shan', '']]",0,1,2023-09-15,2,6,1,2,1,1,16c42a500bff0af731ee505aae23aa4d2937fc91,262013507.0,https://www.semanticscholar.org/paper/16c42a500bff0af731ee505aae23aa4d2937fc91,arXiv.org,2023.0,20.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2148295431', 'name': 'Chan-Jan Hsu'}, {'authorId': '2211001638', 'name': 'Chang-Le Liu'}, {'authorId': '2242711123', 'name': 'Feng-Ting Liao'}, {'authorId': '2242906301', 'name': 'Po-Chun Hsu'}, {'authorId': '2242778881', 'name': 'Yi-Chang Chen'}, {'authorId': '2027686', 'name': 'D. Shiu'}]",['MediaTek (China)'],['China'],2023-09
2309.08532,Qingyan Guo,"Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan,
  Guoqing Liu, Jiang Bian, Yujiu Yang",Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers,Work in progress,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3.5 and Alpaca, on 9 datasets spanning language understanding and generation tasks. EvoPrompt significantly outperforms human-engineered prompts and existing methods for automatic prompt generation by up to 25% and 14% respectively. Furthermore, EvoPrompt demonstrates that connecting LLMs with EAs creates synergies, which could inspire further research on the combination of LLMs and conventional algorithms. ","[{'version': 'v1', 'created': 'Fri, 15 Sep 2023 16:50:09 GMT'}]",2023-09-18,"[['Guo', 'Qingyan', ''], ['Wang', 'Rui', ''], ['Guo', 'Junliang', ''], ['Li', 'Bei', ''], ['Song', 'Kaitao', ''], ['Tan', 'Xu', ''], ['Liu', 'Guoqing', ''], ['Bian', 'Jiang', ''], ['Yang', 'Yujiu', '']]",0,1,2023-09-15,1,9,2,2,0,2,8d17234680db76f99efd22fbcb169f45d2d79d93,262012566.0,https://www.semanticscholar.org/paper/8d17234680db76f99efd22fbcb169f45d2d79d93,arXiv.org,2023.0,69.0,13.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2658956', 'name': 'Qingyan Guo'}, {'authorId': '2243328016', 'name': 'Rui Wang'}, {'authorId': '13838086', 'name': 'Junliang Guo'}, {'authorId': '2242562942', 'name': 'Bei Li'}, {'authorId': '50982078', 'name': 'Kaitao Song'}, {'authorId': '48391466', 'name': 'Xu Tan'}, {'authorId': '2242776123', 'name': 'Guoqing Liu'}, {'authorId': '2192822005', 'name': 'Jiang Bian'}, {'authorId': '2242697141', 'name': 'Yujiu Yang'}, {'authorId': '2242588393', 'name': 'Tsinghua University'}, {'authorId': '68973648', 'name': 'Microsoft Research'}]","['Northeastern University', 'Tsinghua University', 'Microsoft']","['China', 'India']",2023-09
2309.08594,Cheng Qian,"Cheng Qian, Xinran Zhao, Sherry Tongshuang Wu","""Merge Conflicts!"" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) acquire extensive knowledge during pre-training, known as their parametric knowledge. However, in order to remain up-to-date and align with human instructions, LLMs inevitably require external knowledge during their interactions with users. This raises a crucial question: How will LLMs respond when external knowledge interferes with their parametric knowledge? To investigate this question, we propose a framework that systematically elicits LLM parametric knowledge and introduces external knowledge. Specifically, we uncover the impacts by constructing a parametric knowledge graph to reveal the different knowledge structures of LLMs, and introduce external knowledge through distractors of varying degrees, methods, positions, and formats. Our experiments on both black-box and open-source models demonstrate that LLMs tend to produce responses that deviate from their parametric knowledge, particularly when they encounter direct conflicts or confounding changes of information within detailed contexts. We also find that while LLMs are sensitive to the veracity of external knowledge, they can still be distracted by unrelated information. These findings highlight the risk of hallucination when integrating external knowledge, even indirectly, during interactions with current LLMs. All the data and results are publicly available. ","[{'version': 'v1', 'created': 'Fri, 15 Sep 2023 17:47:59 GMT'}]",2023-09-18,"[['Qian', 'Cheng', ''], ['Zhao', 'Xinran', ''], ['Wu', 'Sherry Tongshuang', '']]",0,0,2023-09-15,1,3,1,0,0,0,7bf607c643a1f7f4607db84dfc0f4b63ff0bde70,261875641.0,https://www.semanticscholar.org/paper/7bf607c643a1f7f4607db84dfc0f4b63ff0bde70,arXiv.org,2023.0,36.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2241207237', 'name': 'Cheng Qian'}, {'authorId': '2241234006', 'name': 'Xinran Zhao'}, {'authorId': '2241209809', 'name': 'Sherry Tongshuang Wu'}]","['Carnegie Mellon University', 'Tsinghua University']","['China', 'United States']",2023-09
2309.08637,Deng Cai,"Huayang Li and Siheng Li and Deng Cai and Longyue Wang and Lemao Liu
  and Taro Watanabe and Yujiu Yang and Shuming Shi",TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild,work in progress. https://textbind.github.io/,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large language models with instruction-following abilities have revolutionized the field of artificial intelligence. These models show exceptional generalizability to tackle various real-world tasks through their natural language interfaces. However, their performance heavily relies on high-quality exemplar data, which is often difficult to obtain. This challenge is further exacerbated when it comes to multimodal instruction following. We introduce TextBind, an almost annotation-free framework for empowering larger language models with the multi-turn interleaved multimodal instruction-following capabilities. Our approach requires only image-caption pairs and generates multi-turn multimodal instruction-response conversations from a language model. To accommodate interleaved image-text inputs and outputs, we devise MIM, a language model-centric architecture that seamlessly integrates image encoder and decoder models. We release our dataset, model, and demo to foster future research in the area of multimodal instruction following. ","[{'version': 'v1', 'created': 'Thu, 14 Sep 2023 15:34:01 GMT'}, {'version': 'v2', 'created': 'Tue, 19 Sep 2023 02:04:11 GMT'}]",2023-09-20,"[['Li', 'Huayang', ''], ['Li', 'Siheng', ''], ['Cai', 'Deng', ''], ['Wang', 'Longyue', ''], ['Liu', 'Lemao', ''], ['Watanabe', 'Taro', ''], ['Yang', 'Yujiu', ''], ['Shi', 'Shuming', '']]",0,0,2023-09-14,2,8,2,0,0,0,4eb87eaa193929dbef93fa2db9419245a8e8916f,261898073.0,https://www.semanticscholar.org/paper/4eb87eaa193929dbef93fa2db9419245a8e8916f,arXiv.org,2023.0,59.0,1.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '91956362', 'name': 'Huayang Li'}, {'authorId': '47319720', 'name': 'Siheng Li'}, {'authorId': '1724421', 'name': 'Deng Cai'}, {'authorId': '1800190', 'name': 'Longyue Wang'}, {'authorId': '2978364', 'name': 'Lemao Liu'}, {'authorId': '2110694221', 'name': 'Taro Watanabe'}, {'authorId': '2241653065', 'name': 'Yujiu Yang'}, {'authorId': '34720053', 'name': 'Shuming Shi'}]","['Tencent', 'Tsinghua University', 'Nara Institute of Science and Technology']","['China', 'Japan']",2023-09
2309.08939,Xichen Ding,"Yuqi Gong, Xichen Ding, Yehui Su, Kaiming Shen, Zhongyi Liu, Guannan
  Zhang",An Unified Search and Recommendation Foundation Model for Cold-Start Scenario,"CIKM 2023,6 pages",,10.1145/3583780.3614657,,cs.IR cs.AI,http://creativecommons.org/licenses/by/4.0/,"  In modern commercial search engines and recommendation systems, data from multiple domains is available to jointly train the multi-domain model. Traditional methods train multi-domain models in the multi-task setting, with shared parameters to learn the similarity of multiple tasks, and task-specific parameters to learn the divergence of features, labels, and sample distributions of individual tasks. With the development of large language models, LLM can extract global domain-invariant text features that serve both search and recommendation tasks. We propose a novel framework called S\&R Multi-Domain Foundation, which uses LLM to extract domain invariant features, and Aspect Gating Fusion to merge the ID feature, domain invariant text features and task-specific heterogeneous sparse features to obtain the representations of query and item. Additionally, samples from multiple search and recommendation scenarios are trained jointly with Domain Adaptive Multi-Task module to obtain the multi-domain foundation model. We apply the S\&R Multi-Domain foundation model to cold start scenarios in the pretrain-finetune manner, which achieves better performance than other SOTA transfer learning methods. The S\&R Multi-Domain Foundation model has been successfully deployed in Alipay Mobile Application's online services, such as content query recommendation and service card recommendation, etc. ","[{'version': 'v1', 'created': 'Sat, 16 Sep 2023 10:00:02 GMT'}]",2023-09-19,"[['Gong', 'Yuqi', ''], ['Ding', 'Xichen', ''], ['Su', 'Yehui', ''], ['Shen', 'Kaiming', ''], ['Liu', 'Zhongyi', ''], ['Zhang', 'Guannan', '']]",0,0,2023-09-16,1,6,2,0,0,0,e54c34d1d4b108a85c11bc68251cf2f215d953e7,262045057.0,https://www.semanticscholar.org/paper/e54c34d1d4b108a85c11bc68251cf2f215d953e7,International Conference on Information and Knowledge Management,2023.0,27.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2243273769', 'name': 'Yuqi Gong'}, {'authorId': '2243923570', 'name': 'Xichen Ding'}, {'authorId': '2243315594', 'name': 'Yehui Su'}, {'authorId': '2242792959', 'name': 'Kaiming Shen'}, {'authorId': '2243372613', 'name': 'Zhongyi Liu'}, {'authorId': '119557985', 'name': 'Guannan Zhang'}]",['Alibaba'],['China'],2023-09
2309.08952,Jiaan Wang,"Jiaan Wang, Yunlong Liang, Zengkui Sun, Yuxuan Cao, Jiarong Xu",Cross-Lingual Knowledge Editing in Large Language Models,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge editing aims to change language models' performance on several special cases (i.e., editing scope) by infusing the corresponding expected knowledge into them. With the recent advancements in large language models (LLMs), knowledge editing has been shown as a promising technique to adapt LLMs to new knowledge without retraining from scratch. However, most of the previous studies neglect the multi-lingual nature of some main-stream LLMs (e.g., LLaMA, ChatGPT and GPT-4), and typically focus on monolingual scenarios, where LLMs are edited and evaluated in the same language. As a result, it is still unknown the effect of source language editing on a different target language. In this paper, we aim to figure out this cross-lingual effect in knowledge editing. Specifically, we first collect a large-scale cross-lingual synthetic dataset by translating ZsRE from English to Chinese. Then, we conduct English editing on various knowledge editing methods covering different paradigms, and evaluate their performance in Chinese, and vice versa. To give deeper analyses of the cross-lingual effect, the evaluation includes four aspects, i.e., reliability, generality, locality and portability. Furthermore, we analyze the inconsistent behaviors of the edited models and discuss their specific challenges. ","[{'version': 'v1', 'created': 'Sat, 16 Sep 2023 11:07:52 GMT'}]",2023-09-19,"[['Wang', 'Jiaan', ''], ['Liang', 'Yunlong', ''], ['Sun', 'Zengkui', ''], ['Cao', 'Yuxuan', ''], ['Xu', 'Jiarong', '']]",1,1,2023-09-16,1,5,2,3,1,2,70ae98c70c0235acf3c3af14ea7286c81da1febe,261891487.0,https://www.semanticscholar.org/paper/70ae98c70c0235acf3c3af14ea7286c81da1febe,arXiv.org,2023.0,27.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2118328782', 'name': 'Jiaan Wang'}, {'authorId': '3389712', 'name': 'Yunlong Liang'}, {'authorId': '2241626414', 'name': 'Zengkui Sun'}, {'authorId': '144149886', 'name': 'Yu Cao'}, {'authorId': '2241538357', 'name': 'Jiarong Xu'}]","['Soochow University', 'Zhejiang University']",['China'],2023-09
2309.08968,Mojtaba Valipour,"Parsa Kavehzadeh, Mojtaba Valipour, Marzieh Tahaei, Ali Ghodsi, Boxing
  Chen, Mehdi Rezagholizadeh",Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large Language Models for Dynamic Inference Using Sorted Fine-Tuning (SoFT),,,,,cs.CL cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  The rapid advancement of large language models (LLMs) has revolutionized natural language processing (NLP). While these models excel at understanding and generating human-like text, their widespread deployment can be prohibitively expensive. SortedNet is a recent training technique for enabling dynamic inference for deep neural networks. It leverages network modularity to create sub-models with varying computational loads, sorting them based on computation/accuracy characteristics in a nested manner. We extend SortedNet to generative NLP tasks, making large language models dynamic without any pretraining and by only replacing standard Supervised Fine-Tuning (SFT) with Sorted Fine-Tuning (SoFT) at the same costs. Our approach boosts model efficiency, eliminating the need for multiple models for various scenarios during inference. We show that using this approach, we are able to unlock the potential of intermediate layers of transformers in generating the target output. Our sub-models remain integral components of the original model, minimizing storage requirements and transition costs between different computational/latency budgets. By applying this approach on LLaMa 2 13B for tuning on the Stanford Alpaca dataset and comparing it to normal tuning and early exit via PandaLM benchmark, we show that Sorted Fine-Tuning can deliver models twice as fast as the original model while maintaining or exceeding performance. ","[{'version': 'v1', 'created': 'Sat, 16 Sep 2023 11:58:34 GMT'}]",2023-09-19,"[['Kavehzadeh', 'Parsa', ''], ['Valipour', 'Mojtaba', ''], ['Tahaei', 'Marzieh', ''], ['Ghodsi', 'Ali', ''], ['Chen', 'Boxing', ''], ['Rezagholizadeh', 'Mehdi', '']]",0,0,2023-09-16,1,6,2,2,1,1,8f088f535419ea054f9cc6d073e53eed9715fe5a,261884642.0,https://www.semanticscholar.org/paper/8f088f535419ea054f9cc6d073e53eed9715fe5a,arXiv.org,2023.0,30.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2121286307', 'name': 'P. Kavehzadeh'}, {'authorId': '9200111', 'name': 'Mojtaba Valipour'}, {'authorId': '1996315', 'name': 'Marzieh S. Tahaei'}, {'authorId': '2237425782', 'name': 'Ali Ghodsi'}, {'authorId': '2237517964', 'name': 'Boxing Chen'}, {'authorId': '2066076226', 'name': 'Mehdi Rezagholizadeh'}]","['University of Waterloo', 'Huawei Technologies (China)']","['China', 'Canada']",2023-09
2309.09298,Hongcheng Guo,"Hongcheng Guo, Jian Yang, Jiaheng Liu, Liqun Yang, Linzheng Chai,
  Jiaqi Bai, Junran Peng, Xiaorong Hu, Chao Chen, Dongfeng Zhang, Xu Shi,
  Tieqiao Zheng, Liangfan Zheng, Bo Zhang, Ke Xu, Zhoujun Li",OWL: A Large Language Model for IT Operations,31 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the rapid development of IT operations, it has become increasingly crucial to efficiently manage and analyze large volumes of data for practical applications. The techniques of Natural Language Processing (NLP) have shown remarkable capabilities for various tasks, including named entity recognition, machine translation and dialogue systems. Recently, Large Language Models (LLMs) have achieved significant improvements across various NLP downstream tasks. However, there is a lack of specialized LLMs for IT operations. In this paper, we introduce the OWL, a large language model trained on our collected OWL-Instruct dataset with a wide range of IT-related information, where the mixture-of-adapter strategy is proposed to improve the parameter-efficient tuning across different domains or tasks. Furthermore, we evaluate the performance of our OWL on the OWL-Bench established by us and open IT-related benchmarks. OWL demonstrates superior performance results on IT tasks, which outperforms existing models by significant margins. Moreover, we hope that the findings of our work will provide more insights to revolutionize the techniques of IT operations with specialized LLMs. ","[{'version': 'v1', 'created': 'Sun, 17 Sep 2023 15:19:29 GMT'}]",2023-09-19,"[['Guo', 'Hongcheng', ''], ['Yang', 'Jian', ''], ['Liu', 'Jiaheng', ''], ['Yang', 'Liqun', ''], ['Chai', 'Linzheng', ''], ['Bai', 'Jiaqi', ''], ['Peng', 'Junran', ''], ['Hu', 'Xiaorong', ''], ['Chen', 'Chao', ''], ['Zhang', 'Dongfeng', ''], ['Shi', 'Xu', ''], ['Zheng', 'Tieqiao', ''], ['Zheng', 'Liangfan', ''], ['Zhang', 'Bo', ''], ['Xu', 'Ke', ''], ['Li', 'Zhoujun', '']]",0,0,2023-09-17,1,16,1,0,0,0,62b4e06f5249d22e4a153ec4a2dc934c6a014372,262043747.0,https://www.semanticscholar.org/paper/62b4e06f5249d22e4a153ec4a2dc934c6a014372,arXiv.org,2023.0,73.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2234806', 'name': 'Hongcheng Guo'}, {'authorId': '37081450', 'name': 'Jian Yang'}, {'authorId': '2108421213', 'name': 'Jiaheng Liu'}, {'authorId': '46554649', 'name': 'Liqun Yang'}, {'authorId': '2165382882', 'name': 'Linzheng Chai'}, {'authorId': '2107018151', 'name': 'Jiaqi Bai'}, {'authorId': '2243329942', 'name': 'Junran Peng'}, {'authorId': '2243804125', 'name': 'Xiaorong Hu'}, {'authorId': '2244169322', 'name': 'Chao Chen'}, {'authorId': '2243794847', 'name': 'Dongfeng Zhang'}, {'authorId': '2243024470', 'name': 'Xu Shi'}, {'authorId': '2152309872', 'name': 'Tieqiao Zheng'}, {'authorId': '2182735047', 'name': 'Liangfan Zheng'}, {'authorId': '2243445798', 'name': 'Bo Zhang'}, {'authorId': '2117101322', 'name': 'Ke Xu'}, {'authorId': '2144279674', 'name': 'Zhoujun Li'}]","['Beihang University', 'Cloudwise Research']",['China'],2023-09
2309.09466,YuTeng Ye,"YuTeng Ye, Jiale Cai, Hang Zhou, Guanwen Li, Youjia Zhang, Zikai Song,
  Chenxing Gao, Junqing Yu, Wei Yang",Progressive Text-to-Image Diffusion with Soft Latent Direction,"14 pages, 15 figures",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In spite of the rapidly evolving landscape of text-to-image generation, the synthesis and manipulation of multiple entities while adhering to specific relational constraints pose enduring challenges. This paper introduces an innovative progressive synthesis and editing operation that systematically incorporates entities into the target image, ensuring their adherence to spatial and relational constraints at each sequential step. Our key insight stems from the observation that while a pre-trained text-to-image diffusion model adeptly handles one or two entities, it often falters when dealing with a greater number. To address this limitation, we propose harnessing the capabilities of a Large Language Model (LLM) to decompose intricate and protracted text descriptions into coherent directives adhering to stringent formats. To facilitate the execution of directives involving distinct semantic operations-namely insertion, editing, and erasing-we formulate the Stimulus, Response, and Fusion (SRF) framework. Within this framework, latent regions are gently stimulated in alignment with each operation, followed by the fusion of the responsive latent components to achieve cohesive entity manipulation. Our proposed framework yields notable advancements in object synthesis, particularly when confronted with intricate and lengthy textual inputs. Consequently, it establishes a new benchmark for text-to-image generation tasks, further elevating the field's performance standards. ","[{'version': 'v1', 'created': 'Mon, 18 Sep 2023 04:01:25 GMT'}]",2023-09-19,"[['Ye', 'YuTeng', ''], ['Cai', 'Jiale', ''], ['Zhou', 'Hang', ''], ['Li', 'Guanwen', ''], ['Zhang', 'Youjia', ''], ['Song', 'Zikai', ''], ['Gao', 'Chenxing', ''], ['Yu', 'Junqing', ''], ['Yang', 'Wei', '']]",0,0,2023-09-18,1,9,1,0,0,0,ebd4d665ddd29e67961e3041574cb1ecf9d31336,262044099.0,https://www.semanticscholar.org/paper/ebd4d665ddd29e67961e3041574cb1ecf9d31336,arXiv.org,2023.0,40.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2192689522', 'name': 'Yuteng Ye'}, {'authorId': '2244145726', 'name': 'Jiale Cai'}, {'authorId': '2243737713', 'name': 'Hang Zhou'}, {'authorId': '2243556226', 'name': 'Guanwen Li'}, {'authorId': '2243000579', 'name': 'Youjia Zhang'}, {'authorId': '7576095', 'name': 'Zikai Song'}, {'authorId': '2243086273', 'name': 'Chenxing Gao'}, {'authorId': '2237947276', 'name': 'Junqing Yu'}, {'authorId': '2243650023', 'name': 'Wei Yang'}]",['Huazhong University of Science and Technology'],['China'],2023-09
2309.09506,Zecheng Tang,"Zecheng Tang, Chenfei Wu, Juntao Li, Nan Duan",LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language Models,,,,,cs.CV cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Graphic layout generation, a growing research field, plays a significant role in user engagement and information perception. Existing methods primarily treat layout generation as a numerical optimization task, focusing on quantitative aspects while overlooking the semantic information of layout, such as the relationship between each layout element. In this paper, we propose LayoutNUWA, the first model that treats layout generation as a code generation task to enhance semantic information and harness the hidden layout expertise of large language models~(LLMs). More concretely, we develop a Code Instruct Tuning (CIT) approach comprising three interconnected modules: 1) the Code Initialization (CI) module quantifies the numerical conditions and initializes them as HTML code with strategically placed masks; 2) the Code Completion (CC) module employs the formatting knowledge of LLMs to fill in the masked portions within the HTML code; 3) the Code Rendering (CR) module transforms the completed code into the final layout output, ensuring a highly interpretable and transparent layout generation procedure that directly maps code to a visualized layout. We attain significant state-of-the-art performance (even over 50\% improvements) on multiple datasets, showcasing the strong capabilities of LayoutNUWA. Our code is available at https://github.com/ProjectNUWA/LayoutNUWA. ","[{'version': 'v1', 'created': 'Mon, 18 Sep 2023 06:35:10 GMT'}, {'version': 'v2', 'created': 'Tue, 19 Sep 2023 07:47:52 GMT'}]",2023-09-20,"[['Tang', 'Zecheng', ''], ['Wu', 'Chenfei', ''], ['Li', 'Juntao', ''], ['Duan', 'Nan', '']]",0,0,2023-09-18,2,4,2,0,0,0,1ecf2955a2f4f2039f36b0334e2c376a5c901d6c,261898120.0,https://www.semanticscholar.org/paper/1ecf2955a2f4f2039f36b0334e2c376a5c901d6c,arXiv.org,2023.0,53.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Business', 'source': 's2-fos-model'}]","[{'authorId': '1576234850', 'name': 'Zecheng Tang'}, {'authorId': '2151101534', 'name': 'Chenfei Wu'}, {'authorId': '143959787', 'name': 'Juntao Li'}, {'authorId': '46429989', 'name': 'Nan Duan'}]","['Soochow University', 'Microsoft']",['China'],2023-09
2309.09507,Yupeng Ji,"Yupeng Ji, Yibo Cao, Jiucai Liu",Pruning Large Language Models via Accuracy Predictor,"6 pages, 4 figs",,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models(LLMs) containing tens of billions of parameters (or even more) have demonstrated impressive capabilities in various NLP tasks. However, substantial model size poses challenges to training, inference, and deployment so that it is necessary to compress the model. At present, most model compression for LLMs requires manual design of pruning features, which has problems such as complex optimization pipeline and difficulty in retaining the capabilities of certain parts of the model.Therefore, we propose a novel pruning approach: firstly, a training set of a certain number of architecture-accuracy pairs is established, and then a non-neural model is trained as an accuracy predictor. Using the accuracy predictor to further optimize the search space and search, the optimal model can be automatically selected. Experiments show that our proposed approach is effective and efficient. Compared with the baseline, the perplexity(PPL) on Wikitext2 and PTB dropped by 9.48% and 5,76% respectively, and the average accuracy of MMLU increased by 6.28%. ","[{'version': 'v1', 'created': 'Mon, 18 Sep 2023 06:38:24 GMT'}]",2023-09-19,"[['Ji', 'Yupeng', ''], ['Cao', 'Yibo', ''], ['Liu', 'Jiucai', '']]",0,0,2023-09-18,1,3,2,0,0,0,eaeb213ee78509436c2fbfcff0e171c2a19a36d7,262044180.0,https://www.semanticscholar.org/paper/eaeb213ee78509436c2fbfcff0e171c2a19a36d7,arXiv.org,2023.0,33.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2190588677', 'name': 'Yupeng Ji'}, {'authorId': '2243275640', 'name': 'Yibo Cao'}, {'authorId': '2129850472', 'name': 'Jiu-si Liu'}]","['Tongji University', 'Chongqing University']",['China'],2023-09
2309.09552,Yuang Li,"Yuang Li, Yinglu Li, Min Zhang, Chang Su, Mengyao Piao, Xiaosong Qiao,
  Jiawei Yu, Miaomiao Ma, Yanqing Zhao, Hao Yang",CB-Whisper: Contextual Biasing Whisper using TTS-based Keyword Spotting,"10 pages, 4 figures",,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  End-to-end automatic speech recognition (ASR) systems often struggle to recognize rare name entities, such as personal names, organizations, or technical terms that are not frequently encountered in the training data. This paper presents Contextual Biasing Whisper (CB-Whisper), a novel ASR system based on OpenAI's Whisper model that performs keyword-spotting (KWS) before the decoder. The KWS module leverages text-to-speech (TTS) techniques and a convolutional neural network (CNN) classifier to match the features between the entities and the utterances. Experiments demonstrate that by incorporating predicted entities into a carefully designed spoken form prompt, the mixed-error-rate (MER) and entity recall of the Whisper model is significantly improved on three internal datasets and two open-sourced datasets that cover English-only, Chinese-only, and code-switching scenarios. ","[{'version': 'v1', 'created': 'Mon, 18 Sep 2023 08:03:54 GMT'}]",2023-09-19,"[['Li', 'Yuang', ''], ['Li', 'Yinglu', ''], ['Zhang', 'Min', ''], ['Su', 'Chang', ''], ['Piao', 'Mengyao', ''], ['Qiao', 'Xiaosong', ''], ['Yu', 'Jiawei', ''], ['Ma', 'Miaomiao', ''], ['Zhao', 'Yanqing', ''], ['Yang', 'Hao', '']]",0,0,2023-09-18,1,10,2,0,0,0,7d7b9e31298cbe20c7dac20db690d04f826314e5,262045119.0,https://www.semanticscholar.org/paper/7d7b9e31298cbe20c7dac20db690d04f826314e5,arXiv.org,2023.0,23.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2243955116', 'name': 'Yuang Li'}, {'authorId': '2194427215', 'name': 'Yinglu Li'}, {'authorId': '40093418', 'name': 'Min Zhang'}, {'authorId': '2152082190', 'name': 'Chang Su'}, {'authorId': '2242967066', 'name': 'Mengyao Piao'}, {'authorId': '2165227179', 'name': 'Xiaosong Qiao'}, {'authorId': '2242886635', 'name': 'Jiawei Yu'}, {'authorId': '2178632208', 'name': 'Miaomiao Ma'}, {'authorId': '2132737', 'name': 'Yanqing Zhao'}, {'authorId': '2257352836', 'name': 'Hao Yang'}]",['Huawei Technologies (China)'],['China'],2023-09
2309.09558,Xiao Pu,"Xiao Pu, Mingqi Gao, Xiaojun Wan",Summarization is (Almost) Dead,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  How well can large language models (LLMs) generate summaries? We develop new datasets and conduct human evaluation experiments to evaluate the zero-shot generation capability of LLMs across five distinct summarization tasks. Our findings indicate a clear preference among human evaluators for LLM-generated summaries over human-written summaries and summaries generated by fine-tuned models. Specifically, LLM-generated summaries exhibit better factual consistency and fewer instances of extrinsic hallucinations. Due to the satisfactory performance of LLMs in summarization tasks (even surpassing the benchmark of reference summaries), we believe that most conventional works in the field of text summarization are no longer necessary in the era of LLMs. However, we recognize that there are still some directions worth exploring, such as the creation of novel datasets with higher quality and more reliable evaluation methods. ","[{'version': 'v1', 'created': 'Mon, 18 Sep 2023 08:13:01 GMT'}]",2023-09-19,"[['Pu', 'Xiao', ''], ['Gao', 'Mingqi', ''], ['Wan', 'Xiaojun', '']]",0,0,2023-09-18,1,3,1,0,0,0,aa0b3306f7dd827a6fb8487aeb39d832fdcb97a0,262044218.0,https://www.semanticscholar.org/paper/aa0b3306f7dd827a6fb8487aeb39d832fdcb97a0,arXiv.org,2023.0,36.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2243189215', 'name': 'Xiao Pu'}, {'authorId': '82340188', 'name': 'Mingqi Gao'}, {'authorId': '2262215991', 'name': 'Xiaojun Wan'}]",['Peking University'],['China'],2023-09
2309.09602,Conghui Niu,"Conghui Niu, Mengyang Hu, Lin Bo, Xiaoli He, Dong Yu, Pengyuan Liu",Proposition from the Perspective of Chinese Language: A Chinese Proposition Classification Evaluation Benchmark,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing propositions often rely on logical constants for classification. Compared with Western languages that lean towards hypotaxis such as English, Chinese often relies on semantic or logical understanding rather than logical connectives in daily expressions, exhibiting the characteristics of parataxis. However, existing research has rarely paid attention to this issue. And accurately classifying these propositions is crucial for natural language understanding and reasoning. In this paper, we put forward the concepts of explicit and implicit propositions and propose a comprehensive multi-level proposition classification system based on linguistics and logic. Correspondingly, we create a large-scale Chinese proposition dataset PEACE from multiple domains, covering all categories related to propositions. To evaluate the Chinese proposition classification ability of existing models and explore their limitations, We conduct evaluations on PEACE using several different methods including the Rule-based method, SVM, BERT, RoBERTA, and ChatGPT. Results show the importance of properly modeling the semantic features of propositions. BERT has relatively good proposition classification capability, but lacks cross-domain transferability. ChatGPT performs poorly, but its classification ability can be improved by providing more proposition information. Many issues are still far from being resolved and require further study. ","[{'version': 'v1', 'created': 'Mon, 18 Sep 2023 09:18:39 GMT'}]",2023-09-19,"[['Niu', 'Conghui', ''], ['Hu', 'Mengyang', ''], ['Bo', 'Lin', ''], ['He', 'Xiaoli', ''], ['Yu', 'Dong', ''], ['Liu', 'Pengyuan', '']]",1,1,2023-09-18,1,6,3,1,0,1,a806f6d93dd0d93de083429737597da31730f9f2,262044746.0,https://www.semanticscholar.org/paper/a806f6d93dd0d93de083429737597da31730f9f2,arXiv.org,2023.0,28.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2242910447', 'name': 'Conghui Niu'}, {'authorId': '2244131486', 'name': 'Mengyang Hu'}, {'authorId': '2242969600', 'name': 'Lin Bo'}, {'authorId': '2243911030', 'name': 'Xiaoli He'}, {'authorId': '144580031', 'name': 'Dong Yu'}, {'authorId': '39336958', 'name': 'Peng Liu'}]","['Beijing Language and Culture University', '.National Language Resources Monitoring and Research Center for Print Media']",['China'],2023-09
2309.09727,Yang Zhang,"Yang Zhang, Yufei Wang, Kai Wang, Quan Z. Sheng, Lina Yao, Adnan
  Mahmood, Wei Emma Zhang and Rongying Zhao",When Large Language Models Meet Citation: A Survey,,,,,cs.DL cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Citations in scholarly work serve the essential purpose of acknowledging and crediting the original sources of knowledge that have been incorporated or referenced. Depending on their surrounding textual context, these citations are used for different motivations and purposes. Large Language Models (LLMs) could be helpful in capturing these fine-grained citation information via the corresponding textual context, thereby enabling a better understanding towards the literature. Furthermore, these citations also establish connections among scientific papers, providing high-quality inter-document relationships and human-constructed knowledge. Such information could be incorporated into LLMs pre-training and improve the text representation in LLMs. Therefore, in this paper, we offer a preliminary review of the mutually beneficial relationship between LLMs and citation analysis. Specifically, we review the application of LLMs for in-text citation analysis tasks, including citation classification, citation-based summarization, and citation recommendation. We then summarize the research pertinent to leveraging citation linkage knowledge to improve text representations of LLMs via citation prediction, network structure information, and inter-document relationship. We finally provide an overview of these contemporary methods and put forth potential promising avenues in combining LLMs and citation analysis for further investigation. ","[{'version': 'v1', 'created': 'Mon, 18 Sep 2023 12:48:48 GMT'}]",2023-09-19,"[['Zhang', 'Yang', ''], ['Wang', 'Yufei', ''], ['Wang', 'Kai', ''], ['Sheng', 'Quan Z.', ''], ['Yao', 'Lina', ''], ['Mahmood', 'Adnan', ''], ['Zhang', 'Wei Emma', ''], ['Zhao', 'Rongying', '']]",0,0,2023-09-18,1,8,2,0,0,0,2c7bee70ce519330953ca29742230bda626faaca,262044956.0,https://www.semanticscholar.org/paper/2c7bee70ce519330953ca29742230bda626faaca,arXiv.org,2023.0,70.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2145956344', 'name': 'Yang Zhang'}, {'authorId': '46395829', 'name': 'Yufei Wang'}, {'authorId': '2148895945', 'name': 'Kai Wang'}, {'authorId': '1713128', 'name': 'Quan Z. Sheng'}, {'authorId': '2115514578', 'name': 'Lina Yao'}, {'authorId': '143945626', 'name': 'A. Mahmood'}, {'authorId': '1390725100', 'name': 'Wei Emma Zhang'}, {'authorId': '2052934', 'name': 'Rongying Zhao'}]","['UNSW Sydney', 'Data61', 'Wuhan University', 'Macquarie University', 'University of Adelaide', 'Nanyang Technological University']","['China', 'Singapore', 'Australia']",2023-09
2309.09749,Huachuan Qiu,"Huachuan Qiu, Shuai Zhang, Hongliang He, Anqi Li, Zhenzhong Lan",Facilitating NSFW Text Detection in Open-Domain Dialogue Systems via Knowledge Distillation,"Submitted to ICASSP 2024. Code and data are publicly available at
  https://github.com/qiuhuachuan/CensorChat",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  NSFW (Not Safe for Work) content, in the context of a dialogue, can have severe side effects on users in open-domain dialogue systems. However, research on detecting NSFW language, especially sexually explicit content, within a dialogue context has significantly lagged behind. To address this issue, we introduce CensorChat, a dialogue monitoring dataset aimed at NSFW dialogue detection. Leveraging knowledge distillation techniques involving GPT-4 and ChatGPT, this dataset offers a cost-effective means of constructing NSFW content detectors. The process entails collecting real-life human-machine interaction data and breaking it down into single utterances and single-turn dialogues, with the chatbot delivering the final utterance. ChatGPT is employed to annotate unlabeled data, serving as a training set. Rationale validation and test sets are constructed using ChatGPT and GPT-4 as annotators, with a self-criticism strategy for resolving discrepancies in labeling. A BERT model is fine-tuned as a text classifier on pseudo-labeled data, and its performance is assessed. The study emphasizes the importance of AI systems prioritizing user safety and well-being in digital conversations while respecting freedom of expression. The proposed approach not only advances NSFW content detection but also aligns with evolving user protection needs in AI-driven dialogues. ","[{'version': 'v1', 'created': 'Mon, 18 Sep 2023 13:24:44 GMT'}, {'version': 'v2', 'created': 'Tue, 19 Sep 2023 12:32:21 GMT'}]",2023-09-20,"[['Qiu', 'Huachuan', ''], ['Zhang', 'Shuai', ''], ['He', 'Hongliang', ''], ['Li', 'Anqi', ''], ['Lan', 'Zhenzhong', '']]",1,1,2023-09-18,2,5,1,2,0,2,2c677ae7db7aa46e7a77430d160af9e5a3c997ed,261898127.0,https://www.semanticscholar.org/paper/2c677ae7db7aa46e7a77430d160af9e5a3c997ed,arXiv.org,2023.0,20.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2106604080', 'name': 'Huachuan Qiu'}, {'authorId': '2108433500', 'name': 'Shuai Zhang'}, {'authorId': '144987681', 'name': 'Hongliang He'}, {'authorId': '2141519878', 'name': 'Anqi Li'}, {'authorId': '2362534', 'name': 'Zhenzhong Lan'}]","['Westlake University', 'Zhejiang University']",['China'],2023-09
2309.09809,Wentao Wan,"Wentao Wan, Zeqing Wang, Nan Kang, Keze Wang, Zhiyu Shen, Liang Lin",VisualProg Distiller: Learning to Fine-tune Non-differentiable Visual Programming Frameworks,"12 pages, 10 figures, 6 tables",,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As an interpretable and universal neuro-symbolic paradigm based on Large Language Models, visual programming (VisualProg) can execute compositional visual tasks without training, but its performance is markedly inferior compared to task-specific supervised learning models. To increase its practicality, the performance of VisualProg on specific tasks needs to be improved. However, the non-differentiability of VisualProg limits the possibility of employing the fine-tuning strategy on specific tasks to achieve further improvements. In our analysis, we discovered that significant performance issues in VisualProg's execution originated from errors made by the sub-modules at corresponding visual sub-task steps. To address this, we propose ``VisualProg Distiller"", a method of supplementing and distilling process knowledge to optimize the performance of each VisualProg sub-module on decoupled visual sub-tasks, thus enhancing the overall task performance. Specifically, we choose an end-to-end model that is well-performed on the given task as the teacher and further distill the knowledge of the teacher into the invoked visual sub-modules step-by-step based on the execution flow of the VisualProg-generated programs. In this way, our method is capable of facilitating the fine-tuning of the non-differentiable VisualProg frameworks effectively. Extensive and comprehensive experimental evaluations demonstrate that our method can achieve a substantial performance improvement of VisualProg, and outperforms all the compared state-of-the-art methods by large margins. Furthermore, to provide valuable process supervision for the GQA task, we construct a large-scale dataset by utilizing the distillation process of our method. ","[{'version': 'v1', 'created': 'Mon, 18 Sep 2023 14:28:47 GMT'}]",2023-09-19,"[['Wan', 'Wentao', ''], ['Wang', 'Zeqing', ''], ['Kang', 'Nan', ''], ['Wang', 'Keze', ''], ['Shen', 'Zhiyu', ''], ['Lin', 'Liang', '']]",0,0,2023-09-18,1,6,2,0,0,0,800c290ba10a6150f2364dcaf597d69fd5a25a4e,262044604.0,https://www.semanticscholar.org/paper/800c290ba10a6150f2364dcaf597d69fd5a25a4e,arXiv.org,2023.0,33.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2149237200', 'name': 'Wentao Wan'}, {'authorId': '2243319007', 'name': 'Zeqing Wang'}, {'authorId': '2242970017', 'name': 'Nan Kang'}, {'authorId': '3170394', 'name': 'Keze Wang'}, {'authorId': '2243374489', 'name': 'Zhiyu Shen'}, {'authorId': '2243325954', 'name': 'Liang Lin'}]","['Sun Yat-sen University', 'Central South University']",['China'],2023-09
2309.09825,Minjia Mao,"Xiao Fang, Shangkun Che, Minjia Mao, Hongzhe Zhang, Ming Zhao,
  Xiaohang Zhao",Bias of AI-Generated Content: An Examination of News Produced by Large Language Models,,,,,cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Large language models (LLMs) have the potential to transform our lives and work through the content they generate, known as AI-Generated Content (AIGC). To harness this transformation, we need to understand the limitations of LLMs. Here, we investigate the bias of AIGC produced by seven representative LLMs, including ChatGPT and LLaMA. We collect news articles from The New York Times and Reuters, both known for their dedication to provide unbiased news. We then apply each examined LLM to generate news content with headlines of these news articles as prompts, and evaluate the gender and racial biases of the AIGC produced by the LLM by comparing the AIGC and the original news articles. We further analyze the gender bias of each LLM under biased prompts by adding gender-biased messages to prompts constructed from these news headlines. Our study reveals that the AIGC produced by each examined LLM demonstrates substantial gender and racial biases. Moreover, the AIGC generated by each LLM exhibits notable discrimination against females and individuals of the Black race. Among the LLMs, the AIGC generated by ChatGPT demonstrates the lowest level of bias, and ChatGPT is the sole model capable of declining content generation when provided with biased prompts. ","[{'version': 'v1', 'created': 'Mon, 18 Sep 2023 14:47:24 GMT'}, {'version': 'v2', 'created': 'Tue, 19 Sep 2023 01:13:22 GMT'}]",2023-09-20,"[['Fang', 'Xiao', ''], ['Che', 'Shangkun', ''], ['Mao', 'Minjia', ''], ['Zhang', 'Hongzhe', ''], ['Zhao', 'Ming', ''], ['Zhao', 'Xiaohang', '']]",1,1,2023-09-18,2,6,1,2,1,1,da0b5feee06ae1564dd03623741273bb0ab939e2,261898112.0,https://www.semanticscholar.org/paper/da0b5feee06ae1564dd03623741273bb0ab939e2,Social Science Research Network,2023.0,35.0,1.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Sociology', 'source': 's2-fos-model'}]","[{'authorId': '2087230367', 'name': 'Xiao Fang'}, {'authorId': '2241599054', 'name': 'Shangkun Che'}, {'authorId': '2241598392', 'name': 'Minjia Mao'}, {'authorId': '2241774909', 'name': 'Hongzhe Zhang'}, {'authorId': '2242386091', 'name': 'Ming Zhao'}, {'authorId': '2108811727', 'name': 'Xiaohang Zhao'}]","['Shanghai University of Finance and Economics', 'Chinese University of Hong Kong, Shenzhen', 'Tsinghua University', 'University of Delaware']","['China', 'United States']",2023-09
2309.09826,Andr\'e Storhaug,"Andr\'e Storhaug, Jingyue Li, and Tianyuan Hu",Efficient Avoidance of Vulnerabilities in Auto-completed Smart Contract Code Using Vulnerability-constrained Decoding,"12 pages, 8 figures, 2 tables, 5 listings, accepted to the 34th IEEE
  International Symposium on Software Reliability Engineering (ISSRE 2023)",,,,cs.CR cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Auto-completing code enables developers to speed up coding significantly. Recent advances in transformer-based large language model (LLM) technologies have been applied to code synthesis. However, studies show that many of such synthesized codes contain vulnerabilities. We propose a novel vulnerability-constrained decoding approach to reduce the amount of vulnerable code generated by such models. Using a small dataset of labeled vulnerable lines of code, we fine-tune an LLM to include vulnerability labels when generating code, acting as an embedded classifier. Then, during decoding, we deny the model to generate these labels to avoid generating vulnerable code. To evaluate the method, we chose to automatically complete Ethereum Blockchain smart contracts (SCs) as the case study due to the strict requirements of SC security. We first fine-tuned the 6-billion-parameter GPT-J model using 186,397 Ethereum SCs after removing the duplication from 2,217,692 SCs. The fine-tuning took more than one week using ten GPUs. The results showed that our fine-tuned model could synthesize SCs with an average BLEU (BiLingual Evaluation Understudy) score of 0.557. However, many codes in the auto-completed SCs were vulnerable. Using the code before the vulnerable line of 176 SCs containing different types of vulnerabilities to auto-complete the code, we found that more than 70% of the auto-completed codes were insecure. Thus, we further fine-tuned the model on other 941 vulnerable SCs containing the same types of vulnerabilities and applied vulnerability-constrained decoding. The fine-tuning took only one hour with four GPUs. We then auto-completed the 176 SCs again and found that our approach could identify 62% of the code to be generated as vulnerable and avoid generating 67% of them, indicating the approach could efficiently and effectively avoid vulnerabilities in the auto-completed code. ","[{'version': 'v1', 'created': 'Mon, 18 Sep 2023 14:47:34 GMT'}]",2023-09-19,"[['Storhaug', 'Andr', ''], ['Li', 'Jingyue', ''], ['Hu', 'Tianyuan', '']]",0,1,2023-09-18,1,3,3,0,0,0,1454b9cbd908dcacea404fba9fe739726117eec9,262044680.0,https://www.semanticscholar.org/paper/1454b9cbd908dcacea404fba9fe739726117eec9,IEEE International Symposium on Software Reliability Engineering,2023.0,45.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2243188696', 'name': 'Andr Storhaug'}, {'authorId': '2242979473', 'name': 'Jingyue Li'}, {'authorId': '2242806045', 'name': 'Tianyuan Hu'}]","['Norwegian University of Science and Technology', 'Southeast University']","['China', 'Norway']",2023-09
2309.10202,Baolin Peng,"Baolin Peng and Linfeng Song and Ye Tian and Lifeng Jin and Haitao Mi
  and Dong Yu",Stabilizing RLHF through Advantage Model and Selective Rehearsal,"9 pages, working in progress",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) have revolutionized natural language processing, yet aligning these models with human values and preferences using RLHF remains a significant challenge. This challenge is characterized by various instabilities, such as reward hacking and catastrophic forgetting. In this technical report, we propose two innovations to stabilize RLHF training: 1) Advantage Model, which directly models advantage score i.e., extra reward compared to the expected rewards and regulates score distributions across tasks to prevent reward hacking. 2) Selective Rehearsal, which mitigates catastrophic forgetting by strategically selecting data for PPO training and knowledge rehearsing. Our experimental analysis on public and proprietary datasets reveals that the proposed methods not only increase stability in RLHF training but also achieve higher reward scores and win rates. ","[{'version': 'v1', 'created': 'Mon, 18 Sep 2023 23:06:32 GMT'}]",2023-09-20,"[['Peng', 'Baolin', ''], ['Song', 'Linfeng', ''], ['Tian', 'Ye', ''], ['Jin', 'Lifeng', ''], ['Mi', 'Haitao', ''], ['Yu', 'Dong', '']]",0,0,2023-09-18,1,6,2,0,0,0,0fb61be60088e80e565b84f44e49ba30630b6126,262054217.0,https://www.semanticscholar.org/paper/0fb61be60088e80e565b84f44e49ba30630b6126,arXiv.org,2023.0,35.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1780690', 'name': 'Baolin Peng'}, {'authorId': '1748796', 'name': 'Linfeng Song'}, {'authorId': '2243391254', 'name': 'Ye Tian'}, {'authorId': '2936180', 'name': 'Lifeng Jin'}, {'authorId': '2013337', 'name': 'Haitao Mi'}, {'authorId': '2239076081', 'name': 'Dong Yu'}]",['Tencent'],['China'],2023-09
2309.10238,Chenhao Tang,"Chenhao Tang, Zhengliang Liu, Chong Ma, Zihao Wu, Yiwei Li, Wei Liu,
  Dajiang Zhu, Quanzheng Li, Xiang Li, Tianming Liu, Lei Fan",PolicyGPT: Automated Analysis of Privacy Policies with Large Language Models,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Privacy policies serve as the primary conduit through which online service providers inform users about their data collection and usage procedures. However, in a bid to be comprehensive and mitigate legal risks, these policy documents are often quite verbose. In practical use, users tend to click the Agree button directly rather than reading them carefully. This practice exposes users to risks of privacy leakage and legal issues. Recently, the advent of Large Language Models (LLM) such as ChatGPT and GPT-4 has opened new possibilities for text analysis, especially for lengthy documents like privacy policies. In this study, we investigate a privacy policy text analysis framework PolicyGPT based on the LLM. This framework was tested using two datasets. The first dataset comprises of privacy policies from 115 websites, which were meticulously annotated by legal experts, categorizing each segment into one of 10 classes. The second dataset consists of privacy policies from 304 popular mobile applications, with each sentence manually annotated and classified into one of another 10 categories. Under zero-shot learning conditions, PolicyGPT demonstrated robust performance. For the first dataset, it achieved an accuracy rate of 97%, while for the second dataset, it attained an 87% accuracy rate, surpassing that of the baseline machine learning and neural network models. ","[{'version': 'v1', 'created': 'Tue, 19 Sep 2023 01:22:42 GMT'}]",2023-09-20,"[['Tang', 'Chenhao', ''], ['Liu', 'Zhengliang', ''], ['Ma', 'Chong', ''], ['Wu', 'Zihao', ''], ['Li', 'Yiwei', ''], ['Liu', 'Wei', ''], ['Zhu', 'Dajiang', ''], ['Li', 'Quanzheng', ''], ['Li', 'Xiang', ''], ['Liu', 'Tianming', ''], ['Fan', 'Lei', '']]",1,1,2023-09-19,1,11,1,2,0,2,437a386f3fe4b8c7449a37e2364412a26e0a478c,262054160.0,https://www.semanticscholar.org/paper/437a386f3fe4b8c7449a37e2364412a26e0a478c,arXiv.org,2023.0,32.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2243996656', 'name': 'Chenhao Tang'}, {'authorId': '2145977326', 'name': 'Zheng Liu'}, {'authorId': '120688117', 'name': 'Chong-Yi Ma'}, {'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '2111161355', 'name': 'Yiwei Li'}, {'authorId': '46641573', 'name': 'W. Liu'}, {'authorId': '2181182', 'name': 'Dajiang Zhu'}, {'authorId': '2210054417', 'name': 'Quanzheng Li'}, {'authorId': '113075935', 'name': 'Xiang Li'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}, {'authorId': '2244143682', 'name': 'Lei Fan'}]","['Mayo Clinic', 'Shanghai Jiao Tong University', 'Massachusetts General Hospital', 'The University of Texas at Arlington', 'University of Georgia', 'Northwestern Polytechnical University']","['China', 'United States']",2023-09
2309.10294,Ziyang Ma,"Ziyang Ma, Wen Wu, Zhisheng Zheng, Yiwei Guo, Qian Chen, Shiliang
  Zhang, Xie Chen","Leveraging Speech PTM, Text LLM, and Emotional TTS for Speech Emotion Recognition","This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible",,,,cs.CL cs.AI cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we explored how to boost speech emotion recognition (SER) with the state-of-the-art speech pre-trained model (PTM), data2vec, text generation technique, GPT-4, and speech synthesis technique, Azure TTS. First, we investigated the representation ability of different speech self-supervised pre-trained models, and we found that data2vec has a good representation ability on the SER task. Second, we employed a powerful large language model (LLM), GPT-4, and emotional text-to-speech (TTS) model, Azure TTS, to generate emotionally congruent text and speech. We carefully designed the text prompt and dataset construction, to obtain the synthetic emotional speech data with high quality. Third, we studied different ways of data augmentation to promote the SER task with synthetic speech, including random mixing, adversarial training, transfer learning, and curriculum learning. Experiments and ablation studies on the IEMOCAP dataset demonstrate the effectiveness of our method, compared with other data augmentation methods, and data augmentation with other synthetic data. ","[{'version': 'v1', 'created': 'Tue, 19 Sep 2023 03:52:01 GMT'}]",2023-09-20,"[['Ma', 'Ziyang', ''], ['Wu', 'Wen', ''], ['Zheng', 'Zhisheng', ''], ['Guo', 'Yiwei', ''], ['Chen', 'Qian', ''], ['Zhang', 'Shiliang', ''], ['Chen', 'Xie', '']]",0,1,2023-09-19,1,7,4,1,0,1,1f9e445ce2f6152c8aa966c904cc75e1c5612937,262053526.0,https://www.semanticscholar.org/paper/1f9e445ce2f6152c8aa966c904cc75e1c5612937,arXiv.org,2023.0,43.0,2.0,0.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2116609277', 'name': 'Ziyang Ma'}, {'authorId': '2242972894', 'name': 'Wen Wu'}, {'authorId': '6950445', 'name': 'Zhisheng Zheng'}, {'authorId': '2154593562', 'name': 'Yiwei Guo'}, {'authorId': '47261124', 'name': 'Qian Chen'}, {'authorId': '2241056109', 'name': 'Shiliang Zhang'}, {'authorId': '2190801121', 'name': 'Xie Chen'}]","['Alibaba', 'Shanghai Jiao Tong University', 'University of Cambridge', 'MoE Key Lab of Artificial Intelligence, AI Institute, X-LANCE Lab, Department of Computer Science and Engineering,']","['China', 'United Kingdom']",2023-09
2309.10444,Qiming Bao,"Qiming Bao, Juho Leinonen, Alex Yuxuan Peng, Wanjun Zhong, Tim
  Pistotti, Alice Huang, Paul Denny, Michael Witbrock and Jiamou Liu",Exploring Self-Reinforcement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models,Preprint. Under review,,,,cs.AI cs.CL,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Learnersourcing involves students generating and sharing learning resources with their peers. When learnersourcing multiple-choice questions, creating explanations for the generated questions is a crucial step as it facilitates a deeper understanding of the related concepts. However, it is often difficult for students to craft effective explanations due to limited subject understanding and a tendency to merely restate the question stem, distractors, and correct answer. To help scaffold this task, in this work we propose a self-reinforcement large-language-model framework, with the goal of generating and evaluating explanations automatically. Comprising three modules, the framework generates student-aligned explanations, evaluates these explanations to ensure their quality and iteratively enhances the explanations. If an explanation's evaluation score falls below a defined threshold, the framework iteratively refines and reassesses the explanation. Importantly, our framework emulates the manner in which students compose explanations at the relevant grade level. For evaluation, we had a human subject-matter expert compare the explanations generated by students with the explanations created by the open-source large language model Vicuna-13B, a version of Vicuna-13B that had been fine-tuned using our method, and by GPT-4. We observed that, when compared to other large language models, GPT-4 exhibited a higher level of creativity in generating explanations. We also found that explanations generated by GPT-4 were ranked higher by the human expert than both those created by the other models and the original student-created explanations. Our findings represent a significant advancement in enriching the learnersourcing experience for students and enhancing the capabilities of large language models in educational applications. ","[{'version': 'v1', 'created': 'Tue, 19 Sep 2023 09:04:15 GMT'}]",2023-09-20,"[['Bao', 'Qiming', ''], ['Leinonen', 'Juho', ''], ['Peng', 'Alex Yuxuan', ''], ['Zhong', 'Wanjun', ''], ['Pistotti', 'Tim', ''], ['Huang', 'Alice', ''], ['Denny', 'Paul', ''], ['Witbrock', 'Michael', ''], ['Liu', 'Jiamou', '']]",0,1,2023-09-19,1,9,2,2,1,1,cc09a2e51036b545e27507949758d8f567ba885b,262053922.0,https://www.semanticscholar.org/paper/cc09a2e51036b545e27507949758d8f567ba885b,arXiv.org,2023.0,19.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Education', 'source': 's2-fos-model'}]","[{'authorId': '1491516930', 'name': 'Qiming Bao'}, {'authorId': '34690956', 'name': 'Juho Leinonen'}, {'authorId': '2064347311', 'name': 'A. Peng'}, {'authorId': '2242929528', 'name': 'Wanjun Zhong'}, {'authorId': '2243195221', 'name': 'Tim Pistotti'}, {'authorId': '2242823034', 'name': 'Alice Huang'}, {'authorId': '2243041721', 'name': 'Paul Denny'}, {'authorId': '2819135', 'name': 'M. Witbrock'}, {'authorId': '1688665', 'name': 'J. Liu'}]","['University of Newcastle Australia', 'Sun Yat-sen University', 'University of Sydney', 'University of Auckland']","['China', 'New Zealand', 'Australia']",2023-09
2309.10447,Xin Zheng,"Xin Zheng, Hongyu Lin, Xianpei Han and Le Sun",Toward Unified Controllable Text Generation via Regular Expression Instruction,Accepted on IJCNLP-AACL 2023,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Controllable text generation is a fundamental aspect of natural language generation, with numerous methods proposed for different constraint types. However, these approaches often require significant architectural or decoding modifications, making them challenging to apply to additional constraints or resolve different constraint combinations. To address this, our paper introduces Regular Expression Instruction (REI), which utilizes an instruction-based mechanism to fully exploit regular expressions' advantages to uniformly model diverse constraints. Specifically, our REI supports all popular fine-grained controllable generation constraints, i.e., lexical, positional, and length, as well as their complex combinations, via regular expression-style instructions. Our method only requires fine-tuning on medium-scale language models or few-shot, in-context learning on large language models, and requires no further adjustment when applied to various constraint combinations. Experiments demonstrate that our straightforward approach yields high success rates and adaptability to various constraints while maintaining competitiveness in automatic metrics and outperforming most previous baselines. ","[{'version': 'v1', 'created': 'Tue, 19 Sep 2023 09:05:14 GMT'}, {'version': 'v2', 'created': 'Wed, 20 Sep 2023 02:18:06 GMT'}]",2023-09-21,"[['Zheng', 'Xin', ''], ['Lin', 'Hongyu', ''], ['Han', 'Xianpei', ''], ['Sun', 'Le', '']]",0,0,2023-09-19,2,4,2,0,0,0,711d28a63e084a013b9e566edf46ff6fffadf3b0,261953258.0,https://www.semanticscholar.org/paper/711d28a63e084a013b9e566edf46ff6fffadf3b0,arXiv.org,2023.0,69.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2143713430', 'name': 'Xin Zheng'}, {'authorId': '2116455765', 'name': 'Hongyu Lin'}, {'authorId': '2118233348', 'name': 'Xianpei Han'}, {'authorId': '2110832778', 'name': 'Le Sun'}]","['University of Chinese Academy of Sciences', 'Chinese Academy of Sciences', 'Chinese Information Processing Laboratory']",['China'],2023-09
2309.10654,Dawei Cheng,"Jiangtong Li, Yuxuan Bian, Guoxuan Wang, Yang Lei, Dawei Cheng, Zhijun
  Ding and Changjun Jiang",CFGPT: Chinese Financial Assistant with Large Language Model,"12 pages, 5 figures",,,,cs.CL cs.AI cs.CE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have demonstrated great potential in natural language processing tasks within the financial domain. In this work, we present a Chinese Financial Generative Pre-trained Transformer framework, named CFGPT, which includes a dataset~(CFData) for pre-training and supervised fine-tuning, a financial LLM~(CFLLM) to adeptly manage financial texts, and a deployment framework~(CFAPP) designed to navigate real-world financial applications. The CFData comprising both a pre-training dataset and a supervised fine-tuning dataset, where the pre-training dataset collates Chinese financial data and analytics, alongside a smaller subset of general-purpose text with 584M documents and 141B tokens in total, and the supervised fine-tuning dataset is tailored for six distinct financial tasks, embodying various facets of financial analysis and decision-making with 1.5M instruction pairs and 1.5B tokens in total. The CFLLM, which is based on InternLM-7B to balance the model capability and size, is trained on CFData in two stage, continued pre-training and supervised fine-tuning. The CFAPP is centered on large language models (LLMs) and augmented with additional modules to ensure multifaceted functionality in real-world application. Our codes are released at https://github.com/TongjiFinLab/CFGPT. ","[{'version': 'v1', 'created': 'Tue, 19 Sep 2023 14:34:01 GMT'}, {'version': 'v2', 'created': 'Fri, 22 Sep 2023 09:52:07 GMT'}]",2023-09-25,"[['Li', 'Jiangtong', ''], ['Bian', 'Yuxuan', ''], ['Wang', 'Guoxuan', ''], ['Lei', 'Yang', ''], ['Cheng', 'Dawei', ''], ['Ding', 'Zhijun', ''], ['Jiang', 'Changjun', '']]",0,1,2023-09-19,2,7,3,0,0,0,a9eb336485e148d0a3f5010693d7752facba2875,262054189.0,https://www.semanticscholar.org/paper/a9eb336485e148d0a3f5010693d7752facba2875,arXiv.org,2023.0,41.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2243033869', 'name': 'Jiangtong Li'}, {'authorId': '2243084925', 'name': 'Yuxuan Bian'}, {'authorId': '2243137231', 'name': 'Guoxuan Wang'}, {'authorId': '2243248115', 'name': 'Yang Lei'}, {'authorId': '2242942332', 'name': 'Dawei Cheng'}, {'authorId': '34368858', 'name': 'Zhijun Ding'}, {'authorId': '2243825234', 'name': 'Changjun Jiang'}]","['Shanghai Artificial Intelligence Laboratory', 'Tongji University']",['China'],2023-09
2309.10691,Xingyao Wang,"Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao
  Peng, Heng Ji",MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback,Code will be available at https://xingyaoww.github.io/mint-bench,,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools. However, current evaluation paradigms often focus solely on benchmark performance with single-turn exchanges, neglecting the intricate interactions among the user, LLMs, and external tools, creating a discrepancy between benchmark evaluation and real-world use cases. We introduce MINT benchmark to evaluate LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback. To ensure reproducibility, we provide an evaluation framework where LLMs can access tools by executing Python code and receive natural language feedback from the user simulated with GPT-4. We repurpose a diverse set of established datasets and tasks focusing on reasoning, coding, and decision-making and carefully curate them into a compact subset of instances for efficient evaluation. Our analysis of 20 open- and closed-source LLMs offers intriguing findings. (1) LLMs generally benefit from tool interactions and language feedback, with performance gains (absolute, same below) of 1--8% per additional turn with tool use and 2--17% with natural language feedback. (2) Better single-turn performance does not guarantee better multi-turn performance. (3) Surprisingly, on LLMs we evaluated, we found supervised instruction-finetuning (SIFT) and reinforcement learning from human feedback (RLHF) generally hurt multi-turn capabilities. We hope MINT can help measure progress and incentivize research in improving LLMs' capabilities in multi-turn interactions, especially for open-source communities where multi-turn human evaluation has been less accessible compared to commercial LLMs with a larger user base. ","[{'version': 'v1', 'created': 'Tue, 19 Sep 2023 15:25:42 GMT'}]",2023-09-20,"[['Wang', 'Xingyao', ''], ['Wang', 'Zihan', ''], ['Liu', 'Jiateng', ''], ['Chen', 'Yangyi', ''], ['Yuan', 'Lifan', ''], ['Peng', 'Hao', ''], ['Ji', 'Heng', '']]",0,1,2023-09-19,1,7,3,1,0,1,12b233752c7097ea6525622bed238ae2d2193c5a,262053695.0,https://www.semanticscholar.org/paper/12b233752c7097ea6525622bed238ae2d2193c5a,arXiv.org,2023.0,69.0,14.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2144803999', 'name': 'Xingyao Wang'}, {'authorId': '2243360876', 'name': 'Zihan Wang'}, {'authorId': '33456794', 'name': 'Jiateng Liu'}, {'authorId': '123331686', 'name': 'Yangyi Chen'}, {'authorId': '2152195191', 'name': 'Lifan Yuan'}, {'authorId': '1818378366', 'name': 'Hao Peng'}, {'authorId': '2243197103', 'name': 'Heng Ji'}]","['University of Illinois Urbana-Champaign', 'Renmin University of China']","['China', 'United States']",2023-09
2309.10706,Zecheng Tang,"Juntao Li, Zecheng Tang, Yuyang Ding, Pinzheng Wang, Pei Guo, Wangjie
  You, Dan Qiao, Wenliang Chen, Guohong Fu, Qiaoming Zhu, Guodong Zhou, Min
  Zhang",OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) with billions of parameters have demonstrated outstanding performance on various natural language processing tasks. This report presents OpenBA, an open-sourced 15B bilingual asymmetric seq2seq model, to contribute an LLM variant to the Chinese-oriented open-source model community. We enhance OpenBA with effective and efficient techniques as well as adopt a three-stage training strategy to train the model from scratch. Our solution can also achieve very competitive performance with only 380B tokens, which is better than LLaMA-70B on the BELEBELE benchmark, BLOOM-176B on the MMLU benchmark, GLM-130B on the C-Eval (hard) benchmark. This report provides the main details to pre-train an analogous model, including pre-training data processing, Bilingual Flan data collection, the empirical observations that inspire our model architecture design, training objectives of different stages, and other enhancement techniques. Additionally, we also provide the fine-tuning details of OpenBA on four downstream tasks. We have refactored our code to follow the design principles of the Huggingface Transformers Library, making it more convenient for developers to use, and released checkpoints of different training stages at https://huggingface.co/openBA. More details of our project are available at https://github.com/OpenNLG/openBA.git. ","[{'version': 'v1', 'created': 'Tue, 19 Sep 2023 15:46:40 GMT'}, {'version': 'v2', 'created': 'Sun, 1 Oct 2023 18:21:58 GMT'}]",2023-10-03,"[['Li', 'Juntao', ''], ['Tang', 'Zecheng', ''], ['Ding', 'Yuyang', ''], ['Wang', 'Pinzheng', ''], ['Guo', 'Pei', ''], ['You', 'Wangjie', ''], ['Qiao', 'Dan', ''], ['Chen', 'Wenliang', ''], ['Fu', 'Guohong', ''], ['Zhu', 'Qiaoming', ''], ['Zhou', 'Guodong', ''], ['Zhang', 'Min', '']]",0,0,2023-09-19,2,12,1,4,3,1,bc9f29881c1d93d225f0a74fa700531202c7043a,262053463.0,https://www.semanticscholar.org/paper/bc9f29881c1d93d225f0a74fa700531202c7043a,arXiv.org,2023.0,124.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '143959787', 'name': 'Juntao Li'}, {'authorId': '1576234850', 'name': 'Zecheng Tang'}, {'authorId': '2243263723', 'name': 'Yuyang Ding'}, {'authorId': '2011935736', 'name': 'Pinzheng Wang'}, {'authorId': '2192232942', 'name': 'Peiming Guo'}, {'authorId': '2242971459', 'name': 'Wangjie You'}, {'authorId': '2243234862', 'name': 'Dan Qiao'}, {'authorId': '2463750', 'name': 'Wenliang Chen'}, {'authorId': '2242974585', 'name': 'Guohong Fu'}, {'authorId': '2243490219', 'name': 'Qiaoming Zhu'}, {'authorId': '2243399657', 'name': 'Guodong Zhou'}, {'authorId': '50495870', 'name': 'M. Zhang'}]",['Soochow University'],['China'],2023-09
2309.10929,Ruiqi Xu,"Ruiqi Xu, Yongfeng Huang, Xin Chen, Lin Zhang",Specializing Small Language Models towards Complex Style Transfer via Latent Attribute Pre-Training,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  In this work, we introduce the concept of complex text style transfer tasks, and constructed complex text datasets based on two widely applicable scenarios. Our dataset is the first large-scale data set of its kind, with 700 rephrased sentences and 1,000 sentences from the game Genshin Impact. While large language models (LLM) have shown promise in complex text style transfer, they have drawbacks such as data privacy concerns, network instability, and high deployment costs. To address these issues, we explore the effectiveness of small models (less than T5-3B) with implicit style pre-training through contrastive learning. We also propose a method for automated evaluation of text generation quality based on alignment with human evaluations using ChatGPT. Finally, we compare our approach with existing methods and show that our model achieves state-of-art performances of few-shot text style transfer models. ","[{'version': 'v1', 'created': 'Tue, 19 Sep 2023 21:01:40 GMT'}]",2023-09-21,"[['Xu', 'Ruiqi', ''], ['Huang', 'Yongfeng', ''], ['Chen', 'Xin', ''], ['Zhang', 'Lin', '']]",1,1,2023-09-19,1,4,1,2,1,1,02f17e5918e4a803f81410901c1375fc323eccd2,262064711.0,https://www.semanticscholar.org/paper/02f17e5918e4a803f81410901c1375fc323eccd2,European Conference on Artificial Intelligence,2023.0,26.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2243405915', 'name': 'Ruiqi Xu'}, {'authorId': '2192183418', 'name': 'Y. Huang'}, {'authorId': '48283677', 'name': 'Xin Chen'}, {'authorId': '2243033035', 'name': 'Lin Zhang'}]","['Symbiotic Matrix', 'ORCID', 'Platinum AI Inc., China']","['China', 'United States']",2023-09
2309.11042,Chengyu Wang,"Yukang Xie, Chengyu Wang, Junbing Yan, Jiyong Zhou, Feiqi Deng, Jun
  Huang",Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, Large Language Models (LLMs) have achieved amazing zero-shot learning performance over a variety of Natural Language Processing (NLP) tasks, especially for text generative tasks. Yet, the large size of LLMs often leads to the high computational cost of model training and online deployment. In our work, we present ALTER, a system that effectively builds the multi-tAsk Learners with mixTure-of-task-adaptERs upon small language models (with <1B parameters) to address multiple NLP tasks simultaneously, capturing the commonalities and differences between tasks, in order to support domain-specific applications. Specifically, in ALTER, we propose the Mixture-of-Task-Adapters (MTA) module as an extension to the transformer architecture for the underlying model to capture the intra-task and inter-task knowledge. A two-stage training method is further proposed to optimize the collaboration between adapters at a small computational cost. Experimental results over a mixture of NLP tasks show that our proposed MTA architecture and the two-stage training method achieve good performance. Based on ALTER, we have also produced MTA-equipped language models for various domains. ","[{'version': 'v1', 'created': 'Wed, 20 Sep 2023 03:39:56 GMT'}]",2023-09-21,"[['Xie', 'Yukang', ''], ['Wang', 'Chengyu', ''], ['Yan', 'Junbing', ''], ['Zhou', 'Jiyong', ''], ['Deng', 'Feiqi', ''], ['Huang', 'Jun', '']]",0,0,2023-09-20,1,6,2,0,0,0,643b61ae46fd7a16b4b70e6768a430b1f11a902c,262066265.0,https://www.semanticscholar.org/paper/643b61ae46fd7a16b4b70e6768a430b1f11a902c,arXiv.org,2023.0,17.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2243365993', 'name': 'Yukang Xie'}, {'authorId': '121899912', 'name': 'Chengyu Wang'}, {'authorId': '2243406637', 'name': 'Junbing Yan'}, {'authorId': '2243387639', 'name': 'Jiyong Zhou'}, {'authorId': '2242943051', 'name': 'Feiqi Deng'}, {'authorId': '1697626', 'name': 'Jun Huang'}]","['Alibaba', 'South China University of Technology', 'East China Normal University']",['China'],2023-09
2309.11087,Pavan Holur,"Pavan Holur, K. C. Enevoldsen, Lajoyce Mboning, Thalia Georgiou,
  Louis-S. Bouchard, Matteo Pellegrini and Vwani Roychowdhury",Embed-Search-Align: DNA Sequence Alignment using Transformer Models,"17 pages, Tables 5, Figures 5, Under review, ICLR",,,,q-bio.GN cs.AI,http://creativecommons.org/licenses/by/4.0/,"  DNA sequence alignment involves assigning short DNA reads to the most probable locations on an extensive reference genome. This process is crucial for various genomic analyses, including variant calling, transcriptomics, and epigenomics. Conventional methods, refined over decades, tackle this challenge in two steps: genome indexing followed by efficient search to locate likely positions for given reads. Building on the success of Large Language Models (LLM) in encoding text into embeddings, where the distance metric captures semantic similarity, recent efforts have explored whether the same Transformer architecture can produce numerical representations for DNA sequences. Such models have shown early promise in tasks involving classification of short DNA sequences, such as the detection of coding vs non-coding regions, as well as the identification of enhancer and promoter sequences. Performance at sequence classification tasks does not, however, translate to sequence alignment, where it is necessary to conduct a genome-wide search to successfully align every read. We address this open problem by framing it as an Embed-Search-Align task. In this framework, a novel encoder model DNA-ESA generates representations of reads and fragments of the reference, which are projected into a shared vector space where the read-fragment distance is used as surrogate for alignment. In particular, DNA-ESA introduces: (1) Contrastive loss for self-supervised training of DNA sequence representations, facilitating rich sequence-level embeddings, and (2) a DNA vector store to enable search across fragments on a global scale. DNA-ESA is >97% accurate when aligning 250-length reads onto a human reference genome of 3 gigabases (single-haploid), far exceeds the performance of 6 recent DNA-Transformer model baselines and shows task transfer across chromosomes and species. ","[{'version': 'v1', 'created': 'Wed, 20 Sep 2023 06:30:39 GMT'}]",2023-09-21,"[['Holur', 'Pavan', ''], ['Enevoldsen', 'K. C.', ''], ['Mboning', 'Lajoyce', ''], ['Georgiou', 'Thalia', ''], ['Bouchard', 'Louis-S.', ''], ['Pellegrini', 'Matteo', ''], ['Roychowdhury', 'Vwani', '']]",0,0,2023-09-20,1,7,2,0,0,0,b8d14f9f62d91411add3911d7f52c0a20f542918,262065499.0,https://www.semanticscholar.org/paper/b8d14f9f62d91411add3911d7f52c0a20f542918,arXiv.org,2023.0,51.0,0.0,0.0,True,"['Biology', 'Computer Science']","[{'category': 'Biology', 'source': 'external'}, {'category': 'Computer Science', 'source': 'external'}, {'category': 'Biology', 'source': 's2-fos-model'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1645199194', 'name': 'Pavan Holur'}, {'authorId': '120365560', 'name': 'K. Enevoldsen'}, {'authorId': '2243288713', 'name': 'Lajoyce Mboning'}, {'authorId': '2243133195', 'name': 'Thalia Georgiou'}, {'authorId': '2243229561', 'name': 'Louis-S. Bouchard'}, {'authorId': '2243263422', 'name': 'Matteo Pellegrini'}, {'authorId': '1686063', 'name': 'V. Roychowdhury'}]","['University of California, Los Angeles', 'Center for Excellence in Molecular Cell Science', 'Aarhus University']","['China', 'United States', 'Denmark']",2023-09
2309.11166,Haoyu Wang,"Haoyu Wang, Guozheng Ma, Cong Yu, Ning Gui, Linrui Zhang, Zhiqi Huang,
  Suwei Ma, Yongzhe Chang, Sen Zhang, Li Shen, Xueqian Wang, Peilin Zhao,
  Dacheng Tao",Are Large Language Models Really Robust to Word-Level Perturbations?,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The swift advancement in the scales and capabilities of Large Language Models (LLMs) positions them as promising tools for a variety of downstream tasks. In addition to the pursuit of better performance and the avoidance of violent feedback on a certain prompt, to ensure the responsibility of the LLM, much attention is drawn to the robustness of LLMs. However, existing evaluation methods mostly rely on traditional question answering datasets with predefined supervised labels, which do not align with the superior generation capabilities of contemporary LLMs. To address this issue, we propose a novel rational evaluation approach that leverages pre-trained reward models as diagnostic tools to evaluate the longer conversation generated from more challenging open questions by LLMs, which we refer to as the Reward Model for Reasonable Robustness Evaluation (TREvaL). Longer conversations manifest the comprehensive grasp of language models in terms of their proficiency in understanding questions, a capability not entirely encompassed by individual words or letters, which may exhibit oversimplification and inherent biases. Our extensive empirical experiments demonstrate that TREvaL provides an innovative method for evaluating the robustness of an LLM. Furthermore, our results demonstrate that LLMs frequently exhibit vulnerability to word-level perturbations that are commonplace in daily language usage. Notably, we are surprised to discover that robustness tends to decrease as fine-tuning (SFT and RLHF) is conducted. The code of TREval is available in https://github.com/Harry-mic/TREvaL. ","[{'version': 'v1', 'created': 'Wed, 20 Sep 2023 09:23:46 GMT'}, {'version': 'v2', 'created': 'Wed, 27 Sep 2023 09:53:16 GMT'}]",2023-09-28,"[['Wang', 'Haoyu', ''], ['Ma', 'Guozheng', ''], ['Yu', 'Cong', ''], ['Gui', 'Ning', ''], ['Zhang', 'Linrui', ''], ['Huang', 'Zhiqi', ''], ['Ma', 'Suwei', ''], ['Chang', 'Yongzhe', ''], ['Zhang', 'Sen', ''], ['Shen', 'Li', ''], ['Wang', 'Xueqian', ''], ['Zhao', 'Peilin', ''], ['Tao', 'Dacheng', '']]",0,0,2023-09-20,2,13,2,0,0,0,57207b935fc3484d175f5e9e2980d73ca793f994,262064288.0,https://www.semanticscholar.org/paper/57207b935fc3484d175f5e9e2980d73ca793f994,arXiv.org,2023.0,44.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2256769475', 'name': 'Haoyu Wang'}, {'authorId': '2171444073', 'name': 'Guozheng Ma'}, {'authorId': '2243365066', 'name': 'Cong Yu'}, {'authorId': '2243275935', 'name': 'Ning Gui'}, {'authorId': '2243296871', 'name': 'Linrui Zhang'}, {'authorId': '2243609675', 'name': 'Zhiqi Huang'}, {'authorId': '2243370574', 'name': 'Suwei Ma'}, {'authorId': '3837093', 'name': 'Yongzhe Chang'}, {'authorId': '2243287503', 'name': 'Sen Zhang'}, {'authorId': '2144035454', 'name': 'Li Shen'}, {'authorId': '2243105430', 'name': 'Xueqian Wang'}, {'authorId': '2243527845', 'name': 'Peilin Zhao'}, {'authorId': '2135519749', 'name': 'Dacheng Tao'}]","['University of Sydney', 'Columbia University', 'Tencent', 'Tsinghua University', 'Jingdong']","['China', 'United States', 'Australia']",2023-09
2309.11206,Yike Wu,"Yike Wu, Nan Hu, Sheng Bi, Guilin Qi, Jie Ren, Anhuan Xie, Wei Song",Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite their competitive performance on knowledge-intensive tasks, large language models (LLMs) still have limitations in memorizing all world knowledge especially long tail knowledge. In this paper, we study the KG-augmented language model approach for solving the knowledge graph question answering (KGQA) task that requires rich world knowledge. Existing work has shown that retrieving KG knowledge to enhance LLMs prompting can significantly improve LLMs performance in KGQA. However, their approaches lack a well-formed verbalization of KG knowledge, i.e., they ignore the gap between KG representations and textual representations. To this end, we propose an answer-sensitive KG-to-Text approach that can transform KG knowledge into well-textualized statements most informative for KGQA. Based on this approach, we propose a KG-to-Text enhanced LLMs framework for solving the KGQA task. Experiments on several KGQA benchmarks show that the proposed KG-to-Text augmented LLMs approach outperforms previous KG-augmented LLMs approaches regarding answer accuracy and usefulness of knowledge statements. ","[{'version': 'v1', 'created': 'Wed, 20 Sep 2023 10:42:08 GMT'}, {'version': 'v2', 'created': 'Thu, 21 Sep 2023 04:43:46 GMT'}]",2023-09-22,"[['Wu', 'Yike', ''], ['Hu', 'Nan', ''], ['Bi', 'Sheng', ''], ['Qi', 'Guilin', ''], ['Ren', 'Jie', ''], ['Xie', 'Anhuan', ''], ['Song', 'Wei', '']]",0,0,2023-09-20,2,7,2,0,0,0,30f0abb793772c15f2cdfec97c994685348177c1,262054223.0,https://www.semanticscholar.org/paper/30f0abb793772c15f2cdfec97c994685348177c1,arXiv.org,2023.0,45.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '50118263', 'name': 'Yike Wu'}, {'authorId': '2056375411', 'name': 'Nan Hu'}, {'authorId': '2065970582', 'name': 'Sheng Bi'}, {'authorId': '1730054', 'name': 'G. Qi'}, {'authorId': '143702207', 'name': 'J. Ren'}, {'authorId': '2242906489', 'name': 'Anhuan Xie'}, {'authorId': '2243575849', 'name': 'Wei Song'}]","['Zhejiang Lab', 'Southeast University', 'Go Sheng', 'Guilin University']","['China', 'Kenya']",2023-09
2309.11235,Sijie Cheng,"Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, Yang Liu",OpenChat: Advancing Open-source Language Models with Mixed-Quality Data,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, which is lightweight and avoids costly human preference labeling. Through extensive experiments on three standard benchmarks, our openchat-13b fine-tuned with C-RLFT achieves the highest average performance among all 13b open-source language models. Moreover, we use AGIEval to validate the model generalization performance, in which only openchat-13b surpasses the base model. Finally, we conduct a series of analyses to shed light on the effectiveness and robustness of OpenChat. Our code, data, and models are publicly available at https://github.com/imoneoi/openchat. ","[{'version': 'v1', 'created': 'Wed, 20 Sep 2023 11:54:40 GMT'}]",2023-09-21,"[['Wang', 'Guan', ''], ['Cheng', 'Sijie', ''], ['Zhan', 'Xianyuan', ''], ['Li', 'Xiangang', ''], ['Song', 'Sen', ''], ['Liu', 'Yang', '']]",0,0,2023-09-20,1,6,1,1,1,0,29f032fc875576b5c3c6b1c2d76af8639bacfb88,262064307.0,https://www.semanticscholar.org/paper/29f032fc875576b5c3c6b1c2d76af8639bacfb88,arXiv.org,2023.0,40.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2243452789', 'name': 'Guan Wang'}, {'authorId': '2110844331', 'name': 'Sijie Cheng'}, {'authorId': '2242851906', 'name': 'Xianyuan Zhan'}, {'authorId': '2243283400', 'name': 'Xiangang Li'}, {'authorId': '2243314934', 'name': 'Sen Song'}, {'authorId': '40457423', 'name': 'Yang Liu'}]","['Shanghai Artificial Intelligence Laboratory', 'Tsinghua University']",['China'],2023-09
2309.11268,Bo Zhang,"Renqiu Xia, Bo Zhang, Haoyang Peng, Ning Liao, Peng Ye, Botian Shi,
  Junchi Yan, Yu Qiao","StructChart: Perception, Structuring, Reasoning for Visual Chart Understanding","SimChart9K is available for downloading at:
  https://github.com/UniModal4Reasoning/SimChart9K. 21 pages, 11 figures",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Charts are common in literature across different scientific fields, conveying rich information easily accessible to readers. Current chart-related tasks focus on either chart perception which refers to extracting information from the visual charts, or performing reasoning given the extracted data, e.g. in a tabular form. In this paper, we aim to establish a unified and label-efficient learning paradigm for joint perception and reasoning tasks, which can be generally applicable to different downstream tasks, beyond the question-answering task as specifically studied in peer works. Specifically, StructChart first reformulates the chart information from the popular tubular form (specifically linearized CSV) to the proposed Structured Triplet Representations (STR), which is more friendly for reducing the task gap between chart perception and reasoning due to the employed structured information extraction for charts. We then propose a Structuring Chart-oriented Representation Metric (SCRM) to quantitatively evaluate the performance for the chart perception task. To enrich the dataset for training, we further explore the possibility of leveraging the Large Language Model (LLM), enhancing the chart diversity in terms of both chart visual style and its statistical information. Extensive experiments are conducted on various chart-related tasks, demonstrating the effectiveness and promising potential for a unified chart perception-reasoning paradigm to push the frontier of chart understanding. ","[{'version': 'v1', 'created': 'Wed, 20 Sep 2023 12:51:13 GMT'}, {'version': 'v2', 'created': 'Mon, 25 Sep 2023 06:09:36 GMT'}]",2023-09-26,"[['Xia', 'Renqiu', ''], ['Zhang', 'Bo', ''], ['Peng', 'Haoyang', ''], ['Liao', 'Ning', ''], ['Ye', 'Peng', ''], ['Shi', 'Botian', ''], ['Yan', 'Junchi', ''], ['Qiao', 'Yu', '']]",0,0,2023-09-20,2,8,1,0,0,0,85b9fa14b5fa81e34ed760cbfd255cf7227b84ae,262067829.0,https://www.semanticscholar.org/paper/85b9fa14b5fa81e34ed760cbfd255cf7227b84ae,arXiv.org,2023.0,47.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Business', 'source': 's2-fos-model'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2239108883', 'name': 'Renqiu Xia'}, {'authorId': '2141897317', 'name': 'Bo Zhang'}, {'authorId': '49349645', 'name': 'Hao Peng'}, {'authorId': '2243285638', 'name': 'Ning Liao'}, {'authorId': '144449660', 'name': 'Peng Ye'}, {'authorId': '119700639', 'name': 'Botian Shi'}, {'authorId': '3063894', 'name': 'Junchi Yan'}, {'authorId': '145858545', 'name': 'Y. Qiao'}]","['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'Fudan University']",['China'],2023-09
2309.11325,Shengbin Yue,"Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen,
  Shujun Liu, Yuxuan Zhou, Yao Xiao, Song Yun, Xuanjing Huang, Zhongyu Wei",DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose DISC-LawLLM, an intelligent legal system utilizing large language models (LLMs) to provide a wide range of legal services. We adopt legal syllogism prompting strategies to construct supervised fine-tuning datasets in the Chinese Judicial domain and fine-tune LLMs with legal reasoning capability. We augment LLMs with a retrieval module to enhance models' ability to access and utilize external legal knowledge. A comprehensive legal benchmark, DISC-Law-Eval, is presented to evaluate intelligent legal systems from both objective and subjective dimensions. Quantitative and qualitative results on DISC-Law-Eval demonstrate the effectiveness of our system in serving various users across diverse legal scenarios. The detailed resources are available at https://github.com/FudanDISC/DISC-LawLLM. ","[{'version': 'v1', 'created': 'Wed, 20 Sep 2023 13:50:26 GMT'}, {'version': 'v2', 'created': 'Sat, 23 Sep 2023 18:36:21 GMT'}]",2023-09-26,"[['Yue', 'Shengbin', ''], ['Chen', 'Wei', ''], ['Wang', 'Siyuan', ''], ['Li', 'Bingxuan', ''], ['Shen', 'Chenchen', ''], ['Liu', 'Shujun', ''], ['Zhou', 'Yuxuan', ''], ['Xiao', 'Yao', ''], ['Yun', 'Song', ''], ['Huang', 'Xuanjing', ''], ['Wei', 'Zhongyu', '']]",0,0,2023-09-20,2,11,1,0,0,0,6806ecad90a778aaa7f6a3cd3a539582d823066c,262064568.0,https://www.semanticscholar.org/paper/6806ecad90a778aaa7f6a3cd3a539582d823066c,arXiv.org,2023.0,39.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Law', 'source': 's2-fos-model'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2243009270', 'name': 'Shengbin Yue'}, {'authorId': '2256716476', 'name': 'Wei Chen'}, {'authorId': '2116420560', 'name': 'Siyuan Wang'}, {'authorId': '2244122886', 'name': 'Bingxuan Li'}, {'authorId': '2244117903', 'name': 'Chenchen Shen'}, {'authorId': '2243135769', 'name': 'Shujun Liu'}, {'authorId': '2221195042', 'name': 'Yuxuan Zhou'}, {'authorId': '2243364423', 'name': 'Yao Xiao'}, {'authorId': '2243234513', 'name': 'Song Yun'}, {'authorId': '2243894924', 'name': 'Wei Lin'}, {'authorId': '1790227', 'name': 'Xuanjing Huang'}, {'authorId': '2712533', 'name': 'Zhongyu Wei'}]","['New York University Shanghai', 'Northwest University of Politics and Law', 'Huazhong University of Science and Technology', 'Fudan University']",['China'],2023-09
2309.11436,Zhuosheng Zhang,"Zhuosheng Zhang, Aston Zhang",You Only Look at Screens: Multimodal Chain-of-Action Agents,"21 pages, 10 figures",,,,cs.CL cs.AI cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Autonomous user interface (UI) agents aim to facilitate task automation by interacting with the user interface without manual intervention. Recent studies have investigated eliciting the capabilities of large language models (LLMs) for effective engagement in diverse environments. To align with the input-output requirement of LLMs, existing approaches are developed under a sandbox setting where they rely on external tools and application-specific APIs to parse the environment into textual elements and interpret the predicted actions. Consequently, those approaches often grapple with inference inefficiency and error propagation risks. To mitigate the challenges, we introduce Auto-UI, a multimodal solution that directly interacts with the interface, bypassing the need for environment parsing or reliance on application-dependent APIs. Moreover, we propose a chain-of-action technique -- leveraging a series of intermediate previous action histories and future action plans -- to help the agent decide what action to execute. We evaluate our approach on a new device-control benchmark AITW with 30K unique instructions, spanning multi-step tasks such as application operation, web searching, and web shopping. Experimental results show that Auto-UI achieves state-of-the-art performance with an action type prediction accuracy of 90% and an overall action success rate of 74%. Code is publicly available at https://github.com/cooelf/Auto-UI. ","[{'version': 'v1', 'created': 'Wed, 20 Sep 2023 16:12:32 GMT'}, {'version': 'v2', 'created': 'Thu, 21 Sep 2023 03:00:07 GMT'}]",2023-09-22,"[['Zhang', 'Zhuosheng', ''], ['Zhang', 'Aston', '']]",0,0,2023-09-20,2,2,3,0,0,0,681f12e89606511dc946d8d7a67bc1b4caab0544,262053313.0,https://www.semanticscholar.org/paper/681f12e89606511dc946d8d7a67bc1b4caab0544,arXiv.org,2023.0,44.0,2.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2242979648', 'name': 'Zhuosheng Zhang'}, {'authorId': '2244790002', 'name': 'Aston Zhang'}]",['Shanghai Jiao Tong University'],['China'],2023-09
2309.11478,Hanyi Wang,"Yuqian Sun, Hanyi Wang, Pok Man Chan, Morteza Tabibi, Yan Zhang, Huan
  Lu, Yuheng Chen, Chang Hee Lee, Ali Asadipour","Fictional Worlds, Real Connections: Developing Community Storytelling Social Chatbots through LLMs",,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We address the integration of storytelling and Large Language Models (LLMs) to develop engaging and believable Social Chatbots (SCs) in community settings. Motivated by the potential of fictional characters to enhance social interactions, we introduce Storytelling Social Chatbots (SSCs) and the concept of story engineering to transform fictional game characters into ""live"" social entities within player communities. Our story engineering process includes three steps: (1) Character and story creation, defining the SC's personality and worldview, (2) Presenting Live Stories to the Community, allowing the SC to recount challenges and seek suggestions, and (3) Communication with community members, enabling interaction between the SC and users. We employed the LLM GPT-3 to drive our SSC prototypes, ""David"" and ""Catherine,"" and evaluated their performance in an online gaming community, ""DE (Alias),"" on Discord. Our mixed-method analysis, based on questionnaires (N=15) and interviews (N=8) with community members, reveals that storytelling significantly enhances the engagement and believability of SCs in community settings. ","[{'version': 'v1', 'created': 'Wed, 20 Sep 2023 17:23:05 GMT'}]",2023-09-21,"[['Sun', 'Yuqian', ''], ['Wang', 'Hanyi', ''], ['Chan', 'Pok Man', ''], ['Tabibi', 'Morteza', ''], ['Zhang', 'Yan', ''], ['Lu', 'Huan', ''], ['Chen', 'Yuheng', ''], ['Lee', 'Chang Hee', ''], ['Asadipour', 'Ali', '']]",0,1,2023-09-20,1,9,1,1,0,1,09d8b3e9172ddd08ab6c1a6c7c8217cec438b6d5,262065244.0,https://www.semanticscholar.org/paper/09d8b3e9172ddd08ab6c1a6c7c8217cec438b6d5,arXiv.org,2023.0,25.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1666629650', 'name': 'Yuqian Sun'}, {'authorId': '2243661143', 'name': 'Hanyi Wang'}, {'authorId': '2242918586', 'name': 'Pok Man Chan'}, {'authorId': '2242892622', 'name': 'Morteza Tabibi'}, {'authorId': '2256617256', 'name': 'Yan Zhang'}, {'authorId': '2243367755', 'name': 'Huan Lu'}, {'authorId': '2047427816', 'name': 'Yuheng Chen'}, {'authorId': '2187449929', 'name': 'Chang Hee Lee'}, {'authorId': '2024403', 'name': 'A. Asadipour'}]","['Royal College of Art', 'Independent University', 'Independent, Hong Kong 4 rct.ai, United States', 'Independent, China', 'Korea Advanced Institute of Science and Technology']","['South Korea', 'Bangladesh', 'United States', 'United Kingdom', 'China']",2023-09
2309.11489,Tianbao Xie,"Tianbao Xie and Siheng Zhao and Chen Henry Wu and Yitao Liu and Qian
  Luo and Victor Zhong and Yanchao Yang and Tao Yu",Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning,"23 pages, 10 figures, update",,,,cs.LG cs.AI cs.CL cs.RO,http://creativecommons.org/licenses/by/4.0/,"  Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better task success rates and convergence speed than expert-written reward codes. For locomotion tasks, our method learns six novel locomotion behaviors with a success rate exceeding 94%. Furthermore, we show that the policies trained in the simulator with our method can be deployed in the real world. Finally, Text2Reward further improves the policies by refining their reward functions with human feedback. Video results are available at https://text-to-reward.github.io ","[{'version': 'v1', 'created': 'Wed, 20 Sep 2023 17:39:13 GMT'}, {'version': 'v2', 'created': 'Thu, 21 Sep 2023 15:17:09 GMT'}]",2023-09-22,"[['Xie', 'Tianbao', ''], ['Zhao', 'Siheng', ''], ['Wu', 'Chen Henry', ''], ['Liu', 'Yitao', ''], ['Luo', 'Qian', ''], ['Zhong', 'Victor', ''], ['Yang', 'Yanchao', ''], ['Yu', 'Tao', '']]",0,0,2023-09-20,2,8,4,0,0,0,2d8713cbccfed20bb250ebda6bd0643707c12f91,262053612.0,https://www.semanticscholar.org/paper/2d8713cbccfed20bb250ebda6bd0643707c12f91,arXiv.org,2023.0,61.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2057038673', 'name': 'Tianbao Xie'}, {'authorId': '2243033718', 'name': 'Siheng Zhao'}, {'authorId': '114621402', 'name': 'Chen Henry Wu'}, {'authorId': '2243302973', 'name': 'Yitao Liu'}, {'authorId': '2242962339', 'name': 'Qian Luo'}, {'authorId': '2243226284', 'name': 'Victor Zhong'}, {'authorId': '2243300847', 'name': 'Yanchao Yang'}, {'authorId': '2256865348', 'name': 'Tao Yu'}]","['Nanjing University', 'University of Waterloo', 'Carnegie Mellon University', 'Microsoft', 'University of Hong Kong']","['Canada', 'United States', 'China', 'Hong Kong']",2023-09
2309.11499,Runpei Dong,"Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong
  Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong,
  Xiangyu Zhang, Kaisheng Ma, Li Yi",DreamLLM: Synergistic Multimodal Comprehension and Creation,see project page at https://dreamllm.github.io/,,,,cs.CV cs.CL cs.LG,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. ","[{'version': 'v1', 'created': 'Wed, 20 Sep 2023 17:58:05 GMT'}]",2023-09-21,"[['Dong', 'Runpei', ''], ['Han', 'Chunrui', ''], ['Peng', 'Yuang', ''], ['Qi', 'Zekun', ''], ['Ge', 'Zheng', ''], ['Yang', 'Jinrong', ''], ['Zhao', 'Liang', ''], ['Sun', 'Jianjian', ''], ['Zhou', 'Hongyu', ''], ['Wei', 'Haoran', ''], ['Kong', 'Xiangwen', ''], ['Zhang', 'Xiangyu', ''], ['Ma', 'Kaisheng', ''], ['Yi', 'Li', '']]",0,0,2023-09-20,1,14,3,0,0,0,7b689adb8c156d6158660f90d1c86888ee281f63,261975252.0,https://www.semanticscholar.org/paper/7b689adb8c156d6158660f90d1c86888ee281f63,arXiv.org,2023.0,169.0,10.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2056965063', 'name': 'Runpei Dong'}, {'authorId': '2118643247', 'name': 'Chunrui Han'}, {'authorId': '2211415443', 'name': 'Yuang Peng'}, {'authorId': '3424017', 'name': 'Zekun Qi'}, {'authorId': '2242581956', 'name': 'Zheng Ge'}, {'authorId': '2161319190', 'name': 'Jinrong Yang'}, {'authorId': '48096671', 'name': 'Liang Zhao'}, {'authorId': '26913717', 'name': 'JianYuan Sun'}, {'authorId': '2157473950', 'name': 'Hongyu Zhou'}, {'authorId': '134085586', 'name': 'Hao-Ran Wei'}, {'authorId': '2242523047', 'name': 'Xiangwen Kong'}, {'authorId': '2185865433', 'name': 'Xiangyu Zhang'}, {'authorId': '2244131405', 'name': 'Kaisheng Ma'}, {'authorId': '2242612318', 'name': 'Li Yi'}]","['Tsinghua University', ""Xi'an Jiaotong University"", 'MEGVII Technology']",['China'],2023-09
2309.11751,Yinpeng Dong,"Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang,
  Yichi Zhang, Yu Tian, Hang Su, Jun Zhu",How Robust is Google's Bard to Adversarial Image Attacks?,Technical report,,,,cs.CV cs.AI cs.CR cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26% attack success rate against Bing Chat and a 86% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection and toxicity detection of images. We design corresponding attacks to evade these defenses, demonstrating that the current defenses of Bard are also vulnerable. We hope this work can deepen our understanding on the robustness of MLLMs and facilitate future research on defenses. Our code is available at https://github.com/thu-ml/Attack-Bard. ","[{'version': 'v1', 'created': 'Thu, 21 Sep 2023 03:24:30 GMT'}]",2023-09-22,"[['Dong', 'Yinpeng', ''], ['Chen', 'Huanran', ''], ['Chen', 'Jiawei', ''], ['Fang', 'Zhengwei', ''], ['Yang', 'Xiao', ''], ['Zhang', 'Yichi', ''], ['Tian', 'Yu', ''], ['Su', 'Hang', ''], ['Zhu', 'Jun', '']]",1,1,2023-09-21,1,9,4,1,0,1,9f4c17aebbb181756fab86ade02deadd90d5d4f9,262083772.0,https://www.semanticscholar.org/paper/9f4c17aebbb181756fab86ade02deadd90d5d4f9,arXiv.org,2023.0,64.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '3431029', 'name': 'Yinpeng Dong'}, {'authorId': '2185449429', 'name': 'Huanran Chen'}, {'authorId': '2154737261', 'name': 'Jiawei Chen'}, {'authorId': '2243460917', 'name': 'Zhengwei Fang'}, {'authorId': '48520620', 'name': 'X. Yang'}, {'authorId': '2243430472', 'name': 'Yichi Zhang'}, {'authorId': '2243374631', 'name': 'Yu Tian'}, {'authorId': '2093561216', 'name': 'Hang Su'}, {'authorId': '145254043', 'name': 'Jun Zhu'}]",['Tsinghua University'],['China'],2023-09
2309.11911,Shanglin Lei,"Shanglin Lei, Guanting Dong, Xiaoping Wang, Keheng Wang, Sirui Wang",InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework,,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  The development of emotion recognition in dialogue (ERC) has been consistently hindered by the complexity of pipeline designs, leading to ERC models that often overfit to specific datasets and dialogue patterns. In this study, we propose a novel approach, namely   InstructERC, to reformulates the ERC task from a discriminative framework to a generative framework based on Large Language Models (LLMs) . InstructERC has two significant contributions: Firstly, InstructERC introduces a simple yet effective retrieval template module, which helps the model explicitly integrate multi-granularity dialogue supervision information by concatenating the historical dialog content, label statement, and emotional domain demonstrations with high semantic similarity. Furthermore, we introduce two additional emotion alignment tasks, namely speaker identification and emotion prediction tasks, to implicitly model the dialogue role relationships and future emotional tendencies in conversations. Our LLM-based plug-and-play plugin framework significantly outperforms all previous models and achieves comprehensive SOTA on three commonly used ERC datasets. Extensive analysis of parameter-efficient and data-scaling experiments provide empirical guidance for applying InstructERC in practical scenarios. Our code will be released after blind review. ","[{'version': 'v1', 'created': 'Thu, 21 Sep 2023 09:22:07 GMT'}, {'version': 'v2', 'created': 'Fri, 22 Sep 2023 06:33:53 GMT'}]",2023-09-25,"[['Lei', 'Shanglin', ''], ['Dong', 'Guanting', ''], ['Wang', 'Xiaoping', ''], ['Wang', 'Keheng', ''], ['Wang', 'Sirui', '']]",0,0,2023-09-21,2,5,1,0,0,0,3506b3be3b2472adb748e5c2cc57c200d403d7b5,262084263.0,https://www.semanticscholar.org/paper/3506b3be3b2472adb748e5c2cc57c200d403d7b5,arXiv.org,2023.0,36.0,7.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2241611542', 'name': 'Shanglin Lei'}, {'authorId': '2241576592', 'name': 'Guanting Dong'}, {'authorId': '2241624787', 'name': 'Xiaoping Wang'}, {'authorId': '2234024810', 'name': 'Keheng Wang'}, {'authorId': '2244106962', 'name': 'Sirui Wang'}]","['Beijing University of Posts and Telecommunications', 'Beihang University', 'Huazhong University of Science and Technology', 'Meituan', 'Key Laboratory of Image Processing and Intelligent Control of Education Ministry of China, Wuhan 430074, China']",['China'],2023-09
2309.12053,Fei Yu,"Huang Huang, Fei Yu, Jianqing Zhu, Xuening Sun, Hao Cheng, Dingjie
  Song, Zhihong Chen, Abdulmohsen Alharthi, Bang An, Ziche Liu, Zhiyi Zhang,
  Junying Chen, Jianquan Li, Benyou Wang, Lian Zhang, Ruoyu Sun, Xiang Wan,
  Haizhou Li, Jinchao Xu","AceGPT, Localizing Large Language Models in Arabic",https://github.com/FreedomIntelligence/AceGPT,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  This paper explores the imperative need and methodology for developing a localized Large Language Model (LLM) tailored for Arabic, a language with unique cultural characteristics that are not adequately addressed by current mainstream models like ChatGPT. Key concerns additionally arise when considering cultural sensitivity and local values. To this end, the paper outlines a packaged solution, including further pre-training with Arabic texts, supervised fine-tuning (SFT) using native Arabic instructions and GPT-4 responses in Arabic, and reinforcement learning with AI feedback (RLAIF) using a reward model that is sensitive to local culture and values. The objective is to train culturally aware and value-aligned Arabic LLMs that can serve the diverse application-specific needs of Arabic-speaking communities.   Extensive evaluations demonstrated that the resulting LLM called `AceGPT' is the SOTA open Arabic LLM in various benchmarks, including instruction-following benchmark (i.e., Arabic Vicuna-80 and Arabic AlpacaEval), knowledge benchmark (i.e., Arabic MMLU and EXAMs), as well as the newly-proposed Arabic cultural \& value alignment benchmark. Notably, AceGPT outperforms ChatGPT in the popular Vicuna-80 benchmark when evaluated with GPT-4, despite the benchmark's limited scale. % Natural Language Understanding (NLU) benchmark (i.e., ALUE)   Codes, data, and models are in https://github.com/FreedomIntelligence/AceGPT. ","[{'version': 'v1', 'created': 'Thu, 21 Sep 2023 13:20:13 GMT'}, {'version': 'v2', 'created': 'Fri, 22 Sep 2023 12:34:15 GMT'}]",2023-09-25,"[['Huang', 'Huang', ''], ['Yu', 'Fei', ''], ['Zhu', 'Jianqing', ''], ['Sun', 'Xuening', ''], ['Cheng', 'Hao', ''], ['Song', 'Dingjie', ''], ['Chen', 'Zhihong', ''], ['Alharthi', 'Abdulmohsen', ''], ['An', 'Bang', ''], ['Liu', 'Ziche', ''], ['Zhang', 'Zhiyi', ''], ['Chen', 'Junying', ''], ['Li', 'Jianquan', ''], ['Wang', 'Benyou', ''], ['Zhang', 'Lian', ''], ['Sun', 'Ruoyu', ''], ['Wan', 'Xiang', ''], ['Li', 'Haizhou', ''], ['Xu', 'Jinchao', '']]",1,1,2023-09-21,2,19,1,3,1,2,859d9e9c77ef556fc6257c2a395e9edfcac3b775,262084244.0,https://www.semanticscholar.org/paper/859d9e9c77ef556fc6257c2a395e9edfcac3b775,arXiv.org,2023.0,40.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2243722384', 'name': 'Huang Huang'}, {'authorId': '40471592', 'name': 'Fei Yu'}, {'authorId': '1739258', 'name': 'Jianqing Zhu'}, {'authorId': '2243398169', 'name': 'Xuening Sun'}, {'authorId': '2243373701', 'name': 'Hao Cheng'}, {'authorId': '1610927763', 'name': 'Dingjie Song'}, {'authorId': '46843171', 'name': 'Zhihong Chen'}, {'authorId': '2243337134', 'name': 'Abdulmohsen Alharthi'}, {'authorId': '2243338660', 'name': 'Bang An'}, {'authorId': '2243875495', 'name': 'Ziche Liu'}, {'authorId': '2214794954', 'name': 'Zhiyi Zhang'}, {'authorId': '2108170007', 'name': 'Junying Chen'}, {'authorId': '2130169642', 'name': 'Jianquan Li'}, {'authorId': '2894465', 'name': 'Benyou Wang'}, {'authorId': '2146644453', 'name': 'Lian Zhang'}, {'authorId': '2243688647', 'name': 'Ruoyu Sun'}, {'authorId': '2101317304', 'name': 'Xiang Wan'}, {'authorId': '2218230700', 'name': 'Haizhou Li'}, {'authorId': '2243435091', 'name': 'Jinchao Xu'}]","['Chinese University of Hong Kong, Shenzhen', 'King Abdullah University of Science and Technology', 'Shenzhen Research Institute of Big Data']","['China', 'Saudi Arabia']",2023-09
2309.12109,Zhou Mingjun,"Zhou Mingjun, Daiqing Zhuoma, Qun Nuo, Nyima Tashi",PEFTT: Parameter-Efficient Fine-Tuning for low-resource Tibetan pre-trained language models,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this era of large language models (LLMs), the traditional training of models has become increasingly unimaginable for regular users and institutions. The exploration of efficient fine-tuning for high-resource languages on these models is an undeniable trend that is gradually gaining popularity. However, there has been very little exploration for various low-resource languages, such as Tibetan. Research in Tibetan NLP is inherently scarce and limited. While there is currently no existing large language model for Tibetan due to its low-resource nature, that day will undoubtedly arrive. Therefore, research on efficient fine-tuning for low-resource language models like Tibetan is highly necessary. Our research can serve as a reference to fill this crucial gap. Efficient fine-tuning strategies for pre-trained language models (PLMs) in Tibetan have seen minimal exploration. We conducted three types of efficient fine-tuning experiments on the publicly available TNCC-title dataset: ""prompt-tuning,"" ""Adapter lightweight fine-tuning,"" and ""prompt-tuning + Adapter fine-tuning."" The experimental results demonstrate significant improvements using these methods, providing valuable insights for advancing Tibetan language applications in the context of pre-trained models. ","[{'version': 'v1', 'created': 'Thu, 21 Sep 2023 14:29:23 GMT'}]",2023-09-22,"[['Mingjun', 'Zhou', ''], ['Zhuoma', 'Daiqing', ''], ['Nuo', 'Qun', ''], ['Tashi', 'Nyima', '']]",0,0,2023-09-21,1,4,2,0,0,0,59e78d1d40281f6d5ff5d7dfcfa9a202edabbf22,262084288.0,https://www.semanticscholar.org/paper/59e78d1d40281f6d5ff5d7dfcfa9a202edabbf22,arXiv.org,2023.0,31.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2243488579', 'name': 'Mingjun Zhou'}, {'authorId': '2243398314', 'name': 'Daiqing Zhuoma'}, {'authorId': '30850510', 'name': 'Qun Nuo'}, {'authorId': '6295933', 'name': 'T. Nyima'}]","['Tibet University', 'Collaborative Innovation Center for Tibet Informatization by MOE and Tibet Autonomous Region, , Lhasa, 850000, Tibet, China']",['China'],2023-09
2309.12132,Chunmo Zheng,"Chunmo Zheng, Saika Wong, Xing Su, Yinqiu Tang",A knowledge representation approach for construction contract knowledge modeling,,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The emergence of large language models (LLMs) presents an unprecedented opportunity to automate construction contract management, reducing human errors and saving significant time and costs. However, LLMs may produce convincing yet inaccurate and misleading content due to a lack of domain expertise. To address this issue, expert-driven contract knowledge can be represented in a structured manner to constrain the automatic contract management process. This paper introduces the Nested Contract Knowledge Graph (NCKG), a knowledge representation approach that captures the complexity of contract knowledge using a nested structure. It includes a nested knowledge representation framework, a NCKG ontology built on the framework, and an implementation method. Furthermore, we present the LLM-assisted contract review pipeline enhanced with external knowledge in NCKG. Our pipeline achieves a promising performance in contract risk reviewing, shedding light on the combination of LLM and KG towards more reliable and interpretable contract management. ","[{'version': 'v1', 'created': 'Thu, 21 Sep 2023 14:53:36 GMT'}]",2023-09-22,"[['Zheng', 'Chunmo', ''], ['Wong', 'Saika', ''], ['Su', 'Xing', ''], ['Tang', 'Yinqiu', '']]",0,0,2023-09-21,1,4,1,0,0,0,6ee567f8597a0957b2d97c8101c55972bd9eda11,262084399.0,https://www.semanticscholar.org/paper/6ee567f8597a0957b2d97c8101c55972bd9eda11,arXiv.org,2023.0,70.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Engineering', 'source': 's2-fos-model'}]","[{'authorId': '2228068133', 'name': 'Chunmo Zheng'}, {'authorId': '2243404018', 'name': 'Saika Wong'}, {'authorId': '2243496449', 'name': 'Xing Su'}, {'authorId': '2243415288', 'name': 'Yinqiu Tang'}]","['Powerchina Huadong Engineering Corporation (China)', 'Zhejiang University']",['China'],2023-09
2309.12247,Qiang Sheng,"Beizhe Hu, Qiang Sheng, Juan Cao, Yuhui Shi, Yang Li, Danding Wang,
  Peng Qi","Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection","17 pages, 6 figures, and 9 tables. Work in progress",,,,cs.CL cs.AI cs.CY,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Detecting fake news requires both a delicate sense of diverse clues and a profound understanding of the real-world background, which remains challenging for detectors based on small language models (SLMs) due to their knowledge and capability limitations. Recent advances in large language models (LLMs) have shown remarkable performance in various tasks, but whether and how LLMs could help with fake news detection remains underexplored. In this paper, we investigate the potential of LLMs in fake news detection. First, we conduct an empirical study and find that a sophisticated LLM such as GPT 3.5 could generally expose fake news and provide desirable multi-perspective rationales but still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis attributes such a gap to the LLM's inability to select and integrate rationales properly to conclude. Based on these findings, we propose that current LLMs may not substitute fine-tuned SLMs in fake news detection but can be a good advisor for SLMs by providing multi-perspective instructive rationales. To instantiate this proposal, we design an adaptive rationale guidance network for fake news detection (ARG), in which SLMs selectively acquire insights on news analysis from the LLMs' rationales. We further derive a rationale-free version of ARG by distillation, namely ARG-D, which services cost-sensitive scenarios without inquiring LLMs. Experiments on two real-world datasets demonstrate that ARG and ARG-D outperform three types of baseline methods, including SLM-based, LLM-based, and combinations of small and large language models. ","[{'version': 'v1', 'created': 'Thu, 21 Sep 2023 16:47:30 GMT'}]",2023-09-22,"[['Hu', 'Beizhe', ''], ['Sheng', 'Qiang', ''], ['Cao', 'Juan', ''], ['Shi', 'Yuhui', ''], ['Li', 'Yang', ''], ['Wang', 'Danding', ''], ['Qi', 'Peng', '']]",0,1,2023-09-21,1,7,3,1,0,1,623a0a4b975452d64d610e28f48822ca500ed93f,262084042.0,https://www.semanticscholar.org/paper/623a0a4b975452d64d610e28f48822ca500ed93f,arXiv.org,2023.0,52.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2220699758', 'name': 'Beizhe Hu'}, {'authorId': '46630548', 'name': 'Qiang Sheng'}, {'authorId': '2243364250', 'name': 'Juan Cao'}, {'authorId': '2243553485', 'name': 'Yuhui Shi'}, {'authorId': '2244866324', 'name': 'Yang Li'}, {'authorId': '37292975', 'name': 'Danding Wang'}, {'authorId': '2063066946', 'name': 'Peng Qi'}]","['National University of Singapore', 'University of Chinese Academy of Sciences', 'Chinese Academy of Sciences', 'Institute of Computing Technology']","['China', 'Singapore']",2023-09
2309.12278,Junyi Bian,"Junyi Bian, Jiaxuan Zheng, Yuyi Zhang, Shanfeng Zhu",Inspire the Large Language Model by External Knowledge on BioMedical Named Entity Recognition,"10 pages, 5 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have demonstrated dominating performance in many NLP tasks, especially on generative tasks. However, they often fall short in some information extraction tasks, particularly those requiring domain-specific knowledge, such as Biomedical Named Entity Recognition (NER). In this paper, inspired by Chain-of-thought, we leverage the LLM to solve the Biomedical NER step-by-step: break down the NER task into entity span extraction and entity type determination. Additionally, for entity type determination, we inject entity knowledge to address the problem that LLM's lack of domain knowledge when predicting entity category. Experimental results show a significant improvement in our two-step BioNER approach compared to previous few-shot LLM baseline. Additionally, the incorporation of external knowledge significantly enhances entity category determination performance. ","[{'version': 'v1', 'created': 'Thu, 21 Sep 2023 17:39:53 GMT'}]",2023-09-22,"[['Bian', 'Junyi', ''], ['Zheng', 'Jiaxuan', ''], ['Zhang', 'Yuyi', ''], ['Zhu', 'Shanfeng', '']]",0,0,2023-09-21,1,4,1,0,0,0,f993edb9aca0239decf07bdbec0cef8ba4ca7c83,262083798.0,https://www.semanticscholar.org/paper/f993edb9aca0239decf07bdbec0cef8ba4ca7c83,arXiv.org,2023.0,23.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '14889056', 'name': 'Junyi Bian'}, {'authorId': '2244849099', 'name': 'Jiaxuan Zheng'}, {'authorId': '2243395182', 'name': 'Yuyi Zhang'}, {'authorId': '7472263', 'name': 'Shanfeng Zhu'}]",['Fudan University'],['China'],2023-09
2309.12284,Longhui Yu,"Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu
  Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu",MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models,"Technical Report, Work in Progress. Project Page:
  https://meta-math.github.io/",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4% on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same size by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of 82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use. ","[{'version': 'v1', 'created': 'Thu, 21 Sep 2023 17:45:42 GMT'}, {'version': 'v2', 'created': 'Fri, 22 Sep 2023 15:51:40 GMT'}]",2023-09-25,"[['Yu', 'Longhui', ''], ['Jiang', 'Weisen', ''], ['Shi', 'Han', ''], ['Yu', 'Jincheng', ''], ['Liu', 'Zhengying', ''], ['Zhang', 'Yu', ''], ['Kwok', 'James T.', ''], ['Li', 'Zhenguo', ''], ['Weller', 'Adrian', ''], ['Liu', 'Weiyang', '']]",0,1,2023-09-21,2,10,2,2,1,1,77b1f1c6d1658d120456b9046667cf009ceb39ce,262084051.0,https://www.semanticscholar.org/paper/77b1f1c6d1658d120456b9046667cf009ceb39ce,arXiv.org,2023.0,74.0,13.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2112584251', 'name': 'L. Yu'}, {'authorId': '2152123946', 'name': 'Weisen Jiang'}, {'authorId': '152751416', 'name': 'Han Shi'}, {'authorId': '2193887687', 'name': 'Jincheng Yu'}, {'authorId': '2239065052', 'name': 'Zhengying Liu'}, {'authorId': '2153638098', 'name': 'Yu Zhang'}, {'authorId': '2243335442', 'name': 'James T. Kwok'}, {'authorId': '121544682', 'name': 'Zheng Li'}, {'authorId': '145689461', 'name': 'Adrian Weller'}, {'authorId': '2243412679', 'name': 'Weiyang Liu'}]","['The Alan Turing Institute', 'Hong Kong University of Science and Technology', 'Southern University of Science and Technology', 'University of Cambridge', 'Huawei Technologies (China)', 'Max Planck Institute for Intelligent Systems']","['China', 'Germany', 'United Kingdom']",2023-09
2309.12626,Saika Wong,"Saika Wong, Chunmo Zheng, Xing Su, Yinqiu Tang",Construction contract risk identification based on knowledge-augmented language model,,,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Contract review is an essential step in construction projects to prevent potential losses. However, the current methods for reviewing construction contracts lack effectiveness and reliability, leading to time-consuming and error-prone processes. While large language models (LLMs) have shown promise in revolutionizing natural language processing (NLP) tasks, they struggle with domain-specific knowledge and addressing specialized issues. This paper presents a novel approach that leverages LLMs with construction contract knowledge to emulate the process of contract review by human experts. Our tuning-free approach incorporates construction contract domain knowledge to enhance language models for identifying construction contract risks. The use of a natural language when building the domain knowledge base facilitates practical implementation. We evaluated our method on real construction contracts and achieved solid performance. Additionally, we investigated how large language models employ logical thinking during the task and provide insights and recommendations for future research. ","[{'version': 'v1', 'created': 'Fri, 22 Sep 2023 05:27:06 GMT'}]",2023-09-25,"[['Wong', 'Saika', ''], ['Zheng', 'Chunmo', ''], ['Su', 'Xing', ''], ['Tang', 'Yinqiu', '']]",0,0,2023-09-22,1,4,2,0,0,0,476de9654c840a080276c4b8b5dadfdbe25c663f,262217163.0,https://www.semanticscholar.org/paper/476de9654c840a080276c4b8b5dadfdbe25c663f,arXiv.org,2023.0,63.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Engineering', 'source': 's2-fos-model'}]","[{'authorId': '2243404018', 'name': 'Saika Wong'}, {'authorId': '2228068133', 'name': 'Chunmo Zheng'}, {'authorId': '2243496449', 'name': 'Xing Su'}, {'authorId': '2243415288', 'name': 'Yinqiu Tang'}]","['Powerchina Huadong Engineering Corporation (China)', 'Zhejiang University']",['China'],2023-09
2309.12669,Tongxu Luo,"Tongxu Luo, Fangyu Lei, Jiahe Lei, Weihao Liu, Shihu He, Jun Zhao and
  Kang Liu",HRoT: Hybrid prompt strategy and Retrieval of Thought for Table-Text Hybrid Question Answering,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Answering numerical questions over hybrid contents from the given tables and text(TextTableQA) is a challenging task. Recently, Large Language Models (LLMs) have gained significant attention in the NLP community. With the emergence of large language models, In-Context Learning and Chain-of-Thought prompting have become two particularly popular research topics in this field. In this paper, we introduce a new prompting strategy called Hybrid prompt strategy and Retrieval of Thought for TextTableQA. Through In-Context Learning, we prompt the model to develop the ability of retrieval thinking when dealing with hybrid data. Our method achieves superior performance compared to the fully-supervised SOTA on the MultiHiertt dataset in the few-shot setting. ","[{'version': 'v1', 'created': 'Fri, 22 Sep 2023 07:26:17 GMT'}]",2023-09-25,"[['Luo', 'Tongxu', ''], ['Lei', 'Fangyu', ''], ['Lei', 'Jiahe', ''], ['Liu', 'Weihao', ''], ['He', 'Shihu', ''], ['Zhao', 'Jun', ''], ['Liu', 'Kang', '']]",0,0,2023-09-22,1,7,1,0,0,0,74acec7abaa6193107c6e3dfc630758ad9474af5,262217366.0,https://www.semanticscholar.org/paper/74acec7abaa6193107c6e3dfc630758ad9474af5,arXiv.org,2023.0,25.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2238950879', 'name': 'Tongxu Luo'}, {'authorId': '2257004176', 'name': 'Fangyu Lei'}, {'authorId': '2238951064', 'name': 'Jiahe Lei'}, {'authorId': '2239053076', 'name': 'Weihao Liu'}, {'authorId': '2245950879', 'name': 'Shihu He'}, {'authorId': '1390572170', 'name': 'Jun Zhao'}, {'authorId': '2200096', 'name': 'Kang Liu'}]","['University of Chinese Academy of Sciences', 'Chinese Academy of Sciences', 'University of Science and Technology Beijing']",['China'],2023-09
2309.12767,Yin Zhu,"Yin Zhu, Zhiling Luo, Gong Cheng",Furthest Reasoning with Plan Assessment: Stable Reasoning Path with Retrieval-Augmented Large Language Models,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs), acting as a powerful reasoner and generator, exhibit extraordinary performance across various natural language tasks, such as question answering (QA). Among these tasks, Multi-Hop Question Answering (MHQA) stands as a widely discussed category, necessitating seamless integration between LLMs and the retrieval of external knowledge. Existing methods employ LLM to generate reasoning paths and plans, and utilize IR to iteratively retrieve related knowledge, but these approaches have inherent flaws. On one hand, Information Retriever (IR) is hindered by the low quality of generated queries by LLM. On the other hand, LLM is easily misguided by the irrelevant knowledge by IR. These inaccuracies, accumulated by the iterative interaction between IR and LLM, lead to a disaster in effectiveness at the end. To overcome above barriers, in this paper, we propose a novel pipeline for MHQA called Furthest-Reasoning-with-Plan-Assessment (FuRePA), including an improved framework (Furthest Reasoning) and an attached module (Plan Assessor). 1) Furthest reasoning operates by masking previous reasoning path and generated queries for LLM, encouraging LLM generating chain of thought from scratch in each iteration. This approach enables LLM to break the shackle built by previous misleading thoughts and queries (if any). 2) The Plan Assessor is a trained evaluator that selects an appropriate plan from a group of candidate plans proposed by LLM. Our methods are evaluated on three highly recognized public multi-hop question answering datasets and outperform state-of-the-art on most metrics (achieving a 10%-12% in answer accuracy). ","[{'version': 'v1', 'created': 'Fri, 22 Sep 2023 10:15:13 GMT'}]",2023-09-25,"[['Zhu', 'Yin', ''], ['Luo', 'Zhiling', ''], ['Cheng', 'Gong', '']]",0,0,2023-09-22,1,3,1,0,0,0,a76226438562cb6e7c6800dea7b045676b407f76,262217474.0,https://www.semanticscholar.org/paper/a76226438562cb6e7c6800dea7b045676b407f76,arXiv.org,2023.0,35.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2192049733', 'name': 'Yin Zhu'}, {'authorId': '2244629083', 'name': 'Zhiling Luo'}, {'authorId': '2244623823', 'name': 'Gong Cheng'}]","['Alibaba', 'Nanjing University']",['China'],2023-09
2309.12881,Shutong Feng,"Shutong Feng, Guangzhi Sun, Nurul Lubis, Chao Zhang, Milica
  Ga\v{s}i\'c",Affect Recognition in Conversations Using Large Language Models,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Affect recognition, encompassing emotions, moods, and feelings, plays a pivotal role in human communication. In the realm of conversational artificial intelligence (AI), the ability to discern and respond to human affective cues is a critical factor for creating engaging and empathetic interactions. This study delves into the capacity of large language models (LLMs) to recognise human affect in conversations, with a focus on both open-domain chit-chat dialogues and task-oriented dialogues. Leveraging three diverse datasets, namely IEMOCAP, EmoWOZ, and DAIC-WOZ, covering a spectrum of dialogues from casual conversations to clinical interviews, we evaluated and compared LLMs' performance in affect recognition. Our investigation explores the zero-shot and few-shot capabilities of LLMs through in-context learning (ICL) as well as their model capacities through task-specific fine-tuning. Additionally, this study takes into account the potential impact of automatic speech recognition (ASR) errors on LLM predictions. With this work, we aim to shed light on the extent to which LLMs can replicate human-like affect recognition capabilities in conversations. ","[{'version': 'v1', 'created': 'Fri, 22 Sep 2023 14:11:23 GMT'}]",2023-09-25,"[['Feng', 'Shutong', ''], ['Sun', 'Guangzhi', ''], ['Lubis', 'Nurul', ''], ['Zhang', 'Chao', ''], ['Gai', 'Milica', '']]",0,0,2023-09-22,1,5,1,0,0,0,866dfa1e5487b871894cee44605cd50107916a7a,262217537.0,https://www.semanticscholar.org/paper/866dfa1e5487b871894cee44605cd50107916a7a,arXiv.org,2023.0,44.0,1.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '2113511651', 'name': 'Shutong Feng'}, {'authorId': '2107310187', 'name': 'Guangzhi Sun'}, {'authorId': '143604111', 'name': 'Nurul Lubis'}, {'authorId': '2256775692', 'name': 'Chao Zhang'}, {'authorId': '1676892968', 'name': ""Milica Gavsi'c""}]","['University of Cambridge', 'Tsinghua University', 'Heinrich Heine University Dsseldorf']","['China', 'Germany', 'United Kingdom']",2023-09
2309.12941,Yuxin Deng,"Zezhong Chen, Yuxin Deng, Wenjie Du",Trusta: Reasoning about Assurance Cases with Formal Methods and Large Language Models,38 pages,,,,cs.SE cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Assurance cases can be used to argue for the safety of products in safety engineering. In safety-critical areas, the construction of assurance cases is indispensable. Trustworthiness Derivation Trees (TDTs) enhance assurance cases by incorporating formal methods, rendering it possible for automatic reasoning about assurance cases. We present Trustworthiness Derivation Tree Analyzer (Trusta), a desktop application designed to automatically construct and verify TDTs. The tool has a built-in Prolog interpreter in its backend, and is supported by the constraint solvers Z3 and MONA. Therefore, it can solve constraints about logical formulas involving arithmetic, sets, Horn clauses etc. Trusta also utilizes large language models to make the creation and evaluation of assurance cases more convenient. It allows for interactive human examination and modification. We evaluated top language models like ChatGPT-3.5, ChatGPT-4, and PaLM 2 for generating assurance cases. Our tests showed a 50%-80% similarity between machine-generated and human-created cases. In addition, Trusta can extract formal constraints from text in natural languages, facilitating an easier interpretation and validation process. This extraction is subject to human review and correction, blending the best of automated efficiency with human insight. To our knowledge, this marks the first integration of large language models in automatic creating and reasoning about assurance cases, bringing a novel approach to a traditional challenge. Through several industrial case studies, Trusta has proven to quickly find some subtle issues that are typically missed in manual inspection, demonstrating its practical value in enhancing the assurance case development process. ","[{'version': 'v1', 'created': 'Fri, 22 Sep 2023 15:42:43 GMT'}]",2023-09-25,"[['Chen', 'Zezhong', ''], ['Deng', 'Yuxin', ''], ['Du', 'Wenjie', '']]",1,1,2023-09-22,1,3,2,2,0,2,63980a765ae610ab4cab3a21e035b051407bfc9d,262217414.0,https://www.semanticscholar.org/paper/63980a765ae610ab4cab3a21e035b051407bfc9d,arXiv.org,2023.0,65.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2161339806', 'name': 'Zezhong Chen'}, {'authorId': '152710186', 'name': 'Yu Deng'}, {'authorId': '7816421', 'name': 'Wenjie Du'}]","['Shanghai Normal University', 'Shanghai Key Laboratory of Trustworthy Computing']",['China'],2023-09
2309.13064,Yi Yang,"Yi Yang, Yixuan Tang, Kar Yan Tam",InvestLM: A Large Language Model for Investment using Financial Domain Instruction Tuning,Link: https://github.com/AbaciNLP/InvestLM,,,,q-fin.GN cs.AI cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  We present a new financial domain large language model, InvestLM, tuned on LLaMA-65B (Touvron et al., 2023), using a carefully curated instruction dataset related to financial investment. Inspired by less-is-more-for-alignment (Zhou et al., 2023), we manually curate a small yet diverse instruction dataset, covering a wide range of financial related topics, from Chartered Financial Analyst (CFA) exam questions to SEC filings to Stackexchange quantitative finance discussions. InvestLM shows strong capabilities in understanding financial text and provides helpful responses to investment related questions. Financial experts, including hedge fund managers and research analysts, rate InvestLM's response as comparable to those of state-of-the-art commercial models (GPT-3.5, GPT-4 and Claude-2). Zero-shot evaluation on a set of financial NLP benchmarks demonstrates strong generalizability. From a research perspective, this work suggests that a high-quality domain specific LLM can be tuned using a small set of carefully curated instructions on a well-trained foundation model, which is consistent with the Superficial Alignment Hypothesis (Zhou et al., 2023). From a practical perspective, this work develops a state-of-the-art financial domain LLM with superior capability in understanding financial texts and providing helpful investment advice, potentially enhancing the work efficiency of financial professionals. We release the model parameters to the research community. ","[{'version': 'v1', 'created': 'Fri, 15 Sep 2023 02:59:31 GMT'}]",2023-09-27,"[['Yang', 'Yi', ''], ['Tang', 'Yixuan', ''], ['Tam', 'Kar Yan', '']]",0,1,2023-09-15,1,3,4,4,1,3,844bc3b26b5c63ec3b251ae634c194dcfb41a7d2,262459267.0,https://www.semanticscholar.org/paper/844bc3b26b5c63ec3b251ae634c194dcfb41a7d2,arXiv.org,2023.0,29.0,4.0,0.0,True,"['Computer Science', 'Economics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Economics', 'source': 'external'}, {'category': 'Economics', 'source': 's2-fos-model'}]","[{'authorId': '2246043972', 'name': 'Yi Yang'}, {'authorId': '49578084', 'name': 'Yixuan Tang'}, {'authorId': '1805674', 'name': 'K. Y. Tam'}]","['InvestLM adopts the same licensing terms as LLaMA', 'Hong Kong University of Science and Technology']",['China'],2023-09
2309.13079,Fukai Shang,"Yidong Liu, FuKai Shang, Fang Wang, Rui Xu, Jun Wang, Wei Li, Yao Li,
  Conghui He",MiChao-HuaFen 1.0: A Specialized Pre-trained Corpus Dataset for Domain-specific Large Models,"4 pages,2 figures",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-sa/4.0/,"  With the advancement of deep learning technologies, general-purpose large models such as GPT-4 have demonstrated exceptional capabilities across various domains. Nevertheless, there remains a demand for high-quality, domain-specific outputs in areas like healthcare, law, and finance. This paper first evaluates the existing large models for specialized domains and discusses their limitations. To cater to the specific needs of certain domains, we introduce the ``MiChao-HuaFen 1.0'' pre-trained corpus dataset, tailored for the news and governmental sectors. The dataset, sourced from publicly available internet data from 2022, underwent multiple rounds of cleansing and processing to ensure high quality and reliable origins, with provisions for consistent and stable updates. This dataset not only supports the pre-training of large models for Chinese vertical domains but also aids in propelling deep learning research and applications in related fields. ","[{'version': 'v1', 'created': 'Thu, 21 Sep 2023 09:02:28 GMT'}, {'version': 'v2', 'created': 'Tue, 26 Sep 2023 10:38:19 GMT'}]",2023-09-27,"[['Liu', 'Yidong', ''], ['Shang', 'FuKai', ''], ['Wang', 'Fang', ''], ['Xu', 'Rui', ''], ['Wang', 'Jun', ''], ['Li', 'Wei', ''], ['Li', 'Yao', ''], ['He', 'Conghui', '']]",0,1,2023-09-21,2,8,2,1,0,1,b2ea7690230be22f7781b33bede69b49f7ea5191,262466089.0,https://www.semanticscholar.org/paper/b2ea7690230be22f7781b33bede69b49f7ea5191,arXiv.org,2023.0,12.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2186736278', 'name': 'Yidong Liu'}, {'authorId': '3486481', 'name': 'Conghui He'}, {'authorId': '2256598803', 'name': 'Wei Li'}, {'authorId': '2227474613', 'name': 'Fu-De Shang'}, {'authorId': '66063792', 'name': 'J. Wang'}, {'authorId': '2110976037', 'name': 'Yaoxin Li'}, {'authorId': '2115802652', 'name': 'Rui Xu'}]","['Shanghai Artificial Intelligence Laboratory', 'Shanghai Midu Technology Co., Ltd']",['China'],2023-09
2309.13165,Qianglong Chen,"Chenin Li, Qianglong Chen, Yin Zhang, Yifei Zhang, Hongxiang Yao",Large Language Models Are Also Good Prototypical Commonsense Reasoners,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Commonsense reasoning is a pivotal skill for large language models, yet it presents persistent challenges in specific tasks requiring this competence. Traditional fine-tuning approaches can be resource-intensive and potentially compromise a model's generalization capacity. Furthermore, state-of-the-art language models like GPT-3.5 and Claude are primarily accessible through API calls, which makes fine-tuning models challenging. To address these challenges, we draw inspiration from the outputs of large models for tailored tasks and semi-automatically developed a set of novel prompts from several perspectives, including task-relevance, supportive evidence generation (e.g. chain-of-thought and knowledge), diverse path decoding to aid the model. Experimental results on ProtoQA dataset demonstrate that with better designed prompts we can achieve the new state-of-art(SOTA) on the ProtoQA leaderboard, improving the Max Answer@1 score by 8%, Max Incorrect@1 score by 4% (breakthrough 50% for the first time) compared to the previous SOTA model and achieved an improvement on StrategyQA and CommonsenseQA2.0 (3% and 1%, respectively). Furthermore, with the generated Chain-of-Thought and knowledge, we can improve the interpretability of the model while also surpassing the previous SOTA models. We hope that our work can provide insight for the NLP community to develop better prompts and explore the potential of large language models for more complex reasoning tasks. ","[{'version': 'v1', 'created': 'Fri, 22 Sep 2023 20:07:24 GMT'}]",2023-09-26,"[['Li', 'Chenin', ''], ['Chen', 'Qianglong', ''], ['Zhang', 'Yin', ''], ['Zhang', 'Yifei', ''], ['Yao', 'Hongxiang', '']]",0,1,2023-09-22,1,5,2,2,0,2,0289c15ca813f13491ad1e1fb0036b103f0eb9fb,262465363.0,https://www.semanticscholar.org/paper/0289c15ca813f13491ad1e1fb0036b103f0eb9fb,arXiv.org,2023.0,40.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2245395502', 'name': 'Chenin Li'}, {'authorId': '1500384901', 'name': 'Qianglong Chen'}, {'authorId': '46867455', 'name': 'Yin Zhang'}, {'authorId': '2206998130', 'name': 'Yifei Zhang'}, {'authorId': '50018057', 'name': 'Hongxiang Yao'}]","['Alibaba', 'Northeastern University', 'Zhejiang University']",['China'],2023-09
2309.13192,Wei Gao,"Kai Huang, Hanyun Yin, Heng Huang, Wei Gao",Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation,14 pages,,,,cs.LG cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Fine-tuning is the most effective way of adapting pre-trained large language models (LLMs) to downstream applications. With the fast growth of LLM-enabled AI applications and democratization of open-souced LLMs, fine-tuning has become possible for non-expert individuals, but intensively performed LLM fine-tuning worldwide could result in significantly high energy consumption and carbon footprint, which may bring large environmental impact. Mitigating such environmental impact towards Green AI directly correlates to reducing the FLOPs of fine-tuning, but existing techniques on efficient LLM fine-tuning can only achieve limited reduction of such FLOPs, due to their ignorance of the backpropagation cost in fine-tuning. To address this limitation, in this paper we present GreenTrainer, a new LLM fine-tuning technique that adaptively evaluates different tensors' backpropagation costs and contributions to the fine-tuned model accuracy, to minimize the fine-tuning cost by selecting the most appropriate set of tensors in training. Such selection in GreenTrainer is made based on a given objective of FLOPs reduction, which can flexibly adapt to the carbon footprint in energy supply and the need in Green AI. Experiment results over multiple open-sourced LLM models and abstractive summarization datasets show that, compared to fine-tuning the whole LLM model, GreenTrainer can save up to 64% FLOPs in fine-tuning without any noticeable model accuracy loss. Compared to the existing fine-tuning techniques such as LoRa, GreenTrainer can achieve up to 4% improvement on model accuracy with on-par FLOPs reduction. ","[{'version': 'v1', 'created': 'Fri, 22 Sep 2023 21:55:18 GMT'}]",2023-09-26,"[['Huang', 'Kai', ''], ['Yin', 'Hanyun', ''], ['Huang', 'Heng', ''], ['Gao', 'Wei', '']]",0,0,2023-09-22,1,4,2,0,0,0,33f5c02e55b556ce88c4121be12b6958bc093a63,262460575.0,https://www.semanticscholar.org/paper/33f5c02e55b556ce88c4121be12b6958bc093a63,arXiv.org,2023.0,45.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2112769493', 'name': 'Kai Huang'}, {'authorId': '30974319', 'name': 'Hanyu Yin'}, {'authorId': '2115528613', 'name': 'Heng Huang'}, {'authorId': '48385754', 'name': 'W. Gao'}]","['University of Science and Technology of China', 'University of Pittsburgh', 'University of Maryland, College Park']","['China', 'United States']",2023-09
2309.13308,Yuxuan Liu,"Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang,
  Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang",Calibrating LLM-Based Evaluator,"22 pages,11 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advancements in large language models (LLMs) on language modeling and emergent capabilities make them a promising reference-free evaluator of natural language generation quality, and a competent alternative to human evaluation. However, hindered by the closed-source or high computational demand to host and tune, there is a lack of practice to further calibrate an off-the-shelf LLM-based evaluator towards better human alignment. In this work, we propose AutoCalibrate, a multi-stage, gradient-free approach to automatically calibrate and align an LLM-based evaluator toward human preference. Instead of explicitly modeling human preferences, we first implicitly encompass them within a set of human labels. Then, an initial set of scoring criteria is drafted by the language model itself, leveraging in-context learning on different few-shot examples. To further calibrate this set of criteria, we select the best performers and re-draft them with self-refinement. Our experiments on multiple text quality evaluation datasets illustrate a significant improvement in correlation with expert evaluation through calibration. Our comprehensive qualitative analysis conveys insightful intuitions and observations on the essence of effective scoring criteria. ","[{'version': 'v1', 'created': 'Sat, 23 Sep 2023 08:46:11 GMT'}]",2023-09-26,"[['Liu', 'Yuxuan', ''], ['Yang', 'Tianchi', ''], ['Huang', 'Shaohan', ''], ['Zhang', 'Zihan', ''], ['Huang', 'Haizhen', ''], ['Wei', 'Furu', ''], ['Deng', 'Weiwei', ''], ['Sun', 'Feng', ''], ['Zhang', 'Qi', '']]",0,0,2023-09-23,1,9,1,0,0,0,2d12f95dd521101f3092cc3bb04e7e88aba8f562,262464745.0,https://www.semanticscholar.org/paper/2d12f95dd521101f3092cc3bb04e7e88aba8f562,arXiv.org,2023.0,38.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2116696995', 'name': 'Yuxuan Liu'}, {'authorId': '2122813607', 'name': 'Tianchi Yang'}, {'authorId': '3110003', 'name': 'Shaohan Huang'}, {'authorId': '2144371301', 'name': 'Zihan Zhang'}, {'authorId': '2146285313', 'name': 'Haizhen Huang'}, {'authorId': '49807919', 'name': 'Furu Wei'}, {'authorId': '2066621592', 'name': 'Weiwei Deng'}, {'authorId': '2247156451', 'name': 'Feng Sun'}, {'authorId': '2256972722', 'name': 'Qi Zhang'}]","['Peking University', 'Microsoft']","['China', 'United States']",2023-09
2309.13315,Peiwen Jiang,"Peiwen Jiang, Chao-Kai Wen, Xinping Yi, Xiao Li, Shi Jin, and Jun
  Zhang",Semantic Communications using Foundation Models: Design Approaches and Open Issues,"This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible",,,,eess.SP eess.IV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Foundation models (FMs), including large language models, have become increasingly popular due to their wide-ranging applicability and ability to understand human-like semantics. While previous research has explored the use of FMs in semantic communications to improve semantic extraction and reconstruction, the impact of these models on different system levels, considering computation and memory complexity, requires further analysis. This study focuses on integrating FMs at the effectiveness, semantic, and physical levels, using universal knowledge to profoundly transform system design. Additionally, it examines the use of compact models to balance performance and complexity, comparing three separate approaches that employ FMs. Ultimately, the study highlights unresolved issues in the field that need addressing. ","[{'version': 'v1', 'created': 'Sat, 23 Sep 2023 09:15:17 GMT'}]",2023-09-26,"[['Jiang', 'Peiwen', ''], ['Wen', 'Chao-Kai', ''], ['Yi', 'Xinping', ''], ['Li', 'Xiao', ''], ['Jin', 'Shi', ''], ['Zhang', 'Jun', '']]",0,0,2023-09-23,1,6,2,0,0,0,a4d9bad6e6052008ad5601beba8104a0e86eac64,262465391.0,https://www.semanticscholar.org/paper/a4d9bad6e6052008ad5601beba8104a0e86eac64,,2023.0,15.0,0.0,0.0,False,['Engineering'],"[{'category': 'Engineering', 'source': 'external'}, {'category': 'Business', 'source': 's2-fos-model'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '49113720', 'name': 'Peiwen Jiang'}, {'authorId': '145382786', 'name': 'Chao-Kai Wen'}, {'authorId': '1813783', 'name': 'Xinping Yi'}, {'authorId': '2154738586', 'name': 'X. Li'}, {'authorId': '31185129', 'name': 'Shimei Jin'}, {'authorId': '39879254', 'name': 'Jun Zhang'}]","['Southeast University', 'National Sun Yat-sen University', 'Hong Kong University of Science and Technology']","['China', 'Taiwan']",2023-09
2309.13345,Zican Dong,"Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen",BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have achieved dramatic proficiency over NLP tasks with normal length. Recently, multiple studies have committed to extending the context length and enhancing the long text modeling capabilities of LLMs. To comprehensively evaluate the long context ability of LLMs, we propose BAMBOO, a multi-task long context benchmark. BAMBOO has been designed with four principles: comprehensive capacity evaluation, avoidance of data contamination, accurate automatic evaluation, and different length levels. It consists of 10 datasets from 5 different long text understanding tasks, i.e. question answering, hallucination detection, text sorting, language modeling, and code completion, to cover core capacities and various domains of LLMs. We conduct experiments with five long context models on BAMBOO and further discuss four key research questions of long text. We also qualitatively analyze current long context models and point out future directions for enhancing long text modeling capacities. We release our data, prompts, and code at https://github.com/RUCAIBox/BAMBOO. ","[{'version': 'v1', 'created': 'Sat, 23 Sep 2023 11:36:15 GMT'}]",2023-09-26,"[['Dong', 'Zican', ''], ['Tang', 'Tianyi', ''], ['Li', 'Junyi', ''], ['Zhao', 'Wayne Xin', ''], ['Wen', 'Ji-Rong', '']]",0,0,2023-09-23,1,5,1,0,0,0,42d83576ee920c1b6df318e212047d9ba57fc4fd,262459341.0,https://www.semanticscholar.org/paper/42d83576ee920c1b6df318e212047d9ba57fc4fd,arXiv.org,2023.0,56.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2198280871', 'name': 'Zican Dong'}, {'authorId': '1997234792', 'name': 'Tianyi Tang'}, {'authorId': '2018027', 'name': 'Junyi Li'}, {'authorId': '2542603', 'name': 'Wayne Xin Zhao'}, {'authorId': '2259699', 'name': 'Ji-Rong Wen'}]","['Universit de Montral', 'Renmin University of China']","['Canada', 'China']",2023-09
2309.13574,Shengcheng Yu,"Shengcheng Yu, Chunrong Fang, Yuchen Ling, Chentian Wu, Zhenyu Chen","LLM for Test Script Generation and Migration: Challenges, Capabilities, and Opportunities","Accepted by the 23rd IEEE International Conference on Software
  Quality, Reliability, and Security (QRS 2023)",,,,cs.SE,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  This paper investigates the application of large language models (LLM) in the domain of mobile application test script generation. Test script generation is a vital component of software testing, enabling efficient and reliable automation of repetitive test tasks. However, existing generation approaches often encounter limitations, such as difficulties in accurately capturing and reproducing test scripts across diverse devices, platforms, and applications. These challenges arise due to differences in screen sizes, input modalities, platform behaviors, API inconsistencies, and application architectures. Overcoming these limitations is crucial for achieving robust and comprehensive test automation.   By leveraging the capabilities of LLMs, we aim to address these challenges and explore its potential as a versatile tool for test automation. We investigate how well LLMs can adapt to diverse devices and systems while accurately capturing and generating test scripts. Additionally, we evaluate its cross-platform generation capabilities by assessing its ability to handle operating system variations and platform-specific behaviors. Furthermore, we explore the application of LLMs in cross-app migration, where it generates test scripts across different applications and software environments based on existing scripts.   Throughout the investigation, we analyze its adaptability to various user interfaces, app architectures, and interaction patterns, ensuring accurate script generation and compatibility. The findings of this research contribute to the understanding of LLMs' capabilities in test automation. Ultimately, this research aims to enhance software testing practices, empowering app developers to achieve higher levels of software quality and development efficiency. ","[{'version': 'v1', 'created': 'Sun, 24 Sep 2023 07:58:57 GMT'}]",2023-09-26,"[['Yu', 'Shengcheng', ''], ['Fang', 'Chunrong', ''], ['Ling', 'Yuchen', ''], ['Wu', 'Chentian', ''], ['Chen', 'Zhenyu', '']]",0,0,2023-09-24,1,5,1,0,0,0,737aa146e2d8d044fff40ba830f89e30fd9ae4fd,262459296.0,https://www.semanticscholar.org/paper/737aa146e2d8d044fff40ba830f89e30fd9ae4fd,arXiv.org,2023.0,65.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '150311588', 'name': 'Shengcheng Yu'}, {'authorId': '7595994', 'name': 'Chunrong Fang'}, {'authorId': '2176287286', 'name': 'Yucheng Ling'}, {'authorId': '2047155463', 'name': 'Chentian Wu'}, {'authorId': '2238950128', 'name': 'Zhenyu Chen'}]",['Nanjing University'],['China'],2023-09
2309.13876,Yifan Peng,"Yifan Peng, Jinchuan Tian, Brian Yan, Dan Berrebbi, Xuankai Chang,
  Xinjian Li, Jiatong Shi, Siddhant Arora, William Chen, Roshan Sharma, Wangyou
  Zhang, Yui Sudo, Muhammad Shakeel, Jee-weon Jung, Soumi Maiti, Shinji
  Watanabe",Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data,Accepted at ASRU 2023,,,,cs.CL cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisper-style training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pre-trained models and training logs to promote open science. ","[{'version': 'v1', 'created': 'Mon, 25 Sep 2023 05:01:34 GMT'}, {'version': 'v2', 'created': 'Mon, 2 Oct 2023 01:10:01 GMT'}]",2023-10-03,"[['Peng', 'Yifan', ''], ['Tian', 'Jinchuan', ''], ['Yan', 'Brian', ''], ['Berrebbi', 'Dan', ''], ['Chang', 'Xuankai', ''], ['Li', 'Xinjian', ''], ['Shi', 'Jiatong', ''], ['Arora', 'Siddhant', ''], ['Chen', 'William', ''], ['Sharma', 'Roshan', ''], ['Zhang', 'Wangyou', ''], ['Sudo', 'Yui', ''], ['Shakeel', 'Muhammad', ''], ['Jung', 'Jee-weon', ''], ['Maiti', 'Soumi', ''], ['Watanabe', 'Shinji', '']]",0,0,2023-09-25,2,16,3,0,0,0,d43338451cd8676548811e1ff8f9c92ea987c5bd,262465225.0,https://www.semanticscholar.org/paper/d43338451cd8676548811e1ff8f9c92ea987c5bd,arXiv.org,2023.0,68.0,0.0,0.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2111014429', 'name': 'Yifan Peng'}, {'authorId': '2087131860', 'name': 'Jinchuan Tian'}, {'authorId': '2087059555', 'name': 'Brian Yan'}, {'authorId': '2142561945', 'name': 'Dan Berrebbi'}, {'authorId': '8776560', 'name': 'Xuankai Chang'}, {'authorId': '2108191722', 'name': 'Xinjian Li'}, {'authorId': '1485531923', 'name': 'Jiatong Shi'}, {'authorId': '72401599', 'name': 'Siddhant Arora'}, {'authorId': '2144302389', 'name': 'William Chen'}, {'authorId': '145521253', 'name': 'Roshan Sharma'}, {'authorId': '1390725481', 'name': 'Wangyou Zhang'}, {'authorId': '151265408', 'name': 'Yui Sudo'}, {'authorId': '47551878', 'name': 'M. Shakeel'}, {'authorId': '31930118', 'name': 'Jee-weon Jung'}, {'authorId': '31949212', 'name': 'Soumi Maiti'}, {'authorId': '1746678', 'name': 'Shinji Watanabe'}]","['Carnegie Mellon University', 'Shanghai Jiao Tong University', 'Honda (Japan)']","['China', 'United States', 'Japan']",2023-09
2309.13905,Jianwei Yu,"Jianwei Yu, Hangting Chen, Yanyao Bian, Xiang Li, Yi Luo, Jinchuan
  Tian, Mengyang Liu, Jiayi Jiang, Shuai Wang",AutoPrep: An Automatic Preprocessing Framework for In-the-Wild Speech Data,,,,,eess.AS cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, the utilization of extensive open-sourced text data has significantly advanced the performance of text-based large language models (LLMs). However, the use of in-the-wild large-scale speech data in the speech technology community remains constrained. One reason for this limitation is that a considerable amount of the publicly available speech data is compromised by background noise, speech overlapping, lack of speech segmentation information, missing speaker labels, and incomplete transcriptions, which can largely hinder their usefulness. On the other hand, human annotation of speech data is both time-consuming and costly. To address this issue, we introduce an automatic in-the-wild speech data preprocessing framework (AutoPrep) in this paper, which is designed to enhance speech quality, generate speaker labels, and produce transcriptions automatically. The proposed AutoPrep framework comprises six components: speech enhancement, speech segmentation, speaker clustering, target speech extraction, quality filtering and automatic speech recognition. Experiments conducted on the open-sourced WenetSpeech and our self-collected AutoPrepWild corpora demonstrate that the proposed AutoPrep framework can generate preprocessed data with similar DNSMOS and PDNSMOS scores compared to several open-sourced TTS datasets. The corresponding TTS system can achieve up to 0.68 in-domain speaker similarity. ","[{'version': 'v1', 'created': 'Mon, 25 Sep 2023 07:01:10 GMT'}]",2023-09-26,"[['Yu', 'Jianwei', ''], ['Chen', 'Hangting', ''], ['Bian', 'Yanyao', ''], ['Li', 'Xiang', ''], ['Luo', 'Yi', ''], ['Tian', 'Jinchuan', ''], ['Liu', 'Mengyang', ''], ['Jiang', 'Jiayi', ''], ['Wang', 'Shuai', '']]",0,0,2023-09-25,1,9,2,0,0,0,98c7bb8f9a8048d8098f3c03fa0187644b248d3c,262480519.0,https://www.semanticscholar.org/paper/98c7bb8f9a8048d8098f3c03fa0187644b248d3c,arXiv.org,2023.0,31.0,0.0,0.0,True,"['Engineering', 'Computer Science']","[{'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2161264092', 'name': 'Jianwei Yu'}, {'authorId': '2118023944', 'name': 'Hangting Chen'}, {'authorId': '51110739', 'name': 'Yanyao Bian'}, {'authorId': '47057383', 'name': 'Xiang Li'}, {'authorId': '49513614', 'name': 'Yimin Luo'}, {'authorId': '2087131860', 'name': 'Jinchuan Tian'}, {'authorId': '2152970727', 'name': 'Mengyang Liu'}, {'authorId': '1914623206', 'name': 'Jiayi Jiang'}, {'authorId': '2155449067', 'name': 'Shuai Wang'}]","['Tencent', 'Shenzhen Research Institute of Big Data']",['China'],2023-09
2309.13963,Changli Tang,"Wenyi Yu and Changli Tang and Guangzhi Sun and Xianzhao Chen and Tian
  Tan and Wei Li and Lu Lu and Zejun Ma and Chao Zhang",Connecting Speech Encoder and Large Language Model for ASR,,,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The impressive capability and versatility of large language models (LLMs) have aroused increasing attention in automatic speech recognition (ASR), with several pioneering studies attempting to build integrated ASR models by connecting a speech encoder with an LLM. This paper presents a comparative study of three commonly used structures as connectors, including fully connected layers, multi-head cross-attention, and Q-Former. Speech encoders from the Whisper model series as well as LLMs from the Vicuna model series with different model sizes were studied. Experiments were performed on the commonly used LibriSpeech, Common Voice, and GigaSpeech datasets, where the LLMs with Q-Formers demonstrated consistent and considerable word error rate (WER) reductions over LLMs with other connector structures. Q-Former-based LLMs can generalise well to out-of-domain datasets, where 12% relative WER reductions over the Whisper baseline ASR model were achieved on the Eval2000 test set without using any in-domain training data from Switchboard. Moreover, a novel segment-level Q-Former is proposed to enable LLMs to recognise speech segments with a duration exceeding the limitation of the encoders, which results in 17% relative WER reductions over other connector structures on 90-second-long speech data. ","[{'version': 'v1', 'created': 'Mon, 25 Sep 2023 08:57:07 GMT'}, {'version': 'v2', 'created': 'Tue, 26 Sep 2023 11:09:25 GMT'}]",2023-09-27,"[['Yu', 'Wenyi', ''], ['Tang', 'Changli', ''], ['Sun', 'Guangzhi', ''], ['Chen', 'Xianzhao', ''], ['Tan', 'Tian', ''], ['Li', 'Wei', ''], ['Lu', 'Lu', ''], ['Ma', 'Zejun', ''], ['Zhang', 'Chao', '']]",0,0,2023-09-25,2,9,3,1,1,0,5596bd3e26ec2207666ec1ff3db4415d212f14b9,262465937.0,https://www.semanticscholar.org/paper/5596bd3e26ec2207666ec1ff3db4415d212f14b9,arXiv.org,2023.0,38.0,3.0,0.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '48729312', 'name': 'Wenyi Yu'}, {'authorId': '2247237695', 'name': 'Changli Tang'}, {'authorId': '2107310187', 'name': 'Guangzhi Sun'}, {'authorId': '2135092817', 'name': 'Xianzhao Chen'}, {'authorId': '101432607', 'name': 'T. Tan'}, {'authorId': '2256598801', 'name': 'Wei Li'}, {'authorId': '2215402606', 'name': 'Lu Lu'}, {'authorId': '2919563', 'name': 'Zejun Ma'}, {'authorId': '2256775692', 'name': 'Chao Zhang'}]","['ByteDance', 'Tsinghua University']",['China'],2023-09
2309.14122,Jieming Zhong,"Zhongjie Ba, Jieming Zhong, Jiachen Lei, Peng Cheng, Qinglong Wang,
  Zhan Qin, Zhibo Wang, Kui Ren",SurrogatePrompt: Bypassing the Safety Filter of Text-To-Image Models via Substitution,"14 pages, 11 figures",,,,cs.CV cs.CR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Advanced text-to-image models such as DALL-E 2 and Midjourney possess the capacity to generate highly realistic images, raising significant concerns regarding the potential proliferation of unsafe content. This includes adult, violent, or deceptive imagery of political figures. Despite claims of rigorous safety mechanisms implemented in these models to restrict the generation of not-safe-for-work (NSFW) content, we successfully devise and exhibit the first prompt attacks on Midjourney, resulting in the production of abundant photorealistic NSFW images. We reveal the fundamental principles of such prompt attacks and suggest strategically substituting high-risk sections within a suspect prompt to evade closed-source safety measures. Our novel framework, SurrogatePrompt, systematically generates attack prompts, utilizing large language models, image-to-text, and image-to-image modules to automate attack prompt creation at scale. Evaluation results disclose an 88% success rate in bypassing Midjourney's proprietary safety filter with our attack prompts, leading to the generation of counterfeit images depicting political figures in violent scenarios. Both subjective and objective assessments validate that the images generated from our attack prompts present considerable safety hazards. ","[{'version': 'v1', 'created': 'Mon, 25 Sep 2023 13:20:15 GMT'}]",2023-09-26,"[['Ba', 'Zhongjie', ''], ['Zhong', 'Jieming', ''], ['Lei', 'Jiachen', ''], ['Cheng', 'Peng', ''], ['Wang', 'Qinglong', ''], ['Qin', 'Zhan', ''], ['Wang', 'Zhibo', ''], ['Ren', 'Kui', '']]",0,0,2023-09-25,1,8,2,0,0,0,e1decb86f2a6aba8682d2fc4e427424b0b49e0d0,262465788.0,https://www.semanticscholar.org/paper/e1decb86f2a6aba8682d2fc4e427424b0b49e0d0,arXiv.org,2023.0,29.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '36890675', 'name': 'Zhongjie Ba'}, {'authorId': '2243261433', 'name': 'Jieming Zhong'}, {'authorId': '2192603948', 'name': 'Jiachen Lei'}, {'authorId': '144533942', 'name': 'Pengyu Cheng'}, {'authorId': '2109096844', 'name': 'Qinglong Wang'}, {'authorId': '3330305', 'name': 'Zhan Qin'}, {'authorId': '3623271', 'name': 'Zhibo Wang'}, {'authorId': '2072596898', 'name': 'Kui Ren'}]",['Zhejiang University'],['China'],2023-09
2309.14181,Haoning Wu Mr,"Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao,
  Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, Weisi Lin",Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision,"25 pages, 14 figures, 9 tables, preprint version",,,,cs.CV cs.AI cs.MM,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  The rapid evolution of Multi-modality Large Language Models (MLLMs) has catalyzed a shift in computer vision from specialized models to general-purpose foundation models. Nevertheless, there is still an inadequacy in assessing the abilities of MLLMs on low-level visual perception and understanding. To address this gap, we present Q-Bench, a holistic benchmark crafted to systematically evaluate potential abilities of MLLMs on three realms: low-level visual perception, low-level visual description, and overall visual quality assessment. a) To evaluate the low-level perception ability, we construct the LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped with a human-asked question focusing on its low-level attributes. We then measure the correctness of MLLMs on answering these questions. b) To examine the description ability of MLLMs on low-level information, we propose the LLDescribe dataset consisting of long expert-labelled golden low-level text descriptions on 499 images, and a GPT-involved comparison pipeline between outputs of MLLMs and the golden descriptions. c) Besides these two tasks, we further measure their visual quality assessment ability to align with human opinion scores. Specifically, we design a softmax-based strategy that enables MLLMs to predict quantifiable quality scores, and evaluate them on various existing image quality assessment (IQA) datasets. Our evaluation across the three abilities confirms that MLLMs possess preliminary low-level visual skills. However, these skills are still unstable and relatively imprecise, indicating the need for specific enhancements on MLLMs towards these abilities. We hope that our benchmark can encourage the research community to delve deeper to discover and enhance these untapped potentials of MLLMs. Project Page: https://vqassessment.github.io/Q-Bench. ","[{'version': 'v1', 'created': 'Mon, 25 Sep 2023 14:43:43 GMT'}, {'version': 'v2', 'created': 'Thu, 28 Sep 2023 16:22:23 GMT'}]",2023-09-29,"[['Wu', 'Haoning', ''], ['Zhang', 'Zicheng', ''], ['Zhang', 'Erli', ''], ['Chen', 'Chaofeng', ''], ['Liao', 'Liang', ''], ['Wang', 'Annan', ''], ['Li', 'Chunyi', ''], ['Sun', 'Wenxiu', ''], ['Yan', 'Qiong', ''], ['Zhai', 'Guangtao', ''], ['Lin', 'Weisi', '']]",0,1,2023-09-25,2,11,3,0,0,0,593b42c628b49937bcd7f7c4a7d54d5f97e6b414,262824606.0,https://www.semanticscholar.org/paper/593b42c628b49937bcd7f7c4a7d54d5f97e6b414,arXiv.org,2023.0,56.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '120155330', 'name': 'Haoning Wu'}, {'authorId': '2116459218', 'name': 'Zicheng Zhang'}, {'authorId': '2054909930', 'name': 'Erli Zhang'}, {'authorId': '10025861', 'name': 'Chaofeng Chen'}, {'authorId': '2049450860', 'name': 'Liang Liao'}, {'authorId': '2116562125', 'name': 'Annan Wang'}, {'authorId': '2109738874', 'name': 'Chunyi Li'}, {'authorId': '2246803713', 'name': 'Wenxiu Sun'}, {'authorId': '144479026', 'name': 'Qiong Yan'}, {'authorId': '144826390', 'name': 'Guangtao Zhai'}, {'authorId': '2242389471', 'name': 'Weisi Lin'}]","['Shanghai Jiao Tong University', 'Sensetime Research', 'Nanyang Technological University']","['China', 'Singapore']",2023-09
2309.14345,Qingwen Bu,"Dong Huang, Qingwen Bu, Jie Zhang, Xiaofei Xie, Junjie Chen, Heming
  Cui",Bias Assessment and Mitigation in LLM-based Code Generation,,,,,cs.SE cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Utilizing state-of-the-art Large Language Models (LLMs), automatic code generation models play a pivotal role in enhancing the productivity and efficiency of software development coding procedures. As the adoption of LLMs becomes more widespread in software coding ecosystems, a pressing issue has emerged: does the generated code contain social biases, such as those related to age, gender, and race? This issue concerns the integrity, fairness, and ethical foundation of software applications that depend on the code generated by these models, yet is under-explored in the literature. This paper presents a novel bias assessment framework that is specifically designed for code generation tasks. Based on this framework, we conduct an extensive evaluation on the bias of nine state-of-the-art LLM-based code generation models. Our findings reveal that first, 31.45\% to 79.93\% code functions generated by our evaluated code generation models are biased, and 9.68\% to 37.37\% code functions' functionality are affected by the bias, which means biases not only exist in code generation models but in some cases, directly affect the functionality of the generated code, posing risks of unintended and possibly harmful software behaviors. To mitigate bias from code generation models, we propose three mitigation strategies, which can decrease the biased code ratio to a very low level of 0.4\% to 4.57\%. ","[{'version': 'v1', 'created': 'Sun, 3 Sep 2023 07:14:49 GMT'}]",2023-09-27,"[['Huang', 'Dong', ''], ['Bu', 'Qingwen', ''], ['Zhang', 'Jie', ''], ['Xie', 'Xiaofei', ''], ['Chen', 'Junjie', ''], ['Cui', 'Heming', '']]",0,0,2023-09-03,1,6,2,0,0,0,d5c2947cab82c44e3cca8e90486da10a81e1f697,262824773.0,https://www.semanticscholar.org/paper/d5c2947cab82c44e3cca8e90486da10a81e1f697,arXiv.org,2023.0,36.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '145252513', 'name': 'Dong Huang'}, {'authorId': '9963055', 'name': 'Qi Bu'}, {'authorId': '51250527', 'name': 'J Zhang'}, {'authorId': '2247410731', 'name': 'Xiaofei Xie'}, {'authorId': '123878420', 'name': 'Junjie Chen'}, {'authorId': '2944075', 'name': 'Heming Cui'}]","[""King's College London"", 'Singapore Management University', 'Shanghai Jiao Tong University', 'Tianjin University', 'University of Hong Kong']","['China', 'United Kingdom', 'Singapore', 'Hong Kong']",2023-09
2309.14365,Pengyu Zhao,Pengyu Zhao and Zijian Jin and Ning Cheng,An In-depth Survey of Large Language Model-based Artificial Intelligence Agents,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Due to the powerful capabilities demonstrated by large language model (LLM), there has been a recent surge in efforts to integrate them with AI agents to enhance their performance. In this paper, we have explored the core differences and characteristics between LLM-based AI agents and traditional AI agents. Specifically, we first compare the fundamental characteristics of these two types of agents, clarifying the significant advantages of LLM-based agents in handling natural language, knowledge storage, and reasoning capabilities. Subsequently, we conducted an in-depth analysis of the key components of AI agents, including planning, memory, and tool use. Particularly, for the crucial component of memory, this paper introduced an innovative classification scheme, not only departing from traditional classification methods but also providing a fresh perspective on the design of an AI agent's memory system. We firmly believe that in-depth research and understanding of these core components will lay a solid foundation for the future advancement of AI agent technology. At the end of the paper, we provide directional suggestions for further research in this field, with the hope of offering valuable insights to scholars and researchers in the field. ","[{'version': 'v1', 'created': 'Sat, 23 Sep 2023 11:25:45 GMT'}]",2023-09-27,"[['Zhao', 'Pengyu', ''], ['Jin', 'Zijian', ''], ['Cheng', 'Ning', '']]",0,0,2023-09-23,1,3,2,0,0,0,b3d0871705f246045d58b77bdb9574d5bc3d949f,262824840.0,https://www.semanticscholar.org/paper/b3d0871705f246045d58b77bdb9574d5bc3d949f,arXiv.org,2023.0,113.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2114871845', 'name': 'Pengyu Zhao'}, {'authorId': '2247157734', 'name': 'Zijian Jin'}, {'authorId': '1768839', 'name': 'N. Cheng'}]",['Beijing Jiaotong University'],['China'],2023-09
2309.14494,Yufan Feng,"Hanzhuo Huang, Yufan Feng, Cheng Shi, Lan Xu, Jingyi Yu, Sibei Yang",Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator,"NeurIPS 2023; Project available at:
  https://github.com/SooLab/Free-Bloom",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text-to-video is a rapidly growing research area that aims to generate a semantic, identical, and temporal coherence sequence of frames that accurately align with the input text prompt. This study focuses on zero-shot text-to-video generation considering the data- and cost-efficient. To generate a semantic-coherent video, exhibiting a rich portrayal of temporal semantics such as the whole process of flower blooming rather than a set of ""moving images"", we propose a novel Free-Bloom pipeline that harnesses large language models (LLMs) as the director to generate a semantic-coherence prompt sequence, while pre-trained latent diffusion models (LDMs) as the animator to generate the high fidelity frames. Furthermore, to ensure temporal and identical coherence while maintaining semantic coherence, we propose a series of annotative modifications to adapting LDMs in the reverse process, including joint noise sampling, step-aware attention shift, and dual-path interpolation. Without any video data and training requirements, Free-Bloom generates vivid and high-quality videos, awe-inspiring in generating complex scenes with semantic meaningful frame sequences. In addition, Free-Bloom is naturally compatible with LDMs-based extensions. ","[{'version': 'v1', 'created': 'Mon, 25 Sep 2023 19:42:16 GMT'}]",2023-09-27,"[['Huang', 'Hanzhuo', ''], ['Feng', 'Yufan', ''], ['Shi', 'Cheng', ''], ['Xu', 'Lan', ''], ['Yu', 'Jingyi', ''], ['Yang', 'Sibei', '']]",0,0,2023-09-25,1,6,1,1,1,0,120aca3e415b6641a0b0cd20695ab85ed7789612,262824917.0,https://www.semanticscholar.org/paper/120aca3e415b6641a0b0cd20695ab85ed7789612,arXiv.org,2023.0,59.0,4.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2247091668', 'name': 'Hanzhuo Huang'}, {'authorId': '2246811982', 'name': 'Yufan Feng'}, {'authorId': '2113917364', 'name': 'Cheng Shi'}, {'authorId': '2112162680', 'name': 'Lan Xu'}, {'authorId': '2155403153', 'name': 'Jingyi Yu'}, {'authorId': '3144952', 'name': 'Sibei Yang'}]",['ShanghaiTech University'],['China'],2023-09
2309.14623,Xu Chen,"Jiayi Liao, Xu Chen, Qiang Fu, Lun Du, Xiangnan He, Xiang Wang, Shi
  Han, Dongmei Zhang",Text-to-Image Generation for Abstract Concepts,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent years have witnessed the substantial progress of large-scale models across various domains, such as natural language processing and computer vision, facilitating the expression of concrete concepts. Unlike concrete concepts that are usually directly associated with physical objects, expressing abstract concepts through natural language requires considerable effort, which results from their intricate semantics and connotations. An alternative approach is to leverage images to convey rich visual information as a supplement. Nevertheless, existing Text-to-Image (T2I) models are primarily trained on concrete physical objects and tend to fail to visualize abstract concepts. Inspired by the three-layer artwork theory that identifies critical factors, intent, object and form during artistic creation, we propose a framework of Text-to-Image generation for Abstract Concepts (TIAC). The abstract concept is clarified into a clear intent with a detailed definition to avoid ambiguity. LLMs then transform it into semantic-related physical objects, and the concept-dependent form is retrieved from an LLM-extracted form pattern set. Information from these three aspects will be integrated to generate prompts for T2I models via LLM. Evaluation results from human assessments and our newly designed metric concept score demonstrate the effectiveness of our framework in creating images that can sufficiently express abstract concepts. ","[{'version': 'v1', 'created': 'Tue, 26 Sep 2023 02:22:39 GMT'}, {'version': 'v2', 'created': 'Wed, 27 Sep 2023 05:34:17 GMT'}]",2023-09-28,"[['Liao', 'Jiayi', ''], ['Chen', 'Xu', ''], ['Fu', 'Qiang', ''], ['Du', 'Lun', ''], ['He', 'Xiangnan', ''], ['Wang', 'Xiang', ''], ['Han', 'Shi', ''], ['Zhang', 'Dongmei', '']]",0,0,2023-09-26,2,8,1,0,0,0,0d38f1edac66b4645cf5fa05abaf9d92cba5d5d3,262824861.0,https://www.semanticscholar.org/paper/0d38f1edac66b4645cf5fa05abaf9d92cba5d5d3,arXiv.org,2023.0,49.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2212781448', 'name': 'Jiayi Liao'}, {'authorId': '2118183722', 'name': 'Xu Chen'}, {'authorId': '2089209772', 'name': 'Qiang Fu'}, {'authorId': '12723949', 'name': 'Lun Du'}, {'authorId': '2240825631', 'name': 'Xiangnan He'}, {'authorId': '2240694093', 'name': 'Xiang Wang'}, {'authorId': '2109750123', 'name': 'Shi Han'}, {'authorId': '2140415600', 'name': 'Dongmei Zhang'}]","['University of Science and Technology of China', 'Microsoft']","['China', 'United States']",2023-09
2309.14681,Jiwei Li,"Rui Li, Guoyin Wang, Jiwei Li",Are Human-generated Demonstrations Necessary for In-context Learning?,Pre-print Version,,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the promising few-shot ability of large language models (LLMs), the standard paradigm of In-context Learning (ICL) suffers the disadvantages of susceptibility to selected demonstrations and the intricacy to generate these demonstrations. In this paper, we raise the fundamental question that whether human-generated demonstrations are necessary for ICL. To answer this question, we propose self-contemplation prompting strategy (SEC), a paradigm free from human-crafted demonstrations. The key point of SEC is that, instead of using hand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create demonstrations on their own, based on which the final output is generated. SEC is a flexible framework and can be adapted to both the vanilla ICL and the chain-of-thought (CoT), but with greater ease: as the manual-generation process of both examples and rationale can be saved. Extensive experiments in arithmetic reasoning, commonsense reasoning, multi-task language understanding, and code generation benchmarks, show that SEC, which does not require hand-crafted demonstrations, significantly outperforms the zero-shot learning strategy, and achieves comparable results to ICL with hand-crafted demonstrations. This demonstrates that, for many tasks, contemporary LLMs possess a sufficient level of competence to exclusively depend on their own capacity for decision making, removing the need for external training data. Code is available at https://github.com/ruili33/SEC. ","[{'version': 'v1', 'created': 'Tue, 26 Sep 2023 05:10:08 GMT'}, {'version': 'v2', 'created': 'Sun, 1 Oct 2023 02:52:00 GMT'}]",2023-10-03,"[['Li', 'Rui', ''], ['Wang', 'Guoyin', ''], ['Li', 'Jiwei', '']]",0,0,2023-09-26,2,3,2,0,0,0,0f45608ddc01b3e192f3490330f4c4b8de074f79,262824824.0,https://www.semanticscholar.org/paper/0f45608ddc01b3e192f3490330f4c4b8de074f79,arXiv.org,2023.0,46.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2247421824', 'name': 'Rui Li'}, {'authorId': '2220704972', 'name': 'Guoyin Wang'}, {'authorId': '2172372802', 'name': 'Jiwei Li'}]","['ByteDance', 'University of Science and Technology of China', 'Zhejiang University']",['China'],2023-09
2309.14717,Yuhui Xu,"Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng
  Zhang, Zhensu Chen, Xiaopeng Zhang, Qi Tian",QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models,16 pages,,,,cs.LG cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently years have witnessed a rapid development of large language models (LLMs). Despite the strong ability in many language-understanding tasks, the heavy computational burden largely restricts the application of LLMs especially when one needs to deploy them onto edge devices. In this paper, we propose a quantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies in the imbalanced degrees of freedom of quantization and adaptation, and the solution is to use group-wise operators which increase the degree of freedom of quantization meanwhile decreasing that of adaptation. QA-LoRA is easily implemented with a few lines of code, and it equips the original LoRA with two-fold abilities: (i) during fine-tuning, the LLM's weights are quantized (e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the LLM and auxiliary weights are naturally integrated into a quantized model without loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model families and validate its effectiveness in different fine-tuning datasets and downstream scenarios. Code will be made available at https://github.com/yuhuixu1993/qa-lora. ","[{'version': 'v1', 'created': 'Tue, 26 Sep 2023 07:22:23 GMT'}]",2023-09-28,"[['Xu', 'Yuhui', ''], ['Xie', 'Lingxi', ''], ['Gu', 'Xiaotao', ''], ['Chen', 'Xin', ''], ['Chang', 'Heng', ''], ['Zhang', 'Hengheng', ''], ['Chen', 'Zhensu', ''], ['Zhang', 'Xiaopeng', ''], ['Tian', 'Qi', '']]",0,0,2023-09-26,1,9,2,1,1,0,945db0077b6d19b720f5998b3f61300013c4f885,262825568.0,https://www.semanticscholar.org/paper/945db0077b6d19b720f5998b3f61300013c4f885,arXiv.org,2023.0,64.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '48615640', 'name': 'Yuhui Xu'}, {'authorId': '3041937', 'name': 'Lingxi Xie'}, {'authorId': '7787721', 'name': 'Xiaotao Gu'}, {'authorId': '2145229597', 'name': 'Xin Chen'}, {'authorId': '144188238', 'name': 'Heng Chang'}, {'authorId': '1983351', 'name': 'Hengheng Zhang'}, {'authorId': '2249294812', 'name': 'Zhensu Chen'}, {'authorId': '21458018', 'name': 'Xiaopeng Zhang'}, {'authorId': '2106415186', 'name': 'Qi Tian'}]",['Huawei Technologies (China)'],['China'],2023-09
2309.14726,Yuanhao Gong,Yuanhao Gong,PLMM: Personal Large Models on Mobile Devices,arXiv admin note: substantial text overlap with arXiv:2307.13221,,,,cs.CV cs.AI cs.CE cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Inspired by Federated Learning, in this paper, we propose personal large models that are distilled from traditional large language models but more adaptive to local users' personal information such as education background and hobbies. We classify the large language models into three levels: the personal level, expert level and traditional level. The personal level models are adaptive to users' personal information. They encrypt the users' input and protect their privacy. The expert level models focus on merging specific knowledge such as finance, IT and art. The traditional models focus on the universal knowledge discovery and upgrading the expert models. In such classifications, the personal models directly interact with the user. For the whole system, the personal models have users' (encrypted) personal information. Moreover, such models must be small enough to be performed on personal computers or mobile devices. Finally, they also have to response in real-time for better user experience and produce high quality results. The proposed personal large models can be applied in a wide range of applications such as language and vision tasks. ","[{'version': 'v1', 'created': 'Tue, 26 Sep 2023 07:36:20 GMT'}]",2023-09-27,"[['Gong', 'Yuanhao', '']]",0,0,2023-09-26,1,1,5,0,0,0,2299541d6bb3242f5c56c6dea798c5e581a817d1,262826015.0,https://www.semanticscholar.org/paper/2299541d6bb3242f5c56c6dea798c5e581a817d1,arXiv.org,2023.0,52.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2247105596', 'name': 'Yuanhao Gong'}]",['Shenzhen University'],['China'],2023-09
2309.14763,Chenyang Song,"Chenyang Song, Xu Han, Zheni Zeng, Kuai Li, Chen Chen, Zhiyuan Liu,
  Maosong Sun and Tao Yang",ConPET: Continual Parameter-Efficient Tuning for Large Language Models,"12 pages, 3 figures. This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Continual learning necessitates the continual adaptation of models to newly emerging tasks while minimizing the catastrophic forgetting of old ones. This is extremely challenging for large language models (LLMs) with vanilla full-parameter tuning due to high computation costs, memory consumption, and forgetting issue. Inspired by the success of parameter-efficient tuning (PET), we propose Continual Parameter-Efficient Tuning (ConPET), a generalizable paradigm for continual task adaptation of LLMs with task-number-independent training complexity. ConPET includes two versions with different application scenarios. First, Static ConPET can adapt former continual learning methods originally designed for relatively smaller models to LLMs through PET and a dynamic replay strategy, which largely reduces the tuning costs and alleviates the over-fitting and forgetting issue. Furthermore, to maintain scalability, Dynamic ConPET adopts separate PET modules for different tasks and a PET module selector for dynamic optimal selection. In our extensive experiments, the adaptation of Static ConPET helps multiple former methods reduce the scale of tunable parameters by over 3,000 times and surpass the PET-only baseline by at least 5 points on five smaller benchmarks, while Dynamic ConPET gains its advantage on the largest dataset. The codes and datasets are available at https://github.com/Raincleared-Song/ConPET. ","[{'version': 'v1', 'created': 'Tue, 26 Sep 2023 08:52:04 GMT'}]",2023-09-27,"[['Song', 'Chenyang', ''], ['Han', 'Xu', ''], ['Zeng', 'Zheni', ''], ['Li', 'Kuai', ''], ['Chen', 'Chen', ''], ['Liu', 'Zhiyuan', ''], ['Sun', 'Maosong', ''], ['Yang', 'Tao', '']]",0,0,2023-09-26,1,8,1,0,0,0,352244ac7602f13e16a08424db322364d0a2cef1,262822572.0,https://www.semanticscholar.org/paper/352244ac7602f13e16a08424db322364d0a2cef1,arXiv.org,2023.0,61.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '35714849', 'name': 'Chenyan Song'}, {'authorId': '48506411', 'name': 'Xu Han'}, {'authorId': '1633538428', 'name': 'Zheni Zeng'}, {'authorId': '2223171532', 'name': 'Kuai Li'}, {'authorId': '2248149386', 'name': 'Chen Chen'}, {'authorId': '2141313179', 'name': 'Zhiyuan Liu'}, {'authorId': '1753344', 'name': 'Maosong Sun'}, {'authorId': '1390892946', 'name': 'Taojiannan Yang'}]","['Tencent', 'Tsinghua University']",['China'],2023-09
2309.14771,Chengyu Wang,"Jianing Wang, Chengyu Wang, Chuanqi Tan, Jun Huang, Ming Gao",Boosting In-Context Learning with Factual Knowledge,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In-Context Learning (ICL) over Large language models (LLMs) aims at solving previously unseen tasks by conditioning on a few training examples, eliminating the need for parameter updates and achieving competitive performance. In this paper, we demonstrate that factual knowledge is imperative for the performance of ICL in three core facets, i.e., the inherent knowledge learned in LLMs, the factual knowledge derived from the selected in-context examples, and the knowledge biases in LLMs for output generation. To unleash the power of LLMs in few-shot learning scenarios, we introduce a novel Knowledgeable In-Context Tuning (KICT) framework to further improve the performance of ICL: 1) injecting factual knowledge to LLMs during continual self-supervised pre-training, 2) judiciously selecting the examples with high knowledge relevance, and 3) calibrating the prediction results based on prior knowledge. We evaluate the proposed approaches on auto-regressive LLMs (e.g., GPT-style models) over multiple text classification and question answering tasks. Experimental results demonstrate that KICT substantially outperforms strong baselines, and improves by more than 13% and 7% of accuracy on text classification and question answering tasks, respectively. ","[{'version': 'v1', 'created': 'Tue, 26 Sep 2023 09:06:39 GMT'}]",2023-09-27,"[['Wang', 'Jianing', ''], ['Wang', 'Chengyu', ''], ['Tan', 'Chuanqi', ''], ['Huang', 'Jun', ''], ['Gao', 'Ming', '']]",0,1,2023-09-26,1,5,2,0,0,0,20177a85f632a34d085bcf645507e461733fcc96,262827483.0,https://www.semanticscholar.org/paper/20177a85f632a34d085bcf645507e461733fcc96,arXiv.org,2023.0,56.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '46584777', 'name': 'J. Wang'}, {'authorId': '50097294', 'name': 'Chengyu Wang'}, {'authorId': '2111727840', 'name': 'Chuanqi Tan'}, {'authorId': '1697626', 'name': 'Jun Huang'}, {'authorId': '2147416932', 'name': 'Ming Gao'}]","['Alibaba', 'East China Normal University']",['China'],2023-09
2309.14846,Zimin Chen,"Zimin Chen, Sen Fang and Martin Monperrus",Supersonic: Learning to Generate Source Code Optimizations in C/C++,,,,,cs.SE cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Software optimization refines programs for resource efficiency while preserving functionality. Traditionally, it is a process done by developers and compilers. This paper introduces a third option, automated optimization at the source code level. We present Supersonic, a neural approach targeting minor source code modifications for optimization. Using a seq2seq model, Supersonic is trained on C/C++ program pairs ($x_{t}$, $x_{t+1}$), where $x_{t+1}$ is an optimized version of $x_{t}$, and outputs a diff. Supersonic's performance is benchmarked against OpenAI's GPT-3.5-Turbo and GPT-4 on competitive programming tasks. The experiments show that Supersonic not only outperforms both models on the code optimization task but also minimizes the extent of the change with a model more than 600x smaller than GPT-3.5-Turbo and 3700x smaller than GPT-4. ","[{'version': 'v1', 'created': 'Tue, 26 Sep 2023 11:21:46 GMT'}, {'version': 'v2', 'created': 'Fri, 29 Sep 2023 09:33:47 GMT'}, {'version': 'v3', 'created': 'Mon, 2 Oct 2023 19:47:15 GMT'}]",2023-10-04,"[['Chen', 'Zimin', ''], ['Fang', 'Sen', ''], ['Monperrus', 'Martin', '']]",0,1,2023-09-26,3,3,2,2,0,2,dd06b331be3e72f5e7b74275aa4065b76b4df5dc,262824943.0,https://www.semanticscholar.org/paper/dd06b331be3e72f5e7b74275aa4065b76b4df5dc,arXiv.org,2023.0,61.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '46842516', 'name': 'Zimin Chen'}, {'authorId': '2110316453', 'name': 'Sen Fang'}, {'authorId': '150220964', 'name': 'Monperrus Martin'}]",['Wuhan Institute of Technology'],['China'],2023-09
2309.15025,Tianhao Shen,"Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong,
  Zishan Guo, Xinwei Wu, Yan Liu, Deyi Xiong",Large Language Model Alignment: A Survey,76 pages,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-sa/4.0/,"  Recent years have witnessed remarkable progress made in large language models (LLMs). Such advancements, while garnering significant attention, have concurrently elicited various concerns. The potential of these models is undeniably vast; however, they may yield texts that are imprecise, misleading, or even detrimental. Consequently, it becomes paramount to employ alignment techniques to ensure these models to exhibit behaviors consistent with human values.   This survey endeavors to furnish an extensive exploration of alignment methodologies designed for LLMs, in conjunction with the extant capability research in this domain. Adopting the lens of AI alignment, we categorize the prevailing methods and emergent proposals for the alignment of LLMs into outer and inner alignment. We also probe into salient issues including the models' interpretability, and potential vulnerabilities to adversarial attacks. To assess LLM alignment, we present a wide variety of benchmarks and evaluation methodologies. After discussing the state of alignment research for LLMs, we finally cast a vision toward the future, contemplating the promising avenues of research that lie ahead.   Our aspiration for this survey extends beyond merely spurring research interests in this realm. We also envision bridging the gap between the AI alignment research community and the researchers engrossed in the capability exploration of LLMs for both capable and safe LLMs. ","[{'version': 'v1', 'created': 'Tue, 26 Sep 2023 15:49:23 GMT'}]",2023-09-27,"[['Shen', 'Tianhao', ''], ['Jin', 'Renren', ''], ['Huang', 'Yufei', ''], ['Liu', 'Chuang', ''], ['Dong', 'Weilong', ''], ['Guo', 'Zishan', ''], ['Wu', 'Xinwei', ''], ['Liu', 'Yan', ''], ['Xiong', 'Deyi', '']]",0,0,2023-09-26,1,9,2,0,0,0,73bd1ee5193d1f9fa5913e15fcf111db9f75bc75,262824801.0,https://www.semanticscholar.org/paper/73bd1ee5193d1f9fa5913e15fcf111db9f75bc75,arXiv.org,2023.0,286.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2057973326', 'name': 'Tianhao Shen'}, {'authorId': '2184143149', 'name': 'Renren Jin'}, {'authorId': '9221211', 'name': 'Yufei Huang'}, {'authorId': '2116348128', 'name': 'Chuang Liu'}, {'authorId': '2114049130', 'name': 'Weilong Dong'}, {'authorId': '2246789578', 'name': 'Zishan Guo'}, {'authorId': '2051972795', 'name': 'Xinwei Wu'}, {'authorId': '2246949835', 'name': 'Yan Liu'}, {'authorId': '2694222', 'name': 'Deyi Xiong'}]",['Tianjin University'],['China'],2023-09
2309.15074,Haoyi Xiong,"Haoyi Xiong and Jiang Bian and Sijia Yang and Xiaofei Zhang and Linghe
  Kong and Daqing Zhang",Natural Language based Context Modeling and Reasoning with LLMs: A Tutorial,Under review,,,,cs.CL cs.AI cs.HC cs.NI,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) have become phenomenally surging, since 2018--two decades after introducing context-awareness into computing systems. Through taking into account the situations of ubiquitous devices, users and the societies, context-aware computing has enabled a wide spectrum of innovative applications, such as assisted living, location-based social network services and so on. To recognize contexts and make decisions for actions accordingly, various artificial intelligence technologies, such as Ontology and OWL, have been adopted as representations for context modeling and reasoning. Recently, with the rise of LLMs and their improved natural language understanding and reasoning capabilities, it has become feasible to model contexts using natural language and perform context reasoning by interacting with LLMs such as ChatGPT and GPT-4. In this tutorial, we demonstrate the use of texts, prompts, and autonomous agents (AutoAgents) that enable LLMs to perform context modeling and reasoning without requiring fine-tuning of the model. We organize and introduce works in the related field, and name this computing paradigm as the LLM-driven Context-aware Computing (LCaC). In the LCaC paradigm, users' requests, sensors reading data, and the command to actuators are supposed to be represented as texts. Given the text of users' request and sensor data, the AutoAgent models the context by prompting and sends to the LLM for context reasoning. LLM generates a plan of actions and responds to the AutoAgent, which later follows the action plan to foster context-awareness. To prove the concepts, we use two showcases--(1) operating a mobile z-arm in an apartment for assisted living, and (2) planning a trip and scheduling the itinerary in a context-aware and personalized manner. ","[{'version': 'v1', 'created': 'Sun, 24 Sep 2023 00:15:39 GMT'}]",2023-09-27,"[['Xiong', 'Haoyi', ''], ['Bian', 'Jiang', ''], ['Yang', 'Sijia', ''], ['Zhang', 'Xiaofei', ''], ['Kong', 'Linghe', ''], ['Zhang', 'Daqing', '']]",1,1,2023-09-24,1,6,4,2,0,2,a1a91c24f25598f437596acf9d81b0ff182a6190,262826716.0,https://www.semanticscholar.org/paper/a1a91c24f25598f437596acf9d81b0ff182a6190,arXiv.org,2023.0,124.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2054473008', 'name': 'Haoyi Xiong'}, {'authorId': '2143957850', 'name': 'Jiang Bian'}, {'authorId': '117785201', 'name': 'Sijia Yang'}, {'authorId': '2144525327', 'name': 'Xiaofei Zhang'}, {'authorId': '3254296', 'name': 'L. Kong'}, {'authorId': '2115947583', 'name': 'Daqing Zhang'}]","['Beijing University of Posts and Telecommunications', 'Baidu', 'Institut Polytechnique de Paris', 'Shanghai Jiao Tong University', 'https://huggingface.co/blog/large-language-models']","['China', 'France']",2023-09
2309.15289,Yao Mu Mark,"Zhiqian Lan, Yuxuan Jiang, Yao Mu, Chen Chen, Shengbo Eben Li, Hang
  Zhao, Keqiang Li",SEPT: Towards Efficient Scene Representation Learning for Motion Prediction,,,,,cs.CV cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Motion prediction is crucial for autonomous vehicles to operate safely in complex traffic environments. Extracting effective spatiotemporal relationships among traffic elements is key to accurate forecasting. Inspired by the successful practice of pretrained large language models, this paper presents SEPT, a modeling framework that leverages self-supervised learning to develop powerful spatiotemporal understanding for complex traffic scenes. Specifically, our approach involves three masking-reconstruction modeling tasks on scene inputs including agents' trajectories and road network, pretraining the scene encoder to capture kinematics within trajectory, spatial structure of road network, and interactions among roads and agents. The pretrained encoder is then finetuned on the downstream forecasting task. Extensive experiments demonstrate that SEPT, without elaborate architectural design or manual feature engineering, achieves state-of-the-art performance on the Argoverse 1 and Argoverse 2 motion forecasting benchmarks, outperforming previous methods on all main metrics by a large margin. ","[{'version': 'v1', 'created': 'Tue, 26 Sep 2023 21:56:03 GMT'}, {'version': 'v2', 'created': 'Tue, 3 Oct 2023 08:52:42 GMT'}]",2023-10-04,"[['Lan', 'Zhiqian', ''], ['Jiang', 'Yuxuan', ''], ['Mu', 'Yao', ''], ['Chen', 'Chen', ''], ['Li', 'Shengbo Eben', ''], ['Zhao', 'Hang', ''], ['Li', 'Keqiang', '']]",0,0,2023-09-26,2,7,2,0,0,0,c30550a1a75507c1eb25641c0757e644210ba64d,263152444.0,https://www.semanticscholar.org/paper/c30550a1a75507c1eb25641c0757e644210ba64d,arXiv.org,2023.0,28.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2186979347', 'name': 'Zhiqian Lan'}, {'authorId': '2212387204', 'name': 'Yuxuan Jiang'}, {'authorId': '2253674656', 'name': 'Yao Mu'}, {'authorId': '40590308', 'name': 'Chen Chen'}, {'authorId': '144103688', 'name': 'Shen Li'}, {'authorId': '2249479131', 'name': 'Hang Zhao'}, {'authorId': '2248100352', 'name': 'Keqiang Li'}]",['Tsinghua University'],['China'],2023-09
2309.15461,Mengyuan Liu,"June M. Liu, Donghao Li, He Cao, Tianhe Ren, Zeyi Liao and Jiamin Wu",ChatCounselor: A Large Language Models for Mental Health Support,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents ChatCounselor, a large language model (LLM) solution designed to provide mental health support. Unlike generic chatbots, ChatCounselor is distinguished by its foundation in real conversations between consulting clients and professional psychologists, enabling it to possess specialized knowledge and counseling skills in the field of psychology. The training dataset, Psych8k, was constructed from 260 in-depth interviews, each spanning an hour. To assess the quality of counseling responses, the counseling Bench was devised. Leveraging GPT-4 and meticulously crafted prompts based on seven metrics of psychological counseling assessment, the model underwent evaluation using a set of real-world counseling questions. Impressively, ChatCounselor surpasses existing open-source models in the counseling Bench and approaches the performance level of ChatGPT, showcasing the remarkable enhancement in model capability attained through high-quality domain-specific data. ","[{'version': 'v1', 'created': 'Wed, 27 Sep 2023 07:57:21 GMT'}]",2023-09-28,"[['Liu', 'June M.', ''], ['Li', 'Donghao', ''], ['Cao', 'He', ''], ['Ren', 'Tianhe', ''], ['Liao', 'Zeyi', ''], ['Wu', 'Jiamin', '']]",1,1,2023-09-27,1,6,1,2,0,2,d030361469baabf5acbaee1623ea495e70591bae,262943261.0,https://www.semanticscholar.org/paper/d030361469baabf5acbaee1623ea495e70591bae,arXiv.org,2023.0,42.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Psychology', 'source': 's2-fos-model'}]","[{'authorId': '2247666353', 'name': 'June M. Liu'}, {'authorId': '2248406777', 'name': 'Donghao Li'}, {'authorId': '2153244338', 'name': 'He Cao'}, {'authorId': '2247536954', 'name': 'Tianhe Ren'}, {'authorId': '2247514935', 'name': 'Zeyi Liao'}, {'authorId': '2247759048', 'name': 'Jiamin Wu'}]","['University of Hong Kong', 'The International Digital Economy Academy (IDEA) Shenzhen, China', 'Hong Kong University of Science and Technology', 'The Ohio State University']","['China', 'United States', 'Hong Kong']",2023-09
2309.15606,Dehai Zhao,"Xiaoxue Ren, Xinyuan Ye, Dehai Zhao, Zhenchang Xing, Xiaohu Yang",From Misuse to Mastery: Enhancing Code Generation with Knowledge-Driven AI Chaining,"Accepted by 38th IEEE/ACM International Conference on Automated
  Software Engineering (ASE 2023)",,,,cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have shown promising results in automatic code generation by improving coding efficiency to a certain extent. However, generating high-quality and reliable code remains a formidable task because of LLMs' lack of good programming practice, especially in exception handling. In this paper, we first conduct an empirical study and summarise three crucial challenges of LLMs in exception handling, i.e., incomplete exception handling, incorrect exception handling and abuse of try-catch. We then try prompts with different granularities to address such challenges, finding fine-grained knowledge-driven prompts works best. Based on our empirical study, we propose a novel Knowledge-driven Prompt Chaining-based code generation approach, name KPC, which decomposes code generation into an AI chain with iterative check-rewrite steps and chains fine-grained knowledge-driven prompts to assist LLMs in considering exception-handling specifications. We evaluate our KPC-based approach with 3,079 code generation tasks extracted from the Java official API documentation. Extensive experimental results demonstrate that the KPC-based approach has considerable potential to ameliorate the quality of code generated by LLMs. It achieves this through proficiently managing exceptions and obtaining remarkable enhancements of 109.86% and 578.57% with static evaluation methods, as well as a reduction of 18 runtime bugs in the sampled dataset with dynamic validation. ","[{'version': 'v1', 'created': 'Wed, 27 Sep 2023 12:09:07 GMT'}]",2023-09-28,"[['Ren', 'Xiaoxue', ''], ['Ye', 'Xinyuan', ''], ['Zhao', 'Dehai', ''], ['Xing', 'Zhenchang', ''], ['Yang', 'Xiaohu', '']]",0,0,2023-09-27,1,5,1,0,0,0,648ba65a59b9e907ca0636876bcd2c3732f0931e,263151863.0,https://www.semanticscholar.org/paper/648ba65a59b9e907ca0636876bcd2c3732f0931e,International Conference on Automated Software Engineering,2023.0,59.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1977416789', 'name': 'Xiaoxue Ren'}, {'authorId': '96243837', 'name': 'Xinyuan Ye'}, {'authorId': '2110556397', 'name': 'Dehai Zhao'}, {'authorId': '2247838341', 'name': 'Zhenchang Xing'}, {'authorId': '2247840482', 'name': 'Xiaohu Yang'}]","['Data61', 'Australian National University', 'Zhejiang University']","['China', 'Australia']",2023-09
2309.15698,Li Shen,"Weishi Li, Yong Peng, Miao Zhang, Liang Ding, Han Hu, Li Shen",Deep Model Fusion: A Survey,46 pages,,,,cs.LG cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Deep model fusion/merging is an emerging technique that merges the parameters or predictions of multiple deep learning models into a single one. It combines the abilities of different models to make up for the biases and errors of a single model to achieve better performance. However, deep model fusion on large-scale deep learning models (e.g., LLMs and foundation models) faces several challenges, including high computational cost, high-dimensional parameter space, interference between different heterogeneous models, etc. Although model fusion has attracted widespread attention due to its potential to solve complex real-world tasks, there is still a lack of complete and detailed survey research on this technique. Accordingly, in order to understand the model fusion method better and promote its development, we present a comprehensive survey to summarize the recent progress. Specifically, we categorize existing deep model fusion methods as four-fold: (1) ""Mode connectivity"", which connects the solutions in weight space via a path of non-increasing loss, in order to obtain better initialization for model fusion; (2) ""Alignment"" matches units between neural networks to create better conditions for fusion; (3) ""Weight average"", a classical model fusion method, averages the weights of multiple models to obtain more accurate results closer to the optimal solution; (4) ""Ensemble learning"" combines the outputs of diverse models, which is a foundational technique for improving the accuracy and robustness of the final model. In addition, we analyze the challenges faced by deep model fusion and propose possible research directions for model fusion in the future. Our review is helpful in deeply understanding the correlation between different model fusion methods and practical application methods, which can enlighten the research in the field of deep model fusion. ","[{'version': 'v1', 'created': 'Wed, 27 Sep 2023 14:40:12 GMT'}]",2023-09-28,"[['Li', 'Weishi', ''], ['Peng', 'Yong', ''], ['Zhang', 'Miao', ''], ['Ding', 'Liang', ''], ['Hu', 'Han', ''], ['Shen', 'Li', '']]",0,0,2023-09-27,1,6,2,0,0,0,128217c0d1e99912ebc727c84686cc97a913b55f,262942062.0,https://www.semanticscholar.org/paper/128217c0d1e99912ebc727c84686cc97a913b55f,arXiv.org,2023.0,270.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2243322582', 'name': 'Weishi Li'}, {'authorId': '2243387143', 'name': 'Yong Peng'}, {'authorId': '2243404565', 'name': 'Miao Zhang'}, {'authorId': '2260506014', 'name': 'Liang Ding'}, {'authorId': '2247556302', 'name': 'Han Hu'}, {'authorId': '2248152216', 'name': 'Li Shen'}]","['Jingdong', 'Beijing Institute of Technology', 'National University of Defense Technology']",['China'],2023-09
2309.15729,Jiaxuan Chen,"Jiaxuan Chen, Yu Qi, Yueming Wang, Gang Pan",MindGPT: Interpreting What You See with Non-invasive Brain Recordings,"13 pages, 6 figures, submitted to anonymous conference",,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Decoding of seen visual contents with non-invasive brain recordings has important scientific and practical values. Efforts have been made to recover the seen images from brain signals. However, most existing approaches cannot faithfully reflect the visual contents due to insufficient image quality or semantic mismatches. Compared with reconstructing pixel-level visual images, speaking is a more efficient and effective way to explain visual information. Here we introduce a non-invasive neural decoder, termed as MindGPT, which interprets perceived visual stimuli into natural languages from fMRI signals. Specifically, our model builds upon a visually guided neural encoder with a cross-attention mechanism, which permits us to guide latent neural representations towards a desired language semantic direction in an end-to-end manner by the collaborative use of the large language model GPT. By doing so, we found that the neural representations of the MindGPT are explainable, which can be used to evaluate the contributions of visual properties to language semantics. Our experiments show that the generated word sequences truthfully represented the visual information (with essential details) conveyed in the seen stimuli. The results also suggested that with respect to language decoding tasks, the higher visual cortex (HVC) is more semantically informative than the lower visual cortex (LVC), and using only the HVC can recover most of the semantic information. The code of the MindGPT model will be publicly available at https://github.com/JxuanC/MindGPT. ","[{'version': 'v1', 'created': 'Wed, 27 Sep 2023 15:35:20 GMT'}]",2023-09-28,"[['Chen', 'Jiaxuan', ''], ['Qi', 'Yu', ''], ['Wang', 'Yueming', ''], ['Pan', 'Gang', '']]",0,1,2023-09-27,1,4,2,0,0,0,ad85bffd5fc1ecbdfeac97216e4387c545afafdd,263152435.0,https://www.semanticscholar.org/paper/ad85bffd5fc1ecbdfeac97216e4387c545afafdd,arXiv.org,2023.0,51.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Biology', 'source': 's2-fos-model'}]","[{'authorId': '2248018094', 'name': 'Jiaxuan Chen'}, {'authorId': '2249854335', 'name': 'Yu Qi'}, {'authorId': '2108895611', 'name': 'Yueming Wang'}, {'authorId': '144563871', 'name': 'Gang Pan'}]",['Zhejiang University'],['China'],2023-09
2309.15806,Chuanyang Zheng,"Chuanyang Zheng, Haiming Wang, Enze Xie, Zhengying Liu, Jiankai Sun,
  Huajian Xin, Jianhao Shen, Zhenguo Li, Yu Li",Lyra: Orchestrating Dual Correction in Automated Theorem Proving,Tech Report,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) present an intriguing avenue for exploration in the field of formal theorem proving. Nevertheless, their full potential, particularly concerning the mitigation of hallucinations and refinement through prover error messages, remains an area that has yet to be thoroughly investigated. To enhance the effectiveness of LLMs in the field, we introduce the Lyra, a new framework that employs two distinct correction mechanisms: Tool Correction (TC) and Conjecture Correction (CC). To implement Tool Correction in the post-processing of formal proofs, we leverage prior knowledge to utilize predefined prover tools (e.g., Sledgehammer) for guiding the replacement of incorrect tools. Tool Correction significantly contributes to mitigating hallucinations, thereby improving the overall accuracy of the proof. In addition, we introduce Conjecture Correction, an error feedback mechanism designed to interact with prover to refine formal proof conjectures with prover error messages. Compared to the previous refinement framework, the proposed Conjecture Correction refines generation with instruction but does not collect paired (generation, error & refinement) prompts. Our method has achieved state-of-the-art (SOTA) performance on both miniF2F validation (48.0% -> 55.3%) and test (45.5% -> 51.2%). We also present 3 IMO problems solved by Lyra. We believe Tool Correction (post-process for hallucination mitigation) and Conjecture Correction (subgoal adjustment from interaction with environment) could provide a promising avenue for future research in this field. ","[{'version': 'v1', 'created': 'Wed, 27 Sep 2023 17:29:41 GMT'}, {'version': 'v2', 'created': 'Mon, 2 Oct 2023 10:32:35 GMT'}]",2023-10-03,"[['Zheng', 'Chuanyang', ''], ['Wang', 'Haiming', ''], ['Xie', 'Enze', ''], ['Liu', 'Zhengying', ''], ['Sun', 'Jiankai', ''], ['Xin', 'Huajian', ''], ['Shen', 'Jianhao', ''], ['Li', 'Zhenguo', ''], ['Li', 'Yu', '']]",0,0,2023-09-27,2,9,2,0,0,0,1ddd474f8a7d596b7609d705f960d2d924ef8e00,262941755.0,https://www.semanticscholar.org/paper/1ddd474f8a7d596b7609d705f960d2d924ef8e00,arXiv.org,2023.0,57.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2238892973', 'name': 'Chuanyang Zheng'}, {'authorId': '2109589929', 'name': 'Haiming Wang'}, {'authorId': '2247612880', 'name': 'Enze Xie'}, {'authorId': '2239065052', 'name': 'Zhengying Liu'}, {'authorId': '2247730541', 'name': 'Jiankai Sun'}, {'authorId': '2238628841', 'name': 'Huajian Xin'}, {'authorId': '2115733983', 'name': 'Jianhao Shen'}, {'authorId': '121544682', 'name': 'Zheng Li'}, {'authorId': '2249753981', 'name': 'Yu Li'}]","['Sun Yat-sen University', 'Chinese University of Hong Kong', 'Huawei Technologies (China)']",['China'],2023-09
2309.16134,Zhenyu Wan,"Qing Huang, Zhenyu Wan, Zhenchang Xing, Changjing Wang, Jieshan Chen,
  Xiwei Xu, Qinghua Lu","Let's Chat to Find the APIs: Connecting Human, LLM and Knowledge Graph through AI Chain",Accepted on ASE'2023,,,,cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  API recommendation methods have evolved from literal and semantic keyword matching to query expansion and query clarification. The latest query clarification method is knowledge graph (KG)-based, but limitations include out-of-vocabulary (OOV) failures and rigid question templates. To address these limitations, we propose a novel knowledge-guided query clarification approach for API recommendation that leverages a large language model (LLM) guided by KG. We utilize the LLM as a neural knowledge base to overcome OOV failures, generating fluent and appropriate clarification questions and options. We also leverage the structured API knowledge and entity relationships stored in the KG to filter out noise, and transfer the optimal clarification path from KG to the LLM, increasing the efficiency of the clarification process. Our approach is designed as an AI chain that consists of five steps, each handled by a separate LLM call, to improve accuracy, efficiency, and fluency for query clarification in API recommendation. We verify the usefulness of each unit in our AI chain, which all received high scores close to a perfect 5. When compared to the baselines, our approach shows a significant improvement in MRR, with a maximum increase of 63.9% higher when the query statement is covered in KG and 37.2% when it is not. Ablation experiments reveal that the guidance of knowledge in the KG and the knowledge-guided pathfinding strategy are crucial for our approach's performance, resulting in a 19.0% and 22.2% increase in MAP, respectively. Our approach demonstrates a way to bridge the gap between KG and LLM, effectively compensating for the strengths and weaknesses of both. ","[{'version': 'v1', 'created': 'Thu, 28 Sep 2023 03:31:01 GMT'}]",2023-09-29,"[['Huang', 'Qing', ''], ['Wan', 'Zhenyu', ''], ['Xing', 'Zhenchang', ''], ['Wang', 'Changjing', ''], ['Chen', 'Jieshan', ''], ['Xu', 'Xiwei', ''], ['Lu', 'Qinghua', '']]",0,0,2023-09-28,1,7,1,0,0,0,0de4f608e8264e783548a462e4dde006d24fe94e,263132518.0,https://www.semanticscholar.org/paper/0de4f608e8264e783548a462e4dde006d24fe94e,International Conference on Automated Software Engineering,2023.0,50.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2181288908', 'name': 'Qing Huang'}, {'authorId': '2248164524', 'name': 'Zhenyu Wan'}, {'authorId': '2247838339', 'name': 'Zhenchang Xing'}, {'authorId': '10521673', 'name': 'Changjing Wang'}, {'authorId': '1421235702', 'name': 'Jieshan Chen'}, {'authorId': '3087664', 'name': 'Xiwei Xu'}, {'authorId': '2151674574', 'name': 'Qinghua Lu'}]","['Jiangxi Normal University', 'Data61']","['China', 'Australia']",2023-09
2309.16146,Ming Wang,"Ming Wang, Daling Wang, Wenfang Wu, Shi Feng, Yifei Zhang",T-COL: Generating Counterfactual Explanations for General User Preferences on Variable Machine Learning Systems,,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine learning (ML) based systems have been suffering a lack of interpretability. To address this problem, counterfactual explanations (CEs) have been proposed. CEs are unique as they provide workable suggestions to users, in addition to explaining why a certain outcome was predicted. However, the application of CEs has been hindered by two main challenges, namely general user preferences and variable ML systems. User preferences, in particular, tend to be general rather than specific feature values. Additionally, CEs need to be customized to suit the variability of ML models, while also maintaining robustness even when these validation models change. To overcome these challenges, we propose several possible general user preferences that have been validated by user research and map them to the properties of CEs. We also introduce a new method called \uline{T}ree-based \uline{C}onditions \uline{O}ptional \uline{L}inks (T-COL), which has two optional structures and several groups of conditions for generating CEs that can be adapted to general user preferences. Meanwhile, a group of conditions lead T-COL to generate more robust CEs that have higher validity when the ML model is replaced. We compared the properties of CEs generated by T-COL experimentally under different user preferences and demonstrated that T-COL is better suited for accommodating user preferences and variable ML systems compared to baseline methods including Large Language Models. ","[{'version': 'v1', 'created': 'Thu, 28 Sep 2023 03:51:49 GMT'}]",2023-09-29,"[['Wang', 'Ming', ''], ['Wang', 'Daling', ''], ['Wu', 'Wenfang', ''], ['Feng', 'Shi', ''], ['Zhang', 'Yifei', '']]",0,0,2023-09-28,1,5,1,0,0,0,13ab0fc6cba5020a53d9fe108d02c5996a29345b,263136579.0,https://www.semanticscholar.org/paper/13ab0fc6cba5020a53d9fe108d02c5996a29345b,arXiv.org,2023.0,54.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2249764692', 'name': 'Ming Wang'}, {'authorId': '2111226672', 'name': 'Daling Wang'}, {'authorId': '47203368', 'name': 'Wenfang Wu'}, {'authorId': '2087586948', 'name': 'Shi Feng'}, {'authorId': '2108463824', 'name': 'Yifei Zhang'}]","['University of Gttingen', 'Northeastern University']","['China', 'Germany']",2023-09
2309.16211,Zihao Zhu,"Zihao Zhu, Mingda Zhang, Shaokui Wei, Bingzhe Wu, Baoyuan Wu",VDC: Versatile Data Cleanser for Detecting Dirty Samples via Visual-Linguistic Inconsistency,"22 pages,5 figures,17 tables",,,,cs.CV cs.AI,http://creativecommons.org/licenses/by/4.0/,"  The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence of such dirty samples makes the DNNs vunerable and unreliable.Hence, it is critical to detect dirty samples to improve the quality and realiability of dataset. Existing detectors only focus on detecting poisoned samples or noisy labels, that are often prone to weak generalization when dealing with dirty samples from other domains.In this paper, we find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels. To capture the semantic inconsistency between modalities, we propose versatile data cleanser (VDC) leveraging the surpassing capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning.It consists of three consecutive modules: the visual question generation module to generate insightful questions about the image; the visual question answering module to acquire the semantics of the visual content by answering the questions with MLLM; followed by the visual answer evaluation module to evaluate the inconsistency.Extensive experiments demonstrate its superior performance and generalization to various categories and types of dirty samples. ","[{'version': 'v1', 'created': 'Thu, 28 Sep 2023 07:37:18 GMT'}]",2023-09-29,"[['Zhu', 'Zihao', ''], ['Zhang', 'Mingda', ''], ['Wei', 'Shaokui', ''], ['Wu', 'Bingzhe', ''], ['Wu', 'Baoyuan', '']]",0,0,2023-09-28,1,5,2,0,0,0,bcc700b750ced4796ed6102bd0c865d9e24449b7,263137881.0,https://www.semanticscholar.org/paper/bcc700b750ced4796ed6102bd0c865d9e24449b7,arXiv.org,2023.0,41.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '49659134', 'name': 'Zihao Zhu'}, {'authorId': '2365530', 'name': 'Mingda Zhang'}, {'authorId': '2173786903', 'name': 'Shaokui Wei'}, {'authorId': '2248061879', 'name': 'Bingzhe Wu'}, {'authorId': '2247856263', 'name': 'Baoyuan Wu'}]","['Chinese University of Hong Kong, Shenzhen', 'Tencent']",['China'],2023-09
2309.16231,Hanqing Zhang,"Hanqing Zhang, Sun Si, Haiming Wu, Dawei Song",Controllable Text Generation with Residual Memory Transformer,github:https://github.com/littlehacker26/Residual_Memory_Transformer,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large-scale Causal Language Models (CLMs), e.g., GPT3 and ChatGPT, have brought great success in text generation. However, it is still an open challenge to control the generation process of CLM while balancing flexibility, control granularity, and generation efficiency. In this paper, we provide a new alternative for controllable text generation (CTG), by designing a non-intrusive, lightweight control plugin to accompany the generation of CLM at arbitrary time steps. The proposed control plugin, namely Residual Memory Transformer (RMT), has an encoder-decoder setup, which can accept any types of control conditions and cooperate with CLM through a residual learning paradigm, to achieve a more flexible, general, and efficient CTG. Extensive experiments are carried out on various control tasks, in the form of both automatic and human evaluations. The results show the superiority of RMT over a range of state-of-the-art approaches, proving the effectiveness and versatility of our approach. ","[{'version': 'v1', 'created': 'Thu, 28 Sep 2023 08:13:33 GMT'}]",2023-09-29,"[['Zhang', 'Hanqing', ''], ['Si', 'Sun', ''], ['Wu', 'Haiming', ''], ['Song', 'Dawei', '']]",1,1,2023-09-28,1,4,1,2,0,2,bb2078b91db18ed031aed82e619bc50dad088b8b,263138761.0,https://www.semanticscholar.org/paper/bb2078b91db18ed031aed82e619bc50dad088b8b,arXiv.org,2023.0,35.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2119078848', 'name': 'Hanqing Zhang'}, {'authorId': '2248223530', 'name': 'Sun Si'}, {'authorId': '2248113896', 'name': 'Haiming Wu'}, {'authorId': '2252343724', 'name': 'Dawei Song'}]","['Tsinghua University', 'Beijing Institute of Technology']",['China'],2023-09
2309.16289,Zhiwei Fei,"Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Songyang
  Zhang, Kai Chen, Zongwen Shen, Jidong Ge",LawBench: Benchmarking Legal Knowledge of Large Language Models,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have demonstrated strong capabilities in various aspects. However, when applying them to the highly specialized, safe-critical legal domain, it is unclear how much legal knowledge they possess and whether they can reliably perform legal-related tasks. To address this gap, we propose a comprehensive evaluation benchmark LawBench. LawBench has been meticulously crafted to have precise assessment of the LLMs' legal capabilities from three cognitive levels: (1) Legal knowledge memorization: whether LLMs can memorize needed legal concepts, articles and facts; (2) Legal knowledge understanding: whether LLMs can comprehend entities, events and relationships within legal text; (3) Legal knowledge applying: whether LLMs can properly utilize their legal knowledge and make necessary reasoning steps to solve realistic legal tasks. LawBench contains 20 diverse tasks covering 5 task types: single-label classification (SLC), multi-label classification (MLC), regression, extraction and generation. We perform extensive evaluations of 51 LLMs on LawBench, including 20 multilingual LLMs, 22 Chinese-oriented LLMs and 9 legal specific LLMs. The results show that GPT-4 remains the best-performing LLM in the legal domain, surpassing the others by a significant margin. While fine-tuning LLMs on legal specific text brings certain improvements, we are still a long way from obtaining usable and reliable LLMs in legal tasks. All data, model predictions and evaluation code are released in https://github.com/open-compass/LawBench/. We hope this benchmark provides in-depth understanding of the LLMs' domain-specified capabilities and speed up the development of LLMs in the legal domain. ","[{'version': 'v1', 'created': 'Thu, 28 Sep 2023 09:35:59 GMT'}]",2023-09-29,"[['Fei', 'Zhiwei', ''], ['Shen', 'Xiaoyu', ''], ['Zhu', 'Dawei', ''], ['Zhou', 'Fengzhe', ''], ['Han', 'Zhuo', ''], ['Zhang', 'Songyang', ''], ['Chen', 'Kai', ''], ['Shen', 'Zongwen', ''], ['Ge', 'Jidong', '']]",0,1,2023-09-28,1,9,3,1,0,1,9099ee08e59cc33ed1c88d4708cf5c931bf46dc4,263134950.0,https://www.semanticscholar.org/paper/9099ee08e59cc33ed1c88d4708cf5c931bf46dc4,arXiv.org,2023.0,89.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Law', 'source': 's2-fos-model'}]","[{'authorId': '2248062587', 'name': 'Zhiwei Fei'}, {'authorId': '2249736444', 'name': 'Xiaoyu Shen'}, {'authorId': '47770349', 'name': 'D. Zhu'}, {'authorId': '2248972766', 'name': 'Fengzhe Zhou'}, {'authorId': '2247916656', 'name': 'Zhuo Han'}, {'authorId': '1734973476', 'name': 'Songyang Zhang'}, {'authorId': '2261273470', 'name': 'Kai Chen'}, {'authorId': '2249070589', 'name': 'Zongwen Shen'}, {'authorId': '2248015856', 'name': 'Jidong Ge'}]","['Shanghai Artificial Intelligence Laboratory', 'Nanjing University', 'Saarland University', 'Amazon']","['China', 'Germany', 'United States']",2023-09
2309.16292,Licheng Wen,"Licheng Wen, Daocheng Fu, Xin Li, Xinyu Cai, Tao Ma, Pinlong Cai, Min
  Dou, Botian Shi, Liang He, Yu Qiao",DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models,,,,,cs.RO cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. Leveraging large language models with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. Extensive experiments prove DiLu's capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. Moreover, DiLu is able to directly acquire experiences from real-world datasets which highlights its potential to be deployed on practical autonomous driving systems. To the best of our knowledge, we are the first to instill knowledge-driven capability into autonomous driving systems from the perspective of how humans drive. ","[{'version': 'v1', 'created': 'Thu, 28 Sep 2023 09:41:35 GMT'}]",2023-09-29,"[['Wen', 'Licheng', ''], ['Fu', 'Daocheng', ''], ['Li', 'Xin', ''], ['Cai', 'Xinyu', ''], ['Ma', 'Tao', ''], ['Cai', 'Pinlong', ''], ['Dou', 'Min', ''], ['Shi', 'Botian', ''], ['He', 'Liang', ''], ['Qiao', 'Yu', '']]",0,0,2023-09-28,1,10,2,0,0,0,3cbfe152220de84ecf8059fa50c47587a3134c86,263136146.0,https://www.semanticscholar.org/paper/3cbfe152220de84ecf8059fa50c47587a3134c86,arXiv.org,2023.0,39.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '153109152', 'name': 'Licheng Wen'}, {'authorId': '150967272', 'name': 'Daocheng Fu'}, {'authorId': '2153898327', 'name': 'Xin Li'}, {'authorId': '2239169372', 'name': 'Xinyu Cai'}, {'authorId': '1901958', 'name': 'Tengyu Ma'}, {'authorId': '26978261', 'name': 'Pinlong Cai'}, {'authorId': '2197075911', 'name': 'Min Dou'}, {'authorId': '119700639', 'name': 'Botian Shi'}, {'authorId': '2220657993', 'name': 'Liang He'}, {'authorId': '145858545', 'name': 'Y. Qiao'}]","['Shanghai Artificial Intelligence Laboratory', 'East China Normal University', 'Chinese University of Hong Kong']",['China'],2023-09
2309.16298,Yingwei Ma,"Yingwei Ma and Yue Liu and Yue Yu and Yuanliang Zhang and Yu Jiang and
  Changjian Wang and Shanshan Li",At Which Training Stage Does Code Data Help LLMs Reasoning?,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have exhibited remarkable reasoning capabilities and become the foundation of language technologies. Inspired by the great success of code data in training LLMs, we naturally wonder at which training stage introducing code data can really help LLMs reasoning. To this end, this paper systematically explores the impact of code data on LLMs at different stages. Concretely, we introduce the code data at the pre-training stage, instruction-tuning stage, and both of them, respectively. Then, the reasoning capability of LLMs is comprehensively and fairly evaluated via six reasoning tasks in five domains. We critically analyze the experimental results and provide conclusions with insights. First, pre-training LLMs with the mixture of code and text can significantly enhance LLMs' general reasoning capability almost without negative transfer on other tasks. Besides, at the instruction-tuning stage, code data endows LLMs the task-specific reasoning capability. Moreover, the dynamic mixing strategy of code and text data assists LLMs to learn reasoning capability step-by-step during training. These insights deepen the understanding of LLMs regarding reasoning ability for their application, such as scientific question answering, legal support, etc. The source code and model parameters are released at the link:~\url{https://github.com/yingweima2022/CodeLLM}. ","[{'version': 'v1', 'created': 'Thu, 28 Sep 2023 09:50:27 GMT'}, {'version': 'v2', 'created': 'Sat, 30 Sep 2023 07:01:13 GMT'}]",2023-10-03,"[['Ma', 'Yingwei', ''], ['Liu', 'Yue', ''], ['Yu', 'Yue', ''], ['Zhang', 'Yuanliang', ''], ['Jiang', 'Yu', ''], ['Wang', 'Changjian', ''], ['Li', 'Shanshan', '']]",0,0,2023-09-28,2,7,1,0,0,0,1e0caa706e9d9fdad82d6713fa20b52975a1703b,263134628.0,https://www.semanticscholar.org/paper/1e0caa706e9d9fdad82d6713fa20b52975a1703b,arXiv.org,2023.0,39.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2249023339', 'name': 'Yingwei Ma'}, {'authorId': '2247965929', 'name': 'Yue Liu'}, {'authorId': '1633124736', 'name': 'Yue Yu'}, {'authorId': '2145783381', 'name': 'Yuanliang Zhang'}, {'authorId': '2249522683', 'name': 'Yu Jiang'}, {'authorId': '2249405261', 'name': 'Changjian Wang'}, {'authorId': '2197478793', 'name': 'Shanshan Li'}]","['National University of Defense Technology', 'Tsinghua University']",['China'],2023-09
2309.16639,Xuhai Xu,"Ruolan Wu, Chun Yu, Xiaole Pan, Yujia Liu, Ningning Zhang, Yue Fu,
  Yuhan Wang, Zhi Zheng, Li Chen, Qiaolei Jiang, Xuhai Xu, Yuanchun Shi",MindShift: Leveraging Large Language Models for Mental-States-Based Problematic Smartphone Use Intervention,,,,,cs.CL cs.AI cs.HC,http://creativecommons.org/licenses/by/4.0/,"  Problematic smartphone use negatively affects physical and mental health. Despite the wide range of prior research, existing persuasive techniques are not flexible enough to provide dynamic persuasion content based on users' physical contexts and mental states. We first conduct a Wizard-of-Oz study (N=12) and an interview study (N=10) to summarize the mental states behind problematic smartphone use: boredom, stress, and inertia. This informs our design of four persuasion strategies: understanding, comforting, evoking, and scaffolding habits. We leverage large language models (LLMs) to enable the automatic and dynamic generation of effective persuasion content. We develop MindShift, a novel LLM-powered problematic smartphone use intervention technique. MindShift takes users' in-the-moment physical contexts, mental states, app usage behaviors, users' goals & habits as input, and generates high-quality and flexible persuasive content with appropriate persuasion strategies. We conduct a 5-week field experiment (N=25) to compare MindShift with baseline techniques. The results show that MindShift significantly improves intervention acceptance rates by 17.8-22.5% and reduces smartphone use frequency by 12.1-14.4%. Moreover, users have a significant drop in smartphone addiction scale scores and a rise in self-efficacy. Our study sheds light on the potential of leveraging LLMs for context-aware persuasion in other behavior change domains. ","[{'version': 'v1', 'created': 'Thu, 28 Sep 2023 17:49:03 GMT'}]",2023-09-29,"[['Wu', 'Ruolan', ''], ['Yu', 'Chun', ''], ['Pan', 'Xiaole', ''], ['Liu', 'Yujia', ''], ['Zhang', 'Ningning', ''], ['Fu', 'Yue', ''], ['Wang', 'Yuhan', ''], ['Zheng', 'Zhi', ''], ['Chen', 'Li', ''], ['Jiang', 'Qiaolei', ''], ['Xu', 'Xuhai', ''], ['Shi', 'Yuanchun', '']]",0,0,2023-09-28,1,12,3,0,0,0,2b35579ecf8b487ed7e24bab4fc53941051f4dd0,263134184.0,https://www.semanticscholar.org/paper/2b35579ecf8b487ed7e24bab4fc53941051f4dd0,arXiv.org,2023.0,92.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Psychology', 'source': 's2-fos-model'}]","[{'authorId': '2256330034', 'name': 'Ruolan Wu'}, {'authorId': '2265952293', 'name': 'Chun Yu'}, {'authorId': '2248834172', 'name': 'Xiaole Pan'}, {'authorId': '2247775088', 'name': 'Yujia Liu'}, {'authorId': '2247972147', 'name': 'Ningning Zhang'}, {'authorId': '2257139682', 'name': 'Yue Fu'}, {'authorId': '2247789527', 'name': 'Yuhan Wang'}, {'authorId': '2256386334', 'name': 'Zhi Zheng'}, {'authorId': '2247970182', 'name': 'Li Chen'}, {'authorId': '2249057460', 'name': 'Qiaolei Jiang'}, {'authorId': '2247941519', 'name': 'Xuhai Xu'}, {'authorId': '2152861676', 'name': 'Yuanchun Shi'}]","['Beijing University of Posts and Telecommunications', 'China XUHAI XU,', 'Tsinghua University', 'University of Washington', 'Massachusetts Institute of Technology']","['China', 'United States']",2023-09
2309.16721,Xiaokai Qin,"Xiaokai Qin, Mingda Song, Yangguan Chen, Zhehong Ai, Jing Jiang",GPT-Lab: Next Generation Of Optimal Chemistry Discovery By GPT Driven Robotic Lab,,,,,cs.AI cs.RO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The integration of robots in chemical experiments has enhanced experimental efficiency, but lacking the human intelligence to comprehend literature, they seldom provide assistance in experimental design. Therefore, achieving full-process autonomy from experiment design to validation in self-driven laboratories (SDL) remains a challenge. The introduction of Generative Pre-trained Transformers (GPT), particularly GPT-4, into robotic experimentation offers a solution. We introduce GPT-Lab, a paradigm that employs GPT models to give robots human-like intelligence. With our robotic experimentation platform, GPT-Lab mines literature for materials and methods and validates findings through high-throughput synthesis. As a demonstration, GPT-Lab analyzed 500 articles, identified 18 potential reagents, and successfully produced an accurate humidity colorimetric sensor with a root mean square error (RMSE) of 2.68%. This showcases the rapid materials discovery and validation potential of our system. ","[{'version': 'v1', 'created': 'Fri, 15 Sep 2023 10:51:21 GMT'}]",2023-10-02,"[['Qin', 'Xiaokai', ''], ['Song', 'Mingda', ''], ['Chen', 'Yangguan', ''], ['Ai', 'Zhehong', ''], ['Jiang', 'Jing', '']]",0,1,2023-09-15,1,5,2,1,0,1,91b68391df0b16d22bffbbf4d0c09f13dee36561,263310436.0,https://www.semanticscholar.org/paper/91b68391df0b16d22bffbbf4d0c09f13dee36561,arXiv.org,2023.0,26.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2249728784', 'name': 'Xiaokai Qin'}, {'authorId': '2249576510', 'name': 'Mingda Song'}, {'authorId': '2249556346', 'name': 'Yangguan Chen'}, {'authorId': '2176580121', 'name': 'Zhehong Ai'}, {'authorId': '2249716397', 'name': 'Jing Jiang'}]",['Zhejiang Lab'],['China'],2023-09
2309.16804,Yu Li,"Yu Li, Shang Qu, Jili Shen, Shangchao Min and Zhou Yu",Curriculum-Driven Edubot: A Framework for Developing Language Learning Chatbots Through Synthesizing Conversational Data,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Chatbots have become popular in educational settings, revolutionizing how students interact with material and how teachers teach. We present Curriculum-Driven EduBot, a framework for developing a chatbot that combines the interactive features of chatbots with the systematic material of English textbooks to assist students in enhancing their conversational skills. We begin by extracting pertinent topics from textbooks and then using large language models to generate dialogues related to these topics. We then fine-tune an open-source LLM using our generated conversational data to create our curriculum-driven chatbot. User studies demonstrate that our chatbot outperforms ChatGPT in leading curriculum-based dialogues and adapting its dialogue to match the user's English proficiency level. By combining traditional textbook methodologies with conversational AI, our approach offers learners an interactive tool that aligns with their curriculum and provides user-tailored conversation practice. This facilitates meaningful student-bot dialogues and enriches the overall learning experience within the curriculum's pedagogical framework. ","[{'version': 'v1', 'created': 'Thu, 28 Sep 2023 19:14:18 GMT'}]",2023-10-02,"[['Li', 'Yu', ''], ['Qu', 'Shang', ''], ['Shen', 'Jili', ''], ['Min', 'Shangchao', ''], ['Yu', 'Zhou', '']]",1,1,2023-09-28,1,5,1,1,0,1,6bde8138560851914a9a9426c2a1ec3f11c6509f,263310830.0,https://www.semanticscholar.org/paper/6bde8138560851914a9a9426c2a1ec3f11c6509f,arXiv.org,2023.0,44.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Education', 'source': 's2-fos-model'}]","[{'authorId': '2116612786', 'name': 'Yu Li'}, {'authorId': '2249537855', 'name': 'Shang Qu'}, {'authorId': '2251087810', 'name': 'Jili Shen'}, {'authorId': '2249538586', 'name': 'Shangchao Min'}, {'authorId': '2249545171', 'name': 'Zhou Yu'}]","['University of Science and Technology of China', 'Columbia University', 'Zhejiang University']","['China', 'United States']",2023-09
2309.17061,Xin Cheng,"Xin Cheng and Xun Wang and Tao Ge and Si-Qing Chen and Furu Wei and
  Dongyan Zhao and Rui Yan",SCALE: Synergized Collaboration of Asymmetric Language Translation Engines,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we introduce SCALE, a collaborative framework that connects compact Specialized Translation Models (STMs) and general-purpose Large Language Models (LLMs) as one unified translation engine. By introducing translation from STM into the triplet in-context demonstrations, SCALE unlocks refinement and pivoting ability of LLM, thus mitigating language bias of LLM and parallel data bias of STM, enhancing LLM speciality without sacrificing generality, and facilitating continual learning without expensive LLM fine-tuning. Our comprehensive experiments show that SCALE significantly outperforms both few-shot LLMs (GPT-4) and specialized models (NLLB) in challenging low-resource settings. Moreover, in Xhosa to English translation, SCALE experiences consistent improvement by a 4 BLEURT score without tuning LLM and surpasses few-shot GPT-4 by 2.5 COMET score and 3.8 BLEURT score when equipped with a compact model consisting of merely 600M parameters. SCALE could also effectively exploit the existing language bias of LLMs by using an English-centric STM as a pivot for translation between any language pairs, outperforming few-shot GPT-4 by an average of 6 COMET points across eight translation directions. Furthermore we provide an in-depth analysis of SCALE's robustness, translation characteristics, and latency costs, providing solid foundation for future studies exploring the potential synergy between LLMs and more specialized, task-specific models. ","[{'version': 'v1', 'created': 'Fri, 29 Sep 2023 08:46:38 GMT'}]",2023-10-02,"[['Cheng', 'Xin', ''], ['Wang', 'Xun', ''], ['Ge', 'Tao', ''], ['Chen', 'Si-Qing', ''], ['Wei', 'Furu', ''], ['Zhao', 'Dongyan', ''], ['Yan', 'Rui', '']]",0,1,2023-09-29,1,7,2,2,1,1,e6fd045dc7fdb79dcef2f71e38c0edeb1b11ee0d,263310813.0,https://www.semanticscholar.org/paper/e6fd045dc7fdb79dcef2f71e38c0edeb1b11ee0d,arXiv.org,2023.0,52.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2193630544', 'name': 'Xin Cheng'}, {'authorId': '2193104542', 'name': 'Xun Wang'}, {'authorId': '2547492', 'name': 'Tao Ge'}, {'authorId': '2111638099', 'name': 'Si-Qing Chen'}, {'authorId': '2249539478', 'name': 'Furu Wei'}, {'authorId': '2253232138', 'name': 'Dongyan Zhao'}, {'authorId': '2249533146', 'name': 'Rui Yan'}]","['Peking University', 'Microsoft', 'Renmin University of China']","['China', 'United States']",2023-09
2309.17078,Qian Dong,"Qian Dong, Yiding Liu, Qingyao Ai, Zhijing Wu, Haitao Li, Yiqun Liu,
  Shuaiqiang Wang, Dawei Yin, Shaoping Ma",Aligning the Capabilities of Large Language Models with the Context of Information Retrieval via Contrastive Feedback,Arxiv version of RLCF,,,,cs.IR,http://creativecommons.org/licenses/by/4.0/,"  Information Retrieval (IR), the process of finding information to satisfy user's information needs, plays an essential role in modern people's lives. Recently, large language models (LLMs) have demonstrated remarkable capabilities across various tasks, some of which are important for IR. Nonetheless, LLMs frequently confront the issue of generating responses that lack specificity. This has limited the overall effectiveness of LLMs for IR in many cases. To address these issues, we present an unsupervised alignment framework called Reinforcement Learning from Contrastive Feedback (RLCF), which empowers LLMs to generate both high-quality and context-specific responses that suit the needs of IR tasks. Specifically, we construct contrastive feedback by comparing each document with its similar documents, and then propose a reward function named Batched-MRR to teach LLMs to generate responses that captures the fine-grained information that distinguish documents from their similar ones. To demonstrate the effectiveness of RLCF, we conducted experiments in two typical applications of LLMs in IR, i.e., data augmentation and summarization. The experimental results show that RLCF can effectively improve the performance of LLMs in IR context. ","[{'version': 'v1', 'created': 'Fri, 29 Sep 2023 09:14:53 GMT'}]",2023-10-02,"[['Dong', 'Qian', ''], ['Liu', 'Yiding', ''], ['Ai', 'Qingyao', ''], ['Wu', 'Zhijing', ''], ['Li', 'Haitao', ''], ['Liu', 'Yiqun', ''], ['Wang', 'Shuaiqiang', ''], ['Yin', 'Dawei', ''], ['Ma', 'Shaoping', '']]",0,0,2023-09-29,1,9,1,0,0,0,e99cec6dcee0a4161d6cd25a5631972ff35adb99,263310649.0,https://www.semanticscholar.org/paper/e99cec6dcee0a4161d6cd25a5631972ff35adb99,arXiv.org,2023.0,65.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2203368550', 'name': 'Qian Dong'}, {'authorId': '2249554788', 'name': 'Yiding Liu'}, {'authorId': '144922928', 'name': 'Qingyao Ai'}, {'authorId': '47039225', 'name': 'Zhijing Wu'}, {'authorId': '2108590438', 'name': 'Haitao Li'}, {'authorId': '46399371', 'name': 'Y. Liu'}, {'authorId': '2237948548', 'name': 'Shuaiqiang Wang'}, {'authorId': '2136400100', 'name': 'Dawei Yin'}, {'authorId': '8093158', 'name': 'Shaoping Ma'}]","['Baidu', 'Yiding Liu', 'Beijing Institute of Technology', 'Zhijing Wu', 'Shaoping Ma', 'Technological University Dawei', 'Tsinghua University', 'Linkping University', 'Shuaiqiang Wang']","['China', 'Sweden', 'Myanmar']",2023-09
2309.17176,Wanpeng Zhang,"Wanpeng Zhang, Zongqing Lu",RLAdapter: Bridging Large Language Models to Reinforcement Learning in Open Worlds,,,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While reinforcement learning (RL) shows remarkable success in decision-making problems, it often requires a lot of interactions with the environment, and in sparse-reward environments, it is challenging to learn meaningful policies. Large Language Models (LLMs) can potentially provide valuable guidance to agents in learning policies, thereby enhancing the performance of RL algorithms in such environments. However, LLMs often encounter difficulties in understanding downstream tasks, which hinders their ability to optimally assist agents in these tasks. A common approach to mitigating this issue is to fine-tune the LLMs with task-related data, enabling them to offer useful guidance for RL agents. However, this approach encounters several difficulties, such as inaccessible model weights or the need for significant computational resources, making it impractical. In this work, we introduce RLAdapter, a framework that builds a better connection between RL algorithms and LLMs by incorporating an adapter model. Within the RLAdapter framework, fine-tuning a lightweight language model with information generated during the training process of RL agents significantly aids LLMs in adapting to downstream tasks, thereby providing better guidance for RL agents. We conducted experiments to evaluate RLAdapter in the Crafter environment, and the results show that RLAdapter surpasses the SOTA baselines. Furthermore, agents under our framework exhibit common-sense behaviors that are absent in baseline models. ","[{'version': 'v1', 'created': 'Fri, 29 Sep 2023 12:16:19 GMT'}]",2023-10-02,"[['Zhang', 'Wanpeng', ''], ['Lu', 'Zongqing', '']]",0,0,2023-09-29,1,2,2,0,0,0,8f0051c2fe31e542904b85198aaf97aee5eac0e0,263310497.0,https://www.semanticscholar.org/paper/8f0051c2fe31e542904b85198aaf97aee5eac0e0,arXiv.org,2023.0,44.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2984723', 'name': 'Wanpeng Zhang'}, {'authorId': '2249581379', 'name': 'Zongqing Lu'}]",['Peking University'],['China'],2023-09
2309.17179,Xidong Feng,"Xidong Feng, Ziyu Wan, Muning Wen, Ying Wen, Weinan Zhang, Jun Wang",Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training,,,,,cs.LG cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) typically employ sampling or beam search, accompanied by prompts such as Chain-of-Thought (CoT), to boost reasoning and decoding ability. Recent work like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the reasoning capabilities of LLMs by utilizing tree-search algorithms to guide multi-step reasoning. These methods mainly focus on LLMs' reasoning ability during inference and heavily rely on human-designed prompts to activate LLM as a value function, which lacks general applicability and scalability. To address these limitations, we present an AlphaZero-like tree-search framework for LLMs (termed TS-LLM), systematically illustrating how tree-search with a learned value function can guide LLMs' decoding ability. TS-LLM distinguishes itself in two key ways: (1) Leveraging a learned value function, our approach can be generally applied to different tasks beyond reasoning (such as RLHF alignment), and LLMs of any size, without prompting advanced, large-scale models. (2) It can guide LLM's decoding during both inference and training. Empirical evaluations across reasoning, planning, and RLHF alignment tasks validate the effectiveness of TS-LLM, even on trees with a depth of 64. ","[{'version': 'v1', 'created': 'Fri, 29 Sep 2023 12:20:19 GMT'}]",2023-10-02,"[['Feng', 'Xidong', ''], ['Wan', 'Ziyu', ''], ['Wen', 'Muning', ''], ['Wen', 'Ying', ''], ['Zhang', 'Weinan', ''], ['Wang', 'Jun', '']]",0,0,2023-09-29,1,6,3,0,0,0,e8df1cf6742b50a15500b8dd3dde3942e9c91418,263310590.0,https://www.semanticscholar.org/paper/e8df1cf6742b50a15500b8dd3dde3942e9c91418,arXiv.org,2023.0,53.0,3.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1443767933', 'name': 'Xidong Feng'}, {'authorId': '2249533760', 'name': 'Ziyu Wan'}, {'authorId': '2111875607', 'name': 'Muning Wen'}, {'authorId': '49021167', 'name': 'Ying Wen'}, {'authorId': '2244690305', 'name': 'Weinan Zhang'}, {'authorId': '2256981980', 'name': 'Jun Wang'}]","['University College London', 'Shanghai Jiao Tong University']","['China', 'United Kingdom']",2023-09
2309.17272,Shuai Lu,"Baizhou Huang, Shuai Lu, Weizhu Chen, Xiaojun Wan, Nan Duan",Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency,Preprint version,,,,cs.CL cs.AI cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have exhibited remarkable ability in textual generation. However, in complex reasoning tasks such as code generation, generating the correct answer in a single attempt remains a formidable challenge for LLMs. Previous research has explored solutions by aggregating multiple outputs, leveraging the consistency among them. However, none of them have comprehensively captured this consistency from different perspectives. In this paper, we propose the Multi-Perspective Self-Consistency (MPSC) framework, a novel decoding strategy for LLM that incorporates both inter-consistency across outputs from multiple perspectives and intra-consistency within a single perspective. Specifically, we ask LLMs to sample multiple diverse outputs from various perspectives for a given query and then construct a multipartite graph based on them. With two predefined measures of consistency, we embed both inter- and intra-consistency information into the graph. The optimal choice is then determined based on consistency analysis in the graph. We conduct comprehensive evaluation on the code generation task by introducing solution, specification and test case as three perspectives. We leverage a code interpreter to quantitatively measure the inter-consistency and propose several intra-consistency measure functions. Our MPSC framework significantly boosts the performance on various popular benchmarks, including HumanEval (+17.60%), HumanEval Plus (+17.61%), MBPP (+6.50%) and CodeContests (+11.82%) in Pass@1, when compared to original outputs generated from ChatGPT, and even surpassing GPT-4. ","[{'version': 'v1', 'created': 'Fri, 29 Sep 2023 14:23:26 GMT'}]",2023-10-02,"[['Huang', 'Baizhou', ''], ['Lu', 'Shuai', ''], ['Chen', 'Weizhu', ''], ['Wan', 'Xiaojun', ''], ['Duan', 'Nan', '']]",1,1,2023-09-29,1,5,3,2,0,2,0d22f06a1f5ad9f62b2f35c126b514f927586c85,263310434.0,https://www.semanticscholar.org/paper/0d22f06a1f5ad9f62b2f35c126b514f927586c85,arXiv.org,2023.0,45.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2184278672', 'name': 'Baizhou Huang'}, {'authorId': '2115338656', 'name': 'Shuai Lu'}, {'authorId': '2249538838', 'name': 'Weizhu Chen'}, {'authorId': '2257016300', 'name': 'Xiaojun Wan'}, {'authorId': '46429989', 'name': 'Nan Duan'}]","['Peking University', 'Microsoft']",['China'],2023-09
2309.17288,Guangyao Chen,"Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, B\""orje F.
  Karlsson, Jie Fu, Yemin Shi",AutoAgents: A Framework for Automatic Agent Generation,,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have enabled remarkable advances in automated task-solving with multi-agent systems. However, most existing LLM-based multi-agent approaches rely on predefined agents to handle simple tasks, limiting the adaptability of multi-agent collaboration to different scenarios. Therefore, we introduce AutoAgents, an innovative framework that adaptively generates and coordinates multiple specialized agents to build an AI team according to different tasks. Specifically, AutoAgents couples the relationship between tasks and roles by dynamically generating multiple required agents based on task content and planning solutions for the current task based on the generated expert agents. Multiple specialized agents collaborate with each other to efficiently accomplish tasks. Concurrently, an observer role is incorporated into the framework to reflect on the designated plans and agents' responses and improve upon them. Our experiments on various benchmarks demonstrate that AutoAgents generates more coherent and accurate solutions than the existing multi-agent methods. This underscores the significance of assigning different roles to different tasks and of team cooperation, offering new perspectives for tackling complex tasks. The repository of this project is available at https://github.com/LinkSoul-AI/AutoAgents. ","[{'version': 'v1', 'created': 'Fri, 29 Sep 2023 14:46:30 GMT'}]",2023-10-02,"[['Chen', 'Guangyao', ''], ['Dong', 'Siwei', ''], ['Shu', 'Yu', ''], ['Zhang', 'Ge', ''], ['Sesay', 'Jaward', ''], ['Karlsson', 'Brje F.', ''], ['Fu', 'Jie', ''], ['Shi', 'Yemin', '']]",0,0,2023-09-29,1,8,1,0,0,0,2e085c1b3eb2b9a7e98a46d4ad9fabed5b1bd54d,263310605.0,https://www.semanticscholar.org/paper/2e085c1b3eb2b9a7e98a46d4ad9fabed5b1bd54d,arXiv.org,2023.0,40.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1745485', 'name': 'Guangyao Chen'}, {'authorId': '2252432259', 'name': 'Siwei Dong'}, {'authorId': '2066476157', 'name': 'Yu Shu'}, {'authorId': '2143853895', 'name': 'Ge Zhang'}, {'authorId': '2249549962', 'name': 'Jaward Sesay'}, {'authorId': '2047947436', 'name': 'Brje F. Karlsson'}, {'authorId': '2265967208', 'name': 'Jie Fu'}, {'authorId': '2249539861', 'name': 'Yemin Shi'}]","['Peking University', 'Beijing Academy of Artificial Intelligence', 'University of Waterloo', 'Hong Kong University of Science and Technology']","['China', 'Canada']",2023-09
2309.17382,Zhihan Liu,"Zhihan Liu, Hao Hu, Shenao Zhang, Hongyi Guo, Shuqi Ke, Boyi Liu,
  Zhaoran Wang","Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency",,,,,cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment, e.g., through an internal mechanism of reasoning. To this end, we propose a principled framework with provable regret guarantees to orchestrate reasoning and acting, which we call ``reason for future, act for now"" (\texttt{RAFA}). Specifically, we design a prompt template for reasoning that learns from the memory buffer and plans a future trajectory over a long horizon (``reason for future""). At each step, the LLM agent takes the initial action of the planned trajectory (``act for now""), stores the collected feedback in the memory buffer, and reinvokes the reasoning routine to replan the future trajectory from the new state.   The key idea is to cast reasoning in LLMs as learning and planning in Bayesian adaptive Markov decision processes (MDPs). Correspondingly, we prompt LLMs to form an updated posterior of the unknown environment from the memory buffer (learning) and generate an optimal trajectory for multiple future steps that maximizes a value function (planning). The learning and planning subroutines are performed in an ""in-context"" manner to emulate the actor-critic update for MDPs. Our theoretical analysis proves that the novel combination of long-term reasoning and short-term acting achieves a $\sqrt{T}$ regret. In particular, the regret bound highlights an intriguing interplay between the prior knowledge obtained through pretraining and the uncertainty reduction achieved by reasoning and acting. Our empirical validation shows that it outperforms various existing frameworks and achieves nearly perfect scores on a few benchmarks. ","[{'version': 'v1', 'created': 'Fri, 29 Sep 2023 16:36:39 GMT'}]",2023-10-02,"[['Liu', 'Zhihan', ''], ['Hu', 'Hao', ''], ['Zhang', 'Shenao', ''], ['Guo', 'Hongyi', ''], ['Ke', 'Shuqi', ''], ['Liu', 'Boyi', ''], ['Wang', 'Zhaoran', '']]",0,0,2023-09-29,1,7,2,0,0,0,d3ca116177369bf6fbe27de64506a2f401aca996,263310943.0,https://www.semanticscholar.org/paper/d3ca116177369bf6fbe27de64506a2f401aca996,arXiv.org,2023.0,87.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2249539089', 'name': 'Zhihan Liu'}, {'authorId': '2249545983', 'name': 'Hao Hu'}, {'authorId': '2145522248', 'name': 'Shenao Zhang'}, {'authorId': '2249533404', 'name': 'Hongyi Guo'}, {'authorId': '2249534528', 'name': 'Shuqi Ke'}, {'authorId': '2156641199', 'name': 'Boyi Liu'}, {'authorId': '50218397', 'name': 'Zhaoran Wang'}]","['Tsinghua University', 'Northwestern University', 'Chinese University of Hong Kong']","['China', 'United States']",2023-09
2309.17415,Jiahao Ying,"Jiahao Ying, Yixin Cao, Kai Xiong, Yidong He, Long Cui, Yongbin Liu",Intuitive or Dependent? Investigating LLMs' Robustness to Conflicting Prompts,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  This paper explores the robustness of LLMs' preference to their internal memory or the given prompt, which may contain contrasting information in real-world applications due to noise or task settings. To this end, we establish a quantitative benchmarking framework and conduct the role playing intervention to control LLMs' preference. In specific, we define two types of robustness, factual robustness targeting the ability to identify the correct fact from prompts or memory, and decision style to categorize LLMs' behavior in making consistent choices -- assuming there is no definitive ""right"" answer -- intuitive, dependent, or rational based on cognitive theory. Our findings, derived from extensive experiments on seven open-source and closed-source LLMs, reveal that these models are highly susceptible to misleading prompts, especially for instructing commonsense knowledge. While detailed instructions can mitigate the selection of misleading answers, they also increase the incidence of invalid responses. After Unraveling the preference, we intervene different sized LLMs through specific style of role instruction, showing their varying upper bound of robustness and adaptivity. ","[{'version': 'v1', 'created': 'Fri, 29 Sep 2023 17:26:03 GMT'}, {'version': 'v2', 'created': 'Tue, 3 Oct 2023 05:16:36 GMT'}]",2023-10-04,"[['Ying', 'Jiahao', ''], ['Cao', 'Yixin', ''], ['Xiong', 'Kai', ''], ['He', 'Yidong', ''], ['Cui', 'Long', ''], ['Liu', 'Yongbin', '']]",0,0,2023-09-29,2,6,1,0,0,0,dc16e37e21f08f753889a5defc81245d84eb2f8d,263310759.0,https://www.semanticscholar.org/paper/dc16e37e21f08f753889a5defc81245d84eb2f8d,arXiv.org,2023.0,38.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2249532957', 'name': 'Jiahao Ying'}, {'authorId': '145014675', 'name': 'Yixin Cao'}, {'authorId': '2249539564', 'name': 'Kai Xiong'}, {'authorId': '2256764940', 'name': 'Yidong He'}, {'authorId': '2249538512', 'name': 'Long Cui'}, {'authorId': '2249562267', 'name': 'Yongbin Liu'}]","['Harbin Institute of Technology', 'University of South China', 'Singapore Management University']","['China', 'Singapore']",2023-09
2309.17452,Zhibin Gou,"Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie
  Huang, Nan Duan, Weizhu Chen",ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving,First two authors equal contribution,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath-70B by 22% absolute. ToRA-Code-34B is also the first open-source model that achieves an accuracy exceeding 50% on MATH, which significantly outperforms GPT-4's CoT result, and is competitive with GPT-4 solving problems with programs. Additionally, we conduct a comprehensive analysis of the benefits and remaining challenges of tool interaction for mathematical reasoning, providing valuable insights for future research. ","[{'version': 'v1', 'created': 'Fri, 29 Sep 2023 17:59:38 GMT'}, {'version': 'v2', 'created': 'Wed, 4 Oct 2023 08:13:03 GMT'}]",2023-10-05,"[['Gou', 'Zhibin', ''], ['Shao', 'Zhihong', ''], ['Gong', 'Yeyun', ''], ['Shen', 'Yelong', ''], ['Yang', 'Yujiu', ''], ['Huang', 'Minlie', ''], ['Duan', 'Nan', ''], ['Chen', 'Weizhu', '']]",0,1,2023-09-29,2,8,2,1,0,1,765225535151b5908d99a96a509729795c2eb840,263310365.0,https://www.semanticscholar.org/paper/765225535151b5908d99a96a509729795c2eb840,arXiv.org,2023.0,64.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1797090', 'name': 'Zhibin Gou'}, {'authorId': '144485528', 'name': 'Zhihong Shao'}, {'authorId': '2254121650', 'name': 'Yeyun Gong'}, {'authorId': '2237948786', 'name': 'Yelong Shen'}, {'authorId': '2253837985', 'name': 'Yujiu Yang'}, {'authorId': '2249709440', 'name': 'Minlie Huang'}, {'authorId': '46429989', 'name': 'Nan Duan'}, {'authorId': '2249538838', 'name': 'Weizhu Chen'}]","['Tsinghua University', 'Microsoft']","['China', 'United States']",2023-09
2310.00074,Hangfeng He,"Hangfeng He, Hongming Zhang, Dan Roth",SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  To comprehensively assess the capacity of current models for complex reasoning, it is crucial to assess their step-by-step reasoning in a scalable manner. Established reference-based evaluation metrics rely on human-annotated reasoning chains to assess the model-derived chains. However, such ``gold-standard'' human-written reasoning chains may not be unique and their acquisition is often labor-intensive. Existing reference-free reasoning metrics eliminate the need for human-crafted reasoning chains as references, but they typically require fine-tuning on datasets with human-derived reasoning chains, which complicates the process and raises concerns regarding generalizability across diverse datasets. To address these challenges, we harness GPT-4 to automatically evaluate reasoning chain quality, obviating the need for human-crafted references. Leveraging the Socratic method, we devise tailored prompts to enhance reference-free reasoning evaluation, which we term SocREval (Socratic method for Reasoning Evaluation). Empirical results from four human annotated datasets reveal that SocREval significantly improves GPT-4's performance, surpassing existing reference-free and reference-based reasoning evaluation metrics. Beyond its demonstrated efficacy, our proposed framework, large language models (LLMs) with the Socratic method, proves to be both cost-efficient and robust to prompt writing and example selection, as substantiated by our in-depth analysis. ","[{'version': 'v1', 'created': 'Fri, 29 Sep 2023 18:25:46 GMT'}]",2023-10-03,"[['He', 'Hangfeng', ''], ['Zhang', 'Hongming', ''], ['Roth', 'Dan', '']]",0,1,2023-09-29,1,3,2,1,0,1,12dc76b5f04fb1fc34f94f26632d193305a97e7e,263334142.0,https://www.semanticscholar.org/paper/12dc76b5f04fb1fc34f94f26632d193305a97e7e,arXiv.org,2023.0,45.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2256964816', 'name': 'Hangfeng He'}, {'authorId': '2111112132', 'name': 'Hongming Zhang'}, {'authorId': '2249759427', 'name': 'Dan Roth'}]","['University of Rochester', 'University of Pennsylvania', 'Tencent']","['China', 'United States']",2023-09
2310.00259,Zouying Cao,"Zouying Cao, Yifei Yang, Hai Zhao",AutoHall: Automated Hallucination Dataset Generation for Large Language Models,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While Large language models (LLMs) have garnered widespread applications across various domains due to their powerful language understanding and generation capabilities, the detection of non-factual or hallucinatory content generated by LLMs remains scarce. Currently, one significant challenge in hallucination detection is the laborious task of time-consuming and expensive manual annotation of the hallucinatory generation. To address this issue, this paper first introduces a method for automatically constructing model-specific hallucination datasets based on existing fact-checking datasets called AutoHall. Furthermore, we propose a zero-resource and black-box hallucination detection method based on self-contradiction. We conduct experiments towards prevalent open-/closed-source LLMs, achieving superior hallucination detection performance compared to extant baselines. Moreover, our experiments reveal variations in hallucination proportions and types among different models. ","[{'version': 'v1', 'created': 'Sat, 30 Sep 2023 05:20:02 GMT'}]",2023-10-03,"[['Cao', 'Zouying', ''], ['Yang', 'Yifei', ''], ['Zhao', 'Hai', '']]",0,0,2023-09-30,1,3,1,0,0,0,bb3cc013c462ff2bf3dc5be90f731ebf34996f86,263334406.0,https://www.semanticscholar.org/paper/bb3cc013c462ff2bf3dc5be90f731ebf34996f86,arXiv.org,2023.0,40.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2253004027', 'name': 'Zouying Cao'}, {'authorId': '2108989265', 'name': 'Yifei Yang'}, {'authorId': '2251173128', 'name': 'Hai Zhao'}]",['Shanghai Jiao Tong University'],['China'],2023-09
2310.00297,Jianhao Yan,"Jianhao Yan, Jin Xu, Chiyu Song, Chenming Wu, Yafu Li, Yue Zhang",Understanding In-Context Learning from Repetitions,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  This paper explores the elusive mechanism underpinning in-context learning in Large Language Models (LLMs). Our work provides a novel perspective by examining in-context learning via the lens of surface repetitions. We quantitatively investigate the role of surface features in text generation, and empirically establish the existence of \emph{token co-occurrence reinforcement}, a principle that strengthens the relationship between two tokens based on their contextual co-occurrences. By investigating the dual impacts of these features, our research illuminates the internal workings of in-context learning and expounds on the reasons for its failures. This paper provides an essential contribution to the understanding of in-context learning and its potential limitations, providing a fresh perspective on this exciting capability. ","[{'version': 'v1', 'created': 'Sat, 30 Sep 2023 08:13:49 GMT'}]",2023-10-03,"[['Yan', 'Jianhao', ''], ['Xu', 'Jin', ''], ['Song', 'Chiyu', ''], ['Wu', 'Chenming', ''], ['Li', 'Yafu', ''], ['Zhang', 'Yue', '']]",0,0,2023-09-30,1,6,1,0,0,0,a865d04c266fd2b3ea820b5741b7420779db9f73,263334398.0,https://www.semanticscholar.org/paper/a865d04c266fd2b3ea820b5741b7420779db9f73,arXiv.org,2023.0,45.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2249842908', 'name': 'Jianhao Yan'}, {'authorId': '2252968561', 'name': 'Jin Xu'}, {'authorId': '2254053209', 'name': 'Chiyu Song'}, {'authorId': '2250291793', 'name': 'Chenming Wu'}, {'authorId': '2110450452', 'name': 'Yafu Li'}, {'authorId': '2249762135', 'name': 'Yue Zhang'}]","['Westlake University', 'Baidu', 'Zhejiang University', 'Tsinghua University', 'Institute for Advanced Study']","['China', 'United States']",2023-09
2310.00322,Chengdong Ma,"Chengdong Ma, Ziran Yang, Minquan Gao, Hai Ci, Jun Gao, Xuehai Pan,
  Yaodong Yang",Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language Models,,,,,cs.CL cs.GT,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deployable Large Language Models (LLMs) must conform to the criterion of helpfulness and harmlessness, thereby achieving consistency between LLMs outputs and human values. Red-teaming techniques constitute a critical way towards this criterion. Existing work rely solely on manual red team designs and heuristic adversarial prompts for vulnerability detection and optimization. These approaches lack rigorous mathematical formulation, thus limiting the exploration of diverse attack strategy within quantifiable measure and optimization of LLMs under convergence guarantees. In this paper, we present Red-teaming Game (RTG), a general game-theoretic framework without manual annotation. RTG is designed for analyzing the multi-turn attack and defense interactions between Red-team language Models (RLMs) and Blue-team Language Model (BLM). Within the RTG, we propose Gamified Red-teaming Solver (GRTS) with diversity measure of the semantic space. GRTS is an automated red teaming technique to solve RTG towards Nash equilibrium through meta-game analysis, which corresponds to the theoretically guaranteed optimization direction of both RLMs and BLM. Empirical results in multi-turn attacks with RLMs show that GRTS autonomously discovered diverse attack strategies and effectively improved security of LLMs, outperforming existing heuristic red-team designs. Overall, RTG has established a foundational framework for red teaming tasks and constructed a new scalable oversight technique for alignment. ","[{'version': 'v1', 'created': 'Sat, 30 Sep 2023 09:35:50 GMT'}]",2023-10-03,"[['Ma', 'Chengdong', ''], ['Yang', 'Ziran', ''], ['Gao', 'Minquan', ''], ['Ci', 'Hai', ''], ['Gao', 'Jun', ''], ['Pan', 'Xuehai', ''], ['Yang', 'Yaodong', '']]",0,0,2023-09-30,1,7,2,0,0,0,e33ff3cf8e209762ca27ca320cca248198194833,263334034.0,https://www.semanticscholar.org/paper/e33ff3cf8e209762ca27ca320cca248198194833,arXiv.org,2023.0,88.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2250489869', 'name': 'Chengdong Ma'}, {'authorId': '2249946745', 'name': 'Ziran Yang'}, {'authorId': '2188802636', 'name': 'Minquan Gao'}, {'authorId': '2249760819', 'name': 'Hai Ci'}, {'authorId': '2256594567', 'name': 'Jun Gao'}, {'authorId': '2190800297', 'name': 'Xuehai Pan'}, {'authorId': '2249834874', 'name': 'Yaodong Yang'}]","['Beijing University of Posts and Telecommunications', 'Peking University']",['China'],2023-09
2310.00378,Zhaowei Zhang,"Zhaowei Zhang, Fengshuo Bai, Jun Gao, Yaodong Yang",Measuring Value Understanding in Language Models through Discriminator-Critique Gap,,,,,cs.CL cs.AI cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advancements in Large Language Models (LLMs) have heightened concerns about their potential misalignment with human values. However, evaluating their grasp of these values is complex due to their intricate and adaptable nature. We argue that truly understanding values in LLMs requires considering both ""know what"" and ""know why"". To this end, we present the Value Understanding Measurement (VUM) framework that quantitatively assess both ""know what"" and ""know why"" by measuring the discriminator-critique gap related to human values. Using the Schwartz Value Survey, we specify our evaluation values and develop a thousand-level dialogue dataset with GPT-4. Our assessment looks at both the value alignment of LLM's outputs compared to baseline answers and how LLM responses align with reasons for value recognition versus GPT-4's annotations. We evaluate five representative LLMs and provide strong evidence that the scaling law significantly impacts ""know what"" but not much on ""know why"", which has consistently maintained a high level. This may further suggest that LLMs might craft plausible explanations based on the provided context without truly understanding their inherent value, indicating potential risks. ","[{'version': 'v1', 'created': 'Sat, 30 Sep 2023 13:47:55 GMT'}]",2023-10-03,"[['Zhang', 'Zhaowei', ''], ['Bai', 'Fengshuo', ''], ['Gao', 'Jun', ''], ['Yang', 'Yaodong', '']]",0,1,2023-09-30,1,4,3,1,0,1,559cd16f17e9a77dc505b8ffcc5d5fab32c07a76,263334060.0,https://www.semanticscholar.org/paper/559cd16f17e9a77dc505b8ffcc5d5fab32c07a76,arXiv.org,2023.0,32.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Psychology', 'source': 's2-fos-model'}]","[{'authorId': '2174174943', 'name': 'Zhaowei Zhang'}, {'authorId': '2216334065', 'name': 'Fengshuo Bai'}, {'authorId': '2256594567', 'name': 'Jun Gao'}, {'authorId': '2249834874', 'name': 'Yaodong Yang'}]","['Peking University', 'Beijing Institute for General Artificial Intelligence (BIGAI)', 'Beijing University of Posts and Telecommunications']",['China'],2023-09
2310.00385,Fei Zhao,"Fei Zhao, Taotian Pang, Zhen Wu, Zheng Ma, Shujian Huang, Xinyu Dai",Dynamic Demonstrations Controller for In-Context Learning,Under review,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In-Context Learning (ICL) is a new paradigm for natural language processing (NLP), where a large language model (LLM) observes a small number of demonstrations and a test instance as its input, and directly makes predictions without updating model parameters. Previous studies have revealed that ICL is sensitive to the selection and the ordering of demonstrations. However, there are few studies regarding the impact of the demonstration number on the ICL performance within a limited input length of LLM, because it is commonly believed that the number of demonstrations is positively correlated with model performance. In this paper, we found this conclusion does not always hold true. Through pilot experiments, we discover that increasing the number of demonstrations does not necessarily lead to improved performance. Building upon this insight, we propose a Dynamic Demonstrations Controller (D$^2$Controller), which can improve the ICL performance by adjusting the number of demonstrations dynamically. The experimental results show that D$^2$Controller yields a 5.4% relative improvement on eight different sizes of LLMs across ten datasets. Moreover, we also extend our method to previous ICL models and achieve competitive results. ","[{'version': 'v1', 'created': 'Sat, 30 Sep 2023 14:04:22 GMT'}]",2023-10-03,"[['Zhao', 'Fei', ''], ['Pang', 'Taotian', ''], ['Wu', 'Zhen', ''], ['Ma', 'Zheng', ''], ['Huang', 'Shujian', ''], ['Dai', 'Xinyu', '']]",0,0,2023-09-30,1,6,2,0,0,0,a3cfcd731331dc81884be01e28a617fcf77b2fec,263334229.0,https://www.semanticscholar.org/paper/a3cfcd731331dc81884be01e28a617fcf77b2fec,arXiv.org,2023.0,28.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2152335026', 'name': 'Fei Zhao'}, {'authorId': '2249758857', 'name': 'Taotian Pang'}, {'authorId': '2250121772', 'name': 'Zhen Wu'}, {'authorId': '2249807914', 'name': 'Zheng Ma'}, {'authorId': '2046010', 'name': 'Shujian Huang'}, {'authorId': '3035069', 'name': 'Xinyu Dai'}]",['Nanjing University'],['China'],2023-09
2310.00399,Yan Xiao,"Yan Xiao, Xinyue Zuo, Lei Xue, Kailong Wang, Jin Song Dong and Ivan
  Beschastnikh",Empirical Study on Transformer-based Techniques for Software Engineering,,,,,cs.SE,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Many Transformer-based pre-trained models for code have been developed and applied to code-related tasks. In this paper, we review the existing literature, examine the suitability of model architectures for different tasks, and look at the generalization ability of models on different datasets, and their resource consumption.   We examine three very representative pre-trained models for code: CodeBERT, CodeGPT, and CodeT5, and conduct experiments on the top-4 most targeted software engineering tasks that we found in our literature survey: Code Summarization, Bug Fixing, Bug Detection, and Code Search. In our study, we showcase the capability of decoder-only models (CodeGPT) for specific generation tasks under state-of-the-art evaluation metrics and contest the common belief that the encoder-decoder architecture is optimal for general-purpose coding tasks. Additionally, we found that the most frequently used models are not necessarily the most suitable for certain applications and the developers' needs are not adequately addressed by current research. As well, we found that the benchmark and frequent dataset for Bug Fixing and Code Summarization both fail to enable models to generalize onto other datasets for the same task (the frequent dataset refers to the dataset with the highest frequency used in literature other than the benchmark). We use statistical testing to support our conclusions from experiments. Finally, CodeBERT is highly efficient for understanding tasks, whereas CodeT5's efficiency for generation tasks is in doubt, as the highest resource consumption does not guarantee a consistent better performance on different metrics. We also discuss the numerous practical issues in advancing future research on transformer-based models for code-related tasks. ","[{'version': 'v1', 'created': 'Sat, 30 Sep 2023 14:45:22 GMT'}]",2023-10-03,"[['Xiao', 'Yan', ''], ['Zuo', 'Xinyue', ''], ['Xue', 'Lei', ''], ['Wang', 'Kailong', ''], ['Dong', 'Jin Song', ''], ['Beschastnikh', 'Ivan', '']]",0,1,2023-09-30,1,6,1,0,0,0,0393ec6aad1b5240bb3c9df5ed11b6cbbb398085,263333937.0,https://www.semanticscholar.org/paper/0393ec6aad1b5240bb3c9df5ed11b6cbbb398085,arXiv.org,2023.0,80.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2249865400', 'name': 'Yan Xiao'}, {'authorId': '2249763603', 'name': 'Xinyue Zuo'}, {'authorId': '2250333773', 'name': 'Lei Xue'}, {'authorId': '2249932682', 'name': 'Kailong Wang'}, {'authorId': '2252658766', 'name': 'Jin Song Dong'}, {'authorId': '2637596', 'name': 'Ivan Beschastnikh'}]","['University of British Columbia', 'National University of Singapore', 'Huazhong University of Science and Technology', 'Sun Yat-sen University']","['Canada', 'China', 'Singapore']",2023-09
2310.00492,Xuansheng Wu,"Xuansheng Wu, Wenlin Yao, Jianshu Chen, Xiaoman Pan, Xiaoyang Wang,
  Ninghao Liu, Dong Yu",From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning,"28 pages, 13 figures, 12 tables",,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Large Language Models (LLMs) have achieved remarkable success, demonstrating powerful instruction-following capabilities across diverse tasks. Instruction fine-tuning is critical in enabling LLMs to align with user intentions and effectively follow instructions. In this work, we investigate how instruction fine-tuning modifies pre-trained models, focusing on two perspectives: instruction recognition and knowledge evolution. To study the behavior shift of LLMs, we employ a suite of local and global explanation methods, including a gradient-based approach for input-output attribution and techniques for interpreting patterns and concepts in self-attention and feed-forward layers. Our findings reveal three significant impacts of instruction fine-tuning: 1) It empowers LLMs to better recognize the instruction parts from user prompts, thereby facilitating high-quality response generation and addressing the ``lost-in-the-middle'' issue observed in pre-trained models; 2) It aligns the knowledge stored in feed-forward layers with user-oriented tasks, exhibiting minimal shifts across linguistic levels. 3) It facilitates the learning of word-word relations with instruction verbs through the self-attention mechanism, particularly in the lower and middle layers, indicating enhanced recognition of instruction words. These insights contribute to a deeper understanding of the behavior shifts in LLMs after instruction fine-tuning and lay the groundwork for future research aimed at interpreting and optimizing LLMs for various applications. We will release our code and data soon. ","[{'version': 'v1', 'created': 'Sat, 30 Sep 2023 21:16:05 GMT'}]",2023-10-03,"[['Wu', 'Xuansheng', ''], ['Yao', 'Wenlin', ''], ['Chen', 'Jianshu', ''], ['Pan', 'Xiaoman', ''], ['Wang', 'Xiaoyang', ''], ['Liu', 'Ninghao', ''], ['Yu', 'Dong', '']]",0,0,2023-09-30,1,7,3,0,0,0,04b880e1e32f37b3796d41a47d013fa07095ae32,263334329.0,https://www.semanticscholar.org/paper/04b880e1e32f37b3796d41a47d013fa07095ae32,arXiv.org,2023.0,66.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2145346360', 'name': 'Xuansheng Wu'}, {'authorId': '2087264100', 'name': 'Wenlin Yao'}, {'authorId': '2108276402', 'name': 'Jianshu Chen'}, {'authorId': '2243367575', 'name': 'Xiaoman Pan'}, {'authorId': '2250363276', 'name': 'Xiaoyang Wang'}, {'authorId': '2256183798', 'name': 'Ninghao Liu'}, {'authorId': '2256336899', 'name': 'Dong Yu'}]","['University of Georgia', 'Tencent']","['China', 'United States']",2023-09
2310.01432,Zongjie Li,"Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Tianxiang Li,
  Shuai Wang, Cuiyun Gao, Yang Liu",Split and Merge: Aligning Position Biases in Large Language Model based Evaluators,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have shown promise as automated evaluators for assessing the quality of answers generated by AI systems. However, these LLM-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content. To address this limitation, we propose PORTIA, an alignment-based system designed to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner. Specifically, PORTIA splits the answers into multiple segments, aligns similar content across candidate answers, and then merges them back into a single prompt for evaluation by LLMs. We conducted extensive experiments with six diverse LLMs to evaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances the consistency rates for all the models and comparison forms tested, achieving an average relative improvement of 47.46%. Remarkably, PORTIA enables less advanced GPT models to achieve 88% agreement with the state-of-the-art GPT-4 model at just 10% of the cost. Furthermore, it rectifies around 80% of the position bias instances within the GPT-4 model, elevating its consistency rate up to 98%. Subsequent human evaluations indicate that the PORTIA-enhanced GPT-3.5 model can even surpass the standalone GPT-4 in terms of alignment with human evaluators. These findings highlight PORTIA's ability to correct position bias, improve LLM consistency, and boost performance while keeping cost-efficiency. This represents a valuable step toward a more reliable and scalable use of LLMs for automated evaluations across diverse applications. ","[{'version': 'v1', 'created': 'Fri, 29 Sep 2023 14:38:58 GMT'}]",2023-10-04,"[['Li', 'Zongjie', ''], ['Wang', 'Chaozheng', ''], ['Ma', 'Pingchuan', ''], ['Wu', 'Daoyuan', ''], ['Li', 'Tianxiang', ''], ['Wang', 'Shuai', ''], ['Gao', 'Cuiyun', ''], ['Liu', 'Yang', '']]",0,1,2023-09-29,1,8,2,2,0,2,2e0f66e626a69da131e486da451218bbabfe7d3b,263608872.0,https://www.semanticscholar.org/paper/2e0f66e626a69da131e486da451218bbabfe7d3b,arXiv.org,2023.0,41.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '2118207559', 'name': 'Zongjie Li'}, {'authorId': '2135764153', 'name': 'Chaozheng Wang'}, {'authorId': '1384480816', 'name': 'Pingchuan Ma'}, {'authorId': '2253859864', 'name': 'Daoyuan Wu'}, {'authorId': '2239108164', 'name': 'Tianxiang Li'}, {'authorId': '2243949366', 'name': 'Shuai Wang'}, {'authorId': '2337550', 'name': 'Cuiyun Gao'}, {'authorId': '2254850265', 'name': 'Yang Liu'}]","['Harbin Institute of Technology', 'Hong Kong University of Science and Technology', 'Nanyang Technological University']","['China', 'Singapore']",2023-09
2310.01436,Yang Gao,"Haishuai Wang, Yang Gao, Xin Zheng, Peng Zhang, Hongyang Chen, Jiajun
  Bu",Graph Neural Architecture Search with GPT-4,,,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Graph Neural Architecture Search (GNAS) has shown promising results in automatically designing graph neural networks. However, GNAS still requires intensive human labor with rich domain knowledge to design the search space and search strategy. In this paper, we integrate GPT-4 into GNAS and propose a new GPT-4 based Graph Neural Architecture Search method (GPT4GNAS for short). The basic idea of our method is to design a new class of prompts for GPT-4 to guide GPT-4 toward the generative task of graph neural architectures. The prompts consist of descriptions of the search space, search strategy, and search feedback of GNAS. By iteratively running GPT-4 with the prompts, GPT4GNAS generates more accurate graph neural networks with fast convergence. Experimental results show that embedding GPT-4 into GNAS outperforms the state-of-the-art GNAS methods. ","[{'version': 'v1', 'created': 'Sat, 30 Sep 2023 08:05:59 GMT'}]",2023-10-04,"[['Wang', 'Haishuai', ''], ['Gao', 'Yang', ''], ['Zheng', 'Xin', ''], ['Zhang', 'Peng', ''], ['Chen', 'Hongyang', ''], ['Bu', 'Jiajun', '']]",0,1,2023-09-30,1,6,2,1,0,1,0b8265a63570d08d8d84aeacbec5495611ae3312,263608402.0,https://www.semanticscholar.org/paper/0b8265a63570d08d8d84aeacbec5495611ae3312,arXiv.org,2023.0,29.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2238917689', 'name': 'Haishuai Wang'}, {'authorId': '2166826920', 'name': 'Yang Gao'}, {'authorId': '2253980842', 'name': 'Xin Zheng'}, {'authorId': '2254902351', 'name': 'Peng Zhang'}, {'authorId': '2254017035', 'name': 'Hongyang Chen'}, {'authorId': '2253457926', 'name': 'Jiajun Bu'}]","['Guangzhou University', 'Zhejiang Lab', 'Zhejiang University']",['China'],2023-09

id_x,submitter,authors_x,title,comments,journal-ref,doi,report-no,categories,license,abstract,versions,update_date,authors_parsed,ContainChatGPT,ContainGPT,publish_date_v1,versionsNumber,authorNumber,categoryNumber,modelNumber,OpenModelNumber,CloseModelNumber,id_y,corpusId,url,venue,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy,s2FieldsOfStudy,authors_y,Institute,Country,month_year
1906.09072,Ruijia Yang,"YiGui Luo, RuiJia Yang, Wei Sha, WeiYi Ding, YouTeng Sun, YiSi Wang",Evolution Attack On Neural Networks,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many studies have been done to prove the vulnerability of neural networks to adversarial example. A trained and well-behaved model can be fooled by a visually imperceptible perturbation, i.e., an originally correctly classified image could be misclassified after a slight perturbation. In this paper, we propose a black-box strategy to attack such networks using an evolution algorithm. First, we formalize the generation of an adversarial example into the optimization problem of perturbations that represent the noise added to an original image at each pixel. To solve this optimization problem in a black-box way, we find that an evolution algorithm perfectly meets our requirement since it can work without any gradient information. Therefore, we test various evolution algorithms, including a simple genetic algorithm, a parameter-exploring policy gradient, an OpenAI evolution strategy, and a covariance matrix adaptive evolution strategy. Experimental results show that a covariance matrix adaptive evolution Strategy performs best in this optimization problem. Additionally, we also perform several experiments to explore the effect of different regularizations on improving the quality of an adversarial example. ","[{'version': 'v1', 'created': 'Fri, 21 Jun 2019 11:22:03 GMT'}]",2019-06-24,"[['Luo', 'YiGui', ''], ['Yang', 'RuiJia', ''], ['Sha', 'Wei', ''], ['Ding', 'WeiYi', ''], ['Sun', 'YouTeng', ''], ['Wang', 'YiSi', '']]",0,0,2019-06-21,1,6,1,0,0,0,4d8c6ca1cb8de00e854950ca3029ab2c6e7e9e8c,195316591.0,https://www.semanticscholar.org/paper/4d8c6ca1cb8de00e854950ca3029ab2c6e7e9e8c,arXiv.org,2019.0,25.0,0.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2118264343', 'name': 'Yigui Luo'}, {'authorId': '2115427816', 'name': 'Ruijia Yang'}, {'authorId': '2093764603', 'name': 'Wei Sha'}, {'authorId': '104514106', 'name': 'Weiyi Ding'}, {'authorId': '150352117', 'name': 'YouTeng Sun'}, {'authorId': '2115869699', 'name': 'Yisi Wang'}]","['Software Engineering Institute', 'Tongji University']","['China', 'United States']",2019-06
1910.03756,Qingyang Wu,"Qingyang Wu, Yichi Zhang, Yu Li, Zhou Yu",Alternating Recurrent Dialog Model with Large-scale Pre-trained Language Models,EACL 2021 (oral),,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing dialog system models require extensive human annotations and are difficult to generalize to different tasks. The recent success of large pre-trained language models such as BERT and GPT-2 (Devlin et al., 2019; Radford et al., 2019) have suggested the effectiveness of incorporating language priors in down-stream NLP tasks. However, how much pre-trained language models can help dialog response generation is still under exploration. In this paper, we propose a simple, general, and effective framework: Alternating Roles Dialog Model (ARDM). ARDM models each speaker separately and takes advantage of the large pre-trained language model. It requires no supervision from human annotations such as belief states or dialog acts to achieve effective conversations. ARDM outperforms or is on par with state-of-the-art methods on two popular task-oriented dialog datasets: CamRest676 and MultiWOZ. Moreover, we can generalize ARDM to more challenging, non-collaborative tasks such as persuasion. In persuasion tasks, ARDM is capable of generating human-like responses to persuade people to donate to a charity. ","[{'version': 'v1', 'created': 'Wed, 9 Oct 2019 02:31:37 GMT'}, {'version': 'v2', 'created': 'Sun, 10 Nov 2019 02:01:13 GMT'}, {'version': 'v3', 'created': 'Mon, 26 Apr 2021 19:48:38 GMT'}]",2021-04-28,"[['Wu', 'Qingyang', ''], ['Zhang', 'Yichi', ''], ['Li', 'Yu', ''], ['Yu', 'Zhou', '']]",0,1,2019-10-09,3,4,2,1,1,0,ebb1f10d878e5720ac01796bfb6769290a71149b,203952980.0,https://www.semanticscholar.org/paper/ebb1f10d878e5720ac01796bfb6769290a71149b,Conference of the European Chapter of the Association for Computational Linguistics,2019.0,29.0,54.0,8.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '31060482', 'name': 'Qingyang Wu'}, {'authorId': '46868553', 'name': 'Yichi Zhang'}, {'authorId': '40058381', 'name': 'Yu Li'}, {'authorId': '1564034697', 'name': 'Zhou Yu'}]","['Tsinghua University', 'University of California, Davis']","['China', 'United States']",2019-10
1911.02707,Houyu Zhang,"Houyu Zhang, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu",Grounded Conversation Generation as Guided Traverses in Commonsense Knowledge Graphs,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Human conversations naturally evolve around related concepts and scatter to multi-hop concepts. This paper presents a new conversation generation model, ConceptFlow, which leverages commonsense knowledge graphs to explicitly model conversation flows. By grounding conversations to the concept space, ConceptFlow represents the potential conversation flow as traverses in the concept space along commonsense relations. The traverse is guided by graph attentions in the concept graph, moving towards more meaningful directions in the concept space, in order to generate more semantic and informative responses. Experiments on Reddit conversations demonstrate ConceptFlow's effectiveness over previous knowledge-aware conversation models and GPT-2 based models while using 70% fewer parameters, confirming the advantage of explicit modeling conversation structures. All source codes of this work are available at https://github.com/thunlp/ConceptFlow. ","[{'version': 'v1', 'created': 'Thu, 7 Nov 2019 01:40:39 GMT'}, {'version': 'v2', 'created': 'Mon, 6 Apr 2020 04:02:51 GMT'}, {'version': 'v3', 'created': 'Tue, 5 May 2020 18:12:41 GMT'}]",2020-05-07,"[['Zhang', 'Houyu', ''], ['Liu', 'Zhenghao', ''], ['Xiong', 'Chenyan', ''], ['Liu', 'Zhiyuan', '']]",0,1,2019-11-07,3,4,2,1,1,0,04ef00b711990004b9c4440b1e3aa59b28701cfc,214802401.0,https://www.semanticscholar.org/paper/04ef00b711990004b9c4440b1e3aa59b28701cfc,Annual Meeting of the Association for Computational Linguistics,2019.0,50.0,109.0,15.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '92156482', 'name': 'Houyu Zhang'}, {'authorId': '49047064', 'name': 'Zhenghao Liu'}, {'authorId': '144628574', 'name': 'Chenyan Xiong'}, {'authorId': '49293587', 'name': 'Zhiyuan Liu'}]","['Tsinghua University', 'Microsoft', 'Brown University']","['China', 'United States']",2019-11
1911.11931,Xuhui Zhou,"Xuhui Zhou, Yue Zhang, Leyang Cui, Dandan Huang",Evaluating Commonsense in Pre-trained Language Models,AAAI 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Contextualized representations trained over large raw text data have given remarkable improvements for NLP tasks including question answering and reading comprehension. There have been works showing that syntactic, semantic and word sense knowledge are contained in such representations, which explains why they benefit such tasks. However, relatively little work has been done investigating commonsense knowledge contained in contextualized representations, which is crucial for human question answering and reading comprehension. We study the commonsense ability of GPT, BERT, XLNet, and RoBERTa by testing them on seven challenging benchmarks, finding that language modeling and its variants are effective objectives for promoting models' commonsense ability while bi-directional context and larger training set are bonuses. We additionally find that current models do poorly on tasks require more necessary inference steps. Finally, we test the robustness of models by making dual test cases, which are correlated so that the correct prediction of one sample should lead to correct prediction of the other. Interestingly, the models show confusion on these test cases, which suggests that they learn commonsense at the surface rather than the deep level. We release a test set, named CATs publicly, for future research. ","[{'version': 'v1', 'created': 'Wed, 27 Nov 2019 03:22:40 GMT'}, {'version': 'v2', 'created': 'Thu, 11 Feb 2021 05:14:52 GMT'}]",2021-02-12,"[['Zhou', 'Xuhui', ''], ['Zhang', 'Yue', ''], ['Cui', 'Leyang', ''], ['Huang', 'Dandan', '']]",0,1,2019-11-27,2,4,2,0,0,0,01f2b214962997260020279bd1fd1f8f372249d4,208310123.0,https://www.semanticscholar.org/paper/01f2b214962997260020279bd1fd1f8f372249d4,AAAI Conference on Artificial Intelligence,2019.0,23.0,130.0,8.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '144101734', 'name': 'Xuhui Zhou'}, {'authorId': '1591125925', 'name': 'Yue Zhang'}, {'authorId': '152496687', 'name': 'Leyang Cui'}, {'authorId': '2110409012', 'name': 'Dandan Huang'}]","['University of Washington', 'Westlake University', 'Zhejiang University']","['China', 'United States']",2019-11
2006.05009,Jiahua Liu,"Shi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong, Paul Bennett,
  Jianfeng Gao, Zhiyuan Liu",Few-Shot Generative Conversational Query Rewriting,Accepted by SIGIR 2020,,,,cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Conversational query rewriting aims to reformulate a concise conversational query to a fully specified, context-independent query that can be effectively handled by existing information retrieval systems. This paper presents a few-shot generative approach to conversational query rewriting. We develop two methods, based on rules and self-supervised learning, to generate weak supervision data using large amounts of ad hoc search sessions, and to fine-tune GPT-2 to rewrite conversational queries. On the TREC Conversational Assistance Track, our weakly supervised GPT-2 rewriter improves the state-of-the-art ranking accuracy by 12%, only using very limited amounts of manual query rewrites. In the zero-shot learning setting, the rewriter still gives a comparable result to previous state-of-the-art systems. Our analyses reveal that GPT-2 effectively picks up the task syntax and learns to capture context dependencies, even for hard cases that involve group references and long-turn dependencies. ","[{'version': 'v1', 'created': 'Tue, 9 Jun 2020 01:47:58 GMT'}]",2020-06-11,"[['Yu', 'Shi', ''], ['Liu', 'Jiahua', ''], ['Yang', 'Jingqin', ''], ['Xiong', 'Chenyan', ''], ['Bennett', 'Paul', ''], ['Gao', 'Jianfeng', ''], ['Liu', 'Zhiyuan', '']]",0,1,2020-06-09,1,7,1,1,1,0,ba960d5b53f3795be5d9600da2adea63754bfc9f,219559295.0,https://www.semanticscholar.org/paper/ba960d5b53f3795be5d9600da2adea63754bfc9f,Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,2020.0,5.0,96.0,15.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '150311558', 'name': 'S. Yu'}, {'authorId': '46701066', 'name': 'Jiahua Liu'}, {'authorId': '2121269197', 'name': 'Jingqin Yang'}, {'authorId': '144628574', 'name': 'Chenyan Xiong'}, {'authorId': '144609235', 'name': 'Paul N. Bennett'}, {'authorId': '1800422', 'name': 'Jianfeng Gao'}, {'authorId': '49293587', 'name': 'Zhiyuan Liu'}]","['Tsinghua University', 'Microsoft']","['China', 'United States']",2020-06
2008.00312,Xinyang Zhang,"Xinyang Zhang, Zheng Zhang, Shouling Ji and Ting Wang",Trojaning Language Models for Fun and Profit,"Additional experiments and text editing; To appear in 2021 6th IEEE
  European Symposium on Security and Privacy",,,,cs.CR cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent years have witnessed the emergence of a new paradigm of building natural language processing (NLP) systems: general-purpose, pre-trained language models (LMs) are composed with simple downstream models and fine-tuned for a variety of NLP tasks. This paradigm shift significantly simplifies the system development cycles. However, as many LMs are provided by untrusted third parties, their lack of standardization or regulation entails profound security implications, which are largely unexplored.   To bridge this gap, this work studies the security threats posed by malicious LMs to NLP systems. Specifically, we present TROJAN-LM, a new class of trojaning attacks in which maliciously crafted LMs trigger host NLP systems to malfunction in a highly predictable manner. By empirically studying three state-of-the-art LMs (BERT, GPT-2, XLNet) in a range of security-critical NLP tasks (toxic comment detection, question answering, text completion) as well as user studies on crowdsourcing platforms, we demonstrate that TROJAN-LM possesses the following properties: (i) flexibility - the adversary is able to flexibly dene logical combinations (e.g., 'and', 'or', 'xor') of arbitrary words as triggers, (ii) efficacy - the host systems misbehave as desired by the adversary with high probability when trigger-embedded inputs are present, (iii) specificity - the trojan LMs function indistinguishably from their benign counterparts on clean inputs, and (iv) fluency - the trigger-embedded inputs appear as fluent natural language and highly relevant to their surrounding contexts. We provide analytical justification for the practicality of TROJAN-LM, and further discuss potential countermeasures and their challenges, which lead to several promising research directions. ","[{'version': 'v1', 'created': 'Sat, 1 Aug 2020 18:22:38 GMT'}, {'version': 'v2', 'created': 'Wed, 10 Mar 2021 21:52:58 GMT'}]",2021-03-12,"[['Zhang', 'Xinyang', ''], ['Zhang', 'Zheng', ''], ['Ji', 'Shouling', ''], ['Wang', 'Ting', '']]",0,1,2020-08-01,2,4,3,1,1,0,11fe33206746251656698bf5188fc622aea7fc21,220936152.0,https://www.semanticscholar.org/paper/11fe33206746251656698bf5188fc622aea7fc21,European Symposium on Security and Privacy,2020.0,78.0,76.0,8.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': None, 'name': 'Xinyang Zhang'}, {'authorId': '1852415', 'name': 'Zheng Zhang'}, {'authorId': '2155389584', 'name': 'Ting Wang'}]","['Pennsylvania State University', 'Zhejiang University']","['China', 'United States']",2020-08
2009.05866,Nicholas Capel,"Nicholas Capel, Naifu Zhang",Extended Radial Basis Function Controller for Reinforcement Learning,,,,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There have been attempts in reinforcement learning to exploit a priori knowledge about the structure of the system. This paper proposes a hybrid reinforcement learning controller which dynamically interpolates a model-based linear controller and an arbitrary differentiable policy. The linear controller is designed based on local linearised model knowledge, and stabilises the system in a neighbourhood about an operating point. The coefficients of interpolation between the two controllers are determined by a scaled distance function measuring the distance between the current state and the operating point. The overall hybrid controller is proven to maintain the stability guarantee around the neighborhood of the operating point and still possess the universal function approximation property of the arbitrary non-linear policy. Learning has been done on both model-based (PILCO) and model-free (DDPG) frameworks. Simulation experiments performed in OpenAI gym demonstrate stability and robustness of the proposed hybrid controller. This paper thus introduces a principled method allowing for the direct importing of control methodology into reinforcement learning. ","[{'version': 'v1', 'created': 'Sat, 12 Sep 2020 20:56:48 GMT'}, {'version': 'v2', 'created': 'Wed, 9 Dec 2020 06:44:17 GMT'}]",2020-12-10,"[['Capel', 'Nicholas', ''], ['Zhang', 'Naifu', '']]",0,0,2020-09-12,2,2,2,0,0,0,44d95c9fb06aa06d0c0dbddcd6057e7f659197e3,221655595.0,https://www.semanticscholar.org/paper/44d95c9fb06aa06d0c0dbddcd6057e7f659197e3,arXiv.org,2020.0,37.0,1.0,0.0,False,"['Computer Science', 'Mathematics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Mathematics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '3486120', 'name': 'Nicholas Capel'}, {'authorId': '31513628', 'name': 'Naifu Zhang'}]","['Tsinghua University', 'Amazon']","['China', 'United States']",2020-09
2010.08618,Sudha Rao,"Allison Hegel, Sudha Rao, Asli Celikyilmaz and Bill Dolan",Substance over Style: Document-Level Targeted Content Transfer,This paper has been accepted to be published at EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing language models excel at writing from scratch, but many real-world scenarios require rewriting an existing document to fit a set of constraints. Although sentence-level rewriting has been fairly well-studied, little work has addressed the challenge of rewriting an entire document coherently. In this work, we introduce the task of document-level targeted content transfer and address it in the recipe domain, with a recipe as the document and a dietary restriction (such as vegan or dairy-free) as the targeted constraint. We propose a novel model for this task based on the generative pre-trained language model (GPT-2) and train on a large number of roughly-aligned recipe pairs (https://github.com/microsoft/document-level-targeted-content-transfer). Both automatic and human evaluations show that our model out-performs existing methods by generating coherent and diverse rewrites that obey the constraint while remaining close to the original document. Finally, we analyze our model's rewrites to assess progress toward the goal of making language generation more attuned to constraints that are substantive rather than stylistic. ","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 20:26:10 GMT'}]",2020-10-20,"[['Hegel', 'Allison', ''], ['Rao', 'Sudha', ''], ['Celikyilmaz', 'Asli', ''], ['Dolan', 'Bill', '']]",0,1,2020-10-16,1,4,1,1,1,0,03763454e488be8530468b3f99c93d5211c7748f,224703411.0,https://www.semanticscholar.org/paper/03763454e488be8530468b3f99c93d5211c7748f,Conference on Empirical Methods in Natural Language Processing,2020.0,41.0,1.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '70414766', 'name': 'Allison Hegel'}, {'authorId': '1845230025', 'name': 'Sudha Rao'}, {'authorId': '1709797', 'name': 'Asli Celikyilmaz'}, {'authorId': '66648221', 'name': 'Bill Dolan'}]","['Microsoft', 'Lexion, Seattle, WA, USA']","['China', 'United States']",2020-10
2011.07956,Dong-Ho Lee,"Wangchunshu Zhou, Dong-Ho Lee, Ravi Kiran Selvam, Seyeon Lee, Bill
  Yuchen Lin, Xiang Ren",Pre-training Text-to-Text Transformers for Concept-centric Common Sense,"15 pages, 4 figures. Code and Data: https://github.com/INK-USC/CALM/",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational commonsense knowledge about everyday concepts, which is crucial to many downstream tasks that need common sense to understand or generate. To augment PTLMs with concept-centric commonsense knowledge, in this paper, we propose both generative and contrastive objectives for learning common sense from the text, and use them as intermediate self-supervised learning tasks for incrementally pre-training PTLMs (before task-specific fine-tuning on downstream datasets). Furthermore, we develop a joint pre-training framework to unify generative and contrastive objectives so that they can mutually reinforce each other. Extensive experimental results show that our method, concept-aware language model (CALM), can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge graphs, yielding better performance on both NLU and NLG tasks. We show that while only incrementally pre-trained on a relatively small corpus for a few steps, CALM outperforms baseline methods by a consistent margin and even comparable with some larger PTLMs, which suggests that CALM can serve as a general, plug-and-play method for improving the commonsense reasoning ability of a PTLM. ","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 07:00:37 GMT'}, {'version': 'v2', 'created': 'Wed, 25 Nov 2020 04:53:38 GMT'}]",2020-11-26,"[['Zhou', 'Wangchunshu', ''], ['Lee', 'Dong-Ho', ''], ['Selvam', 'Ravi Kiran', ''], ['Lee', 'Seyeon', ''], ['Lin', 'Bill Yuchen', ''], ['Ren', 'Xiang', '']]",0,0,2020-10-24,2,6,3,1,1,0,abaadb4c6affc4d874c4f59bfac60686e851cb5e,226964491.0,https://www.semanticscholar.org/paper/abaadb4c6affc4d874c4f59bfac60686e851cb5e,International Conference on Learning Representations,2020.0,45.0,61.0,7.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '150341221', 'name': 'Wangchunshu Zhou'}, {'authorId': '73037511', 'name': 'Dong-Ho Lee'}, {'authorId': '1838125907', 'name': 'Ravi Kiran Selvam'}, {'authorId': '2108701349', 'name': 'Seyeon Lee'}, {'authorId': '51583409', 'name': 'Bill Yuchen Lin'}, {'authorId': '1384550891', 'name': 'Xiang Ren'}]","['University of Southern California', 'Beihang University']","['China', 'United States']",2020-10
2012.00744,Harry Wang,"Jinggang Zhuo, Ling Fan, Harry Jiannan Wang",A Framework and Dataset for Abstract Art Generation via CalligraphyGAN,"Accepted by NeurIPS 2020 Workshop on Machine Learning for Creativity
  and Design, Vancouver, Canada",,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  With the advancement of deep learning, artificial intelligence (AI) has made many breakthroughs in recent years and achieved superhuman performance in various tasks such as object detection, reading comprehension, and video games. Generative Modeling, such as various Generative Adversarial Networks (GAN) models, has been applied to generate paintings and music. Research in Natural Language Processing (NLP) also had a leap forward in 2018 since the release of the pre-trained contextual neural language models such as BERT and recently released GPT3. Despite the exciting AI applications aforementioned, AI is still significantly lagging behind humans in creativity, which is often considered the ultimate moonshot for AI. Our work is inspired by Chinese calligraphy, which is a unique form of visual art where the character itself is an aesthetic painting. We also draw inspirations from paintings of the Abstract Expressionist movement in the 1940s and 1950s, such as the work by American painter Franz Kline. In this paper, we present a creative framework based on Conditional Generative Adversarial Networks and Contextual Neural Language Model to generate abstract artworks that have intrinsic meaning and aesthetic value, which is different from the existing work, such as image captioning and text-to-image generation, where the texts are the descriptions of the images. In addition, we have publicly released a Chinese calligraphy image dataset and demonstrate our framework using a prototype system and a user study. ","[{'version': 'v1', 'created': 'Wed, 2 Dec 2020 16:24:20 GMT'}]",2020-12-03,"[['Zhuo', 'Jinggang', ''], ['Fan', 'Ling', ''], ['Wang', 'Harry Jiannan', '']]",0,1,2020-12-02,1,3,1,1,0,1,2adbd8f367c5ab43dcaba87c980aabd3747d4994,227247926.0,https://www.semanticscholar.org/paper/2adbd8f367c5ab43dcaba87c980aabd3747d4994,arXiv.org,2020.0,6.0,1.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Art', 'source': 's2-fos-model'}]","[{'authorId': '2030709785', 'name': 'Jinggang Zhuo'}, {'authorId': '1406286691', 'name': 'Ling Fan'}, {'authorId': '2145329798', 'name': 'Harry J. Wang'}]","['Tongji University', 'University of Delaware']","['China', 'United States']",2020-12
2012.02469,Ju Fan,"Nan Tang, Ju Fan, Fangyi Li, Jianhong Tu, Xiaoyong Du, Guoliang Li,
  Sam Madden, Mourad Ouzzani",RPT: Relational Pre-trained Transformer Is Almost All You Need towards Democratizing Data Preparation,,,,,cs.LG cs.DB,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Can AI help automate human-easy but computer-hard data preparation tasks that burden data scientists, practitioners, and crowd workers? We answer this question by presenting RPT, a denoising auto-encoder for tuple-to-X models (X could be tuple, token, label, JSON, and so on). RPT is pre-trained for a tuple-to-tuple model by corrupting the input tuple and then learning a model to reconstruct the original tuple. It adopts a Transformer-based neural translation architecture that consists of a bidirectional encoder (similar to BERT) and a left-to-right autoregressive decoder (similar to GPT), leading to a generalization of both BERT and GPT. The pre-trained RPT can already support several common data preparation tasks such as data cleaning, auto-completion and schema matching. Better still, RPT can be fine-tuned on a wide range of data preparation tasks, such as value normalization, data transformation, data annotation, etc. To complement RPT, we also discuss several appealing techniques such as collaborative training and few-shot learning for entity resolution, and few-shot learning and NLP question-answering for information extraction. In addition, we identify a series of research opportunities to advance the field of data preparation. ","[{'version': 'v1', 'created': 'Fri, 4 Dec 2020 08:52:05 GMT'}, {'version': 'v2', 'created': 'Wed, 31 Mar 2021 08:28:30 GMT'}]",2021-04-01,"[['Tang', 'Nan', ''], ['Fan', 'Ju', ''], ['Li', 'Fangyi', ''], ['Tu', 'Jianhong', ''], ['Du', 'Xiaoyong', ''], ['Li', 'Guoliang', ''], ['Madden', 'Sam', ''], ['Ouzzani', 'Mourad', '']]",0,1,2020-12-04,2,8,2,0,0,0,bfe6cce534d76df1aa7ebd629c09e30d6b8abdd4,232432173.0,https://www.semanticscholar.org/paper/bfe6cce534d76df1aa7ebd629c09e30d6b8abdd4,Proceedings of the VLDB Endowment,2020.0,65.0,42.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '8669763', 'name': 'N. Tang'}, {'authorId': '1704755170', 'name': 'Ju Fan'}, {'authorId': '2146330398', 'name': 'Fangyi Li'}, {'authorId': '49365463', 'name': 'Jianhong Tu'}, {'authorId': '144589756', 'name': 'Xiaoyong Du'}, {'authorId': '2108491555', 'name': 'Guoliang Li'}, {'authorId': '144478906', 'name': 'S. Madden'}, {'authorId': '2168047', 'name': 'M. Ouzzani'}]","['Massachusetts Institute of Technology', 'Tsinghua University', 'Hamad bin Khalifa University', 'Renmin University of China']","['China', 'United States', 'Qatar']",2020-12
2012.14535,Linfeng Song,"Jie Hao, Linfeng Song, Liwei Wang, Kun Xu, Zhaopeng Tu and Dong Yu",Robust Dialogue Utterance Rewriting as Sequence Tagging,11 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The task of dialogue rewriting aims to reconstruct the latest dialogue utterance by copying the missing content from the dialogue context. Until now, the existing models for this task suffer from the robustness issue, i.e., performances drop dramatically when testing on a different domain. We address this robustness issue by proposing a novel sequence-tagging-based model so that the search space is significantly reduced, yet the core of this task is still well covered. As a common issue of most tagging models for text generation, the model's outputs may lack fluency. To alleviate this issue, we inject the loss signal from BLEU or GPT-2 under a REINFORCE framework. Experiments show huge improvements of our model over the current state-of-the-art systems on domain transfer. ","[{'version': 'v1', 'created': 'Tue, 29 Dec 2020 00:05:35 GMT'}]",2021-01-01,"[['Hao', 'Jie', ''], ['Song', 'Linfeng', ''], ['Wang', 'Liwei', ''], ['Xu', 'Kun', ''], ['Tu', 'Zhaopeng', ''], ['Yu', 'Dong', '']]",0,1,2020-12-29,1,6,1,1,1,0,0e635104e5378bc226b0e07e429fcef44f959fc8,229923054.0,https://www.semanticscholar.org/paper/0e635104e5378bc226b0e07e429fcef44f959fc8,arXiv.org,2020.0,29.0,6.0,2.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '145485960', 'name': 'Jie Hao'}, {'authorId': '1748796', 'name': 'Linfeng Song'}, {'authorId': '39060743', 'name': 'Liwei Wang'}, {'authorId': '151485141', 'name': 'Kun Xu'}, {'authorId': '2909321', 'name': 'Zhaopeng Tu'}, {'authorId': '2111505433', 'name': 'Dong Yu'}]","['Tencent', 'Chinese University of Hong Kong', 'Florida State University']","['China', 'United States']",2020-12
2101.00416,Wangchunshu Zhou,"Wangchunshu Zhou, Tao Ge, Canwen Xu, Ke Xu, Furu Wei",Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we generalize text infilling (e.g., masked language models) by proposing Sequence Span Rewriting (SSR) as a self-supervised sequence-to-sequence (seq2seq) pre-training objective. SSR provides more fine-grained learning signals for text representations by supervising the model to rewrite imperfect spans to ground truth, and it is more consistent than text infilling with many downstream seq2seq tasks that rewrite a source sentences into a target sentence. Our experiments with T5 models on various seq2seq tasks show that SSR can substantially improve seq2seq pre-training. Moreover, we observe SSR is especially helpful to improve pre-training a small-size seq2seq model with a powerful imperfect span generator, which indicates a new perspective of transferring knowledge from a large model to a smaller model for seq2seq pre-training. ","[{'version': 'v1', 'created': 'Sat, 2 Jan 2021 10:27:11 GMT'}, {'version': 'v2', 'created': 'Fri, 24 Sep 2021 07:45:41 GMT'}]",2021-09-27,"[['Zhou', 'Wangchunshu', ''], ['Ge', 'Tao', ''], ['Xu', 'Canwen', ''], ['Xu', 'Ke', ''], ['Wei', 'Furu', '']]",0,0,2021-01-02,2,5,1,1,1,0,124b25a74691a993673359288637716d83899a06,230437660.0,https://www.semanticscholar.org/paper/124b25a74691a993673359288637716d83899a06,Conference on Empirical Methods in Natural Language Processing,2021.0,67.0,14.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '150341221', 'name': 'Wangchunshu Zhou'}, {'authorId': '50251691', 'name': 'Tao Ge'}, {'authorId': '145389711', 'name': 'Ke Xu'}, {'authorId': '49807919', 'name': 'Furu Wei'}]","['Beihang University', 'University of California, San Diego']","['China', 'United States']",2021-01
2103.01403,Qing Li,"Qing Li, Siyuan Huang, Yining Hong, Yixin Zhu, Ying Nian Wu, Song-Chun
  Zhu","A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics",ICLR 2023. website: https://liqing-ustc.github.io/HINT,,,,cs.LG cs.AI cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Inspired by humans' exceptional ability to master arithmetic and generalize to new problems, we present a new dataset, Handwritten arithmetic with INTegers (HINT), to examine machines' capability of learning generalizable concepts at three levels: perception, syntax, and semantics. In HINT, machines are tasked with learning how concepts are perceived from raw signals such as images (i.e., perception), how multiple concepts are structurally combined to form a valid expression (i.e., syntax), and how concepts are realized to afford various reasoning tasks (i.e., semantics), all in a weakly supervised manner. Focusing on systematic generalization, we carefully design a five-fold test set to evaluate both the interpolation and the extrapolation of learned concepts w.r.t. the three levels. Further, we design a few-shot learning split to determine whether or not models can rapidly learn new concepts and generalize them to more complex scenarios. To comprehend existing models' limitations, we undertake extensive experiments with various sequence-to-sequence models, including RNNs, Transformers, and GPT-3 (with the chain of thought prompting). The results indicate that current models struggle to extrapolate to long-range syntactic dependency and semantics. Models exhibit a considerable gap toward human-level generalization when evaluated with new concepts in a few-shot setting. Moreover, we discover that it is infeasible to solve HINT by merely scaling up the dataset and the model size; this strategy contributes little to the extrapolation of syntax and semantics. Finally, in zero-shot GPT-3 experiments, the chain of thought prompting exhibits impressive results and significantly boosts the test accuracy. We believe the HINT dataset and the experimental findings are of great interest to the learning community on systematic generalization. ","[{'version': 'v1', 'created': 'Tue, 2 Mar 2021 01:32:54 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Sep 2022 02:16:59 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Apr 2023 07:54:24 GMT'}]",2023-04-19,"[['Li', 'Qing', ''], ['Huang', 'Siyuan', ''], ['Hong', 'Yining', ''], ['Zhu', 'Yixin', ''], ['Wu', 'Ying Nian', ''], ['Zhu', 'Song-Chun', '']]",0,1,2021-03-02,3,6,3,1,0,1,268fedc5d786fa197b294dccab7eea02dc08038a,252383681.0,https://www.semanticscholar.org/paper/268fedc5d786fa197b294dccab7eea02dc08038a,International Conference on Learning Representations,2021.0,117.0,4.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2117895397', 'name': 'Qing Li'}, {'authorId': '1713084', 'name': 'Siyuan Huang'}, {'authorId': '151261268', 'name': 'Yining Hong'}, {'authorId': '2118318556', 'name': 'Yixin Zhu'}, {'authorId': '39092098', 'name': 'Y. Wu'}, {'authorId': '145380991', 'name': 'Song-Chun Zhu'}]","['Peking University', 'University of California, Los Angeles', 'The University of Tokyo']","['China', 'United States', 'Japan']",2021-03
2106.03983,Hai Hu,"Hai Hu, He Zhou, Zuoyu Tian, Yiwen Zhang, Yina Ma, Yanting Li, Yixin
  Nie, Kyle Richardson",Investigating Transfer Learning in Multilingual Pre-trained Language Models through Chinese Natural Language Inference,accepted to ACL Findings 2021,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multilingual transformers (XLM, mT5) have been shown to have remarkable transfer skills in zero-shot settings. Most transfer studies, however, rely on automatically translated resources (XNLI, XQuAD), making it hard to discern the particular linguistic knowledge that is being transferred, and the role of expert annotated monolingual datasets when developing task-specific models. We investigate the cross-lingual transfer abilities of XLM-R for Chinese and English natural language inference (NLI), with a focus on the recent large-scale Chinese dataset OCNLI. To better understand linguistic transfer, we created 4 categories of challenge and adversarial tasks (totaling 17 new datasets) for Chinese that build on several well-known resources for English (e.g., HANS, NLI stress-tests). We find that cross-lingual models trained on English NLI do transfer well across our Chinese tasks (e.g., in 3/4 of our challenge categories, they perform as well/better than the best monolingual models, even on 3/5 uniquely Chinese linguistic phenomena such as idioms, pro drop). These results, however, come with important caveats: cross-lingual models often perform best when trained on a mixture of English and high-quality monolingual NLI data (OCNLI), and are often hindered by automatically translated resources (XNLI-zh). For many phenomena, all models continue to struggle, highlighting the need for our new diagnostics to help benchmark Chinese and cross-lingual models. All new datasets/code are released at https://github.com/huhailinguist/ChineseNLIProbing. ","[{'version': 'v1', 'created': 'Mon, 7 Jun 2021 22:00:18 GMT'}]",2021-06-09,"[['Hu', 'Hai', ''], ['Zhou', 'He', ''], ['Tian', 'Zuoyu', ''], ['Zhang', 'Yiwen', ''], ['Ma', 'Yina', ''], ['Li', 'Yanting', ''], ['Nie', 'Yixin', ''], ['Richardson', 'Kyle', '']]",0,0,2021-06-07,1,8,2,1,1,0,1cde1aa4f7bcebc47b35518cec452893ea6b824c,235367897.0,https://www.semanticscholar.org/paper/1cde1aa4f7bcebc47b35518cec452893ea6b824c,Findings,2021.0,58.0,6.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '145309512', 'name': 'Hai Hu'}, {'authorId': '1596810069', 'name': 'He Zhou'}, {'authorId': '105042132', 'name': 'Zuoyu Tian'}, {'authorId': '2108139880', 'name': 'Yiwen Zhang'}, {'authorId': '2146277459', 'name': 'Yina Ma'}, {'authorId': '2155221186', 'name': 'Yanting Li'}, {'authorId': '40383658', 'name': 'Yixin Nie'}, {'authorId': '46666605', 'name': 'Kyle Richardson'}]","['Northwestern University', 'University of North Carolina at Chapel Hill', 'Shanghai Jiao Tong University', 'Allen Institute for Artificial Intelligence', 'Indiana University Bloomington', 'Brigham Young University']","['China', 'United States']",2021-06
2108.11023,Hongbin Liu,"Hongbin Liu, Jinyuan Jia, Wenjie Qu, Neil Zhenqiang Gong",EncoderMI: Membership Inference against Pre-trained Encoders in Contrastive Learning,"To appear in ACM Conference on Computer and Communications Security
  (CCS), 2021",,,,cs.CR cs.CV cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Given a set of unlabeled images or (image, text) pairs, contrastive learning aims to pre-train an image encoder that can be used as a feature extractor for many downstream tasks. In this work, we propose EncoderMI, the first membership inference method against image encoders pre-trained by contrastive learning. In particular, given an input and a black-box access to an image encoder, EncoderMI aims to infer whether the input is in the training dataset of the image encoder. EncoderMI can be used 1) by a data owner to audit whether its (public) data was used to pre-train an image encoder without its authorization or 2) by an attacker to compromise privacy of the training data when it is private/sensitive. Our EncoderMI exploits the overfitting of the image encoder towards its training data. In particular, an overfitted image encoder is more likely to output more (or less) similar feature vectors for two augmented versions of an input in (or not in) its training dataset. We evaluate EncoderMI on image encoders pre-trained on multiple datasets by ourselves as well as the Contrastive Language-Image Pre-training (CLIP) image encoder, which is pre-trained on 400 million (image, text) pairs collected from the Internet and released by OpenAI. Our results show that EncoderMI can achieve high accuracy, precision, and recall. We also explore a countermeasure against EncoderMI via preventing overfitting through early stopping. Our results show that it achieves trade-offs between accuracy of EncoderMI and utility of the image encoder, i.e., it can reduce the accuracy of EncoderMI, but it also incurs classification accuracy loss of the downstream classifiers built based on the image encoder. ","[{'version': 'v1', 'created': 'Wed, 25 Aug 2021 03:00:45 GMT'}]",2021-08-26,"[['Liu', 'Hongbin', ''], ['Jia', 'Jinyuan', ''], ['Qu', 'Wenjie', ''], ['Gong', 'Neil Zhenqiang', '']]",0,0,2021-08-25,1,4,3,0,0,0,dcf115bd311988638bf4791bf6f88b1aeed0322d,237290083.0,https://www.semanticscholar.org/paper/dcf115bd311988638bf4791bf6f88b1aeed0322d,Conference on Computer and Communications Security,2021.0,51.0,46.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2110279247', 'name': 'Hongbin Liu'}, {'authorId': '143987304', 'name': 'Jinyuan Jia'}, {'authorId': '2124940443', 'name': 'Wenjie Qu'}, {'authorId': '144516687', 'name': 'N. Gong'}]","['Huazhong University of Science and Technology', 'Duke University']","['China', 'United States']",2021-08
2108.13679,Weizhi Wang,"Weizhi Wang, Zhirui Zhang, Junliang Guo, Yinpei Dai, Boxing Chen and
  Weihua Luo",Task-Oriented Dialogue System as Natural Language Generation,SIGIR 2022,,10.1145/3477495.3531920,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we propose to formulate the task-oriented dialogue system as the purely natural language generation task, so as to fully leverage the large-scale pre-trained models like GPT-2 and simplify complicated delexicalization prepossessing. However, directly applying this method heavily suffers from the dialogue entity inconsistency caused by the removal of delexicalized tokens, as well as the catastrophic forgetting problem of the pre-trained model during fine-tuning, leading to unsatisfactory performance. To alleviate these problems, we design a novel GPT-Adapter-CopyNet network, which incorporates the lightweight adapter and CopyNet modules into GPT-2 to achieve better performance on transfer learning and dialogue entity generation. Experimental results conducted on the DSTC8 Track 1 benchmark and MultiWOZ dataset demonstrate that our proposed approach significantly outperforms baseline models with a remarkable performance on automatic and human evaluations. ","[{'version': 'v1', 'created': 'Tue, 31 Aug 2021 08:36:42 GMT'}, {'version': 'v2', 'created': 'Wed, 1 Sep 2021 07:33:04 GMT'}, {'version': 'v3', 'created': 'Sun, 24 Apr 2022 12:30:31 GMT'}]",2022-04-26,"[['Wang', 'Weizhi', ''], ['Zhang', 'Zhirui', ''], ['Guo', 'Junliang', ''], ['Dai', 'Yinpei', ''], ['Chen', 'Boxing', ''], ['Luo', 'Weihua', '']]",0,1,2021-08-31,3,6,2,1,1,0,3b0615caf74c7c84a33be78813ec7ebd0f53033d,237363762.0,https://www.semanticscholar.org/paper/3b0615caf74c7c84a33be78813ec7ebd0f53033d,Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,2021.0,39.0,24.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108438929', 'name': 'Weizhi Wang'}, {'authorId': '4947404', 'name': 'Zhirui Zhang'}, {'authorId': '2117224175', 'name': 'Junliang Guo'}, {'authorId': '30087809', 'name': 'Yinpei Dai'}, {'authorId': '2152687324', 'name': 'Boxing Chen'}, {'authorId': '48244817', 'name': 'Weihua Luo'}]","['Alibaba', 'Tencent', 'Microsoft', 'University of California, Santa Barbara']","['China', 'United States']",2021-08
2109.05256,Zewei Sun,"Zewei Sun, Mingxuan Wang and Lei Li",Multilingual Translation via Grafting Pre-trained Language Models,Accepted in EMNLP 2021 (Findings),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Can pre-trained BERT for one language and GPT for another be glued together to translate texts? Self-supervised training using only monolingual data has led to the success of pre-trained (masked) language models in many NLP tasks. However, directly connecting BERT as an encoder and GPT as a decoder can be challenging in machine translation, for GPT-like models lack a cross-attention component that is needed in seq2seq decoders. In this paper, we propose Graformer to graft separately pre-trained (masked) language models for machine translation. With monolingual data for pre-training and parallel data for grafting training, we maximally take advantage of the usage of both types of data. Experiments on 60 directions show that our method achieves average improvements of 5.8 BLEU in x2en and 2.9 BLEU in en2x directions comparing with the multilingual Transformer of the same size. ","[{'version': 'v1', 'created': 'Sat, 11 Sep 2021 10:57:45 GMT'}]",2021-09-14,"[['Sun', 'Zewei', ''], ['Wang', 'Mingxuan', ''], ['Li', 'Lei', '']]",0,1,2021-09-11,1,3,1,0,0,0,10b9a1f65d70964f85430d8edf419de736939d41,237490358.0,https://www.semanticscholar.org/paper/10b9a1f65d70964f85430d8edf419de736939d41,Conference on Empirical Methods in Natural Language Processing,2021.0,58.0,14.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '48064977', 'name': 'Zewei Sun'}, {'authorId': '50468534', 'name': 'Mingxuan Wang'}, {'authorId': '143900005', 'name': 'Lei Li'}]","['ByteDance', 'University of California, Santa Barbara']","['China', 'United States']",2021-09
2109.07684,Genta Indra Winata,"Genta Indra Winata, Andrea Madotto, Zhaojiang Lin, Rosanne Liu, Jason
  Yosinski, Pascale Fung",Language Models are Few-shot Multilingual Learners,14 pages,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  General-purpose language models have demonstrated impressive capabilities, performing on par with state-of-the-art approaches on a range of downstream natural language processing (NLP) tasks and benchmarks when inferring instructions from very few examples. Here, we evaluate the multilingual skills of the GPT and T5 models in conducting multi-class classification on non-English languages without any parameter updates. We show that, given a few English examples as context, pre-trained language models can predict not only English test samples but also non-English ones. Finally, we find the in-context few-shot cross-lingual prediction results of language models are significantly better than random prediction, and they are competitive compared to the existing state-of-the-art cross-lingual models. ","[{'version': 'v1', 'created': 'Thu, 16 Sep 2021 03:08:22 GMT'}]",2021-09-17,"[['Winata', 'Genta Indra', ''], ['Madotto', 'Andrea', ''], ['Lin', 'Zhaojiang', ''], ['Liu', 'Rosanne', ''], ['Yosinski', 'Jason', ''], ['Fung', 'Pascale', '']]",0,1,2021-09-16,1,6,2,1,1,0,42fc019b2668c9d9d984154d4c57f6c6d5a91619,237532173.0,https://www.semanticscholar.org/paper/42fc019b2668c9d9d984154d4c57f6c6d5a91619,MRL,2021.0,75.0,71.0,8.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '9162688', 'name': 'Genta Indra Winata'}, {'authorId': '2111680936', 'name': 'Andrea Madotto'}, {'authorId': '2146396528', 'name': 'Zhaojiang Lin'}, {'authorId': '48757909', 'name': 'Rosanne Liu'}, {'authorId': '2965424', 'name': 'J. Yosinski'}, {'authorId': '40539650', 'name': 'Pascale Fung'}]","['ML Collective', 'Google', 'Hong Kong University of Science and Technology']","['China', 'United States']",2021-09
2110.00987,Zaixi Zhang,"Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, Chee-Kong Lee",Motif-based Graph Self-Supervised Learning for Molecular Property Prediction,Accepted by NeurIPS'21,,,,q-bio.QM cs.AI cs.LG,http://creativecommons.org/licenses/by-sa/4.0/,"  Predicting molecular properties with data-driven methods has drawn much attention in recent years. Particularly, Graph Neural Networks (GNNs) have demonstrated remarkable success in various molecular generation and prediction tasks. In cases where labeled data is scarce, GNNs can be pre-trained on unlabeled molecular data to first learn the general semantic and structural information before being fine-tuned for specific tasks. However, most existing self-supervised pre-training frameworks for GNNs only focus on node-level or graph-level tasks. These approaches cannot capture the rich information in subgraphs or graph motifs. For example, functional groups (frequently-occurred subgraphs in molecular graphs) often carry indicative information about the molecular properties. To bridge this gap, we propose Motif-based Graph Self-supervised Learning (MGSSL) by introducing a novel self-supervised motif generation framework for GNNs. First, for motif extraction from molecular graphs, we design a molecule fragmentation method that leverages a retrosynthesis-based algorithm BRICS and additional rules for controlling the size of motif vocabulary. Second, we design a general motif-based generative pre-training framework in which GNNs are asked to make topological and label predictions. This generative framework can be implemented in two different ways, i.e., breadth-first or depth-first. Finally, to take the multi-scale information in molecular graphs into consideration, we introduce a multi-level self-supervised pre-training. Extensive experiments on various downstream benchmark tasks show that our methods outperform all state-of-the-art baselines. ","[{'version': 'v1', 'created': 'Sun, 3 Oct 2021 11:45:51 GMT'}, {'version': 'v2', 'created': 'Sat, 16 Oct 2021 00:48:00 GMT'}]",2021-10-19,"[['Zhang', 'Zaixi', ''], ['Liu', 'Qi', ''], ['Wang', 'Hao', ''], ['Lu', 'Chengqiang', ''], ['Lee', 'Chee-Kong', '']]",0,1,2021-10-03,2,5,3,0,0,0,2ced2ac19a88439b52e519d2e6ce44cccf08e191,238259534.0,https://www.semanticscholar.org/paper/2ced2ac19a88439b52e519d2e6ce44cccf08e191,Neural Information Processing Systems,2021.0,55.0,111.0,15.0,False,"['Computer Science', 'Biology']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Biology', 'source': 'external'}, {'category': 'Chemistry', 'source': 's2-fos-model'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2129455190', 'name': 'Zaixin Zhang'}, {'authorId': '2144831836', 'name': 'Qi Liu'}, {'authorId': '2144219662', 'name': 'Hao Wang'}, {'authorId': '46655401', 'name': 'Chengqiang Lu'}, {'authorId': '153897134', 'name': 'Chee-Kong Lee'}]","['Tencent', 'University of Science and Technology of China']","['China', 'United States']",2021-10
2111.04613,Jiayu Chen,"Jiayu Chen, Yuanxin Zhang, Yuanfan Xu, Huimin Ma, Huazhong Yang,
  Jiaming Song, Yu Wang, Yi Wu",Variational Automatic Curriculum Learning for Sparse-Reward Cooperative Multi-Agent Problems,In NeurIPS 2021,,,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  We introduce a curriculum learning algorithm, Variational Automatic Curriculum Learning (VACL), for solving challenging goal-conditioned cooperative multi-agent reinforcement learning problems. We motivate our paradigm through a variational perspective, where the learning objective can be decomposed into two terms: task learning on the current task distribution, and curriculum update to a new task distribution. Local optimization over the second term suggests that the curriculum should gradually expand the training tasks from easy to hard. Our VACL algorithm implements this variational paradigm with two practical components, task expansion and entity progression, which produces training curricula over both the task configurations as well as the number of entities in the task. Experiment results show that VACL solves a collection of sparse-reward problems with a large number of agents. Particularly, using a single desktop machine, VACL achieves 98% coverage rate with 100 agents in the simple-spread benchmark and reproduces the ramp-use behavior originally shown in OpenAI's hide-and-seek project. Our project website is at https://sites.google.com/view/vacl-neurips-2021. ","[{'version': 'v1', 'created': 'Mon, 8 Nov 2021 16:35:08 GMT'}, {'version': 'v2', 'created': 'Wed, 22 Dec 2021 08:11:07 GMT'}]",2021-12-23,"[['Chen', 'Jiayu', ''], ['Zhang', 'Yuanxin', ''], ['Xu', 'Yuanfan', ''], ['Ma', 'Huimin', ''], ['Yang', 'Huazhong', ''], ['Song', 'Jiaming', ''], ['Wang', 'Yu', ''], ['Wu', 'Yi', '']]",0,0,2021-11-08,2,8,1,0,0,0,ef5c29f6d78f81c78b2d7a625d4d4053f51fcd6e,243848161.0,https://www.semanticscholar.org/paper/ef5c29f6d78f81c78b2d7a625d4d4053f51fcd6e,Neural Information Processing Systems,2021.0,36.0,20.0,4.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108327767', 'name': 'Jiayu Chen'}, {'authorId': '2145784019', 'name': 'Yuanxin Zhang'}, {'authorId': '2110355949', 'name': 'Yuanfan Xu'}, {'authorId': '2008031104', 'name': 'Huimin Ma'}, {'authorId': '39150998', 'name': 'Huazhong Yang'}, {'authorId': '2112626470', 'name': 'Jiaming Song'}, {'authorId': '2153607473', 'name': 'Yu Wang'}, {'authorId': '2108052525', 'name': 'Yi Wu'}]","['Stanford University', 'Tsinghua University', 'University of Science and Technology Beijing']","['China', 'United States']",2021-11
2111.07658,Sha Yuan,"Hanyu Zhao, Sha Yuan, Jiahong Leng, Xiang Pan, Guoqiang Wang, Ledell
  Wu and Jie Tang",Calculating Question Similarity is Enough: A New Method for KBQA Tasks,"We want to withdraw this submission, and add some experiments to make
  it more valuable",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge Base Question Answering (KBQA) aims to answer natural language questions with the help of an external knowledge base. The core idea is to find the link between the internal knowledge behind questions and known triples of the knowledge base. Traditional KBQA task pipelines contain several steps, including entity recognition, entity linking, answering selection, etc. In this kind of pipeline methods, errors in any procedure will inevitably propagate to the final prediction. To address this challenge, this paper proposes a Corpus Generation - Retrieve Method (CGRM) with Pre-training Language Model (PLM) for the KBQA task. The major novelty lies in the design of the new method, wherein our approach, the knowledge enhanced T5 (kT5) model aims to generate natural language QA pairs based on Knowledge Graph triples and directly solve the QA by retrieving the synthetic dataset. The new method can extract more information about the entities from PLM to improve accuracy and simplify the processes. We test our method on NLPCC-ICCPOL 2016 KBQA dataset, and the results show that our method improves the performance of KBQA and the out straight-forward method is competitive with the state-of-the-art. ","[{'version': 'v1', 'created': 'Mon, 15 Nov 2021 10:31:46 GMT'}, {'version': 'v2', 'created': 'Sat, 11 Dec 2021 05:44:46 GMT'}, {'version': 'v3', 'created': 'Wed, 9 Feb 2022 07:55:59 GMT'}, {'version': 'v4', 'created': 'Sun, 1 May 2022 02:49:10 GMT'}]",2022-05-03,"[['Zhao', 'Hanyu', ''], ['Yuan', 'Sha', ''], ['Leng', 'Jiahong', ''], ['Pan', 'Xiang', ''], ['Wang', 'Guoqiang', ''], ['Wu', 'Ledell', ''], ['Tang', 'Jie', '']]",0,0,2021-11-15,4,7,2,1,1,0,920598a31d3e6284129ad24ffcc8d957a88068d0,244117403.0,https://www.semanticscholar.org/paper/920598a31d3e6284129ad24ffcc8d957a88068d0,arXiv.org,2021.0,47.0,0.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1907469334', 'name': 'Hanyu Zhao'}, {'authorId': '3411347', 'name': 'Shaoqing Yuan'}, {'authorId': '2124048409', 'name': 'Jiahong Leng'}, {'authorId': '2119600282', 'name': 'X. Pan'}, {'authorId': '2142449761', 'name': 'Guoqiang Wang'}]","['New York University', 'Tsinghua University', 'Beijing Academy of Artificial Intelligence']","['China', 'United States']",2021-11
2112.00029,Tri Dao,"Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri
  Rudra, Christopher R\'e",Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models,"International Conference on Learning Representations (ICLR) 2022
  spotlight",,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Overparameterized neural networks generalize well but are expensive to train. Ideally, one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty in sparsifying all model components. The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is difficult and expensive. To address this, our main insight is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. As butterfly matrices are not hardware efficient, we propose simple variants of butterfly (block and flat) to take advantage of modern hardware. Our method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP). We empirically validate that Pixelated Butterfly is 3x faster than butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs. On the ImageNet classification and WikiText-103 language modeling tasks, our sparse models train up to 2.5x faster than the dense MLP-Mixer, Vision Transformer, and GPT-2 medium with no drop in accuracy. ","[{'version': 'v1', 'created': 'Tue, 30 Nov 2021 19:00:03 GMT'}, {'version': 'v2', 'created': 'Wed, 11 May 2022 03:59:56 GMT'}]",2022-05-12,"[['Dao', 'Tri', ''], ['Chen', 'Beidi', ''], ['Liang', 'Kaizhao', ''], ['Yang', 'Jiaming', ''], ['Song', 'Zhao', ''], ['Rudra', 'Atri', ''], ['Ré', 'Christopher', '']]",0,1,2021-11-30,2,7,1,1,1,0,90b21dbad8969b74d704eed15a3d98722a88e464,244773609.0,https://www.semanticscholar.org/paper/90b21dbad8969b74d704eed15a3d98722a88e464,International Conference on Learning Representations,2021.0,108.0,45.0,7.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '4319427', 'name': 'Beidi Chen'}, {'authorId': '24593911', 'name': 'Tri Dao'}, {'authorId': '102461072', 'name': 'Kaizhao Liang'}, {'authorId': '2142772202', 'name': 'Jiaming Yang'}, {'authorId': '2119235975', 'name': 'Zhao Song'}, {'authorId': '1755572', 'name': 'A. Rudra'}, {'authorId': '2114485554', 'name': 'C. Ré'}]","['Adobe Research', 'University at Buffalo, State University of New York', 'Peking University']","['China', 'United States']",2021-11
2201.05966,Chen Henry Wu,"Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak,
  Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang,
  Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao,
  Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke
  Zettlemoyer, Tao Yu",UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models,EMNLP 2022,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests, such as semantic parsing over databases and question answering over knowledge bases. Since the inputs and outputs of SKG tasks are heterogeneous, they have been studied separately by different communities, which limits systematic and compatible research on SKG. In this paper, we overcome this limitation by proposing the UnifiedSKG framework, which unifies 21 SKG tasks into a text-to-text format, aiming to promote systematic SKG research, instead of being exclusive to a single task, domain, or dataset. We use UnifiedSKG to benchmark T5 with different sizes and show that T5, with simple modifications when necessary, achieves state-of-the-art performance on almost all of the 21 tasks. We further demonstrate that multi-task prefix-tuning improves the performance on most tasks, largely improving the overall performance. UnifiedSKG also facilitates the investigation of zero-shot and few-shot learning, and we show that T0, GPT-3, and Codex struggle in zero-shot and few-shot learning for SKG. We also use UnifiedSKG to conduct a series of controlled experiments on structured knowledge encoding variants across SKG tasks. UnifiedSKG is easily extensible to more tasks, and it is open-sourced at https://github.com/hkunlp/unifiedskg. ","[{'version': 'v1', 'created': 'Sun, 16 Jan 2022 04:36:18 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Jan 2022 03:20:45 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Oct 2022 15:56:01 GMT'}]",2022-10-19,"[['Xie', 'Tianbao', ''], ['Wu', 'Chen Henry', ''], ['Shi', 'Peng', ''], ['Zhong', 'Ruiqi', ''], ['Scholak', 'Torsten', ''], ['Yasunaga', 'Michihiro', ''], ['Wu', 'Chien-Sheng', ''], ['Zhong', 'Ming', ''], ['Yin', 'Pengcheng', ''], ['Wang', 'Sida I.', ''], ['Zhong', 'Victor', ''], ['Wang', 'Bailin', ''], ['Li', 'Chengzu', ''], ['Boyle', 'Connor', ''], ['Ni', 'Ansong', ''], ['Yao', 'Ziyu', ''], ['Radev', 'Dragomir', ''], ['Xiong', 'Caiming', ''], ['Kong', 'Lingpeng', ''], ['Zhang', 'Rui', ''], ['Smith', 'Noah A.', ''], ['Zettlemoyer', 'Luke', ''], ['Yu', 'Tao', '']]",0,1,2022-01-16,3,23,1,4,2,2,53c0abe83fe9b4fdaf2208295d8504fcf5241694,246016124.0,https://www.semanticscholar.org/paper/53c0abe83fe9b4fdaf2208295d8504fcf5241694,Conference on Empirical Methods in Natural Language Processing,2022.0,121.0,189.0,12.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2057038673', 'name': 'Tianbao Xie'}, {'authorId': '114621402', 'name': 'Chen Henry Wu'}, {'authorId': '2055357805', 'name': 'Peng Shi'}, {'authorId': '51011000', 'name': 'Ruiqi Zhong'}, {'authorId': '11869783', 'name': 'Torsten Scholak'}, {'authorId': '19168196', 'name': 'Michihiro Yasunaga'}, {'authorId': '30340989', 'name': 'Chien-Sheng Wu'}, {'authorId': '1606040932', 'name': 'Ming Zhong'}, {'authorId': '38253388', 'name': 'Pengcheng Yin'}, {'authorId': '8729431', 'name': 'Sida I. Wang'}, {'authorId': '3428769', 'name': 'Victor Zhong'}, {'authorId': '2118640406', 'name': 'Bailin Wang'}, {'authorId': '2155795167', 'name': 'Chengzu Li'}, {'authorId': '2143195008', 'name': 'Connor Boyle'}, {'authorId': '33981736', 'name': 'Ansong Ni'}, {'authorId': '3366595', 'name': 'Ziyu Yao'}, {'authorId': '9215251', 'name': 'Dragomir R. Radev'}, {'authorId': '2054594326', 'name': 'Caiming Xiong'}, {'authorId': '47648549', 'name': 'Lingpeng Kong'}, {'authorId': '15176410', 'name': 'Rui Zhang'}, {'authorId': '144365875', 'name': 'Noah A. Smith'}, {'authorId': '1982950', 'name': 'Luke Zettlemoyer'}, {'authorId': '48881008', 'name': 'Tao Yu'}]","['University of Waterloo', 'Shanghai Artificial Intelligence Laboratory', 'Carnegie Mellon University', 'Allen Institute for Artificial Intelligence', 'University of Washington', 'University of Edinburgh', 'University of Hong Kong', 'Meta']","['Canada', 'United States', 'United Kingdom', 'China', 'Hong Kong']",2022-01
2201.08531,Shizhe Diao,"Shizhe Diao, Zhichao Huang, Ruijia Xu, Xuechun Li, Yong Lin, Xiao
  Zhou, Tong Zhang",Black-box Prompt Learning for Pre-trained Language Models,To appear in the Transactions on Machine Learning Research (TMLR),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The increasing scale of general-purpose Pre-trained Language Models (PLMs) necessitates the study of more efficient adaptation across different downstream tasks. In this paper, we establish a Black-box Discrete Prompt Learning (BDPL) to resonate with pragmatic interactions between the cloud infrastructure and edge devices. Particularly, instead of fine-tuning the model in the cloud, we adapt PLMs by prompt learning, which efficiently optimizes only a few parameters of the discrete prompts. Moreover, we consider the scenario that we do not have access to the parameters and gradients of the pre-trained models, except for its outputs given inputs. This black-box setting secures the cloud infrastructure from potential attack and misuse to cause a single-point failure, which is preferable to the white-box counterpart by current infrastructures. Under this black-box constraint, we apply a variance-reduced policy gradient algorithm to estimate the gradients of parameters in the categorical distribution of each discrete prompt. In light of our method, the user devices can efficiently tune their tasks by querying the PLMs bounded by a range of API calls. Our experiments on RoBERTa and GPT-3 demonstrate that the proposed algorithm achieves significant improvement on eight benchmarks in a cloud-device collaboration manner. Finally, we conduct in-depth case studies to comprehensively analyze our method in terms of various data sizes, prompt lengths, training budgets, optimization objectives, prompt transferability, and explanations of the learned prompts. Our code will be available at https://github.com/shizhediao/Black-Box-Prompt-Learning. ","[{'version': 'v1', 'created': 'Fri, 21 Jan 2022 03:53:19 GMT'}, {'version': 'v2', 'created': 'Mon, 14 Nov 2022 17:56:31 GMT'}, {'version': 'v3', 'created': 'Thu, 23 Feb 2023 18:42:07 GMT'}]",2023-02-24,"[['Diao', 'Shizhe', ''], ['Huang', 'Zhichao', ''], ['Xu', 'Ruijia', ''], ['Li', 'Xuechun', ''], ['Lin', 'Yong', ''], ['Zhou', 'Xiao', ''], ['Zhang', 'Tong', '']]",0,1,2022-01-21,3,7,1,1,0,1,5faa744dcc28cbbdd9bd67eb703320c6e2d85e52,246210164.0,https://www.semanticscholar.org/paper/5faa744dcc28cbbdd9bd67eb703320c6e2d85e52,Trans. Mach. Learn. Res.,2022.0,85.0,34.0,1.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '50826757', 'name': 'Shizhe Diao'}, {'authorId': '2181353249', 'name': 'Xuechun Li'}, {'authorId': '2154487280', 'name': 'Yong Lin'}, {'authorId': '50294051', 'name': 'Zhichao Huang'}, {'authorId': '2109127048', 'name': 'Xiao Zhou'}, {'authorId': '2117882335', 'name': 'Tong Zhang'}]","['Hong Kong University of Science and Technology', 'University of California, San Diego']","['China', 'United States']",2022-01
2202.05262,David Bau,"Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov",Locating and Editing Factual Associations in GPT,"NeurIPS 2022. 35 pages, 30 figures. Code and data at
  https://rome.baulab.info/",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/ ","[{'version': 'v1', 'created': 'Thu, 10 Feb 2022 18:59:54 GMT'}, {'version': 'v2', 'created': 'Mon, 21 Mar 2022 15:13:09 GMT'}, {'version': 'v3', 'created': 'Wed, 1 Jun 2022 18:56:44 GMT'}, {'version': 'v4', 'created': 'Sun, 23 Oct 2022 18:07:20 GMT'}, {'version': 'v5', 'created': 'Fri, 13 Jan 2023 15:16:16 GMT'}]",2023-01-16,"[['Meng', 'Kevin', ''], ['Bau', 'David', ''], ['Andonian', 'Alex', ''], ['Belinkov', 'Yonatan', '']]",0,1,2022-02-10,5,4,2,0,0,0,996445d847f06e99b0bd259345408a0cf1bce87e,255825985.0,https://www.semanticscholar.org/paper/996445d847f06e99b0bd259345408a0cf1bce87e,Neural Information Processing Systems,2022.0,56.0,235.0,44.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '153615419', 'name': 'Kevin Meng'}, {'authorId': '144159726', 'name': 'David Bau'}, {'authorId': '50112310', 'name': 'A. Andonian'}, {'authorId': '2083259', 'name': 'Yonatan Belinkov'}]","['Massachusetts Institute of Technology', 'Technion – Israel Institute of Technology', 'Northeastern University']","['China', 'United States', 'Israel']",2022-02
2202.07922,Jiacheng Ye,"Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong
  Wu, Tao Yu, Lingpeng Kong",ZeroGen: Efficient Zero-shot Learning via Dataset Generation,Accepted by EMNLP 2022 (Main Conference),,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There is a growing interest in dataset generation recently due to the superior generative capacity of large pre-trained language models (PLMs). In this paper, we study a flexible and efficient zero-short learning method, \textsc{ZeroGen}. Given a zero-shot task, we first generate a dataset from scratch using PLMs in an unsupervised manner. Then, we train a tiny task model (e.g., LSTM) under the supervision of the synthesized dataset. This approach allows highly efficient inference as the final task model only has orders of magnitude fewer parameters comparing to PLMs (e.g., GPT2-XL). Apart from being annotation-free and efficient, we argue that \textsc{ZeroGen} can also provide useful insights from the perspective of data-free model-agnostic knowledge distillation, and unreferenced text generation evaluation. Experiments and analysis on different NLP tasks, namely, text classification, question answering, and natural language inference, show the effectiveness of \textsc{ZeroGen}. ","[{'version': 'v1', 'created': 'Wed, 16 Feb 2022 08:18:02 GMT'}, {'version': 'v2', 'created': 'Sat, 22 Oct 2022 01:32:03 GMT'}]",2022-10-25,"[['Ye', 'Jiacheng', ''], ['Gao', 'Jiahui', ''], ['Li', 'Qintong', ''], ['Xu', 'Hang', ''], ['Feng', 'Jiangtao', ''], ['Wu', 'Zhiyong', ''], ['Yu', 'Tao', ''], ['Kong', 'Lingpeng', '']]",0,1,2022-02-16,2,8,2,1,1,0,2145fcceeb69385e108bf1796d52f974854d4c0b,246867045.0,https://www.semanticscholar.org/paper/2145fcceeb69385e108bf1796d52f974854d4c0b,Conference on Empirical Methods in Natural Language Processing,2022.0,95.0,65.0,7.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '65846898', 'name': 'Jiacheng Ye'}, {'authorId': '144407296', 'name': 'Jiahui Gao'}, {'authorId': '47422209', 'name': 'Qintong Li'}, {'authorId': '47995165', 'name': 'Hang Xu'}, {'authorId': '2093485', 'name': 'Jiangtao Feng'}, {'authorId': '150358371', 'name': 'Zhiyong Wu'}, {'authorId': '2117900202', 'name': 'Tao Yu'}, {'authorId': '47648549', 'name': 'Lingpeng Kong'}]","['Shanghai Artificial Intelligence Laboratory', 'University of Washington', 'University of Hong Kong', 'Huawei Technologies (China)']","['China', 'United States', 'Hong Kong']",2022-02
2203.12054,Tianyu Hua,"Tianyu Hua, Yonglong Tian, Sucheng Ren, Michalis Raptis, Hang Zhao,
  Leonid Sigal",Self-supervision through Random Segments with Autoregressive Coding (RandSAC),,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Inspired by the success of self-supervised autoregressive representation learning in natural language (GPT and its variants), and advances in recent visual architecture design with Vision Transformers (ViTs), in this paper, we explore the effect various design choices have on the success of applying such training strategies for visual feature learning. Specifically, we introduce a novel strategy that we call Random Segments with Autoregressive Coding (RandSAC). In RandSAC, we group patch representations (image tokens) into hierarchically arranged segments; within each segment, tokens are predicted in parallel, similar to BERT, while across segment predictions are sequential, similar to GPT. We illustrate that randomized serialization of the segments significantly improves the performance and results in distribution over spatially-long (across-segments) and -short (within-segment) predictions which are effective for feature learning. We illustrate the pertinence of these design choices and explore alternatives on a number of datasets (e.g., CIFAR10, CIFAR100, ImageNet). While our pre-training strategy works with a vanilla Transformer, we also propose a conceptually simple, but highly effective, addition to the decoder that allows learnable skip-connections to encoder$'$s feature layers, which further improves the performance. ","[{'version': 'v1', 'created': 'Tue, 22 Mar 2022 21:28:55 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Oct 2022 03:59:43 GMT'}]",2022-10-27,"[['Hua', 'Tianyu', ''], ['Tian', 'Yonglong', ''], ['Ren', 'Sucheng', ''], ['Raptis', 'Michalis', ''], ['Zhao', 'Hang', ''], ['Sigal', 'Leonid', '']]",0,1,2022-03-22,2,6,2,0,0,0,4bc8fa1ca83cf4a6b93044b3103fe622b012d90d,247618909.0,https://www.semanticscholar.org/paper/4bc8fa1ca83cf4a6b93044b3103fe622b012d90d,International Conference on Learning Representations,2022.0,64.0,6.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1419971650', 'name': 'Tianyu Hua'}, {'authorId': '2476765', 'name': 'Yonglong Tian'}, {'authorId': '1823941979', 'name': 'Sucheng Ren'}, {'authorId': '2146231364', 'name': 'Hang Zhao'}, {'authorId': '144398147', 'name': 'L. Sigal'}]","['South China University of Technology', 'Google', 'Vector Institute', 'University of British Columbia', 'Tsinghua University', 'Canadian Institute for Advanced Research', 'Massachusetts Institute of Technology']","['China', 'United States', 'Canada']",2022-03
2206.02369,Jin Xu,"Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, Jian Li",Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation,"Accepted by NeurIPS 2022. Code is released at
  https://github.com/Jxu-Thu/DITTO",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  While large-scale neural language models, such as GPT2 and BART, have achieved impressive results on various text generation tasks, they tend to get stuck in undesirable sentence-level loops with maximization-based decoding algorithms (\textit{e.g.}, greedy search). This phenomenon is counter-intuitive since there are few consecutive sentence-level repetitions in human corpora (e.g., 0.02\% in Wikitext-103). To investigate the underlying reasons for generating consecutive sentence-level repetitions, we study the relationship between the probabilities of the repetitive tokens and their previous repetitions in the context. Through our quantitative experiments, we find that 1) Language models have a preference to repeat the previous sentence; 2) The sentence-level repetitions have a \textit{self-reinforcement effect}: the more times a sentence is repeated in the context, the higher the probability of continuing to generate that sentence; 3) The sentences with higher initial probabilities usually have a stronger self-reinforcement effect. Motivated by our findings, we propose a simple and effective training method \textbf{DITTO} (Pseu\underline{D}o-Repet\underline{IT}ion Penaliza\underline{T}i\underline{O}n), where the model learns to penalize probabilities of sentence-level repetitions from pseudo repetitive data. Although our method is motivated by mitigating repetitions, experiments show that DITTO not only mitigates the repetition issue without sacrificing perplexity, but also achieves better generation quality. Extensive experiments on open-ended text generation (Wikitext-103) and text summarization (CNN/DailyMail) demonstrate the generality and effectiveness of our method. ","[{'version': 'v1', 'created': 'Mon, 6 Jun 2022 05:51:12 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Oct 2022 07:53:01 GMT'}]",2022-10-11,"[['Xu', 'Jin', ''], ['Liu', 'Xiaojiang', ''], ['Yan', 'Jianhao', ''], ['Cai', 'Deng', ''], ['Li', 'Huayang', ''], ['Li', 'Jian', '']]",0,1,2022-06-06,2,6,1,1,1,0,6151ee4af6a3fe78f2df7c605598cd9e02b23c5b,249395390.0,https://www.semanticscholar.org/paper/6151ee4af6a3fe78f2df7c605598cd9e02b23c5b,Neural Information Processing Systems,2022.0,44.0,24.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2110641353', 'name': 'Jin Xu'}, {'authorId': '3028405', 'name': 'Xiaojiang Liu'}, {'authorId': '134233854', 'name': 'Jianhao Yan'}, {'authorId': '2053327987', 'name': 'Deng Cai'}, {'authorId': '91956362', 'name': 'Huayang Li'}, {'authorId': '2151968158', 'name': 'Jian Li'}]","['Westlake University', 'Apple', 'Chinese University of Hong Kong', 'Tsinghua University']","['China', 'United States']",2022-06
2206.06888,Daoguang Zan,"Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan,
  Yongji Wang, Weizhu Chen, Jian-Guang Lou",CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation,Accepted for publication at IJCAI-ECAI 2022,,,,cs.SE cs.CL cs.PL,http://creativecommons.org/licenses/by/4.0/,"  Code generation is a longstanding challenge, aiming to generate a code snippet based on a natural language description. Usually, expensive text-code paired data is essential for training a code generation model. Recently, thanks to the success of pre-training techniques, large language models are trained on large-scale unlabelled code corpora and perform well in code generation. In this paper, we investigate how to leverage an unlabelled code corpus to train a model for library-oriented code generation. Since it is a common practice for programmers to reuse third-party libraries, in which case the text-code paired data are harder to obtain due to the huge number of libraries. We observe that library-oriented code snippets are more likely to share similar code sketches. Hence, we present CERT with two steps: a sketcher generates the sketch, then a generator fills the details in the sketch. Both the sketcher and the generator are continually pre-trained upon a base model using unlabelled data. Furthermore, we craft two benchmarks named PandasEval and NumpyEval to evaluate library-oriented code generation. Experimental results demonstrate the impressive performance of CERT. For example, it surpasses the base model by an absolute 15.67% improvement in terms of pass@1 on PandasEval. Our work is available at https://github.com/microsoft/PyCodeGPT. ","[{'version': 'v1', 'created': 'Tue, 14 Jun 2022 14:44:34 GMT'}]",2022-06-15,"[['Zan', 'Daoguang', ''], ['Chen', 'Bei', ''], ['Yang', 'Dejian', ''], ['Lin', 'Zeqi', ''], ['Kim', 'Minsu', ''], ['Guan', 'Bei', ''], ['Wang', 'Yongji', ''], ['Chen', 'Weizhu', ''], ['Lou', 'Jian-Guang', '']]",0,1,2022-06-14,1,9,3,0,0,0,a08a3b08a5a1de6462a7da2906b1cd81691d6c18,249642442.0,https://www.semanticscholar.org/paper/a08a3b08a5a1de6462a7da2906b1cd81691d6c18,International Joint Conference on Artificial Intelligence,2022.0,27.0,36.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2134434187', 'name': 'Daoguang Zan'}, {'authorId': None, 'name': 'Bei Chen'}, {'authorId': '2111181262', 'name': 'Dejian Yang'}, {'authorId': '2284174', 'name': 'Zeqi Lin'}, {'authorId': '2174816942', 'name': 'Minsu Kim'}, {'authorId': '36923691', 'name': 'Bei Guan'}, {'authorId': '2108097250', 'name': 'Yongji Wang'}, {'authorId': '2109136147', 'name': 'Weizhu Chen'}, {'authorId': '153249455', 'name': 'Jian-Guang Lou'}]","['University of Chinese Academy of Sciences', 'Microsoft', 'Chinese Academy of Sciences', 'Korea University']","['China', 'United States', 'South Korea']",2022-06
2206.11064,Ning Gui Prof. dr.,"Jiawen Wei, Fangyuan Wang, Wanxin Zeng, Wenwei Lin and Ning Gui",An Embedded Feature Selection Framework for Control,"9 pages, 9 figures, accepted by SIGKDD 2022",,10.1145/3534678.3539290,,cs.LG cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Reducing sensor requirements while keeping optimal control performance is crucial to many industrial control applications to achieve robust, low-cost, and computation-efficient controllers. However, existing feature selection solutions for the typical machine learning domain can hardly be applied in the domain of control with changing dynamics. In this paper, a novel framework, namely the Dual-world embedded Attentive Feature Selection (D-AFS), can efficiently select the most relevant sensors for the system under dynamic control. Rather than the one world used in most Deep Reinforcement Learning (DRL) algorithms, D-AFS has both the real world and its virtual peer with twisted features. By analyzing the DRL's response in two worlds, D-AFS can quantitatively identify respective features' importance towards control. A well-known active flow control problem, cylinder drag reduction, is used for evaluation. Results show that D-AFS successfully finds an optimized five-probes layout with 18.7\% drag reduction than the state-of-the-art solution with 151 probes and 49.2\% reduction than five-probes layout by human experts. We also apply this solution to four OpenAI classical control cases. In all cases, D-AFS achieves the same or better sensor configurations than originally provided solutions. Results highlight, we argued, a new way to achieve efficient and optimal sensor designs for experimental or industrial systems. Our source codes are made publicly available at https://github.com/G-AILab/DAFSFluid. ","[{'version': 'v1', 'created': 'Sun, 19 Jun 2022 07:03:40 GMT'}]",2022-06-23,"[['Wei', 'Jiawen', ''], ['Wang', 'Fangyuan', ''], ['Zeng', 'Wanxin', ''], ['Lin', 'Wenwei', ''], ['Gui', 'Ning', '']]",0,0,2022-06-19,1,5,2,0,0,0,f03ae909e81f0489bee1cfe6c20664e47edcc4f3,249926422.0,https://www.semanticscholar.org/paper/f03ae909e81f0489bee1cfe6c20664e47edcc4f3,Knowledge Discovery and Data Mining,2022.0,35.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '145604950', 'name': 'Jiawen Wei'}, {'authorId': '2145902572', 'name': 'Fangyuan Wang'}, {'authorId': '3013856', 'name': 'Wan-Ting Zeng'}, {'authorId': '49661300', 'name': 'Wen-Jiun Lin'}, {'authorId': '1780829', 'name': 'Ning Gui'}]","['New York, NY, USA, 10 pages.', 'Central South University', 'Zhejiang Sci-Tech University']","['China', 'United States']",2022-06
2206.11494,Tairan Huang,"Tairan Huang, Xu Li, Hao Li, Mingming Sun, Ping Li",CGAR: Critic Guided Action Redistribution in Reinforcement Leaning,"IEEE Conference on Games (CoG), 2022",,,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Training a game-playing reinforcement learning agent requires multiple interactions with the environment. Ignorant random exploration may cause a waste of time and resources. It's essential to alleviate such waste. As discussed in this paper, under the settings of the off-policy actor critic algorithms, we demonstrate that the critic can bring more expected discounted rewards than or at least equal to the actor. Thus, the Q value predicted by the critic is a better signal to redistribute the action originally sampled from the policy distribution predicted by the actor. This paper introduces the novel Critic Guided Action Redistribution (CGAR) algorithm and tests it on the OpenAI MuJoCo tasks. The experimental results demonstrate that our method improves the sample efficiency and achieves state-of-the-art performance. Our code can be found at https://github.com/tairanhuang/CGAR. ","[{'version': 'v1', 'created': 'Thu, 23 Jun 2022 06:33:14 GMT'}]",2022-06-24,"[['Huang', 'Tairan', ''], ['Li', 'Xu', ''], ['Li', 'Hao', ''], ['Sun', 'Mingming', ''], ['Li', 'Ping', '']]",0,0,2022-06-23,1,5,1,0,0,0,c5db0f5620a9d7be50248ef90ba140cfca7682da,249954000.0,https://www.semanticscholar.org/paper/c5db0f5620a9d7be50248ef90ba140cfca7682da,2022 IEEE Conference on Games (CoG),2022.0,16.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '51932854', 'name': 'Tairan Huang'}, {'authorId': '2116327105', 'name': 'Xu Li'}, {'authorId': '2145537996', 'name': 'Haoyuan Li'}, {'authorId': '1893044063', 'name': 'Mingming Sun'}, {'authorId': '2420746', 'name': 'P. Li'}]","['Peking University', 'Beihang University', 'Baidu']","['China', 'United States']",2022-06
2206.13778,Fan Xu,Fan Xu and Yunxiang Zhang and Xiaojun Wan,CC-Riddle: A Question Answering Dataset of Chinese Character Riddles,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The Chinese character riddle is a unique form of cultural entertainment specific to the Chinese language. It typically comprises two parts: the riddle description and the solution. The solution to the riddle is a single character, while the riddle description primarily describes the glyph of the solution, occasionally supplemented with its explanation and pronunciation. Solving Chinese character riddles is a challenging task that demands understanding of character glyph, general knowledge, and a grasp of figurative language. In this paper, we construct a \textbf{C}hinese \textbf{C}haracter riddle dataset named CC-Riddle, which covers the majority of common simplified Chinese characters. The construction process is a combination of web crawling, language model generation and manual filtering. In generation stage, we input the Chinese phonetic alphabet, glyph and meaning of the solution character into the generation model, which then produces multiple riddle descriptions. The generated riddles are then manually filtered and the final CC-Riddle dataset is composed of both human-written riddles and these filtered, generated riddles. In order to assess the performance of language models on the task of solving character riddles, we use retrieval-based, generative and multiple-choice QA strategies to test three language models: BERT, ChatGPT and ChatGLM. The test results reveal that current language models still struggle to solve Chinese character riddles. CC-Riddle is publicly available at \url{https://github.com/pku0xff/CC-Riddle}. ","[{'version': 'v1', 'created': 'Tue, 28 Jun 2022 06:23:13 GMT'}, {'version': 'v2', 'created': 'Sun, 24 Sep 2023 05:15:51 GMT'}]",2023-09-26,"[['Xu', 'Fan', ''], ['Zhang', 'Yunxiang', ''], ['Wan', 'Xiaojun', '']]",1,1,2022-06-28,2,3,1,2,1,1,db4906c7cc08cfd324bcc8a78a8faa747b78ddff,249172717.0,https://www.semanticscholar.org/paper/db4906c7cc08cfd324bcc8a78a8faa747b78ddff,arXiv.org,2022.0,29.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2111431951', 'name': 'Fan Xu'}, {'authorId': '2124948532', 'name': 'Yunxiang Zhang'}, {'authorId': '9714242', 'name': 'Xiao-Yi Wan'}]","['Peking University', 'University of Michigan–Ann Arbor']","['China', 'United States']",2022-06
2208.00638,Guangyi Liu,"Guangyi Liu, Zeyu Feng, Yuan Gao, Zichao Yang, Xiaodan Liang, Junwei
  Bao, Xiaodong He, Shuguang Cui, Zhen Li, Zhiting Hu",Composable Text Controls in Latent Space with ODEs,"27 Pages, Code: https://github.com/guangyliu/LatentOps",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Real-world text applications often involve composing a wide range of text control operations, such as editing the text w.r.t. an attribute, manipulating keywords and structure, and generating new text of desired properties. Prior work typically learns/finetunes a language model (LM) to perform individual or specific subsets of operations. Recent research has studied combining operations in a plug-and-play manner, often with costly search or optimization in the complex sequence space. This paper proposes a new efficient approach for composable text operations in the compact latent space of text. The low-dimensionality and differentiability of the text latent vector allow us to develop an efficient sampler based on ordinary differential equations (ODEs) given arbitrary plug-in operators (e.g., attribute classifiers). By connecting pretrained LMs (e.g., GPT2) to the latent space through efficient adaption, we then decode the sampled vectors into desired text sequences. The flexible approach permits diverse control operators (sentiment, tense, formality, keywords, etc.) acquired using any relevant data from different domains. Experiments show that composing those operators within our approach manages to generate or edit high-quality text, substantially improving over previous methods in terms of generation quality and efficiency. ","[{'version': 'v1', 'created': 'Mon, 1 Aug 2022 06:51:45 GMT'}, {'version': 'v2', 'created': 'Thu, 15 Sep 2022 06:16:01 GMT'}]",2022-09-16,"[['Liu', 'Guangyi', ''], ['Feng', 'Zeyu', ''], ['Gao', 'Yuan', ''], ['Yang', 'Zichao', ''], ['Liang', 'Xiaodan', ''], ['Bao', 'Junwei', ''], ['He', 'Xiaodong', ''], ['Cui', 'Shuguang', ''], ['Li', 'Zhen', ''], ['Hu', 'Zhiting', '']]",0,1,2022-08-01,2,10,2,1,1,0,291a32c20db76eab85d4f626d0a8c16bcf965379,252280511.0,https://www.semanticscholar.org/paper/291a32c20db76eab85d4f626d0a8c16bcf965379,,2022.0,65.0,9.0,2.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2152756082', 'name': 'Guangyi Liu'}, {'authorId': '35398411', 'name': 'Zeyu Feng'}, {'authorId': '2185168382', 'name': 'Yuan Gao'}, {'authorId': '8387085', 'name': 'Zichao Yang'}, {'authorId': '2153397698', 'name': 'Xiaodan Liang'}, {'authorId': '3299718', 'name': 'Junwei Bao'}, {'authorId': '144137069', 'name': 'Xiaodong He'}, {'authorId': '1745056', 'name': 'Shuguang Cui'}, {'authorId': '2110120874', 'name': 'Zhen Li'}, {'authorId': '2749311', 'name': 'Zhiting Hu'}]","['Jingdong', 'Carnegie Mellon University', 'DarkMatter AI Research,']","['China', 'United States']",2022-08
2208.06677,Xingyu Xie,Xingyu Xie and Pan Zhou and Huan Li and Zhouchen Lin and Shuicheng Yan,Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models,,,,,cs.LG math.OC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In deep learning, different kinds of deep networks typically need different optimizers, which have to be chosen after multiple trials, making the training process inefficient. To relieve this issue and consistently improve the model training speed across deep networks, we propose the ADAptive Nesterov momentum algorithm, Adan for short. Adan first reformulates the vanilla Nesterov acceleration to develop a new Nesterov momentum estimation (NME) method, which avoids the extra overhead of computing gradient at the extrapolation point. Then Adan adopts NME to estimate the gradient's first- and second-order moments in adaptive gradient algorithms for convergence acceleration. Besides, we prove that Adan finds an $\epsilon$-approximate first-order stationary point within $O(\epsilon^{-3.5})$ stochastic gradient complexity on the non-convex stochastic problems (e.g., deep learning problems), matching the best-known lower bound. Extensive experimental results show that Adan consistently surpasses the corresponding SoTA optimizers on vision, language, and RL tasks and sets new SoTAs for many popular networks and frameworks, e.g., ResNet, ConvNext, ViT, Swin, MAE, DETR, GPT-2, Transformer-XL, and BERT. More surprisingly, Adan can use half of the training cost (epochs) of SoTA optimizers to achieve higher or comparable performance on ViT, GPT-2, MAE, e.t.c., and also shows great tolerance to a large range of minibatch size, e.g., from 1k to 32k. Code is released at https://github.com/sail-sg/Adan, and has been used in multiple popular deep learning frameworks or projects. ","[{'version': 'v1', 'created': 'Sat, 13 Aug 2022 16:04:39 GMT'}, {'version': 'v2', 'created': 'Thu, 1 Sep 2022 15:48:38 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Oct 2022 09:32:37 GMT'}, {'version': 'v4', 'created': 'Mon, 27 Feb 2023 14:58:59 GMT'}]",2023-02-28,"[['Xie', 'Xingyu', ''], ['Zhou', 'Pan', ''], ['Li', 'Huan', ''], ['Lin', 'Zhouchen', ''], ['Yan', 'Shuicheng', '']]",0,1,2022-08-13,4,5,2,1,1,0,568eb10d17f1643228303670fe0f1d6608bd6f4d,251564316.0,https://www.semanticscholar.org/paper/568eb10d17f1643228303670fe0f1d6608bd6f4d,arXiv.org,2022.0,92.0,29.0,3.0,True,"['Computer Science', 'Mathematics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Mathematics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2543387', 'name': 'Xingyu Xie'}, {'authorId': '2153245275', 'name': 'Pan Zhou'}, {'authorId': '3092681', 'name': 'Huan Li'}, {'authorId': '33383055', 'name': 'Zhouchen Lin'}, {'authorId': '143653681', 'name': 'Shuicheng Yan'}]","['Peking University', 'Nankai University', 'Applied Research Laboratory at the University of Hawai‘i']","['China', 'United States']",2022-08
2210.02875,Tianbao Xie,"Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni,
  Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer,
  Noah A. Smith, Tao Yu",Binding Language Models in Symbolic Languages,"ICLR 2023 camera ready, 27 pages, 10 figures",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at https://github.com/HKUNLP/Binder . ","[{'version': 'v1', 'created': 'Thu, 6 Oct 2022 12:55:17 GMT'}, {'version': 'v2', 'created': 'Wed, 1 Mar 2023 03:21:40 GMT'}]",2023-03-02,"[['Cheng', 'Zhoujun', ''], ['Xie', 'Tianbao', ''], ['Shi', 'Peng', ''], ['Li', 'Chengzu', ''], ['Nadkarni', 'Rahul', ''], ['Hu', 'Yushi', ''], ['Xiong', 'Caiming', ''], ['Radev', 'Dragomir', ''], ['Ostendorf', 'Mari', ''], ['Zettlemoyer', 'Luke', ''], ['Smith', 'Noah A.', ''], ['Yu', 'Tao', '']]",0,1,2022-10-06,2,12,1,2,0,2,f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab,252734772.0,https://www.semanticscholar.org/paper/f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab,International Conference on Learning Representations,2022.0,67.0,77.0,6.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1471878967', 'name': 'Zhoujun Cheng'}, {'authorId': '2057038673', 'name': 'Tianbao Xie'}, {'authorId': '2055356856', 'name': 'Peng Shi'}, {'authorId': '2155795167', 'name': 'Chengzu Li'}, {'authorId': '40027281', 'name': 'R.K. Nadkarni'}, {'authorId': '2112209725', 'name': 'Yushi Hu'}, {'authorId': '2054594326', 'name': 'Caiming Xiong'}, {'authorId': '9215251', 'name': 'Dragomir R. Radev'}, {'authorId': '81444299', 'name': 'M. Ostendorf'}, {'authorId': '2137813791', 'name': 'Luke Zettlemoyer'}, {'authorId': '2116827887', 'name': 'N. A. Smith'}, {'authorId': None, 'name': 'Tao Yu'}]","['University of Waterloo', 'Yale University', 'Shanghai Jiao Tong University', 'Allen Institute for Artificial Intelligence', 'University of Washington', 'Salesforce', 'University of Hong Kong', 'Meta']","['China', 'United States', 'Canada', 'Hong Kong']",2022-10
2210.05075,Haoyu Dong,"Fan Zhou, Haoyu Dong, Qian Liu, Zhoujun Cheng, Shi Han, Dongmei Zhang",Reflection of Thought: Inversely Eliciting Numerical Reasoning in Language Models via Solving Linear Systems,,,,,cs.CL cs.IR cs.NA math.NA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Numerical reasoning over natural language has been a long-standing goal for the research community. However, cutting-edge language models have proven difficult to reliably generalize to a broad range of numbers, although they have shown proficiency in reasoning over common and simple numbers. In this paper, we propose a novel method to elicit and exploit the numerical reasoning knowledge hidden in pre-trained language models using simple anchor numbers. Concretely, we first leverage simple numbers as anchors to probe the implicitly inferred arithmetic expressions from language models, and then explicitly apply the expressions on complex numbers to get corresponding answers. To inversely elicit arithmetic expressions, we transform and formulate the task as an analytically solvable linear system. Experimental results on several numerical reasoning benchmarks demonstrate that our approach significantly improves numerical reasoning capabilities of existing LMs. More importantly, our approach is training-free and simply works in the inference phase, making it highly portable and achieving consistent performance benefits across a variety of language models (GPT-3, T5, BART, etc) in all zero-shot, few-shot, and fine-tuning scenarios. ","[{'version': 'v1', 'created': 'Tue, 11 Oct 2022 00:57:19 GMT'}]",2022-10-12,"[['Zhou', 'Fan', ''], ['Dong', 'Haoyu', ''], ['Liu', 'Qian', ''], ['Cheng', 'Zhoujun', ''], ['Han', 'Shi', ''], ['Zhang', 'Dongmei', '']]",0,1,2022-10-11,1,6,4,2,1,1,712f21411526e8450036d7199637808590be3579,252815431.0,https://www.semanticscholar.org/paper/712f21411526e8450036d7199637808590be3579,arXiv.org,2022.0,37.0,5.0,1.0,True,"['Computer Science', 'Mathematics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Mathematics', 'source': 'external'}, {'category': 'Mathematics', 'source': 's2-fos-model'}]","[{'authorId': '2153433679', 'name': 'Fan Zhou'}, {'authorId': '2153721991', 'name': 'Haoyu Dong'}, {'authorId': '1409707585', 'name': 'Qian Liu'}, {'authorId': '1471878967', 'name': 'Zhoujun Cheng'}, {'authorId': '2109750123', 'name': 'Shi Han'}, {'authorId': '2140415600', 'name': 'Dongmei Zhang'}]","['Shanghai Jiao Tong University', 'Microsoft', 'Applied Research Laboratory at the University of Hawai‘i']","['China', 'United States']",2022-10
2210.05549,Zixuan Ke,"Zixuan Ke, Haowei Lin, Yijia Shao, Hu Xu, Lei Shu, and Bing Liu",Continual Training of Language Models for Few-Shot Learning,,EMNLP 2022,,,cs.CL cs.AI cs.LG cs.NE,http://creativecommons.org/publicdomain/zero/1.0/,"  Recent work on applying large language models (LMs) achieves impressive performance in many NLP applications. Adapting or posttraining an LM using an unlabeled domain corpus can produce even better performance for end-tasks in the domain. This paper proposes the problem of continually extending an LM by incrementally post-train the LM with a sequence of unlabeled domain corpora to expand its knowledge without forgetting its previous skills. The goal is to improve the few-shot end-task learning in these domains. The resulting system is called CPT (Continual PostTraining), which to our knowledge, is the first continual post-training system. Experimental results verify its effectiveness. ","[{'version': 'v1', 'created': 'Tue, 11 Oct 2022 15:43:58 GMT'}]",2022-10-12,"[['Ke', 'Zixuan', ''], ['Lin', 'Haowei', ''], ['Shao', 'Yijia', ''], ['Xu', 'Hu', ''], ['Shu', 'Lei', ''], ['Liu', 'Bing', '']]",0,0,2022-10-11,1,6,4,0,0,0,e053be7f36a0772b68eaaa14f15650c14071e4ab,252815848.0,https://www.semanticscholar.org/paper/e053be7f36a0772b68eaaa14f15650c14071e4ab,Conference on Empirical Methods in Natural Language Processing,2022.0,62.0,15.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '9623089', 'name': 'Zixuan Ke'}, {'authorId': '2152782471', 'name': 'Hao Lin'}, {'authorId': '74175857', 'name': 'Yijia Shao'}, {'authorId': '33464127', 'name': 'Hu Xu'}, {'authorId': '145142456', 'name': 'Lei Shu'}, {'authorId': '47655556', 'name': 'Bin Liu'}]","['Peking University', 'University of Illinois at Chicago', 'Google']","['China', 'United States']",2022-10
2210.06726,Shiyang Li,"Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun
  Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, Wenhu Chen and Xifeng Yan",Explanations from Large Language Models Make Small Reasoners Better,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Integrating free-text explanations to in-context learning of large language models (LLM) is shown to elicit strong reasoning capabilities along with reasonable explanations. In this paper, we consider the problem of leveraging the explanations generated by LLM to improve the training of small reasoners, which are more favorable in real-production deployment due to their low cost. We systematically explore three explanation generation approaches from LLM and utilize a multi-task learning framework to facilitate small models to acquire strong reasoning power together with explanation generation capabilities. Experiments on multiple reasoning tasks show that our method can consistently and significantly outperform finetuning baselines across different settings, and even perform better than finetuning/prompting a 60x larger GPT-3 (175B) model by up to 9.5% in accuracy. As a side benefit, human evaluation further shows that our method can generate high-quality explanations to justify its predictions, moving towards the goal of explainable AI. ","[{'version': 'v1', 'created': 'Thu, 13 Oct 2022 04:50:02 GMT'}]",2022-10-14,"[['Li', 'Shiyang', ''], ['Chen', 'Jianshu', ''], ['Shen', 'Yelong', ''], ['Chen', 'Zhiyu', ''], ['Zhang', 'Xinlu', ''], ['Li', 'Zekun', ''], ['Wang', 'Hong', ''], ['Qian', 'Jing', ''], ['Peng', 'Baolin', ''], ['Mao', 'Yi', ''], ['Chen', 'Wenhu', ''], ['Yan', 'Xifeng', '']]",0,1,2022-10-13,1,12,1,1,0,1,7d29a84a589aa5655e5d3fed8d725ea472816599,252873123.0,https://www.semanticscholar.org/paper/7d29a84a589aa5655e5d3fed8d725ea472816599,arXiv.org,2022.0,54.0,46.0,7.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '50341591', 'name': 'SHIYANG LI'}, {'authorId': '2108276402', 'name': 'Jianshu Chen'}, {'authorId': '1752875', 'name': 'Yelong Shen'}, {'authorId': '2142370346', 'name': 'Zhiyu Chen'}, {'authorId': '2108030191', 'name': 'Xinlu Zhang'}, {'authorId': '2109964198', 'name': 'Zekun Li'}, {'authorId': '46507182', 'name': 'Hong Wang'}, {'authorId': '66735072', 'name': 'Jingu Qian'}, {'authorId': '1780690', 'name': 'Baolin Peng'}, {'authorId': '145469202', 'name': 'Yi Mao'}, {'authorId': '2928777', 'name': 'Wenhu Chen'}, {'authorId': '1740249', 'name': 'Xifeng Yan'}]","['Tencent', 'Microsoft', 'University of Waterloo', 'University of California, Santa Barbara']","['Canada', 'United States', 'China']",2022-10
2210.07229,Arnab Sen Sharma,"Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, David
  Bau",Mass-Editing Memory in a Transformer,"18 pages, 11 figures. Code and data at https://memit.baulab.info",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by orders of magnitude. Our code and data are at https://memit.baulab.info. ","[{'version': 'v1', 'created': 'Thu, 13 Oct 2022 17:55:53 GMT'}, {'version': 'v2', 'created': 'Tue, 1 Aug 2023 18:41:52 GMT'}]",2023-08-03,"[['Meng', 'Kevin', ''], ['Sharma', 'Arnab Sen', ''], ['Andonian', 'Alex', ''], ['Belinkov', 'Yonatan', ''], ['Bau', 'David', '']]",0,1,2022-10-13,2,5,2,0,0,0,2fe1ac0b09cc0f50eb83eef6c7c6b45ac8b12413,252873467.0,https://www.semanticscholar.org/paper/2fe1ac0b09cc0f50eb83eef6c7c6b45ac8b12413,International Conference on Learning Representations,2022.0,59.0,101.0,15.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '153615419', 'name': 'Kevin Meng'}, {'authorId': '2109373982', 'name': 'Arnab Sharma'}, {'authorId': '50112310', 'name': 'A. Andonian'}, {'authorId': '2083259', 'name': 'Yonatan Belinkov'}, {'authorId': '144159726', 'name': 'David Bau'}]","['Massachusetts Institute of Technology', 'Technion – Israel Institute of Technology', 'Northeastern University']","['China', 'United States', 'Israel']",2022-10
2210.07652,Yejin Bang,"Yejin Bang, Tiezheng Yu, Andrea Madotto, Zhaojiang Lin, Mona Diab,
  Pascale Fung",Enabling Classifiers to Make Judgements Explicitly Aligned with Human Values,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many NLP classification tasks, such as sexism/racism detection or toxicity detection, are based on human values. Yet, human values can vary under diverse cultural conditions. Therefore, we introduce a framework for value-aligned classification that performs prediction based on explicitly written human values in the command. Along with the task, we propose a practical approach that distills value-aligned knowledge from large-scale language models (LLMs) to construct value-aligned classifiers in two steps. First, we generate value-aligned training data from LLMs by prompt-based few-shot learning. Next, we fine-tune smaller classification models with the generated data for the task. Empirical results show that our VA-Models surpass multiple baselines by at least 15.56% on the F1-score, including few-shot learning with OPT-175B and existing text augmentation methods. We suggest that using classifiers with explicit human value input improves both inclusivity & explainability in AI. ","[{'version': 'v1', 'created': 'Fri, 14 Oct 2022 09:10:49 GMT'}]",2022-10-17,"[['Bang', 'Yejin', ''], ['Yu', 'Tiezheng', ''], ['Madotto', 'Andrea', ''], ['Lin', 'Zhaojiang', ''], ['Diab', 'Mona', ''], ['Fung', 'Pascale', '']]",0,0,2022-10-14,1,6,2,1,1,0,9c36c8f398a074801d6098287c4353bcf87a1d6c,252907375.0,https://www.semanticscholar.org/paper/9c36c8f398a074801d6098287c4353bcf87a1d6c,TRUSTNLP,2022.0,49.0,5.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '23672613', 'name': 'Yejin Bang'}, {'authorId': '1660855299', 'name': 'Tiezheng Yu'}, {'authorId': '2111680936', 'name': 'Andrea Madotto'}, {'authorId': '100466830', 'name': 'Zhaojiang Lin'}, {'authorId': '2138579860', 'name': 'Mona T. Diab'}, {'authorId': '2057151752', 'name': 'Pascale Fung'}]","['Hong Kong University of Science and Technology', 'Meta']","['China', 'United States']",2022-10
2210.10341,Renqian Luo,"Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon,
  Tie-Yan Liu",BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining,"Published at Briefings in Bioinformatics. Code is available at
  https://github.com/microsoft/BioGPT","Briefings in Bioinformatics, 2022;, bbac409",10.1093/bib/bbac409,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e., BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large scale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms. Code is available at https://github.com/microsoft/BioGPT. ","[{'version': 'v1', 'created': 'Wed, 19 Oct 2022 07:17:39 GMT'}, {'version': 'v2', 'created': 'Fri, 13 Jan 2023 12:00:04 GMT'}, {'version': 'v3', 'created': 'Mon, 3 Apr 2023 06:49:33 GMT'}]",2023-04-04,"[['Luo', 'Renqian', ''], ['Sun', 'Liai', ''], ['Xia', 'Yingce', ''], ['Qin', 'Tao', ''], ['Zhang', 'Sheng', ''], ['Poon', 'Hoifung', ''], ['Liu', 'Tie-Yan', '']]",0,1,2022-10-19,3,7,2,0,0,0,44279244407a64431810f982be6d0c7da4429dd7,252542956.0,https://www.semanticscholar.org/paper/44279244407a64431810f982be6d0c7da4429dd7,Briefings Bioinform.,2022.0,58.0,230.0,29.0,True,"['Computer Science', 'Medicine']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Medicine', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Medicine', 'source': 's2-fos-model'}]","[{'authorId': '3440939', 'name': 'Renqian Luo'}, {'authorId': '2186266093', 'name': 'Liai Sun'}, {'authorId': '2111056280', 'name': 'Yingce Xia'}, {'authorId': '143826491', 'name': 'Tao Qin'}, {'authorId': '72655349', 'name': 'Sheng Zhang'}, {'authorId': '1759772', 'name': 'Hoifung Poon'}, {'authorId': '2110264337', 'name': 'Tie-Yan Liu'}]","['Peking University', 'Microsoft']","['China', 'United States', 'Netherlands']",2022-10
2210.13382,Kenneth Li,"Kenneth Li, Aspen K. Hopkins, David Bau, Fernanda Vi\'egas, Hanspeter
  Pfister, Martin Wattenberg",Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task,"ICLR 2023 oral (notable-top-5%):
  https://openreview.net/forum?id=DeG07_TcZvT ; code:
  https://github.com/likenneth/othello_world",,,,cs.LG cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Language models show a surprising range of capabilities, but the source of their apparent competence is unclear. Do these networks just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see? We investigate this question by applying a variant of the GPT model to the task of predicting legal moves in a simple board game, Othello. Although the network has no a priori knowledge of the game or its rules, we uncover evidence of an emergent nonlinear internal representation of the board state. Interventional experiments indicate this representation can be used to control the output of the network and create ""latent saliency maps"" that can help explain predictions in human terms. ","[{'version': 'v1', 'created': 'Mon, 24 Oct 2022 16:29:55 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Oct 2022 13:47:00 GMT'}, {'version': 'v3', 'created': 'Wed, 25 Jan 2023 20:05:29 GMT'}, {'version': 'v4', 'created': 'Mon, 27 Feb 2023 17:09:15 GMT'}]",2023-02-28,"[['Li', 'Kenneth', ''], ['Hopkins', 'Aspen K.', ''], ['Bau', 'David', ''], ['Viégas', 'Fernanda', ''], ['Pfister', 'Hanspeter', ''], ['Wattenberg', 'Martin', '']]",0,1,2022-10-24,4,6,3,0,0,0,b34a7d4f40d7653b15f4e2e04405cf59b8821d5d,253098566.0,https://www.semanticscholar.org/paper/b34a7d4f40d7653b15f4e2e04405cf59b8821d5d,International Conference on Learning Representations,2022.0,34.0,67.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2149140708', 'name': 'Kenneth Li'}, {'authorId': '1934459700', 'name': 'Aspen K. Hopkins'}, {'authorId': '144159726', 'name': 'David Bau'}, {'authorId': '2064951472', 'name': ""Fernanda Vi'egas""}, {'authorId': '143758236', 'name': 'H. Pfister'}, {'authorId': '145233583', 'name': 'M. Wattenberg'}]","['Massachusetts Institute of Technology', 'Harvard University', 'Northeastern University']","['China', 'United States']",2022-10
2210.13432,Hao Liu,"Hao Liu, Xinyang Geng, Lisa Lee, Igor Mordatch, Sergey Levine, Sharan
  Narang, Pieter Abbeel",Towards Better Few-Shot and Finetuning Performance with Forgetful Causal Language Models,Added T-FCM and better FCM results,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLM) trained using the next-token-prediction objective, such as GPT3 and PaLM, have revolutionized natural language processing in recent years by showing impressive zero-shot and few-shot capabilities across a wide range of tasks. In this work, we propose a simple technique that significantly boosts the performance of LLMs without adding computational cost. Our key observation is that, by performing the next token prediction task with randomly selected past tokens masked out, we can improve the quality of the learned representations for downstream language understanding tasks. We hypothesize that randomly masking past tokens prevents over-attending to recent tokens and encourages attention to tokens in the distant past. We find that our method, Forgetful Causal Masking (FCM), significantly improves both few-shot and finetuning performance of PaLM. We further consider a simple extension, T-FCM, which introduces bidirectional context to causal language model without altering the sequence order, and further improves finetuning performance. ","[{'version': 'v1', 'created': 'Mon, 24 Oct 2022 17:46:57 GMT'}, {'version': 'v2', 'created': 'Tue, 31 Jan 2023 08:08:18 GMT'}]",2023-02-01,"[['Liu', 'Hao', ''], ['Geng', 'Xinyang', ''], ['Lee', 'Lisa', ''], ['Mordatch', 'Igor', ''], ['Levine', 'Sergey', ''], ['Narang', 'Sharan', ''], ['Abbeel', 'Pieter', '']]",0,1,2022-10-24,2,7,1,2,0,2,075f83cac2742dbb36ee49d30f7aee2a322f3127,256416540.0,https://www.semanticscholar.org/paper/075f83cac2742dbb36ee49d30f7aee2a322f3127,,2022.0,62.0,2.0,2.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2143856672', 'name': 'Hao Liu'}, {'authorId': '3468192', 'name': 'Xinyang Geng'}, {'authorId': '87068304', 'name': 'Lisa Lee'}, {'authorId': '2080746', 'name': 'Igor Mordatch'}, {'authorId': '1736651', 'name': 'S. Levine'}, {'authorId': '46617804', 'name': 'Sharan Narang'}, {'authorId': '1689992', 'name': 'P. Abbeel'}]","['Google', 'Xinyang Normal University', 'University of California, Berkeley']","['China', 'United States']",2022-10
2210.14128,Xiao Liu,"Chenguang Wang, Xiao Liu, Dawn Song",IELM: An Open Information Extraction Benchmark for Pre-Trained Language Models,"EMNLP 2022. arXiv admin note: substantial text overlap with
  arXiv:2010.11967",,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  We introduce a new open information extraction (OIE) benchmark for pre-trained language models (LM). Recent studies have demonstrated that pre-trained LMs, such as BERT and GPT, may store linguistic and relational knowledge. In particular, LMs are able to answer ``fill-in-the-blank'' questions when given a pre-defined relation category. Instead of focusing on pre-defined relations, we create an OIE benchmark aiming to fully examine the open relational information present in the pre-trained LMs. We accomplish this by turning pre-trained LMs into zero-shot OIE systems. Surprisingly, pre-trained LMs are able to obtain competitive performance on both standard OIE datasets (CaRB and Re-OIE2016) and two new large-scale factual OIE datasets (TAC KBP-OIE and Wikidata-OIE) that we establish via distant supervision. For instance, the zero-shot pre-trained LMs outperform the F1 score of the state-of-the-art supervised OIE methods on our factual OIE datasets without needing to use any training sets. Our code and datasets are available at https://github.com/cgraywang/IELM ","[{'version': 'v1', 'created': 'Tue, 25 Oct 2022 16:25:00 GMT'}]",2022-10-26,"[['Wang', 'Chenguang', ''], ['Liu', 'Xiao', ''], ['Song', 'Dawn', '']]",0,1,2022-10-25,1,3,3,0,0,0,1933a0ef47f8d2ba4a8277d702d522a06319302c,253107490.0,https://www.semanticscholar.org/paper/1933a0ef47f8d2ba4a8277d702d522a06319302c,Conference on Empirical Methods in Natural Language Processing,2022.0,70.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108755854', 'name': 'Chenguang Wang'}, {'authorId': '2111312892', 'name': 'Xiao Liu'}, {'authorId': '2112739650', 'name': 'Dawn Song'}]","['Tsinghua University', 'Washington University in St. Louis']","['China', 'United States']",2022-10
2211.06778,Qiuhao Lu,"Qiuhao Lu, Dejing Dou, Thien Huu Nguyen",Textual Data Augmentation for Patient Outcomes Prediction,BIBM 2021,,10.1109/BIBM52615.2021.9669861,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep learning models have demonstrated superior performance in various healthcare applications. However, the major limitation of these deep models is usually the lack of high-quality training data due to the private and sensitive nature of this field. In this study, we propose a novel textual data augmentation method to generate artificial clinical notes in patients' Electronic Health Records (EHRs) that can be used as additional training data for patient outcomes prediction. Essentially, we fine-tune the generative language model GPT-2 to synthesize labeled text with the original training data. More specifically, We propose a teacher-student framework where we first pre-train a teacher model on the original data, and then train a student model on the GPT-augmented data under the guidance of the teacher. We evaluate our method on the most common patient outcome, i.e., the 30-day readmission rate. The experimental results show that deep models can improve their predictive performance with the augmented data, indicating the effectiveness of the proposed architecture. ","[{'version': 'v1', 'created': 'Sun, 13 Nov 2022 01:07:23 GMT'}]",2022-11-15,"[['Lu', 'Qiuhao', ''], ['Dou', 'Dejing', ''], ['Nguyen', 'Thien Huu', '']]",0,1,2022-11-13,1,3,2,1,1,0,3fed6f440e4f2cc67312524f2444434f733811a7,245142427.0,https://www.semanticscholar.org/paper/3fed6f440e4f2cc67312524f2444434f733811a7,IEEE International Conference on Bioinformatics and Biomedicine,2021.0,24.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '35539899', 'name': 'Qiuhao Lu'}, {'authorId': '1721158', 'name': 'D. Dou'}, {'authorId': '1811211', 'name': 'Thien Huu Nguyen'}]","['University of Oregon', 'Baidu']","['China', 'United States']",2022-11
2211.08073,Linyi Yang,"Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng
  Liu, Jindong Wang, Xing Xie, Yue Zhang",GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective,Accepted to ACL-23 Findings,,,,cs.CL cs.AI cs.LG cs.PF,http://creativecommons.org/licenses/by-sa/4.0/,"  Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods. This paper presents the first attempt at creating a unified benchmark named GLUE-X for evaluating OOD robustness in NLP models, highlighting the importance of OOD robustness and providing insights on how to measure the robustness of a model and how to improve it. The benchmark includes 13 publicly available datasets for OOD testing, and evaluations are conducted on 8 classic NLP tasks over 21 popularly used PLMs, including GPT-3 and GPT-3.5. Our findings confirm the need for improved OOD accuracy in NLP tasks, as significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy. ","[{'version': 'v1', 'created': 'Tue, 15 Nov 2022 11:53:55 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Dec 2022 16:03:56 GMT'}, {'version': 'v3', 'created': 'Fri, 12 May 2023 02:14:58 GMT'}, {'version': 'v4', 'created': 'Mon, 22 May 2023 11:55:52 GMT'}]",2023-05-23,"[['Yang', 'Linyi', ''], ['Zhang', 'Shuibai', ''], ['Qin', 'Libo', ''], ['Li', 'Yafu', ''], ['Wang', 'Yidong', ''], ['Liu', 'Hanmeng', ''], ['Wang', 'Jindong', ''], ['Xie', 'Xing', ''], ['Zhang', 'Yue', '']]",0,1,2022-11-15,4,9,4,1,0,1,0e127ff323cf37cc32bdb227ca8498ea7e6ee389,253523094.0,https://www.semanticscholar.org/paper/0e127ff323cf37cc32bdb227ca8498ea7e6ee389,Annual Meeting of the Association for Computational Linguistics,2022.0,101.0,26.0,6.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2145500840', 'name': 'Linyi Yang'}, {'authorId': '2190935184', 'name': 'Shuibai Zhang'}, {'authorId': '49169076', 'name': 'Libo Qin'}, {'authorId': '2110450452', 'name': 'Yafu Li'}, {'authorId': '2108024273', 'name': 'Yidong Wang'}, {'authorId': '2118960911', 'name': 'Hanmeng Liu'}, {'authorId': '1519290245', 'name': 'Jindong Wang'}, {'authorId': '1576441343', 'name': 'Xingxu Xie'}, {'authorId': '39939186', 'name': 'Yue Zhang'}]","['Westlake University', 'Xidian University', 'Institute for Advanced Study', 'Microsoft', 'Central South University']","['China', 'United States']",2022-11
2211.08316,Changlong Yu,"Changlong Yu, Weiqi Wang, Xin Liu, Jiaxin Bai, Yangqiu Song, Zheng Li,
  Yifan Gao, Tianyu Cao, and Bing Yin",FolkScope: Intention Knowledge Graph Construction for E-commerce Commonsense Discovery,ACL Findings 2023,,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  Understanding users' intentions in e-commerce platforms requires commonsense knowledge. In this paper, we present FolkScope, an intention knowledge graph construction framework to reveal the structure of humans' minds about purchasing items. As commonsense knowledge is usually ineffable and not expressed explicitly, it is challenging to perform information extraction. Thus, we propose a new approach that leverages the generation power of large language models~(LLMs) and human-in-the-loop annotation to semi-automatically construct the knowledge graph. LLMs first generate intention assertions via e-commerce-specific prompts to explain shopping behaviors, where the intention can be an open reason or a predicate falling into one of 18 categories aligning with ConceptNet, e.g., IsA, MadeOf, UsedFor, etc. Then we annotate plausibility and typicality labels of sampled intentions as training data in order to populate human judgments to all automatic generations. Last, to structurize the assertions, we propose pattern mining and conceptualization to form more condensed and abstract knowledge. Extensive evaluations and studies demonstrate that our constructed knowledge graph can well model e-commerce knowledge and have many potential applications. ","[{'version': 'v1', 'created': 'Tue, 15 Nov 2022 17:20:40 GMT'}, {'version': 'v2', 'created': 'Thu, 11 May 2023 16:33:50 GMT'}]",2023-05-12,"[['Yu', 'Changlong', ''], ['Wang', 'Weiqi', ''], ['Liu', 'Xin', ''], ['Bai', 'Jiaxin', ''], ['Song', 'Yangqiu', ''], ['Li', 'Zheng', ''], ['Gao', 'Yifan', ''], ['Cao', 'Tianyu', ''], ['Yin', 'Bing', '']]",0,0,2022-11-15,2,9,1,0,0,0,af1ec939e26e2467fb4c51f4ca3c85f12579c5a1,258615139.0,https://www.semanticscholar.org/paper/af1ec939e26e2467fb4c51f4ca3c85f12579c5a1,Annual Meeting of the Association for Computational Linguistics,2022.0,78.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '49778308', 'name': 'Changlong Yu'}, {'authorId': '1587728690', 'name': 'Weiqi Wang'}, {'authorId': '89121677', 'name': 'Xin Liu'}, {'authorId': '145677395', 'name': 'Jiaxin Bai'}, {'authorId': '1809614', 'name': 'Yangqiu Song'}, {'authorId': '2146249169', 'name': 'Zheng Li'}, {'authorId': '1921742', 'name': 'Yifan Gao'}, {'authorId': '2151070', 'name': 'Tianyu Cao'}, {'authorId': '2021632793', 'name': 'Bing Yin'}]","['Amazon', 'Hong Kong University of Science and Technology']","['China', 'United States']",2022-11
2211.09783,Yulong Chen,"Yulong Chen, Yang Liu, Ruochen Xu, Ziyi Yang, Chenguang Zhu, Michael
  Zeng, Yue Zhang",UniSumm and SummZoo: Unified Model and Diverse Benchmark for Few-Shot Summarization,ACL2023 main conference,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The high annotation costs and diverse demands of various summarization tasks motivate the development of few-shot summarization. However, despite the emergence of many summarization tasks and datasets, the current training paradigm for few-shot summarization systems ignores potentially shareable knowledge in heterogeneous datasets. To this end, we propose \textsc{UniSumm}, a unified few-shot summarization model pre-trained with multiple summarization tasks and can be prefix-tuned to excel at any few-shot summarization task. Meanwhile, to better evaluate few-shot summarizers, under the principles of diversity and robustness, we assemble and release a new benchmark \textsc{SummZoo}. It consists of $8$ summarization tasks with multiple sets of few-shot samples for each task, covering diverse domains. Experimental results and analysis show that \textsc{UniSumm} outperforms strong baselines by a large margin across all sub-tasks in \textsc{SummZoo} under both automatic and human evaluations and achieves comparable results in human evaluation compared with a GPT-3.5 model. ","[{'version': 'v1', 'created': 'Thu, 17 Nov 2022 18:54:47 GMT'}, {'version': 'v2', 'created': 'Mon, 21 Nov 2022 15:16:40 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Dec 2022 08:54:22 GMT'}, {'version': 'v4', 'created': 'Tue, 13 Dec 2022 14:57:14 GMT'}, {'version': 'v5', 'created': 'Mon, 19 Dec 2022 05:15:58 GMT'}, {'version': 'v6', 'created': 'Sat, 27 May 2023 19:28:00 GMT'}]",2023-05-30,"[['Chen', 'Yulong', ''], ['Liu', 'Yang', ''], ['Xu', 'Ruochen', ''], ['Yang', 'Ziyi', ''], ['Zhu', 'Chenguang', ''], ['Zeng', 'Michael', ''], ['Zhang', 'Yue', '']]",0,1,2022-11-17,6,7,1,1,0,1,732fbf857fb46cbafb19eba691cded67be43143d,258958993.0,https://www.semanticscholar.org/paper/732fbf857fb46cbafb19eba691cded67be43143d,Annual Meeting of the Association for Computational Linguistics,2022.0,72.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2109404730', 'name': 'Yulong Chen'}, {'authorId': '2152797401', 'name': 'Yang Liu'}, {'authorId': '8233965', 'name': 'Ruochen Xu'}, {'authorId': '2155459391', 'name': 'Ziyi Yang'}, {'authorId': '8652308', 'name': 'Chenguang Zhu'}, {'authorId': '48262024', 'name': 'Michael Zeng'}, {'authorId': '2145913600', 'name': 'Yue Zhang'}]","['Westlake University', 'Institute for Advanced Study', 'Microsoft', 'Zhejiang University']","['China', 'India', 'United States']",2022-11
2211.10438,Guangxuan Xiao,"Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, Song
  Han",SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models,ICML 2023. First two authors contributed equally to this work,,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, and LLaMA family. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs. Code is available at https://github.com/mit-han-lab/smoothquant. ","[{'version': 'v1', 'created': 'Fri, 18 Nov 2022 18:59:33 GMT'}, {'version': 'v2', 'created': 'Mon, 28 Nov 2022 18:59:16 GMT'}, {'version': 'v3', 'created': 'Wed, 4 Jan 2023 14:36:07 GMT'}, {'version': 'v4', 'created': 'Tue, 14 Feb 2023 21:31:03 GMT'}, {'version': 'v5', 'created': 'Mon, 5 Jun 2023 21:21:28 GMT'}]",2023-06-07,"[['Xiao', 'Guangxuan', ''], ['Lin', 'Ji', ''], ['Seznec', 'Mickael', ''], ['Wu', 'Hao', ''], ['Demouth', 'Julien', ''], ['Han', 'Song', '']]",0,0,2022-11-18,5,6,3,5,4,1,2c994fadbb84fb960d8306ee138dbeef41a5b323,253708271.0,https://www.semanticscholar.org/paper/2c994fadbb84fb960d8306ee138dbeef41a5b323,International Conference on Machine Learning,2022.0,45.0,116.0,28.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2046958974', 'name': 'Guangxuan Xiao'}, {'authorId': '46698300', 'name': 'Ji Lin'}, {'authorId': '66890356', 'name': 'Mickael Seznec'}, {'authorId': '32604218', 'name': 'Julien Demouth'}, {'authorId': '143840275', 'name': 'Song Han'}]","['Jilin University', 'Massachusetts Institute of Technology']","['China', 'United States']",2022-11
2212.01539,Xuechen Li,"Jiyan He, Xuechen Li, Da Yu, Huishuai Zhang, Janardhan Kulkarni, Yin
  Tat Lee, Arturs Backurs, Nenghai Yu, Jiang Bian",Exploring the Limits of Differentially Private Deep Learning with Group-wise Clipping,25 pages,,,,cs.LG stat.ML,http://creativecommons.org/licenses/by/4.0/,"  Differentially private deep learning has recently witnessed advances in computational efficiency and privacy-utility trade-off. We explore whether further improvements along the two axes are possible and provide affirmative answers leveraging two instantiations of \emph{group-wise clipping}. To reduce the compute time overhead of private learning, we show that \emph{per-layer clipping}, where the gradient of each neural network layer is clipped separately, allows clipping to be performed in conjunction with backpropagation in differentially private optimization. This results in private learning that is as memory-efficient and almost as fast per training update as non-private learning for many workflows of interest. While per-layer clipping with constant thresholds tends to underperform standard flat clipping, per-layer clipping with adaptive thresholds matches or outperforms flat clipping under given training epoch constraints, hence attaining similar or better task performance within less wall time. To explore the limits of scaling (pretrained) models in differentially private deep learning, we privately fine-tune the 175 billion-parameter GPT-3. We bypass scaling challenges associated with clipping gradients that are distributed across multiple devices with \emph{per-device clipping} that clips the gradient of each model piece separately on its host device. Privately fine-tuning GPT-3 with per-device clipping achieves a task performance at $\epsilon=1$ better than what is attainable by non-privately fine-tuning the largest GPT-2 on a summarization task. ","[{'version': 'v1', 'created': 'Sat, 3 Dec 2022 05:20:15 GMT'}]",2022-12-06,"[['He', 'Jiyan', ''], ['Li', 'Xuechen', ''], ['Yu', 'Da', ''], ['Zhang', 'Huishuai', ''], ['Kulkarni', 'Janardhan', ''], ['Lee', 'Yin Tat', ''], ['Backurs', 'Arturs', ''], ['Yu', 'Nenghai', ''], ['Bian', 'Jiang', '']]",0,1,2022-12-03,1,9,2,2,1,1,0ba0091c60c0346493b9ffb46ac682eee5453a53,254247299.0,https://www.semanticscholar.org/paper/0ba0091c60c0346493b9ffb46ac682eee5453a53,International Conference on Learning Representations,2022.0,70.0,16.0,3.0,True,"['Computer Science', 'Mathematics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Mathematics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2167236091', 'name': 'Jiyan He'}, {'authorId': '2145429039', 'name': 'Xuechen Li'}, {'authorId': '2111467167', 'name': 'Da Yu'}, {'authorId': '2973831', 'name': 'Huishuai Zhang'}, {'authorId': '2876647', 'name': 'Janardhan Kulkarni'}, {'authorId': '2109308930', 'name': 'Y. Lee'}, {'authorId': '2064251', 'name': 'A. Backurs'}, {'authorId': '1708598', 'name': 'Nenghai Yu'}, {'authorId': '143901037', 'name': 'J. Bian'}]","['Stanford University', 'Sun Yat-sen University', 'University of Science and Technology of China', 'Microsoft']","['China', 'United States', 'India']",2022-12
2212.03827,Collin Burns,"Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt",Discovering Latent Knowledge in Language Models Without Supervision,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels. ","[{'version': 'v1', 'created': 'Wed, 7 Dec 2022 18:17:56 GMT'}]",2022-12-08,"[['Burns', 'Collin', ''], ['Ye', 'Haotian', ''], ['Klein', 'Dan', ''], ['Steinhardt', 'Jacob', '']]",0,0,2022-12-07,1,4,3,0,0,0,89c3bd70ad33c4f8832f00ab98872b77861ee0ec,254366253.0,https://www.semanticscholar.org/paper/89c3bd70ad33c4f8832f00ab98872b77861ee0ec,International Conference on Learning Representations,2022.0,75.0,72.0,13.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '90909974', 'name': 'Collin Burns'}, {'authorId': '5935398', 'name': 'Hao-Tong Ye'}, {'authorId': '38666915', 'name': 'D. Klein'}, {'authorId': '5164568', 'name': 'J. Steinhardt'}]","['Peking University', 'University of California, Berkeley']","['China', 'United States']",2022-12
2212.08966,Shaopeng Wei,"Shaopeng Wei, Yu Zhao, Xingyan Chen, Qing Li, Fuzhen Zhuang, Ji Liu,
  Gang Kou",Graph Learning and Its Applications: A Holistic Survey,"20 pages, 8 figures, 3 tables",,,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Graph learning is a prevalent domain that endeavors to learn the intricate relationships among nodes and the topological structure of graphs. Over the years, graph learning has transcended from graph theory to graph data mining. With the advent of representation learning, it has attained remarkable performance in diverse scenarios. Owing to its extensive application prospects, graph learning attracts copious attention. While some researchers have accomplished impressive surveys on graph learning, they failed to connect related objectives, methods, and applications in a more coherent way. As a result, they did not encompass current ample scenarios and challenging problems due to the rapid expansion of graph learning. Particularly, large language models have recently had a disruptive effect on human life, but they also show relative weakness in structured scenarios. The question of how to make these models more powerful with graph learning remains open. Different from previous surveys on graph learning, we provide a holistic review that analyzes current works from the perspective of graph structure, and discusses the latest applications, trends, and challenges in graph learning. Specifically, we commence by proposing a taxonomy and then summarize the methods employed in graph learning. We then provide a detailed elucidation of mainstream applications. Finally, we propose future directions. ","[{'version': 'v1', 'created': 'Sat, 17 Dec 2022 22:05:07 GMT'}, {'version': 'v2', 'created': 'Sat, 11 Mar 2023 17:00:20 GMT'}, {'version': 'v3', 'created': 'Sat, 3 Jun 2023 18:36:37 GMT'}]",2023-06-06,"[['Wei', 'Shaopeng', ''], ['Zhao', 'Yu', ''], ['Chen', 'Xingyan', ''], ['Li', 'Qing', ''], ['Zhuang', 'Fuzhen', ''], ['Liu', 'Ji', ''], ['Kou', 'Gang', '']]",0,0,2022-12-17,3,7,1,0,0,0,5c743bea9cdd2bf0a3efb603c67309ee27907aa0,257496081.0,https://www.semanticscholar.org/paper/5c743bea9cdd2bf0a3efb603c67309ee27907aa0,,2022.0,248.0,1.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1409866591', 'name': 'Shaopeng Wei'}, {'authorId': '97522134', 'name': 'Yu Zhao'}, {'authorId': '2143791763', 'name': 'Xingyan Chen'}, {'authorId': '2117895423', 'name': 'Qing Li'}, {'authorId': '2162961864', 'name': 'Fuzhen Zhuang'}, {'authorId': '2155375528', 'name': 'Ji Liu'}, {'authorId': '2147326459', 'name': 'Gang Kou'}]","['Xidian University', 'Beihang University', 'are with Fintech Innovation Center, Financial Intelligence and Financial Engineering Key Laboratory of Sichuan Province,', 'Southwestern University of Finance and Economics', 'Meta', 'Zhongguancun Laboratory, Beijing, China. J.']","['China', 'United States']",2022-12
2212.09278,Yingwen Fu,"Yingwen Fu, Wenjie Ou, Zhou Yu, and Yue Lin",MIGA: A Unified Multi-task Generation Framework for Conversational Text-to-SQL,Accepted by AAAI23,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Conversational text-to-SQL is designed to translate multi-turn natural language questions into their corresponding SQL queries. Most state-of-the-art conversational text- to-SQL methods are incompatible with generative pre-trained language models (PLMs), such as T5. In this paper, we present a two-stage unified MultI-task Generation frAmework (MIGA) that leverages PLMs' ability to tackle conversational text-to-SQL. In the pre-training stage, MIGA first decomposes the main task into several related sub-tasks and then unifies them into the same sequence-to-sequence (Seq2Seq) paradigm with task-specific natural language prompts to boost the main task from multi-task training. Later in the fine-tuning stage, we propose four SQL perturbations to alleviate the error propagation problem. MIGA tends to achieve state-of-the-art performance on two benchmarks (SparC and CoSQL). We also provide extensive analyses and discussions to shed light on some new perspectives for conversational text-to-SQL. ","[{'version': 'v1', 'created': 'Mon, 19 Dec 2022 07:14:32 GMT'}]",2022-12-20,"[['Fu', 'Yingwen', ''], ['Ou', 'Wenjie', ''], ['Yu', 'Zhou', ''], ['Lin', 'Yue', '']]",0,1,2022-12-19,1,4,2,1,1,0,642e04ece55c5b532ff7e5408d8723c7d9c835db,254853639.0,https://www.semanticscholar.org/paper/642e04ece55c5b532ff7e5408d8723c7d9c835db,AAAI Conference on Artificial Intelligence,2022.0,38.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2110659921', 'name': 'Yingwen Fu'}, {'authorId': '46223131', 'name': 'Wenjie Ou'}, {'authorId': '144007938', 'name': 'Zhou Yu'}, {'authorId': '2143785210', 'name': 'Yue Lin'}]","['Guangdong University of Foreign Studies', 'NetEase', 'Columbia University']","['China', 'United States']",2022-12
2212.09535,Zheng-Xin Yong,"Zheng-Xin Yong, Hailey Schoelkopf, Niklas Muennighoff, Alham Fikri
  Aji, David Ifeoluwa Adelani, Khalid Almubarak, M Saiful Bari, Lintang
  Sutawika, Jungo Kasai, Ahmed Baruwa, Genta Indra Winata, Stella Biderman,
  Edward Raff, Dragomir Radev and Vassilina Nikoulina",BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting,ACL 2023,,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  The BLOOM model is a large publicly available multilingual language model, but its pretraining was limited to 46 languages. To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining. In this work, we apply existing language adaptation strategies to BLOOM and benchmark its zero-shot prompting performance on eight new languages in a resource-constrained setting. We find language adaptation to be effective at improving zero-shot performance in new languages. Surprisingly, we find that adapter-based finetuning is more effective than continued pretraining for large models. In addition, we discover that prompting performance is not significantly affected by language specifics, such as the writing system. It is primarily determined by the size of the language adaptation data. We also add new languages to BLOOMZ, which is a multitask finetuned version of BLOOM capable of following task instructions zero-shot. We find including a new language in the multitask fine-tuning mixture to be the most effective method to teach BLOOMZ a new language. We conclude that with sufficient training data language adaptation can generalize well to diverse languages. Our code is available at https://github.com/bigscience-workshop/multilingual-modeling. ","[{'version': 'v1', 'created': 'Mon, 19 Dec 2022 15:24:45 GMT'}, {'version': 'v2', 'created': 'Thu, 25 May 2023 10:50:40 GMT'}, {'version': 'v3', 'created': 'Sat, 27 May 2023 05:48:38 GMT'}]",2023-05-30,"[['Yong', 'Zheng-Xin', ''], ['Schoelkopf', 'Hailey', ''], ['Muennighoff', 'Niklas', ''], ['Aji', 'Alham Fikri', ''], ['Adelani', 'David Ifeoluwa', ''], ['Almubarak', 'Khalid', ''], ['Bari', 'M Saiful', ''], ['Sutawika', 'Lintang', ''], ['Kasai', 'Jungo', ''], ['Baruwa', 'Ahmed', ''], ['Winata', 'Genta Indra', ''], ['Biderman', 'Stella', ''], ['Raff', 'Edward', ''], ['Radev', 'Dragomir', ''], ['Nikoulina', 'Vassilina', '']]",0,0,2022-12-19,3,15,3,2,2,0,34c2939d3147946b2ac218e7857e1bc4c8902679,254854009.0,https://www.semanticscholar.org/paper/34c2939d3147946b2ac218e7857e1bc4c8902679,Annual Meeting of the Association for Computational Linguistics,2022.0,78.0,20.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '1725420331', 'name': 'Zheng-Xin Yong'}, {'authorId': '2184031883', 'name': 'Hailey Schoelkopf'}, {'authorId': '2037383772', 'name': 'Niklas Muennighoff'}, {'authorId': '8129718', 'name': 'Alham Fikri Aji'}, {'authorId': '2518906', 'name': 'David Ifeoluwa Adelani'}, {'authorId': '90615055', 'name': 'Khalid Almubarak'}, {'authorId': '31773000', 'name': 'M Saiful Bari'}, {'authorId': '35566806', 'name': 'Lintang Sutawika'}, {'authorId': '11348687', 'name': 'Jungo Kasai'}, {'authorId': '114850513', 'name': 'Ahmed Baruwa'}, {'authorId': '9162688', 'name': 'Genta Indra Winata'}, {'authorId': '103476203', 'name': 'Stella Rose Biderman'}, {'authorId': '9215251', 'name': 'Dragomir R. Radev'}, {'authorId': '2841761', 'name': 'Vassilina Nikoulina'}]","['University College London', 'Hong Kong University of Science and Technology', 'Yale University', 'Hugging Face', 'University of Oregon', 'NAVER', 'Nanyang Technological University', 'Brown University', 'University of Washington', 'Mohamed bin Zayed University of Artificial Intelligence']","['Singapore', 'Canada', 'United States', 'United Kingdom', 'France', 'China', 'United Arab Emirates']",2022-12
2212.09561,Yixuan Weng,"Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, Jun
  Zhao",Large Language Models are Better Reasoners with Self-Verification,,,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, with the chain of thought (CoT) prompting, large language models (LLMs), e.g., GPT-3, have shown strong reasoning ability in several natural language processing tasks such as arithmetic, commonsense, and logical reasoning. However, LLMs with CoT require multi-step prompting and multi-token prediction, which is highly sensitive to individual mistakes and vulnerable to error accumulation. The above issues make the LLMs need the ability to verify the answers. In fact, after inferring conclusions in some thinking decision tasks, people often check them by re-verifying steps to avoid some mistakes. In this paper, we propose and prove that LLMs also have similar self-verification abilities. We take the conclusion obtained by CoT as one of the conditions for solving the original problem. By taking turns masking the original conditions and predicting their results, we calculate an explainable answer verification score based on whether the re-predicted conditions are correct. Experimental results demonstrate that the proposed method can improve the reasoning performance on various arithmetic, commonsense, and logical reasoning datasets. Our code is publicly available at: https://github.com/WENGSYX/Self-Verification. ","[{'version': 'v1', 'created': 'Mon, 19 Dec 2022 15:51:52 GMT'}, {'version': 'v2', 'created': 'Wed, 29 Mar 2023 11:52:10 GMT'}, {'version': 'v3', 'created': 'Tue, 23 May 2023 16:39:09 GMT'}, {'version': 'v4', 'created': 'Wed, 24 May 2023 09:34:20 GMT'}]",2023-05-25,"[['Weng', 'Yixuan', ''], ['Zhu', 'Minjun', ''], ['Xia', 'Fei', ''], ['Li', 'Bin', ''], ['He', 'Shizhu', ''], ['Liu', 'Kang', ''], ['Zhao', 'Jun', '']]",0,1,2022-12-19,4,7,2,1,0,1,7715ba5e75f5256e1061c7473afe61bb0dbb9065,258840837.0,https://www.semanticscholar.org/paper/7715ba5e75f5256e1061c7473afe61bb0dbb9065,,2022.0,55.0,19.0,2.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2142839441', 'name': 'Yixuan Weng'}, {'authorId': '2187504043', 'name': 'Minjun Zhu'}, {'authorId': '2066079622', 'name': 'Fei Xia'}, {'authorId': '2156072001', 'name': 'Bin Li'}, {'authorId': '1954845', 'name': 'Shizhu He'}, {'authorId': '2200096', 'name': 'Kang Liu'}, {'authorId': '11447228', 'name': 'Jun Zhao'}]","['University of Chinese Academy of Sciences', 'Hunan University', 'Shanghai Artificial Intelligence Laboratory', 'Case Western Reserve University', 'Tianjin University']","['China', 'United States']",2022-12
2212.10461,Jingjing Xu,"Jingjing Xu, Qingxiu Dong, Hongyi Liu and Lei Li",Go-tuning: Improving Zero-shot Learning Abilities of Smaller Language Models,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  With increasing scale, large language models demonstrate both quantitative improvement and new qualitative capabilities, especially as zero-shot learners, like GPT-3. However, these results rely heavily on delicate prompt design and large computation. In this work, we explore whether the strong zero-shot ability could be achieved at a smaller model scale without any external supervised data. To achieve this goal, we revisit masked language modeling and present a geometry-guided self-supervised learning method (Go-tuningfor short) by taking a small number of task-aware self-supervised data to update language models further. Experiments show that Go-tuning can enable T5-small (80M) competitive zero-shot results compared with large language models, such as T5-XL (3B). We also apply Go-tuning on multi-task settings and develop a multi-task model, mgo-T5 (250M). It can reach the average performance of OPT (175B) on 9 datasets. ","[{'version': 'v1', 'created': 'Tue, 20 Dec 2022 17:36:49 GMT'}]",2022-12-21,"[['Xu', 'Jingjing', ''], ['Dong', 'Qingxiu', ''], ['Liu', 'Hongyi', ''], ['Li', 'Lei', '']]",0,1,2022-12-20,1,4,1,3,2,1,96bdc84fba47a71f2a4dbdeb58439fa16693873f,254877425.0,https://www.semanticscholar.org/paper/96bdc84fba47a71f2a4dbdeb58439fa16693873f,arXiv.org,2022.0,25.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '47883405', 'name': 'Jingjing Xu'}, {'authorId': '2047143813', 'name': 'Qingxiu Dong'}, {'authorId': '2115669628', 'name': 'Hongyi Liu'}, {'authorId': None, 'name': 'Lei Li'}]","['Peking University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'University of California, Santa Barbara']","['China', 'United States']",2022-12
2212.10559,Damai Dai,"Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, Furu
  Wei",Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers,Accepted to ACL 2023 findings,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as meta-optimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. We comprehensively compare the behaviors of in-context learning and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspectives. Inspired by the dual form between Transformer attention and gradient descent, we design a momentum-based attention by analogy with gradient descent with momentum. The improved performance over vanilla attention further supports our understanding from another perspective, and more importantly, shows the potential to utilize our understanding for future model design. The code is available at \url{https://aka.ms/icl}. ","[{'version': 'v1', 'created': 'Tue, 20 Dec 2022 18:58:48 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Dec 2022 08:36:47 GMT'}, {'version': 'v3', 'created': 'Mon, 15 May 2023 11:45:12 GMT'}]",2023-05-16,"[['Dai', 'Damai', ''], ['Sun', 'Yutao', ''], ['Dong', 'Li', ''], ['Hao', 'Yaru', ''], ['Ma', 'Shuming', ''], ['Sui', 'Zhifang', ''], ['Wei', 'Furu', '']]",0,1,2022-12-20,3,7,1,0,0,0,69c85405cc1986a41f6387d869aa1648a5668d6f,258686544.0,https://www.semanticscholar.org/paper/69c85405cc1986a41f6387d869aa1648a5668d6f,,2022.0,29.0,31.0,3.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '10780897', 'name': 'Damai Dai'}, {'authorId': '2108540694', 'name': 'Yutao Sun'}, {'authorId': '145307652', 'name': 'Li Dong'}, {'authorId': '34128716', 'name': 'Y. Hao'}, {'authorId': '2118866998', 'name': 'Shuming Ma'}, {'authorId': '3335836', 'name': 'Zhifang Sui'}, {'authorId': '49807919', 'name': 'Furu Wei'}]","['Peking University', 'Tsinghua University', 'Microsoft']","['China', 'United States']",2022-12
2301.00234,Qingxiu Dong,"Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang,
  Xu Sun, Jingjing Xu, Lei Li and Zhifang Sui",A Survey on In-context Learning,Papers collected until 2023/05/22,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few examples. It has been a new trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL. ","[{'version': 'v1', 'created': 'Sat, 31 Dec 2022 15:57:09 GMT'}, {'version': 'v2', 'created': 'Wed, 8 Feb 2023 02:59:46 GMT'}, {'version': 'v3', 'created': 'Thu, 1 Jun 2023 12:23:40 GMT'}]",2023-06-02,"[['Dong', 'Qingxiu', ''], ['Li', 'Lei', ''], ['Dai', 'Damai', ''], ['Zheng', 'Ce', ''], ['Wu', 'Zhiyong', ''], ['Chang', 'Baobao', ''], ['Sun', 'Xu', ''], ['Xu', 'Jingjing', ''], ['Li', 'Lei', ''], ['Sui', 'Zhifang', '']]",0,0,2022-12-31,3,10,2,0,0,0,30c0cdc414f68211d5d0514df027cec22e005174,255372865.0,https://www.semanticscholar.org/paper/30c0cdc414f68211d5d0514df027cec22e005174,,2022.0,128.0,39.0,1.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2047143813', 'name': 'Qingxiu Dong'}, {'authorId': '49192881', 'name': 'Lei Li'}, {'authorId': '10780897', 'name': 'Damai Dai'}, {'authorId': '2113919886', 'name': 'Ce Zheng'}, {'authorId': '150358371', 'name': 'Zhiyong Wu'}, {'authorId': '7267809', 'name': 'Baobao Chang'}, {'authorId': '2116530295', 'name': 'Xu Sun'}, {'authorId': '47883405', 'name': 'Jingjing Xu'}, {'authorId': '3335836', 'name': 'Zhifang Sui'}]","['Shanghai Artificial Intelligence Laboratory', 'Peking University', 'University of California, Santa Barbara']","['China', 'United States']",2022-12
2301.00303,Hangfeng He,"Hangfeng He, Hongming Zhang, Dan Roth",Rethinking with Retrieval: Faithful Large Language Model Inference,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs. ","[{'version': 'v1', 'created': 'Sat, 31 Dec 2022 22:35:34 GMT'}]",2023-01-03,"[['He', 'Hangfeng', ''], ['Zhang', 'Hongming', ''], ['Roth', 'Dan', '']]",0,1,2022-12-31,1,3,2,1,0,1,490d8006851b1562cfd9ec1f057471f2868289d1,255372320.0,https://www.semanticscholar.org/paper/490d8006851b1562cfd9ec1f057471f2868289d1,arXiv.org,2022.0,58.0,59.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '7146703', 'name': 'Hangfeng He'}, {'authorId': '2111112132', 'name': 'Hongming Zhang'}, {'authorId': '144590225', 'name': 'D. Roth'}]","['University of Rochester', 'University of Pennsylvania', 'Tencent']","['China', 'United States']",2022-12
2301.08721,Zhoujun Cheng,"Zhoujun Cheng, Jungo Kasai, Tao Yu",Batch Prompting: Efficient Inference with Large Language Model APIs,"18 pages, 9 figures",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Performing inference on hundreds of thousands of samples with large language models (LLMs) can be computationally and financially costly. We propose batch prompting, a simple alternative prompting approach that enables the LLM to run inference in batches, instead of one sample at a time. Our method reduces both token and time costs while retaining downstream performance. We theoretically demonstrate that under a few-shot in-context learning setting, the inference costs decrease almost inverse linearly with the number of samples in each batch. We extensively validate the effectiveness of batch prompting on ten datasets across commonsense QA, arithmetic reasoning, and NLI/NLU: batch prompting significantly~(up to $5\times$ with six samples in batch) reduces the LLM (Codex) inference token and time costs while achieving better or comparable performance. Our analysis shows that the number of samples in each batch and the complexity of tasks affect its performance. Further, batch prompting can be applied across different LLMs and reasoning methods. ","[{'version': 'v1', 'created': 'Thu, 19 Jan 2023 02:29:23 GMT'}]",2023-01-23,"[['Cheng', 'Zhoujun', ''], ['Kasai', 'Jungo', ''], ['Yu', 'Tao', '']]",0,0,2023-01-19,1,3,2,1,0,1,27f0ce04403158b61328716ae4aaab5840c0d123,256080546.0,https://www.semanticscholar.org/paper/27f0ce04403158b61328716ae4aaab5840c0d123,arXiv.org,2023.0,80.0,13.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1471878967', 'name': 'Zhoujun Cheng'}, {'authorId': '11348687', 'name': 'Jungo Kasai'}, {'authorId': '2117900202', 'name': 'Tao Yu'}]","['University of Washington', 'Shanghai Jiao Tong University', 'University of Hong Kong']","['China', 'United States', 'Hong Kong']",2023-01
2301.08984,Youshan Miao,"Zhiqi Lin, Youshan Miao, Guodong Liu, Xiaoxiang Shi, Quanlu Zhang, Fan
  Yang, Saeed Maleki, Yi Zhu, Xu Cao, Cheng Li, Mao Yang, Lintao Zhang, Lidong
  Zhou",SuperScaler: Supporting Flexible DNN Parallelization via a Unified Abstraction,,,,,cs.DC cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the growing model size, deep neural networks (DNN) are increasingly trained over massive GPU accelerators, which demands a proper parallelization plan that transforms a DNN model into fine-grained tasks and then schedules them to GPUs for execution. Due to the large search space, the contemporary parallelization plan generators often rely on empirical rules that couple transformation and scheduling, and fall short in exploring more flexible schedules that yield better memory usage and compute efficiency. This tension can be exacerbated by the emerging models with increasing complexity in their structure and model size. SuperScaler is a system that facilitates the design and generation of highly flexible parallelization plans. It formulates the plan design and generation into three sequential phases explicitly: model transformation, space-time scheduling, and data dependency preserving. Such a principled approach decouples multiple seemingly intertwined factors and enables the composition of highly flexible parallelization plans. As a result, SuperScaler can not only generate empirical parallelization plans, but also construct new plans that achieve up to 3.5X speedup compared to state-of-the-art solutions like DeepSpeed, Megatron and Alpa, for emerging DNN models like Swin-Transformer and AlphaFold2, as well as well-optimized models like GPT-3. ","[{'version': 'v1', 'created': 'Sat, 21 Jan 2023 17:47:55 GMT'}]",2023-01-24,"[['Lin', 'Zhiqi', ''], ['Miao', 'Youshan', ''], ['Liu', 'Guodong', ''], ['Shi', 'Xiaoxiang', ''], ['Zhang', 'Quanlu', ''], ['Yang', 'Fan', ''], ['Maleki', 'Saeed', ''], ['Zhu', 'Yi', ''], ['Cao', 'Xu', ''], ['Li', 'Cheng', ''], ['Yang', 'Mao', ''], ['Zhang', 'Lintao', ''], ['Zhou', 'Lidong', '']]",0,1,2023-01-21,1,13,2,1,0,1,f6ba3fab410cf2182fde171beeeec57bffa3e90b,256105230.0,https://www.semanticscholar.org/paper/f6ba3fab410cf2182fde171beeeec57bffa3e90b,arXiv.org,2023.0,60.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108651040', 'name': 'Zhiqi Lin'}, {'authorId': '11009920', 'name': 'Youshan Miao'}, {'authorId': '2158131767', 'name': 'Guodong Liu'}, {'authorId': '144120884', 'name': 'Xiaoxiang Shi'}, {'authorId': '3123382', 'name': 'Quanlu Zhang'}, {'authorId': '145338263', 'name': 'Fan Yang'}, {'authorId': '144834015', 'name': 'Saeed Maleki'}, {'authorId': '2117913672', 'name': 'Yi Zhu'}, {'authorId': '2125517683', 'name': 'Xu Cao'}, {'authorId': '15060444', 'name': 'Cheng-Wu Li'}, {'authorId': '2168609907', 'name': 'Mao Yang'}, {'authorId': '1978393184', 'name': 'Lintao Zhang'}, {'authorId': '2143359114', 'name': 'Lidong Zhou'}]","['Shanghai Jiao Tong University', 'University of Science and Technology of China', 'Microsoft']","['China', 'United States']",2023-01
2302.06476,Chengwei Qin,"Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro
  Yasunaga, Diyi Yang",Is ChatGPT a General-Purpose Natural Language Processing Task Solver?,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-sa/4.0/,"  Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies. ","[{'version': 'v1', 'created': 'Wed, 8 Feb 2023 09:44:51 GMT'}, {'version': 'v2', 'created': 'Wed, 15 Feb 2023 17:46:20 GMT'}]",2023-02-16,"[['Qin', 'Chengwei', ''], ['Zhang', 'Aston', ''], ['Zhang', 'Zhuosheng', ''], ['Chen', 'Jiaao', ''], ['Yasunaga', 'Michihiro', ''], ['Yang', 'Diyi', '']]",1,1,2023-02-08,2,6,2,1,0,1,873a581320d928249609d3c07229d5af182a379c,256827430.0,https://www.semanticscholar.org/paper/873a581320d928249609d3c07229d5af182a379c,arXiv.org,2023.0,88.0,265.0,18.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2084609980', 'name': 'Chengwei Qin'}, {'authorId': '2085709', 'name': 'Aston Zhang'}, {'authorId': '3322871', 'name': 'Zhuosheng Zhang'}, {'authorId': '47739850', 'name': 'Jiaao Chen'}, {'authorId': '19168196', 'name': 'Michihiro Yasunaga'}, {'authorId': '2143919864', 'name': 'Diyi Yang'}]","['Stanford University', 'Shanghai Jiao Tong University', 'Nanyang Technological University']","['China', 'United States', 'Singapore']",2023-02
2302.08722,Yizhe Zhang,Yizhe Zhang and Danny Z. Chen,GPT4MIA: Utilizing Generative Pre-trained Transformer (GPT-3) as A Plug-and-Play Transductive Model for Medical Image Analysis,"Version 3: Added appendix with more results and visualizations.
  Questions and suggestions are welcome",,,,cs.CV cs.AI cs.LG cs.MM,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we propose a novel approach (called GPT4MIA) that utilizes Generative Pre-trained Transformer (GPT) as a plug-and-play transductive inference tool for medical image analysis (MIA). We provide theoretical analysis on why a large pre-trained language model such as GPT-3 can be used as a plug-and-play transductive inference model for MIA. At the methodological level, we develop several technical treatments to improve the efficiency and effectiveness of GPT4MIA, including better prompt structure design, sample selection, and prompt ordering of representative samples/features. We present two concrete use cases (with workflow) of GPT4MIA: (1) detecting prediction errors and (2) improving prediction accuracy, working in conjecture with well-established vision-based models for image classification (e.g., ResNet). Experiments validate that our proposed method is effective for these two tasks. We further discuss the opportunities and challenges in utilizing Transformer-based large language models for broader MIA applications. ","[{'version': 'v1', 'created': 'Fri, 17 Feb 2023 06:33:06 GMT'}, {'version': 'v2', 'created': 'Sat, 4 Mar 2023 06:45:27 GMT'}, {'version': 'v3', 'created': 'Tue, 21 Mar 2023 12:59:20 GMT'}]",2023-03-22,"[['Zhang', 'Yizhe', ''], ['Chen', 'Danny Z.', '']]",0,1,2023-02-17,3,2,4,1,0,1,9021c16bca0e9e8b98607e352722b0ff6561e8fd,257364761.0,https://www.semanticscholar.org/paper/9021c16bca0e9e8b98607e352722b0ff6561e8fd,,2023.0,8.0,0.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2000095290', 'name': 'Yizhe Zhang'}, {'authorId': '2132585133', 'name': 'Da Chen'}]","['Nanjing University of Science and Technology', 'University of Notre Dame']","['China', 'United States']",2023-02
2302.09185,Albert Lu,"Albert Lu, Hongxin Zhang, Yanzhe Zhang, Xuezhi Wang, Diyi Yang",Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints,"27 pages, 13 figures, 11 tables, to be published in EACL 2023
  Findings",,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  The limits of open-ended generative models are unclear, yet increasingly important. What causes them to succeed and what causes them to fail? In this paper, we take a prompt-centric approach to analyzing and bounding the abilities of open-ended generative models. We present a generic methodology of analysis with two challenging prompt constraint types: structural and stylistic. These constraint types are categorized into a set of well-defined constraints that are analyzable by a single prompt. We then systematically create a diverse set of simple, natural, and useful prompts to robustly analyze each individual constraint. Using the GPT-3 text-davinci-002 model as a case study, we generate outputs from our collection of prompts and analyze the model's generative failures. We also show the generalizability of our proposed method on other large models like BLOOM and OPT. Our results and our in-context mitigation strategies reveal open challenges for future research. We have publicly released our code at https://github.com/SALT-NLP/Bound-Cap-LLM. ","[{'version': 'v1', 'created': 'Fri, 17 Feb 2023 23:30:28 GMT'}]",2023-02-21,"[['Lu', 'Albert', ''], ['Zhang', 'Hongxin', ''], ['Zhang', 'Yanzhe', ''], ['Wang', 'Xuezhi', ''], ['Yang', 'Diyi', '']]",0,1,2023-02-17,1,5,3,3,2,1,3f83582c08a62e5bd02398fafc93f7eaf1e4b84e,257039031.0,https://www.semanticscholar.org/paper/3f83582c08a62e5bd02398fafc93f7eaf1e4b84e,Findings,2023.0,36.0,8.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2208657585', 'name': 'Albert Lu'}, {'authorId': '2118083343', 'name': 'Hongxin Zhang'}, {'authorId': '2121290295', 'name': 'Yanzhe Zhang'}, {'authorId': '1524732527', 'name': 'Xuezhi Wang'}, {'authorId': '2143919864', 'name': 'Diyi Yang'}]","['Stanford University', 'Google', 'Georgia Institute of Technology', 'Shanghai Jiao Tong University']","['China', 'United States']",2023-02
2302.09419,Ce Zhou,"Ce Zhou (1), Qian Li (2), Chen Li (2), Jun Yu (3), Yixin Liu (3),
  Guangjing Wang (1), Kai Zhang (3), Cheng Ji (2), Qiben Yan (1), Lifang He
  (3), Hao Peng (2), Jianxin Li (2), Jia Wu (4), Ziwei Liu (5), Pengtao Xie
  (6), Caiming Xiong (7), Jian Pei (8), Philip S. Yu (9), Lichao Sun (3) ((1)
  Michigan State University, (2) Beihang University, (3) Lehigh University, (4)
  Macquarie University, (5) Nanyang Technological University, (6) University of
  California San Diego, (7) Salesforce AI Research, (8) Duke University, (9)
  University of Illinois at Chicago)",A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT,"99 pages, 16 figures",,,,cs.AI cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of recent research advancements, challenges, and opportunities for PFMs in text, image, graph, as well as other data modalities. The review covers the basic components and existing pretraining methods used in natural language processing, computer vision, and graph learning. Additionally, it explores advanced PFMs used for different data modalities and unified PFMs that consider data quality and quantity. The review also discusses research related to the fundamentals of PFMs, such as model efficiency and compression, security, and privacy. Finally, the study provides key implications, future research directions, challenges, and open problems in the field of PFMs. Overall, this survey aims to shed light on the research of the PFMs on scalability, security, logical reasoning ability, cross-domain learning ability, and the user-friendly interactive ability for artificial general intelligence. ","[{'version': 'v1', 'created': 'Sat, 18 Feb 2023 20:51:09 GMT'}, {'version': 'v2', 'created': 'Thu, 30 Mar 2023 14:44:09 GMT'}, {'version': 'v3', 'created': 'Mon, 1 May 2023 07:48:05 GMT'}]",2023-05-02,"[['Zhou', 'Ce', ''], ['Li', 'Qian', ''], ['Li', 'Chen', ''], ['Yu', 'Jun', ''], ['Liu', 'Yixin', ''], ['Wang', 'Guangjing', ''], ['Zhang', 'Kai', ''], ['Ji', 'Cheng', ''], ['Yan', 'Qiben', ''], ['He', 'Lifang', ''], ['Peng', 'Hao', ''], ['Li', 'Jianxin', ''], ['Wu', 'Jia', ''], ['Liu', 'Ziwei', ''], ['Xie', 'Pengtao', ''], ['Xiong', 'Caiming', ''], ['Pei', 'Jian', ''], ['Yu', 'Philip S.', ''], ['Sun', 'Lichao', '']]",1,1,2023-02-18,3,19,3,2,0,2,3599a236f285af48782fc30b1341d13ec7320735,257039063.0,https://www.semanticscholar.org/paper/3599a236f285af48782fc30b1341d13ec7320735,arXiv.org,2023.0,0.0,161.0,8.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2187669795', 'name': 'Ce Zhou'}, {'authorId': '2117126771', 'name': 'Qian Li'}, {'authorId': '2116521405', 'name': 'Chen Li'}, {'authorId': None, 'name': 'Jun Yu'}, {'authorId': None, 'name': 'Yixin Liu'}, {'authorId': '2152582885', 'name': 'Guan Wang'}, {'authorId': '2158520914', 'name': 'Kaichao Zhang'}, {'authorId': '2052296239', 'name': 'Cheng Ji'}, {'authorId': '2072779826', 'name': 'Qi Yan'}, {'authorId': '40901820', 'name': 'Lifang He'}, {'authorId': '49349645', 'name': 'Hao Peng'}, {'authorId': '47785906', 'name': 'Jianxin Li'}, {'authorId': '2144139161', 'name': 'Jia Wu'}, {'authorId': '2145254462', 'name': 'Ziwei Liu'}, {'authorId': '40526720', 'name': 'P. Xie'}, {'authorId': '2054594326', 'name': 'Caiming Xiong'}, {'authorId': '2143960747', 'name': 'Jian Pei'}, {'authorId': '152297693', 'name': 'Philip S. Yu'}, {'authorId': '2208907959', 'name': 'Lichao Sun Michigan State University'}, {'authorId': '88729424', 'name': 'B. University'}, {'authorId': '103217915', 'name': 'Lehigh University'}, {'authorId': '2095706829', 'name': 'M. University'}, {'authorId': '88740224', 'name': 'Nanyang Technological University'}, {'authorId': '102788302', 'name': 'University of California at San Diego'}, {'authorId': '102879328', 'name': 'D. University'}, {'authorId': '6106131', 'name': 'U. Chicago'}, {'authorId': '2208653278', 'name': 'Salesforce AI Research'}]","['Michigan State University', 'University of California, Riverside', 'Macquarie University', 'Lehigh University', 'Salesforce AI Research,', 'Duke University', 'Beihang University', 'University of Illinois at Chicago', 'Nanyang Technological University']","['China', 'United States', 'Singapore', 'Australia']",2023-02
2302.09582,Dan Zhang,"Ming Li, Yusheng Su, Hsiu-Yuan Huang, Jiali Cheng, Xin Hu, Xinmiao
  Zhang, Huadong Wang, Yujia Qin, Xiaozhi Wang, Zhiyuan Liu, Dan Zhang",Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference,"39 pages, 13 figures, 2 tables, fix formatting errors",,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Understanding how language supports emotion inference remains a topic of debate in emotion science. The present study investigated whether language-derived emotion-concept knowledge would causally support emotion inference by manipulating the language-specific knowledge representations in large language models. Using the prompt technique, 14 attributes of emotion concepts were found to be represented by distinct artificial neuron populations. By manipulating these attribute-related neurons, the majority of the emotion inference tasks showed performance deterioration compared to random manipulations. The attribute-specific performance deterioration was related to the importance of different attributes in human mental space. Our findings provide causal evidence in support of a language-based mechanism for emotion inference and highlight the contributions of emotion-concept knowledge. ","[{'version': 'v1', 'created': 'Sun, 19 Feb 2023 14:21:33 GMT'}, {'version': 'v2', 'created': 'Tue, 21 Feb 2023 07:28:04 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Jul 2023 09:04:14 GMT'}, {'version': 'v4', 'created': 'Mon, 21 Aug 2023 09:44:19 GMT'}]",2023-08-22,"[['Li', 'Ming', ''], ['Su', 'Yusheng', ''], ['Huang', 'Hsiu-Yuan', ''], ['Cheng', 'Jiali', ''], ['Hu', 'Xin', ''], ['Zhang', 'Xinmiao', ''], ['Wang', 'Huadong', ''], ['Qin', 'Yujia', ''], ['Wang', 'Xiaozhi', ''], ['Liu', 'Zhiyuan', ''], ['Zhang', 'Dan', '']]",0,0,2023-02-19,4,11,2,0,0,0,25e48d36c34d958c89244953501638cfcb7a2839,259833437.0,https://www.semanticscholar.org/paper/25e48d36c34d958c89244953501638cfcb7a2839,,2023.0,133.0,1.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Psychology', 'source': 's2-fos-model'}]","[{'authorId': '2222963088', 'name': 'Ming Li'}, {'authorId': '48576745', 'name': 'Yusheng Su'}, {'authorId': '2209072917', 'name': 'Hsiu-Yuan Huang'}, {'authorId': '2112798028', 'name': 'Jialing Cheng'}, {'authorId': '2110048706', 'name': 'Xin Hu'}, {'authorId': '2208908966', 'name': 'Xinmiao Zhang'}, {'authorId': '2155242767', 'name': 'Huadong Wang'}, {'authorId': '50625437', 'name': 'Yujia Qin'}, {'authorId': '48631777', 'name': 'Xiaozhi Wang'}, {'authorId': '46270592', 'name': 'Zhi-Yun Liu'}, {'authorId': '2122440589', 'name': 'Dan Zhang'}]","['University of Massachusetts Lowell', 'Tsinghua University', 'University of Pittsburgh', 'University of Science and Technology Beijing']","['China', 'United States']",2023-02
2302.11978,Shengnan An,"Shengnan An, Zeqi Lin, Bei Chen, Qiang Fu, Nanning Zheng, Jian-Guang
  Lou",Does Deep Learning Learn to Abstract? A Systematic Probing Framework,ICLR 2023,,,,cs.LG cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Abstraction is a desirable capability for deep learning models, which means to induce abstract concepts from concrete instances and flexibly apply them beyond the learning context. At the same time, there is a lack of clear understanding about both the presence and further characteristics of this capability in deep learning models. In this paper, we introduce a systematic probing framework to explore the abstraction capability of deep learning models from a transferability perspective. A set of controlled experiments are conducted based on this framework, providing strong evidence that two probed pre-trained language models (PLMs), T5 and GPT2, have the abstraction capability. We also conduct in-depth analysis, thus shedding further light: (1) the whole training phase exhibits a ""memorize-then-abstract"" two-stage process; (2) the learned abstract concepts are gathered in a few middle-layer attention heads, rather than being evenly distributed throughout the model; (3) the probed abstraction capabilities exhibit robustness against concept mutations, and are more robust to low-level/source-side mutations than high-level/target-side ones; (4) generic pre-training is critical to the emergence of abstraction capability, and PLMs exhibit better abstraction with larger model sizes and data scales. ","[{'version': 'v1', 'created': 'Thu, 23 Feb 2023 12:50:02 GMT'}]",2023-02-24,"[['An', 'Shengnan', ''], ['Lin', 'Zeqi', ''], ['Chen', 'Bei', ''], ['Fu', 'Qiang', ''], ['Zheng', 'Nanning', ''], ['Lou', 'Jian-Guang', '']]",0,1,2023-02-23,1,6,2,2,2,0,2ae807688d5bae0a7331992793be066b93d7655f,257102348.0,https://www.semanticscholar.org/paper/2ae807688d5bae0a7331992793be066b93d7655f,International Conference on Learning Representations,2023.0,69.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2119217081', 'name': 'Shengnan An'}, {'authorId': '2284174', 'name': 'Zeqi Lin'}, {'authorId': '143876723', 'name': 'B. Chen'}, {'authorId': '2113771309', 'name': 'Qiang Fu'}, {'authorId': '2144620206', 'name': 'Nanning Zheng'}, {'authorId': '153249455', 'name': 'Jian-Guang Lou'}]","[""Xi'an Jiaotong University"", 'Microsoft']","['China', 'United States']",2023-02
2302.12095,Jindong Wang,"Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong
  Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, Binxin Jiao, Yue Zhang,
  Xing Xie",On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective,"Highlighted paper at ICLR 2023 workshop on Trustworthy and Reliable
  Large-Scale Machine Learning Models; code is at:
  https://github.com/microsoft/robustlearn; more works:
  https://llm-eval.github.io/",,,,cs.AI cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  ChatGPT is a recent chatbot service released by OpenAI and is receiving increasing attention over the past few months. While evaluations of various aspects of ChatGPT have been done, its robustness, i.e., the performance to unexpected inputs, is still unclear to the public. Robustness is of particular concern in responsible AI, especially for safety-critical applications. In this paper, we conduct a thorough evaluation of the robustness of ChatGPT from the adversarial and out-of-distribution (OOD) perspective. To do so, we employ the AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart review and DDXPlus medical diagnosis datasets for OOD evaluation. We select several popular foundation models as baselines. Results show that ChatGPT shows consistent advantages on most adversarial and OOD classification and translation tasks. However, the absolute performance is far from perfection, which suggests that adversarial and OOD robustness remains a significant threat to foundation models. Moreover, ChatGPT shows astounding performance in understanding dialogue-related texts and we find that it tends to provide informal suggestions for medical tasks instead of definitive answers. Finally, we present in-depth discussions of possible research directions. ","[{'version': 'v1', 'created': 'Wed, 22 Feb 2023 11:01:20 GMT'}, {'version': 'v2', 'created': 'Mon, 27 Feb 2023 02:13:38 GMT'}, {'version': 'v3', 'created': 'Thu, 2 Mar 2023 08:33:04 GMT'}, {'version': 'v4', 'created': 'Wed, 29 Mar 2023 14:21:51 GMT'}, {'version': 'v5', 'created': 'Tue, 29 Aug 2023 05:34:25 GMT'}]",2023-08-30,"[['Wang', 'Jindong', ''], ['Hu', 'Xixu', ''], ['Hou', 'Wenxin', ''], ['Chen', 'Hao', ''], ['Zheng', 'Runkai', ''], ['Wang', 'Yidong', ''], ['Yang', 'Linyi', ''], ['Huang', 'Haojun', ''], ['Ye', 'Wei', ''], ['Geng', 'Xiubo', ''], ['Jiao', 'Binxin', ''], ['Zhang', 'Yue', ''], ['Xie', 'Xing', '']]",1,1,2023-02-22,5,13,3,1,0,1,5c7353fac22a8fdc43fc2f5c006b5d6902c47e75,257102461.0,https://www.semanticscholar.org/paper/5c7353fac22a8fdc43fc2f5c006b5d6902c47e75,arXiv.org,2023.0,74.0,86.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1519290245', 'name': 'Jindong Wang'}, {'authorId': '2186046594', 'name': 'Xixu Hu'}, {'authorId': '32216189', 'name': 'Wenxin Hou'}, {'authorId': '2051536212', 'name': 'Hao Chen'}, {'authorId': '2008039497', 'name': 'Runkai Zheng'}, {'authorId': '2108024273', 'name': 'Yidong Wang'}, {'authorId': '2145500840', 'name': 'Linyi Yang'}, {'authorId': None, 'name': 'Haojun Huang'}, {'authorId': '2147205193', 'name': 'Weirong Ye'}, {'authorId': '2442662', 'name': 'Xiubo Geng'}, {'authorId': '24128606', 'name': 'Binxing Jiao'}, {'authorId': '2211964951', 'name': 'Yue Zhang'}, {'authorId': '1576441343', 'name': 'Xingxu Xie'}]","['Westlake University', 'Chinese University of Hong Kong, Shenzhen', 'City University of Hong Kong', 'Carnegie Mellon University', 'Microsoft']","['China', 'United States']",2023-02
2302.12343,Denis Jered McInerney,"Denis Jered McInerney, Geoffrey Young, Jan-Willem van de Meent, Byron
  C. Wallace",CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have yielded fast and dramatic progress in NLP, and now offer strong few- and zero-shot capabilities on new tasks, reducing the need for annotation. This is especially exciting for the medical domain, in which supervision is often scant and expensive. At the same time, model predictions are rarely so accurate that they can be trusted blindly. Clinicians therefore tend to favor ""interpretable"" classifiers over opaque LLMs. For example, risk prediction tools are often linear models defined over manually crafted predictors that must be laboriously extracted from EHRs. We propose CHiLL (Crafting High-Level Latents), which uses LLMs to permit natural language specification of high-level features for linear models via zero-shot feature extraction using expert-composed queries. This approach has the promise to empower physicians to use their domain expertise to craft features which are clinically meaningful for a downstream task of interest, without having to manually extract these from raw EHR (as often done now). We are motivated by a real-world risk prediction task, but as a reproducible proxy, we use MIMIC-III and MIMIC-CXR data and standard predictive tasks (e.g., 30-day readmission) to evaluate our approach. We find that linear models using automatically extracted features are comparably performant to models using reference features, and provide greater interpretability than linear models using ""Bag-of-Words"" features. We verify that learned feature weights align well with clinical expectations. ","[{'version': 'v1', 'created': 'Thu, 23 Feb 2023 21:23:06 GMT'}]",2023-02-27,"[['McInerney', 'Denis Jered', ''], ['Young', 'Geoffrey', ''], ['van de Meent', 'Jan-Willem', ''], ['Wallace', 'Byron C.', '']]",0,0,2023-02-23,1,4,3,0,0,0,d78eb600f987334b051d0a2ba69c72f9f01849ea,257205986.0,https://www.semanticscholar.org/paper/d78eb600f987334b051d0a2ba69c72f9f01849ea,arXiv.org,2023.0,23.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1621628526', 'name': 'Denis Jered McInerney'}, {'authorId': '2106234673', 'name': 'Geoffrey S. Young'}, {'authorId': '2086966519', 'name': 'J.-W. van de Meent'}, {'authorId': '1912476', 'name': 'Byron C. Wallace'}]","['University of Amsterdam', 'Northeastern University', ""Brigham and Women's Hospital""]","['China', 'United States', 'Netherlands']",2023-02
2302.13136,Rui Wang,"Rui Wang, Pengyu Cheng, Ricardo Henao",Toward Fairness in Text Generation via Mutual Information Minimization based on Importance Sampling,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pretrained language models (PLMs), such as GPT2, have achieved remarkable empirical performance in text generation tasks. However, pretrained on large-scale natural language corpora, the generated text from PLMs may exhibit social bias against disadvantaged demographic groups. To improve the fairness of PLMs in text generation, we propose to minimize the mutual information between the semantics in the generated text sentences and their demographic polarity, i.e., the demographic group to which the sentence is referring. In this way, the mentioning of a demographic group (e.g., male or female) is encouraged to be independent from how it is described in the generated text, thus effectively alleviating the social bias. Moreover, we propose to efficiently estimate the upper bound of the above mutual information via importance sampling, leveraging a natural language corpus. We also propose a distillation mechanism that preserves the language modeling ability of the PLMs after debiasing. Empirical results on real-world benchmarks demonstrate that the proposed method yields superior performance in term of both fairness and language modeling ability. ","[{'version': 'v1', 'created': 'Sat, 25 Feb 2023 18:29:02 GMT'}]",2023-02-28,"[['Wang', 'Rui', ''], ['Cheng', 'Pengyu', ''], ['Henao', 'Ricardo', '']]",0,1,2023-02-25,1,3,2,1,1,0,812867453701f0929579e481bd15dfb0ebee1d7e,257220117.0,https://www.semanticscholar.org/paper/812867453701f0929579e481bd15dfb0ebee1d7e,International Conference on Artificial Intelligence and Statistics,2023.0,26.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '39077217', 'name': 'Rui Wang'}, {'authorId': '144533942', 'name': 'Pengyu Cheng'}, {'authorId': '145153424', 'name': 'Ricardo Henao'}]","['Duke University', 'Tencent']","['China', 'United States']",2023-02
2302.13939,Rui-Jie Zhu,"Rui-Jie Zhu, Qihang Zhao, Guoqi Li, Jason K. Eshraghian",SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks,,,,,cs.CL cs.LG cs.NE,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language. We achieve this by modifying the transformer block to replace multi-head self attention to reduce quadratic computational complexity O(N^2) to linear complexity O(N) with increasing sequence length. Input tokens are instead streamed in sequentially to our attention mechanism (as with typical SNNs). Our preliminary experiments show that SpikeGPT remains competitive with non-spiking models on tested benchmarks, while maintaining 20x fewer operations when processed on neuromorphic hardware that can leverage sparse, event-driven activations. ","[{'version': 'v1', 'created': 'Mon, 27 Feb 2023 16:43:04 GMT'}, {'version': 'v2', 'created': 'Tue, 28 Feb 2023 06:28:43 GMT'}, {'version': 'v3', 'created': 'Mon, 26 Jun 2023 02:38:07 GMT'}, {'version': 'v4', 'created': 'Tue, 27 Jun 2023 02:55:23 GMT'}]",2023-06-28,"[['Zhu', 'Rui-Jie', ''], ['Zhao', 'Qihang', ''], ['Li', 'Guoqi', ''], ['Eshraghian', 'Jason K.', '']]",0,1,2023-02-27,4,4,3,0,0,0,9f52317ea9c5a6804b978987ff2a6557f98b5b2c,257219477.0,https://www.semanticscholar.org/paper/9f52317ea9c5a6804b978987ff2a6557f98b5b2c,arXiv.org,2023.0,61.0,23.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '144649570', 'name': 'Rui Zhu'}, {'authorId': '2110483969', 'name': 'Qihang Zhao'}, {'authorId': '3444950', 'name': 'J. Eshraghian'}]","['University of California, Santa Cruz', 'Chinese Academy of Sciences', 'Kuaishou Technology Co. Ltd']","['China', 'United States']",2023-02
2302.14229,Jiaan Wang,"Jiaan Wang, Yunlong Liang, Fandong Meng, Beiqi Zou, Zhixu Li, Jianfeng
  Qu, Jie Zhou",Zero-Shot Cross-Lingual Summarization via Large Language Models,"Technical Report, 12 pages",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Given a document in a source language, cross-lingual summarization (CLS) aims to generate a summary in a different target language. Recently, the emergence of Large Language Models (LLMs), such as GPT-3.5, ChatGPT and GPT-4, has attracted wide attention from the computational linguistics community. However, it is not yet known the performance of LLMs on CLS. In this report, we empirically use various prompts to guide LLMs to perform zero-shot CLS from different paradigms (i.e., end-to-end and pipeline), and provide a preliminary evaluation on the generated summaries. We find that ChatGPT and GPT-4 originally prefer to produce lengthy summaries with detailed information. These two LLMs can further balance informativeness and conciseness with the help of an interactive prompt, significantly improving their CLS performance. Experimental results on three widely-used CLS datasets show that GPT-4 achieves state-of-the-art zero-shot CLS performance, and performs competitively compared with the fine-tuned mBART-50. Moreover, we also find some multi-lingual and bilingual LLMs (i.e., BLOOMZ, ChatGLM-6B, Vicuna-13B and ChatYuan) have limited zero-shot CLS ability. Due to the composite nature of CLS, which requires models to perform summarization and translation simultaneously, accomplishing this task in a zero-shot manner is even a challenge for LLMs. Therefore, we sincerely hope and recommend future LLM research could use CLS as a testbed. ","[{'version': 'v1', 'created': 'Tue, 28 Feb 2023 01:27:37 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Apr 2023 09:27:37 GMT'}, {'version': 'v3', 'created': 'Mon, 3 Jul 2023 14:11:38 GMT'}]",2023-07-04,"[['Wang', 'Jiaan', ''], ['Liang', 'Yunlong', ''], ['Meng', 'Fandong', ''], ['Zou', 'Beiqi', ''], ['Li', 'Zhixu', ''], ['Qu', 'Jianfeng', ''], ['Zhou', 'Jie', '']]",1,1,2023-02-28,3,7,2,6,3,3,90e0a949da276b66d13920a185a6e35042337518,257985556.0,https://www.semanticscholar.org/paper/90e0a949da276b66d13920a185a6e35042337518,,2023.0,69.0,3.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2118328782', 'name': 'Jiaan Wang'}, {'authorId': '3389712', 'name': 'Yunlong Liang'}, {'authorId': '33427918', 'name': 'Fandong Meng'}, {'authorId': '2151794700', 'name': 'Beiqi Zou'}, {'authorId': '115419489', 'name': 'Zhixu Li'}, {'authorId': '2069479032', 'name': 'Jianfeng Qu'}, {'authorId': '48128428', 'name': 'Jie Zhou'}]","['Princeton University', 'Beijing Jiaotong University', 'Tencent', 'Soochow University', 'Fudan University']","['China', 'United States']",2023-02
2303.02868,Xiaonan Nie,"Xiaonan Nie, Yi Liu, Fangcheng Fu, Jinbao Xue, Dian Jiao, Xupeng Miao,
  Yangyu Tao, Bin Cui",Angel-PTM: A Scalable and Economical Large-scale Pre-training System in Tencent,,,,,cs.LG cs.DC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent years have witnessed the unprecedented achievements of large-scale pre-trained models, especially the Transformer models. Many products and services in Tencent Inc., such as WeChat, QQ, and Tencent Advertisement, have been opted in to gain the power of pre-trained models. In this work, we present Angel-PTM, a productive deep learning system designed for pre-training and fine-tuning Transformer models. Angel-PTM can train extremely large-scale models with hierarchical memory efficiently. The key designs of Angel-PTM are the fine-grained memory management via the Page abstraction and a unified scheduling method that coordinate the computations, data movements, and communications. Furthermore, Angel-PTM supports extreme model scaling with SSD storage and implements the lock-free updating mechanism to address the SSD I/O bandwidth bottlenecks. Experimental results demonstrate that Angel-PTM outperforms existing systems by up to 114.8% in terms of maximum model scale as well as up to 88.9% in terms of training throughput. Additionally, experiments on GPT3-175B and T5-MoE-1.2T models utilizing hundreds of GPUs verify the strong scalability of Angel-PTM. ","[{'version': 'v1', 'created': 'Mon, 6 Mar 2023 03:36:26 GMT'}]",2023-03-07,"[['Nie', 'Xiaonan', ''], ['Liu', 'Yi', ''], ['Fu', 'Fangcheng', ''], ['Xue', 'Jinbao', ''], ['Jiao', 'Dian', ''], ['Miao', 'Xupeng', ''], ['Tao', 'Yangyu', ''], ['Cui', 'Bin', '']]",0,1,2023-03-06,1,8,2,2,1,1,d7e00702bbb5a0cccc97033f0405b634ae9e2d3c,257365347.0,https://www.semanticscholar.org/paper/d7e00702bbb5a0cccc97033f0405b634ae9e2d3c,Proceedings of the VLDB Endowment,2023.0,69.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2113588952', 'name': 'Xiaonan Nie'}, {'authorId': '2231085434', 'name': 'Yi Liu'}, {'authorId': '46182701', 'name': 'Fangcheng Fu'}, {'authorId': '2067731583', 'name': 'J. Xue'}, {'authorId': '2210796850', 'name': 'Dian Jiao'}, {'authorId': '1720763480', 'name': 'Xupeng Miao'}, {'authorId': '2105710451', 'name': 'Yangyu Tao'}, {'authorId': '2143385941', 'name': 'Bin Cui'}]","['Peking University', 'Carnegie Mellon University', 'Tencent']","['China', 'United States']",2023-03
2303.03032,Wei Li,"Wei Li, Linchao Zhu, Longyin Wen, Yi Yang",DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training,"Accepted by ICLR 2023. Code is available at
  https://github.com/dhg-wei/DeCap",,,,cs.CV cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large-scale pre-trained multi-modal models (e.g., CLIP) demonstrate strong zero-shot transfer capability in many discriminative tasks. Their adaptation to zero-shot image-conditioned text generation tasks has drawn increasing interest. Prior arts approach to zero-shot captioning by either utilizing the existing large language models (e.g., GPT-2) or pre-training the encoder-decoder network in an end-to-end manner. In this work, we propose a simple framework, named DeCap, for zero-shot captioning. We introduce a lightweight visual-aware language decoder. This decoder is both data-efficient and computation-efficient: 1) it only requires the text data for training, easing the burden on the collection of paired data. 2) it does not require end-to-end training. When trained with text-only data, the decoder takes the text embedding extracted from the off-the-shelf CLIP encoder as a prefix embedding. The challenge is that the decoder is trained on the text corpus but at the inference stage, it needs to generate captions based on visual inputs. The modality gap issue is widely observed in multi-modal contrastive models that prevents us from directly taking the visual embedding as the prefix embedding. We propose a training-free mechanism to reduce the modality gap. We project the visual embedding into the CLIP text embedding space, while the projected embedding retains the information of the visual input. Taking the projected embedding as the prefix embedding, the decoder generates high-quality descriptions that match the visual input. The experiments show that DeCap outperforms other zero-shot captioning methods and unpaired captioning methods on the typical image captioning benchmarks, i.e., MSCOCO and NoCaps. ","[{'version': 'v1', 'created': 'Mon, 6 Mar 2023 11:02:47 GMT'}]",2023-03-07,"[['Li', 'Wei', ''], ['Zhu', 'Linchao', ''], ['Wen', 'Longyin', ''], ['Yang', 'Yi', '']]",0,1,2023-03-06,1,4,3,1,1,0,a20e7e5e4338644d24145e71a9b89100af87e5d0,257365203.0,https://www.semanticscholar.org/paper/a20e7e5e4338644d24145e71a9b89100af87e5d0,International Conference on Learning Representations,2023.0,63.0,18.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '153021098', 'name': 'Wei Li'}, {'authorId': '2948393', 'name': 'Linchao Zhu'}, {'authorId': '39774417', 'name': 'Longyin Wen'}, {'authorId': '2143684866', 'name': 'Yi Yang'}]","['ByteDance', 'Zhejiang University']","['China', 'United States']",2023-03
2303.03915,Paulo Villegas,"Hugo Lauren\c{c}on, Lucile Saulnier, Thomas Wang, Christopher Akiki,
  Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou,
  Eduardo Gonz\'alez Ponferrada, Huu Nguyen, J\""org Frohberg, Mario
  \v{S}a\v{s}ko, Quentin Lhoest, Angelina McMillan-Major, Gerard Dupont, Stella
  Biderman, Anna Rogers, Loubna Ben allal, Francesco De Toni, Giada Pistilli,
  Olivier Nguyen, Somaieh Nikpoor, Maraim Masoud, Pierre Colombo, Javier de la
  Rosa, Paulo Villegas, Tristan Thrush, Shayne Longpre, Sebastian Nagel, Leon
  Weber, Manuel Mu\~noz, Jian Zhu, Daniel Van Strien, Zaid Alyafeai, Khalid
  Almubarak, Minh Chien Vu, Itziar Gonzalez-Dios, Aitor Soroa, Kyle Lo, Manan
  Dey, Pedro Ortiz Suarez, Aaron Gokaslan, Shamik Bose, David Adelani, Long
  Phan, Hieu Tran, Ian Yu, Suhas Pai, Jenny Chim, Violette Lepercq, Suzana
  Ilic, Margaret Mitchell, Sasha Alexandra Luccioni, Yacine Jernite",The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset,"NeurIPS 2022, Datasets and Benchmarks Track",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings. The BigScience workshop, a 1-year international and multidisciplinary initiative, was formed with the goal of researching and training large language models as a values-driven undertaking, putting issues of ethics, harm, and governance in the foreground. This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM) language model. We further release a large initial subset of the corpus and analyses thereof, and hope to empower large-scale monolingual and multilingual modeling projects with both the data and the processing tools, as well as stimulate research around this large multilingual corpus. ","[{'version': 'v1', 'created': 'Tue, 7 Mar 2023 14:25:44 GMT'}]",2023-03-08,"[['Laurençon', 'Hugo', ''], ['Saulnier', 'Lucile', ''], ['Wang', 'Thomas', ''], ['Akiki', 'Christopher', ''], ['del Moral', 'Albert Villanova', ''], ['Scao', 'Teven Le', ''], ['Von Werra', 'Leandro', ''], ['Mou', 'Chenghao', ''], ['Ponferrada', 'Eduardo González', ''], ['Nguyen', 'Huu', ''], ['Frohberg', 'Jörg', ''], ['Šaško', 'Mario', ''], ['Lhoest', 'Quentin', ''], ['McMillan-Major', 'Angelina', ''], ['Dupont', 'Gerard', ''], ['Biderman', 'Stella', ''], ['Rogers', 'Anna', ''], ['allal', 'Loubna Ben', ''], ['De Toni', 'Francesco', ''], ['Pistilli', 'Giada', ''], ['Nguyen', 'Olivier', ''], ['Nikpoor', 'Somaieh', ''], ['Masoud', 'Maraim', ''], ['Colombo', 'Pierre', ''], ['de la Rosa', 'Javier', ''], ['Villegas', 'Paulo', ''], ['Thrush', 'Tristan', ''], ['Longpre', 'Shayne', ''], ['Nagel', 'Sebastian', ''], ['Weber', 'Leon', ''], ['Muñoz', 'Manuel', ''], ['Zhu', 'Jian', ''], ['Van Strien', 'Daniel', ''], ['Alyafeai', 'Zaid', ''], ['Almubarak', 'Khalid', ''], ['Vu', 'Minh Chien', ''], ['Gonzalez-Dios', 'Itziar', ''], ['Soroa', 'Aitor', ''], ['Lo', 'Kyle', ''], ['Dey', 'Manan', ''], ['Suarez', 'Pedro Ortiz', ''], ['Gokaslan', 'Aaron', ''], ['Bose', 'Shamik', ''], ['Adelani', 'David', ''], ['Phan', 'Long', ''], ['Tran', 'Hieu', ''], ['Yu', 'Ian', ''], ['Pai', 'Suhas', ''], ['Chim', 'Jenny', ''], ['Lepercq', 'Violette', ''], ['Ilic', 'Suzana', ''], ['Mitchell', 'Margaret', ''], ['Luccioni', 'Sasha Alexandra', ''], ['Jernite', 'Yacine', '']]",0,0,2023-03-07,1,54,2,1,1,0,16c64f74ce0e6a59b0709c0d8e66596a5bc08ed6,257378329.0,https://www.semanticscholar.org/paper/16c64f74ce0e6a59b0709c0d8e66596a5bc08ed6,Neural Information Processing Systems,2023.0,136.0,76.0,8.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2172404846', 'name': 'Hugo Laurenccon'}, {'authorId': '2113836860', 'name': 'Lucile Saulnier'}, {'authorId': '2135734748', 'name': 'Thomas Wang'}, {'authorId': '2003696840', 'name': 'Christopher Akiki'}, {'authorId': '46219923', 'name': 'Albert Villanova del Moral'}, {'authorId': '1379806208', 'name': 'Teven Le Scao'}, {'authorId': '51128119', 'name': 'Leandro von Werra'}, {'authorId': '35966970', 'name': 'Chenghao Mou'}, {'authorId': '79512668', 'name': 'E. G. Ponferrada'}, {'authorId': '2168170616', 'name': 'Huu Nguyen'}, {'authorId': '2146695800', 'name': 'Jorg Frohberg'}, {'authorId': '2125821515', 'name': 'Mario vSavsko'}, {'authorId': '2113836945', 'name': 'Quentin Lhoest'}, {'authorId': '1584940075', 'name': 'Angelina McMillan-Major'}, {'authorId': '13656138', 'name': 'Gérard Dupont'}, {'authorId': '103476203', 'name': 'Stella Rose Biderman'}, {'authorId': '145046059', 'name': 'Anna Rogers'}, {'authorId': '2190281230', 'name': 'Loubna Ben Allal'}, {'authorId': '2067891070', 'name': 'F. Toni'}, {'authorId': '2158858559', 'name': 'Giada Pistilli'}, {'authorId': '2089233725', 'name': 'Olivier Nguyen'}, {'authorId': '2099315138', 'name': 'So-maieh Nikpoor'}, {'authorId': '153528116', 'name': 'Maraim Masoud'}, {'authorId': '46985469', 'name': 'Pierre Colombo'}, {'authorId': '144979591', 'name': 'Javier de la Rosa'}, {'authorId': '2176184659', 'name': 'Paulo Villegas'}, {'authorId': '1500242049', 'name': 'Tristan Thrush'}, {'authorId': '29909347', 'name': 'S. Longpre'}, {'authorId': '47351277', 'name': 'Sebastian Nagel'}, {'authorId': '20308468', 'name': 'Leon Weber'}, {'authorId': '2067766446', 'name': 'M. Muñoz'}, {'authorId': '144549416', 'name': 'Jian Zhu'}, {'authorId': '71075073', 'name': 'Daniel Alexander van Strien'}, {'authorId': '25098419', 'name': 'Zaid Alyafeai'}, {'authorId': '90615055', 'name': 'Khalid Almubarak'}, {'authorId': '1484109150', 'name': 'Minh Chien Vu'}, {'authorId': '1404791152', 'name': 'Itziar Gonzalez-Dios'}, {'authorId': '2078619062', 'name': 'Aitor Soroa Etxabe'}, {'authorId': '46258841', 'name': 'Kyle Lo'}, {'authorId': '1879591269', 'name': 'Manan Dey'}, {'authorId': '147846651', 'name': 'Pedro Ortiz Suarez'}, {'authorId': '8278433', 'name': 'Aaron Gokaslan'}, {'authorId': '2795685', 'name': 'Shamik Bose'}, {'authorId': '2518906', 'name': 'David Ifeoluwa Adelani'}, {'authorId': '2066589762', 'name': 'Long Phan'}, {'authorId': '2057078797', 'name': 'H. Tran'}, {'authorId': '52141399', 'name': 'I. Yu'}, {'authorId': '2053516473', 'name': 'S. Pai'}, {'authorId': '2164872258', 'name': 'Jenny Chim'}, {'authorId': '2190280574', 'name': 'Violette Lepercq'}, {'authorId': '2066663276', 'name': 'Suzana Ilic'}, {'authorId': '49501003', 'name': 'Margaret Mitchell'}, {'authorId': '2165225321', 'name': 'Sasha Luccioni'}, {'authorId': '2262249', 'name': 'Yacine Jernite'}]","['King Fahd University of Petroleum and Minerals', 'Western University', 'Ferrum College', 'Humboldt-Universität zu Berlin', 'University of Copenhagen', 'Queen Mary University of London', 'Puer University', 'University of Michigan–Ann Arbor', 'University of Washington', 'Leipzig University']","['Germany', 'Saudi Arabia', 'Denmark', 'Canada', 'United States', 'United Kingdom', 'China']",2023-03
2303.07263,Alexey Svyatkovskiy,"Matthew Jin, Syed Shahriar, Michele Tufano, Xin Shi, Shuai Lu, Neel
  Sundaresan, Alexey Svyatkovskiy",InferFix: End-to-End Program Repair with LLMs,,,,,cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Software development life cycle is profoundly influenced by bugs: their introduction, identification, and eventual resolution account for a significant portion of software cost. This has motivated software engineering researchers and practitioners to propose different approaches for automating the identification and repair of software defects. Large language models have been adapted to the program repair task through few-shot demonstration learning and instruction prompting, treating this as an infilling task. However, these models have only focused on learning general bug-fixing patterns for uncategorized bugs mined from public repositories. In this paper, we propose InferFix: a transformer-based program repair framework paired with a state-of-the-art static analyzer to fix critical security and performance bugs. InferFix combines a Retriever -- transformer encoder model pretrained via contrastive learning objective, which aims at searching for semantically equivalent bugs and corresponding fixes; and a Generator -- a large language model (Codex Cushman) finetuned on supervised bug-fix data with prompts augmented via bug type annotations and semantically similar fixes retrieved from an external non-parametric memory. To train and evaluate our approach, we curated InferredBugs, a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories. Our evaluation demonstrates that InferFix outperforms strong LLM baselines, with a top-1 accuracy of 65.6% for generating fixes in C# and 76.8% in Java. We discuss the deployment of InferFix alongside Infer at Microsoft which offers an end-to-end solution for detection, classification, and localization of bugs, as well as fixing and validation of candidate patches, integrated in the continuous integration pipeline to automate the software development workflow. ","[{'version': 'v1', 'created': 'Mon, 13 Mar 2023 16:42:47 GMT'}]",2023-03-14,"[['Jin', 'Matthew', ''], ['Shahriar', 'Syed', ''], ['Tufano', 'Michele', ''], ['Shi', 'Xin', ''], ['Lu', 'Shuai', ''], ['Sundaresan', 'Neel', ''], ['Svyatkovskiy', 'Alexey', '']]",0,0,2023-03-13,1,7,1,1,0,1,34d24b2d9f116f8f652c112d4ac924afcf11bd0d,257495997.0,https://www.semanticscholar.org/paper/34d24b2d9f116f8f652c112d4ac924afcf11bd0d,arXiv.org,2023.0,34.0,13.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2072904201', 'name': 'Ma Jin'}, {'authorId': '2211425830', 'name': 'Syed Shahriar'}, {'authorId': '40626221', 'name': 'Michele Tufano'}, {'authorId': '2159544304', 'name': 'Xin Shi'}, {'authorId': '2115338656', 'name': 'Shuai Lu'}, {'authorId': '145507437', 'name': 'Neel Sundaresan'}, {'authorId': '2061625488', 'name': 'Alexey Svyatkovskiy'}]","['University of California, Los Angeles', 'Microsoft Research Beijing, China Neel Sundaresan Microsoft Redmond, WA, USA', ""Conference'17, July 2017, Washington, DC, USA"", 'Microsoft']","['China', 'United States']",2023-03
2303.09618,Shu Zhang,"Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu,
  Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong and Ran
  Xu",HIVE: Harnessing Human Feedback for Instructional Visual Editing,,,,,cs.CV cs.AI cs.CL cs.HC cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Incorporating human feedback has been shown to be crucial to align text generated by large language models to human preferences. We hypothesize that state-of-the-art instructional image editing models, where outputs are generated based on an input image and an editing instruction, could similarly benefit from human feedback, as their outputs may not adhere to the correct instructions and preferences of users. In this paper, we present a novel framework to harness human feedback for instructional visual editing (HIVE). Specifically, we collect human feedback on the edited images and learn a reward function to capture the underlying user preferences. We then introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward. Besides, to mitigate the bias brought by the limitation of data, we contribute a new 1M training dataset, a 3.6K reward dataset for rewards learning, and a 1K evaluation dataset to boost the performance of instructional image editing. We conduct extensive empirical experiments quantitatively and qualitatively, showing that HIVE is favored over previous state-of-the-art instructional image editing approaches by a large margin. ","[{'version': 'v1', 'created': 'Thu, 16 Mar 2023 19:47:41 GMT'}]",2023-03-20,"[['Zhang', 'Shu', ''], ['Yang', 'Xinyi', ''], ['Feng', 'Yihao', ''], ['Qin', 'Can', ''], ['Chen', 'Chia-Chih', ''], ['Yu', 'Ning', ''], ['Chen', 'Zeyuan', ''], ['Wang', 'Huan', ''], ['Savarese', 'Silvio', ''], ['Ermon', 'Stefano', ''], ['Xiong', 'Caiming', ''], ['Xu', 'Ran', '']]",0,0,2023-03-16,1,12,5,0,0,0,372bc41602bbd21f192305775f0a58de9880e454,257622925.0,https://www.semanticscholar.org/paper/372bc41602bbd21f192305775f0a58de9880e454,arXiv.org,2023.0,58.0,13.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108088736', 'name': 'Shu Zhang'}, {'authorId': '2150441485', 'name': 'Xinyi Yang'}, {'authorId': '22758695', 'name': 'Yihao Feng'}, {'authorId': '12282768', 'name': 'Can Qin'}, {'authorId': '2211960332', 'name': 'Chia-Chih Chen'}, {'authorId': '2211974297', 'name': 'Ning Yu'}, {'authorId': '5478513', 'name': 'Zeyuan Chen'}, {'authorId': '46507194', 'name': 'Haiquan Wang'}, {'authorId': '1702137', 'name': 'S. Savarese'}, {'authorId': '2490652', 'name': 'Stefano Ermon'}, {'authorId': '2054594326', 'name': 'Caiming Xiong'}, {'authorId': '2115800155', 'name': 'Ran Xu'}]","['Stanford University', 'Salesforce AI Research,', 'Northeastern University']","['China', 'United States']",2023-03
2303.11032,Xiang Li,"Zhengliang Liu, Xiaowei Yu, Lu Zhang, Zihao Wu, Chao Cao, Haixing Dai,
  Lin Zhao, Wei Liu, Dinggang Shen, Quanzheng Li, Tianming Liu, Dajiang Zhu,
  Xiang Li",DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4,,,,,cs.CL cs.CY,http://creativecommons.org/licenses/by/4.0/,"  The digitization of healthcare has facilitated the sharing and re-using of medical data but has also raised concerns about confidentiality and privacy. HIPAA (Health Insurance Portability and Accountability Act) mandates removing re-identifying information before the dissemination of medical records. Thus, effective and efficient solutions for de-identifying medical data, especially those in free-text forms, are highly needed. While various computer-assisted de-identification methods, including both rule-based and learning-based, have been developed and used in prior practice, such solutions still lack generalizability or need to be fine-tuned according to different scenarios, significantly imposing restrictions in wider use. The advancement of large language models (LLM), such as ChatGPT and GPT-4, have shown great potential in processing text data in the medical domain with zero-shot in-context learning, especially in the task of privacy protection, as these models can identify confidential information by their powerful named entity recognition (NER) capability. In this work, we developed a novel GPT4-enabled de-identification framework (""DeID-GPT"") to automatically identify and remove the identifying information. Compared to existing commonly used medical text data de-identification methods, our developed DeID-GPT showed the highest accuracy and remarkable reliability in masking private information from the unstructured medical text while preserving the original structure and meaning of the text. This study is one of the earliest to utilize ChatGPT and GPT-4 for medical text data processing and de-identification, which provides insights for further research and solution development on the use of LLMs such as ChatGPT/GPT-4 in healthcare. Codes and benchmarking data information are available at https://github.com/yhydhx/ChatGPT-API. ","[{'version': 'v1', 'created': 'Mon, 20 Mar 2023 11:34:37 GMT'}]",2023-03-21,"[['Liu', 'Zhengliang', ''], ['Yu', 'Xiaowei', ''], ['Zhang', 'Lu', ''], ['Wu', 'Zihao', ''], ['Cao', 'Chao', ''], ['Dai', 'Haixing', ''], ['Zhao', 'Lin', ''], ['Liu', 'Wei', ''], ['Shen', 'Dinggang', ''], ['Li', 'Quanzheng', ''], ['Liu', 'Tianming', ''], ['Zhu', 'Dajiang', ''], ['Li', 'Xiang', '']]",1,1,2023-03-20,1,13,2,2,0,2,cff26bda86237d113ed01c812ad8bedd0afbe070,257632030.0,https://www.semanticscholar.org/paper/cff26bda86237d113ed01c812ad8bedd0afbe070,arXiv.org,2023.0,95.0,55.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '70995262', 'name': 'Zheng-Long Liu'}, {'authorId': '2139760758', 'name': 'Xiao-Xing Yu'}, {'authorId': '2156146823', 'name': 'Lu Zhang'}, {'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '5762440', 'name': 'Chao-Yang Cao'}, {'authorId': '29944950', 'name': 'Haixing Dai'}, {'authorId': '2111641126', 'name': 'Lin Zhao'}, {'authorId': '46641573', 'name': 'W. Liu'}, {'authorId': '2150038187', 'name': 'Dinggang Shen'}, {'authorId': '1762919', 'name': 'Quanzheng Li'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}, {'authorId': '2181182', 'name': 'Dajiang Zhu'}, {'authorId': '113075935', 'name': 'Xiang Li'}]","['Harvard Medical School', 'Lehigh University', 'ShanghaiTech University', 'Shanghai United Imaging Intelligence Co., Ltd., Shanghai, China;', 'Mayo Clinic Hospital', 'Shanghai Clinical Research Center', 'Massachusetts General Hospital', 'University of Georgia', 'The University of Texas at Arlington']","['China', 'United States']",2023-03
2303.11366,Noah Shinn,"Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik
  Narasimhan, Shunyu Yao",Reflexion: Language Agents with Verbal Reinforcement Learning,v3 contains additional citations,,,,cs.AI cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance. ","[{'version': 'v1', 'created': 'Mon, 20 Mar 2023 18:08:50 GMT'}, {'version': 'v2', 'created': 'Sun, 21 May 2023 06:20:36 GMT'}, {'version': 'v3', 'created': 'Sat, 10 Jun 2023 04:32:30 GMT'}]",2023-06-13,"[['Shinn', 'Noah', ''], ['Cassano', 'Federico', ''], ['Labash', 'Beck', ''], ['Gopinath', 'Ashwin', ''], ['Narasimhan', 'Karthik', ''], ['Yao', 'Shunyu', '']]",0,1,2023-03-20,3,6,3,1,0,1,0671fd553dd670a4e820553a974bc48040ba0819,258833055.0,https://www.semanticscholar.org/paper/0671fd553dd670a4e820553a974bc48040ba0819,,2023.0,35.0,123.0,17.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2212367248', 'name': 'Noah Shinn'}, {'authorId': '2197063831', 'name': 'Federico Cassano'}, {'authorId': '2212367414', 'name': 'Beck Labash'}, {'authorId': '2162047785', 'name': 'A. Gopinath'}, {'authorId': '144958935', 'name': 'Karthik Narasimhan'}, {'authorId': '47188964', 'name': 'Shunyu Yao'}]","['Massachusetts Institute of Technology', 'Princeton University', 'Northeastern University']","['China', 'United States']",2023-03
2303.11568,Jianing Qiu,"Jianing Qiu, Lin Li, Jiankai Sun, Jiachuan Peng, Peilun Shi, Ruiyang
  Zhang, Yinzhao Dong, Kyle Lam, Frank P.-W. Lo, Bo Xiao, Wu Yuan, Ningli Wang,
  Dong Xu, Benny Lo","Large AI Models in Health Informatics: Applications, Challenges, and the Future","This article has been accepted for publication in IEEE Journal of
  Biomedical and Health Informatics","JBHI, 2023",10.1109/JBHI.2023.3316750,,cs.AI cs.CY,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Large AI models, or foundation models, are models recently emerging with massive scales both parameter-wise and data-wise, the magnitudes of which can reach beyond billions. Once pretrained, large AI models demonstrate impressive performance in various downstream tasks. A prime example is ChatGPT, whose capability has compelled people's imagination about the far-reaching influence that large AI models can have and their potential to transform different domains of our lives. In health informatics, the advent of large AI models has brought new paradigms for the design of methodologies. The scale of multi-modal data in the biomedical and health domain has been ever-expanding especially since the community embraced the era of deep learning, which provides the ground to develop, validate, and advance large AI models for breakthroughs in health-related areas. This article presents a comprehensive review of large AI models, from background to their applications. We identify seven key sectors in which large AI models are applicable and might have substantial influence, including 1) bioinformatics; 2) medical diagnosis; 3) medical imaging; 4) medical informatics; 5) medical education; 6) public health; and 7) medical robotics. We examine their challenges, followed by a critical discussion about potential future directions and pitfalls of large AI models in transforming the field of health informatics. ","[{'version': 'v1', 'created': 'Tue, 21 Mar 2023 03:28:33 GMT'}, {'version': 'v2', 'created': 'Sun, 24 Sep 2023 06:09:58 GMT'}]",2023-09-26,"[['Qiu', 'Jianing', ''], ['Li', 'Lin', ''], ['Sun', 'Jiankai', ''], ['Peng', 'Jiachuan', ''], ['Shi', 'Peilun', ''], ['Zhang', 'Ruiyang', ''], ['Dong', 'Yinzhao', ''], ['Lam', 'Kyle', ''], ['Lo', 'Frank P. -W.', ''], ['Xiao', 'Bo', ''], ['Yuan', 'Wu', ''], ['Wang', 'Ningli', ''], ['Xu', 'Dong', ''], ['Lo', 'Benny', '']]",1,1,2023-03-21,2,14,2,1,0,1,23684a07517870cffd1f97fafbaae16ba22bd2b7,257636930.0,https://www.semanticscholar.org/paper/23684a07517870cffd1f97fafbaae16ba22bd2b7,IEEE journal of biomedical and health informatics,2023.0,345.0,23.0,0.0,True,"['Computer Science', 'Medicine']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Medicine', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '9720064', 'name': 'Jianing Qiu'}, {'authorId': '47681623', 'name': 'Lin Li'}, {'authorId': '2282025', 'name': 'Jiankai Sun'}, {'authorId': '2182888388', 'name': 'Jiachuan Peng'}, {'authorId': '2182698296', 'name': 'Peilun Shi'}, {'authorId': '144142354', 'name': 'Rui Zhang'}, {'authorId': '97436309', 'name': 'Yinzhao Dong'}, {'authorId': '2212174676', 'name': 'Kyle Lam'}, {'authorId': '7281521', 'name': 'F. P. Lo'}, {'authorId': '2212367961', 'name': 'Bo Xiao'}, {'authorId': '2212037281', 'name': 'Wu Yuan'}, {'authorId': '2212008585', 'name': 'Dong Xu'}, {'authorId': '2106084676', 'name': 'Benny P. L. Lo'}]","[""King's College London"", 'Precision Robotics (Hong Kong) Ltd., Hong Kong SAR', 'Imperial College London', 'Capital Medical University', 'Stanford University', 'Chinese University of Hong Kong', 'Hong Kong Polytechnic University', 'University of Missouri', 'University of Hong Kong', 'Large AI Models in Health Informatics']","['China', 'United States', 'United Kingdom', 'Hong Kong']",2023-03
2303.12135,Xin Jin,"Xin Jin, Yuchen Wang",Understand Legal Documents with Contextualized Large Language Models,SemEval 2023,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The growth of pending legal cases in populous countries, such as India, has become a major issue. Developing effective techniques to process and understand legal documents is extremely useful in resolving this problem. In this paper, we present our systems for SemEval-2023 Task 6: understanding legal texts (Modi et al., 2023). Specifically, we first develop the Legal-BERT-HSLN model that considers the comprehensive context information in both intra- and inter-sentence levels to predict rhetorical roles (subtask A) and then train a Legal-LUKE model, which is legal-contextualized and entity-aware, to recognize legal entities (subtask B). Our evaluations demonstrate that our designed models are more accurate than baselines, e.g., with an up to 15.0% better F1 score in subtask B. We achieved notable performance in the task leaderboard, e.g., 0.834 micro F1 score, and ranked No.5 out of 27 teams in subtask A. ","[{'version': 'v1', 'created': 'Tue, 21 Mar 2023 18:48:11 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Jun 2023 03:25:37 GMT'}, {'version': 'v3', 'created': 'Sun, 2 Jul 2023 22:51:52 GMT'}, {'version': 'v4', 'created': 'Wed, 19 Jul 2023 05:30:31 GMT'}]",2023-07-20,"[['Jin', 'Xin', ''], ['Wang', 'Yuchen', '']]",0,0,2023-03-21,4,2,1,0,0,0,77f1fd9d5d0deaaba2ae9b2b7d0f8df5b5762268,257663840.0,https://www.semanticscholar.org/paper/77f1fd9d5d0deaaba2ae9b2b7d0f8df5b5762268,arXiv.org,2023.0,41.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2149170954', 'name': 'Xin Jin'}, {'authorId': '2108898835', 'name': 'Yuchen Wang'}]","['The Ohio State University', 'Northwestern Polytechnical University']","['China', 'United States']",2023-03
2303.13547,Aiwei Liu,"Aiwei Liu, Xuming Hu, Lijie Wen, Philip S. Yu",A comprehensive evaluation of ChatGPT's zero-shot Text-to-SQL capability,"6 pages, 1 figures",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  This paper presents the first comprehensive analysis of ChatGPT's Text-to-SQL ability. Given the recent emergence of large-scale conversational language model ChatGPT and its impressive capabilities in both conversational abilities and code generation, we sought to evaluate its Text-to-SQL performance. We conducted experiments on 12 benchmark datasets with different languages, settings, or scenarios, and the results demonstrate that ChatGPT has strong text-to-SQL abilities. Although there is still a gap from the current state-of-the-art (SOTA) model performance, considering that the experiment was conducted in a zero-shot scenario, ChatGPT's performance is still impressive. Notably, in the ADVETA (RPL) scenario, the zero-shot ChatGPT even outperforms the SOTA model that requires fine-tuning on the Spider dataset by 4.1\%, demonstrating its potential for use in practical applications. To support further research in related fields, we have made the data generated by ChatGPT publicly available at https://github.com/THU-BPM/chatgpt-sql. ","[{'version': 'v1', 'created': 'Sun, 12 Mar 2023 04:22:01 GMT'}]",2023-03-27,"[['Liu', 'Aiwei', ''], ['Hu', 'Xuming', ''], ['Wen', 'Lijie', ''], ['Yu', 'Philip S.', '']]",1,1,2023-03-12,1,4,2,1,0,1,6034a1ed44de01ffc18cd2265223ed2bf1d216cd,257757019.0,https://www.semanticscholar.org/paper/6034a1ed44de01ffc18cd2265223ed2bf1d216cd,arXiv.org,2023.0,30.0,51.0,11.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '10017193', 'name': 'Aiwei Liu'}, {'authorId': '2109906988', 'name': 'Xuming Hu'}, {'authorId': '2114092431', 'name': 'Lijie Wen'}, {'authorId': '2191036692', 'name': 'Philip S. Yu'}]","['University of Illinois at Chicago', 'Tsinghua University']","['China', 'United States']",2023-03
2303.13809,Liang Ding,"Qingyu Lu, Baopu Qiu, Liang Ding, Liping Xie, Dacheng Tao",Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT,,,,,cs.CL,http://creativecommons.org/publicdomain/zero/1.0/,"  Generative large language models (LLMs), e.g., ChatGPT, have demonstrated remarkable proficiency across several NLP tasks such as machine translation, question answering, text summarization, and natural language understanding. Recent research has shown that utilizing ChatGPT for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but performs poorly at the segment level. To further improve the performance of LLMs on MT quality assessment, we conducted an investigation into several prompting methods. Our results indicate that by combining Chain-of-Thoughts and Error Analysis, a new prompting method called \textbf{\texttt{Error Analysis Prompting}}, LLMs like ChatGPT can \textit{generate human-like MT evaluations at both the system and segment level}. Additionally, we discovered some limitations of ChatGPT as an MT evaluator, such as unstable scoring and biases when provided with multiple translations in a single query. Our findings aim to provide a preliminary experience for appropriately evaluating translation quality on ChatGPT while offering a variety of tricks in designing prompts for in-context learning. We anticipate that this report will shed new light on advancing the field of translation evaluation with LLMs by enhancing both the accuracy and reliability of metrics. The project can be found in \url{https://github.com/Coldmist-Lu/ErrorAnalysis_Prompt}. ","[{'version': 'v1', 'created': 'Fri, 24 Mar 2023 05:05:03 GMT'}]",2023-03-27,"[['Lu', 'Qingyu', ''], ['Qiu', 'Baopu', ''], ['Ding', 'Liang', ''], ['Xie', 'Liping', ''], ['Tao', 'Dacheng', '']]",1,1,2023-03-24,1,5,1,1,0,1,8bc313e04cbd39847eb50b22af0a698ff2971a35,257756967.0,https://www.semanticscholar.org/paper/8bc313e04cbd39847eb50b22af0a698ff2971a35,arXiv.org,2023.0,51.0,29.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2117523622', 'name': 'Qingyu Lu'}, {'authorId': '2136339354', 'name': 'Baopu Qiu'}, {'authorId': '46573238', 'name': 'Liang Ding'}, {'authorId': '153164376', 'name': 'Liping Xie'}, {'authorId': '2140448089', 'name': 'Dacheng Tao'}]","['Southeast University', 'Nanjing University', 'Microsoft', 'Jingdong']","['China', 'United States']",2023-03
2303.14325,Xukun Zhou,"Xukun Zhou, Jiwei Li, Tianwei Zhang, Lingjuan Lyu, Muqiao Yang, Jun He",Backdoor Attacks with Input-unique Triggers in NLP,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Backdoor attack aims at inducing neural models to make incorrect predictions for poison data while keeping predictions on the clean dataset unchanged, which creates a considerable threat to current natural language processing (NLP) systems. Existing backdoor attacking systems face two severe issues:firstly, most backdoor triggers follow a uniform and usually input-independent pattern, e.g., insertion of specific trigger words, synonym replacement. This significantly hinders the stealthiness of the attacking model, leading the trained backdoor model being easily identified as malicious by model probes. Secondly, trigger-inserted poisoned sentences are usually disfluent, ungrammatical, or even change the semantic meaning from the original sentence, making them being easily filtered in the pre-processing stage. To resolve these two issues, in this paper, we propose an input-unique backdoor attack(NURA), where we generate backdoor triggers unique to inputs. IDBA generates context-related triggers by continuing writing the input with a language model like GPT2. The generated sentence is used as the backdoor trigger. This strategy not only creates input-unique backdoor triggers, but also preserves the semantics of the original input, simultaneously resolving the two issues above. Experimental results show that the IDBA attack is effective for attack and difficult to defend: it achieves high attack success rate across all the widely applied benchmarks, while is immune to existing defending methods. In addition, it is able to generate fluent, grammatical, and diverse backdoor inputs, which can hardly be recognized through human inspection. ","[{'version': 'v1', 'created': 'Sat, 25 Mar 2023 01:41:54 GMT'}]",2023-03-28,"[['Zhou', 'Xukun', ''], ['Li', 'Jiwei', ''], ['Zhang', 'Tianwei', ''], ['Lyu', 'Lingjuan', ''], ['Yang', 'Muqiao', ''], ['He', 'Jun', '']]",0,1,2023-03-25,1,6,1,1,1,0,02396a82642ca3a3c7e653e764cea193beaf88ba,257767146.0,https://www.semanticscholar.org/paper/02396a82642ca3a3c7e653e764cea193beaf88ba,arXiv.org,2023.0,57.0,3.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2212241158', 'name': 'Xukun Zhou'}, {'authorId': '2172372802', 'name': 'Jiwei Li'}, {'authorId': '2146331573', 'name': 'Tianwei Zhang'}, {'authorId': '3366777', 'name': 'L. Lyu'}, {'authorId': '8724126', 'name': 'Muqiao Yang'}, {'authorId': '2109932574', 'name': 'Jun He'}]","['National Youth Policy Institute', 'Carnegie Mellon University', 'Shandong University', 'Nanyang Technological University', 'Renmin University of China']","['South Korea', 'United States', 'China', 'Singapore']",2023-03
2303.14524,Tao Sheng,"Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, Jiawei
  Zhang",Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System,,,,,cs.IR cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) have demonstrated their significant potential to be applied for addressing various application tasks. However, traditional recommender systems continue to face great challenges such as poor interactivity and explainability, which actually also hinder their broad deployment in real-world systems. To address these limitations, this paper proposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender System) that innovatively augments LLMs for building conversational recommender systems by converting user profiles and historical interactions into prompts. Chat-Rec is demonstrated to be effective in learning user preferences and establishing connections between users and products through in-context learning, which also makes the recommendation process more interactive and explainable. What's more, within the Chat-Rec framework, user's preferences can transfer to different products for cross-domain recommendations, and prompt-based injection of information into LLMs can also handle the cold-start scenarios with new items. In our experiments, Chat-Rec effectively improve the results of top-k recommendations and performs better in zero-shot rating prediction task. Chat-Rec offers a novel approach to improving recommender systems and presents new practical scenarios for the implementation of AIGC (AI generated content) in recommender system studies. ","[{'version': 'v1', 'created': 'Sat, 25 Mar 2023 17:37:43 GMT'}, {'version': 'v2', 'created': 'Tue, 4 Apr 2023 03:51:27 GMT'}]",2023-04-05,"[['Gao', 'Yunfan', ''], ['Sheng', 'Tao', ''], ['Xiang', 'Youlin', ''], ['Xiong', 'Yun', ''], ['Wang', 'Haofen', ''], ['Zhang', 'Jiawei', '']]",1,1,2023-03-25,2,6,3,1,0,1,0cfdd655100055f234fd23ebecd915504b8e00e3,257766541.0,https://www.semanticscholar.org/paper/0cfdd655100055f234fd23ebecd915504b8e00e3,arXiv.org,2023.0,28.0,62.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1390781327', 'name': 'Yunfan Gao'}, {'authorId': '2169937292', 'name': 'Tao Sheng'}, {'authorId': '2152318443', 'name': 'Youlin Xiang'}, {'authorId': '2212411539', 'name': 'Yun Xiong'}, {'authorId': '21606013', 'name': 'Haofen Wang'}, {'authorId': '2144138716', 'name': 'Jiawei Zhang'}]","['Fudan University', 'Tongji University', 'University of California, Davis']","['China', 'United States']",2023-03
2303.14624,Hongyang Du,"Jiacheng Wang, Hongyang Du, Dusit Niyato, Zehui Xiong, Jiawen Kang,
  Shiwen Mao, and Xuemin (Sherman) Shen",Guiding AI-Generated Digital Content with Wireless Perception,,,,,cs.AI cs.HC cs.NI,http://creativecommons.org/licenses/by/4.0/,"  Recent advances in artificial intelligence (AI), coupled with a surge in training data, have led to the widespread use of AI for digital content generation, with ChatGPT serving as a representative example. Despite the increased efficiency and diversity, the inherent instability of AI models poses a persistent challenge in guiding these models to produce the desired content for users. In this paper, we introduce an integration of wireless perception (WP) with AI-generated content (AIGC) and propose a unified WP-AIGC framework to improve the quality of digital content production. The framework employs a novel multi-scale perception technology to read user's posture, which is difficult to describe accurately in words, and transmits it to the AIGC model as skeleton images. Based on these images and user's service requirements, the AIGC model generates corresponding digital content. Since the production process imposes the user's posture as a constraint on the AIGC model, it makes the generated content more aligned with the user's requirements. Additionally, WP-AIGC can also accept user's feedback, allowing adjustment of computing resources at edge server to improve service quality. Experiments results verify the effectiveness of the WP-AIGC framework, highlighting its potential as a novel approach for guiding AI models in the accurate generation of digital content. ","[{'version': 'v1', 'created': 'Sun, 26 Mar 2023 04:39:03 GMT'}]",2023-03-28,"[['Wang', 'Jiacheng', '', 'Sherman'], ['Du', 'Hongyang', '', 'Sherman'], ['Niyato', 'Dusit', '', 'Sherman'], ['Xiong', 'Zehui', '', 'Sherman'], ['Kang', 'Jiawen', '', 'Sherman'], ['Mao', 'Shiwen', '', 'Sherman'], ['Xuemin', '', '', 'Sherman'], ['Shen', '', '']]",1,1,2023-03-26,1,8,3,1,0,1,618fc40c8574668e0807d49fa8871433e073d961,257766435.0,https://www.semanticscholar.org/paper/618fc40c8574668e0807d49fa8871433e073d961,arXiv.org,2023.0,15.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2110175408', 'name': 'Jiacheng Wang'}, {'authorId': '2175043468', 'name': 'Hongyang Du'}, {'authorId': '1713586', 'name': 'D. Niyato'}, {'authorId': '2943819', 'name': 'Zehui Xiong'}, {'authorId': '145993626', 'name': 'Jiawen Kang'}, {'authorId': '2056609621', 'name': 'Shiwen Mao'}, {'authorId': '2158524414', 'name': 'X. Shen'}]","['Singapore University of Technology and Design', 'Auburn University', 'Nanyang Technological University', 'Guangdong University of Technology']","['China', 'United States', 'Singapore']",2023-03
2303.15772,Rishi Bommasani,"Rishi Bommasani and Dilara Soylu and Thomas I. Liao and Kathleen A.
  Creel and Percy Liang",Ecosystem Graphs: The Social Footprint of Foundation Models,"Authored by the Center for Research on Foundation Models (CRFM) at
  the Stanford Institute for Human-Centered Artificial Intelligence (HAI).
  Ecosystem Graphs available at https://crfm.stanford.edu/ecosystem-graphs/",,,,cs.LG cs.AI cs.CY,http://creativecommons.org/licenses/by/4.0/,"  Foundation models (e.g. ChatGPT, StableDiffusion) pervasively influence society, warranting immediate social attention. While the models themselves garner much attention, to accurately characterize their impact, we must consider the broader sociotechnical ecosystem. We propose Ecosystem Graphs as a documentation framework to transparently centralize knowledge of this ecosystem. Ecosystem Graphs is composed of assets (datasets, models, applications) linked together by dependencies that indicate technical (e.g. how Bing relies on GPT-4) and social (e.g. how Microsoft relies on OpenAI) relationships. To supplement the graph structure, each asset is further enriched with fine-grained metadata (e.g. the license or training emissions). We document the ecosystem extensively at https://crfm.stanford.edu/ecosystem-graphs/. As of March 16, 2023, we annotate 262 assets (64 datasets, 128 models, 70 applications) from 63 organizations linked by 356 dependencies. We show Ecosystem Graphs functions as a powerful abstraction and interface for achieving the minimum transparency required to address myriad use cases. Therefore, we envision Ecosystem Graphs will be a community-maintained resource that provides value to stakeholders spanning AI researchers, industry professionals, social scientists, auditors and policymakers. ","[{'version': 'v1', 'created': 'Tue, 28 Mar 2023 07:18:29 GMT'}]",2023-03-29,"[['Bommasani', 'Rishi', ''], ['Soylu', 'Dilara', ''], ['Liao', 'Thomas I.', ''], ['Creel', 'Kathleen A.', ''], ['Liang', 'Percy', '']]",1,1,2023-03-28,1,5,3,2,0,2,8ed7c9ba7cdb33e816135381ca502ace649c7985,257771875.0,https://www.semanticscholar.org/paper/8ed7c9ba7cdb33e816135381ca502ace649c7985,arXiv.org,2023.0,110.0,9.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '150272855', 'name': 'Rishi Bommasani'}, {'authorId': '1914569491', 'name': 'Dilara Soylu'}, {'authorId': '145636331', 'name': 'Thomas Liao'}, {'authorId': '1383066965', 'name': 'Kathleen A. Creel'}, {'authorId': '145419642', 'name': 'Percy Liang'}]","['Northeastern University', 'Stanford University']","['China', 'United States']",2023-03
2303.16129,Hongyang Du,"Minrui Xu, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Shiwen
  Mao, Zhu Han, Abbas Jamalipour, Dong In Kim, Xuemin (Sherman) Shen, Victor C.
  M. Leung, and H. Vincent Poor",Unleashing the Power of Edge-Cloud Generative AI in Mobile Networks: A Survey of AIGC Services,,,,,cs.NI,http://creativecommons.org/licenses/by/4.0/,"  Artificial Intelligence-Generated Content (AIGC) is an automated method for generating, manipulating, and modifying valuable and diverse data using AI algorithms creatively. This survey paper focuses on the deployment of AIGC applications, e.g., ChatGPT and Dall-E, at mobile edge networks, namely mobile AIGC networks, that provide personalized and customized AIGC services in real time while maintaining user privacy. We begin by introducing the background and fundamentals of generative models and the lifecycle of AIGC services at mobile AIGC networks, which includes data collection, training, finetuning, inference, and product management. We then discuss the collaborative cloud-edge-mobile infrastructure and technologies required to support AIGC services and enable users to access AIGC at mobile edge networks. Furthermore, we explore AIGCdriven creative applications and use cases for mobile AIGC networks. Additionally, we discuss the implementation, security, and privacy challenges of deploying mobile AIGC networks. Finally, we highlight some future research directions and open issues for the full realization of mobile AIGC networks. ","[{'version': 'v1', 'created': 'Tue, 28 Mar 2023 16:52:05 GMT'}, {'version': 'v2', 'created': 'Wed, 29 Mar 2023 16:21:20 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Jul 2023 05:27:51 GMT'}]",2023-07-20,"[['Xu', 'Minrui', '', 'Sherman'], ['Du', 'Hongyang', '', 'Sherman'], ['Niyato', 'Dusit', '', 'Sherman'], ['Kang', 'Jiawen', '', 'Sherman'], ['Xiong', 'Zehui', '', 'Sherman'], ['Mao', 'Shiwen', '', 'Sherman'], ['Han', 'Zhu', '', 'Sherman'], ['Jamalipour', 'Abbas', '', 'Sherman'], ['Kim', 'Dong In', '', 'Sherman'], ['Xuemin', '', '', 'Sherman'], ['Shen', '', ''], ['Leung', 'Victor C. M.', ''], ['Poor', 'H. Vincent', '']]",1,1,2023-03-28,3,13,1,1,0,1,f8e77bd3d573d0daee0744443c65c40e3b5dc10f,257771737.0,https://www.semanticscholar.org/paper/f8e77bd3d573d0daee0744443c65c40e3b5dc10f,arXiv.org,2023.0,306.0,23.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1454018677', 'name': 'Minrui Xu'}, {'authorId': '2175043468', 'name': 'Hongyang Du'}, {'authorId': '1713586', 'name': 'D. Niyato'}, {'authorId': '145993626', 'name': 'Jiawen Kang'}, {'authorId': '2943819', 'name': 'Zehui Xiong'}, {'authorId': '2056609621', 'name': 'Shiwen Mao'}, {'authorId': '2155438325', 'name': 'Zhu Han'}, {'authorId': '1725427', 'name': 'A. Jamalipour'}, {'authorId': '2155182631', 'name': 'Dong In Kim'}, {'authorId': '2158524414', 'name': 'X. Shen'}, {'authorId': '2144885207', 'name': 'Victor C. M. Leung'}, {'authorId': '2113229282', 'name': 'H. Poor'}]","['University of Houston', 'University of Waterloo', 'University of British Columbia', 'Singapore University of Technology and Design', 'University of Sydney', 'Princeton University', 'Auburn University', 'Shenzhen University', 'Nanyang Technological University', 'Guangdong University of Technology', 'Kyung Hee University', 'Sungkyunkwan University']","['Singapore', 'Canada', 'South Korea', 'United States', 'China', 'Australia']",2023-03
2303.16199,Renrui Zhang,"Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei
  Hu, Shilin Yan, Pan Lu, Hongsheng Li, Yu Qiao",LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention,Code is available at https://github.com/OpenGVLab/LLaMA-Adapter,,,,cs.CV cs.AI cs.CL cs.LG cs.MM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the word tokens at higher transformer layers. Then, a zero-initialized attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter can generate high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, our approach can be simply extended to multi-modal instructions for learning image-conditioned LLaMA model, which achieves superior reasoning performance on ScienceQA and COCO Caption benchmarks. Furthermore, we also evaluate the zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on traditional vision and language tasks, demonstrating the superior generalization capacity of our approach. Code is released at https://github.com/OpenGVLab/LLaMA-Adapter. ","[{'version': 'v1', 'created': 'Tue, 28 Mar 2023 17:59:12 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Jun 2023 17:31:32 GMT'}]",2023-06-16,"[['Zhang', 'Renrui', ''], ['Han', 'Jiaming', ''], ['Liu', 'Chris', ''], ['Gao', 'Peng', ''], ['Zhou', 'Aojun', ''], ['Hu', 'Xiangfei', ''], ['Yan', 'Shilin', ''], ['Lu', 'Pan', ''], ['Li', 'Hongsheng', ''], ['Qiao', 'Yu', '']]",0,0,2023-03-28,2,10,5,2,1,1,a757999ed260d7bc45484dc6b4456bf33fe6f679,257771811.0,https://www.semanticscholar.org/paper/a757999ed260d7bc45484dc6b4456bf33fe6f679,arXiv.org,2023.0,101.0,157.0,12.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2115713503', 'name': 'Renrui Zhang'}, {'authorId': '150147382', 'name': 'Jiaming Han'}, {'authorId': '9548994', 'name': 'Aojun Zhou'}, {'authorId': '2193028455', 'name': 'Xiangfei Hu'}, {'authorId': '2210554979', 'name': 'Shilin Yan'}, {'authorId': '2887562', 'name': 'Pan Lu'}, {'authorId': '47893312', 'name': 'Hongsheng Li'}, {'authorId': '144740494', 'name': 'Peng Gao'}, {'authorId': '2059129841', 'name': 'Y. Qiao'}]","['Shanghai Artificial Intelligence Laboratory', 'University of California, Los Angeles', 'Chinese University of Hong Kong']","['China', 'United States']",2023-03
2303.16854,Xingwei He,"Xingwei He, Zhenghao Lin, Yeyun Gong, A-Long Jin, Hang Zhang, Chen
  Lin, Jian Jiao, Siu Ming Yiu, Nan Duan, Weizhu Chen",AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many natural language processing (NLP) tasks rely on labeled data to train machine learning models to achieve high performance. However, data annotation can be a time-consuming and expensive process, especially when the task involves a large amount of data or requires specialized domains. Recently, GPT-3.5 series models have demonstrated remarkable few-shot and zero-shot ability across various NLP tasks. In this paper, we first claim that large language models (LLMs), such as GPT-3.5, can serve as an excellent crowdsourced annotator by providing them with sufficient guidance and demonstrated examples. To make LLMs to be better annotators, we propose a two-step approach, 'explain-then-annotate'. To be more precise, we begin by creating prompts for every demonstrated example, which we subsequently utilize to prompt a LLM to provide an explanation for why the specific ground truth answer/label was chosen for that particular example. Following this, we construct the few-shot chain-of-thought prompt with the self-generated explanation and employ it to annotate the unlabeled data. We conduct experiments on three tasks, including user input and keyword relevance assessment, BoolQ and WiC. The annotation results from GPT-3.5 surpasses those from crowdsourced annotation for user input and keyword relevance assessment. Additionally, for the other two tasks, GPT-3.5 achieves results that are comparable to those obtained through crowdsourced annotation. ","[{'version': 'v1', 'created': 'Wed, 29 Mar 2023 17:03:21 GMT'}]",2023-03-30,"[['He', 'Xingwei', ''], ['Lin', 'Zhenghao', ''], ['Gong', 'Yeyun', ''], ['Jin', 'A-Long', ''], ['Zhang', 'Hang', ''], ['Lin', 'Chen', ''], ['Jiao', 'Jian', ''], ['Yiu', 'Siu Ming', ''], ['Duan', 'Nan', ''], ['Chen', 'Weizhu', '']]",0,1,2023-03-29,1,10,1,1,0,1,70da4fb798a86cbe8cad96c27ced0415885bbd9d,257805087.0,https://www.semanticscholar.org/paper/70da4fb798a86cbe8cad96c27ced0415885bbd9d,arXiv.org,2023.0,32.0,35.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1754500', 'name': 'Xingwei He'}, {'authorId': '31113759', 'name': 'Zheng-Wen Lin'}, {'authorId': '2171182', 'name': 'Yeyun Gong'}, {'authorId': '15796861', 'name': 'Alex Jin'}, {'authorId': '2119077859', 'name': 'Hang Zhang'}, {'authorId': '2186278677', 'name': 'Chen Lin'}, {'authorId': '2143968416', 'name': 'Jian Jiao'}, {'authorId': '2184000509', 'name': 'S. Yiu'}, {'authorId': '46429989', 'name': 'Nan Duan'}, {'authorId': '2109136147', 'name': 'Weizhu Chen'}]","['Xiamen University', 'Microsoft Research Asia, 4 Microsoft', 'Microsoft', 'University of Hong Kong']","['China', 'United States', 'Hong Kong']",2023-03
2304.01196,Canwen Xu,Canwen Xu and Daya Guo and Nan Duan and Julian McAuley,Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data,Baize v2,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Chat models, such as ChatGPT, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field. We propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself. Subsequently, we employ parameter-efficient tuning to enhance LLaMA, an open-source large language model. The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. Furthermore, we propose a new technique called Self-Distill with Feedback, to further improve the performance of the Baize models with feedback from ChatGPT. The Baize models and data are released for research purposes only at https://github.com/project-baize/baize-chatbot. An online demo is also available at https://huggingface.co/spaces/project-baize/chat-with-baize. ","[{'version': 'v1', 'created': 'Mon, 3 Apr 2023 17:59:09 GMT'}, {'version': 'v2', 'created': 'Tue, 4 Apr 2023 08:34:16 GMT'}, {'version': 'v3', 'created': 'Tue, 23 May 2023 19:40:03 GMT'}]",2023-05-25,"[['Xu', 'Canwen', ''], ['Guo', 'Daya', ''], ['Duan', 'Nan', ''], ['McAuley', 'Julian', '']]",1,1,2023-04-03,3,4,2,2,1,1,42e741e0be43954ae684d14333e4074f4d0ae961,257912848.0,https://www.semanticscholar.org/paper/42e741e0be43954ae684d14333e4074f4d0ae961,arXiv.org,2023.0,26.0,103.0,12.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '66247317', 'name': 'Canwen Xu'}, {'authorId': '51223794', 'name': 'Daya Guo'}, {'authorId': '46429989', 'name': 'Nan Duan'}, {'authorId': '35660011', 'name': 'Julian McAuley'}]","['Sun Yat-sen University', 'Microsoft', 'University of California, San Diego']","['China', 'United States']",2023-04
2304.01433,Cliff Young,"Norman P. Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan,
  Lifeng Nai, Nishant Patil, Suvinay Subramanian, Andy Swing, Brian Towles,
  Cliff Young, Xiang Zhou, Zongwei Zhou, and David Patterson",TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings,"15 pages; 16 figures; to be published at ISCA 2023 (the International
  Symposium on Computer Architecture)",,,,cs.AR cs.AI cs.LG cs.PF,http://creativecommons.org/licenses/by/4.0/,"  In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are <5% of system cost and <3% of system power. Each TPU v4 includes SparseCores, dataflow processors that accelerate models that rely on embeddings by 5x-7x yet use only 5% of die area and power. Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves performance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chips and thus ~10x faster overall, which along with OCS flexibility helps large language models. For similar sized systems, it is ~4.3x-4.5x faster than the Graphcore IPU Bow and is 1.2x-1.7x faster and uses 1.3x-1.9x less power than the Nvidia A100. TPU v4s inside the energy-optimized warehouse scale computers of Google Cloud use ~3x less energy and produce ~20x less CO2e than contemporary DSAs in a typical on-premise data center. ","[{'version': 'v1', 'created': 'Tue, 4 Apr 2023 00:52:46 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Apr 2023 14:50:57 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Apr 2023 22:25:51 GMT'}]",2023-04-24,"[['Jouppi', 'Norman P.', ''], ['Kurian', 'George', ''], ['Li', 'Sheng', ''], ['Ma', 'Peter', ''], ['Nagarajan', 'Rahul', ''], ['Nai', 'Lifeng', ''], ['Patil', 'Nishant', ''], ['Subramanian', 'Suvinay', ''], ['Swing', 'Andy', ''], ['Towles', 'Brian', ''], ['Young', 'Cliff', ''], ['Zhou', 'Xiang', ''], ['Zhou', 'Zongwei', ''], ['Patterson', 'David', '']]",0,0,2023-04-04,3,14,4,0,0,0,7c25adf2ddb35df05a61c697da97efb8583d77df,257921908.0,https://www.semanticscholar.org/paper/7c25adf2ddb35df05a61c697da97efb8583d77df,International Symposium on Computer Architecture,2023.0,65.0,44.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1715454', 'name': 'N. Jouppi'}, {'authorId': '1753079661', 'name': 'George Kurian'}, {'authorId': '2153701529', 'name': 'Sheng Li'}, {'authorId': '49735130', 'name': 'Peter C. Ma'}, {'authorId': '1395811464', 'name': 'R. Nagarajan'}, {'authorId': '2144577', 'name': 'Lifeng Nai'}, {'authorId': '2056800684', 'name': 'Nishant Patil'}, {'authorId': '1929462', 'name': 'Suvinay Subramanian'}, {'authorId': '1394189636', 'name': 'Andy Swing'}, {'authorId': '1762455', 'name': 'Brian Towles'}, {'authorId': '39660914', 'name': 'C. Young'}, {'authorId': '50177639', 'name': 'Xiaoping Zhou'}, {'authorId': '2109465503', 'name': 'Zongwei Zhou'}, {'authorId': '2052996328', 'name': 'David A. Patterson'}]","['ISCA Technologies (United States)', 'Google', 'General Hospital of Guangzhou Military Command']","['China', 'United States']",2023-04
2304.01852,Yiheng Liu,"Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang,
  Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, Zihao Wu, Lin
  Zhao, Dajiang Zhu, Xiang Li, Ning Qiang, Dingang Shen, Tianming Liu, Bao Ge",Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models,"21 pages, 4 figures, accepted by Meta-Radiology",Meta-Radiology (2023)100017,10.1016/j.metrad.2023.100017,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  This paper presents a comprehensive survey of ChatGPT-related (GPT-3.5 and GPT-4) research, state-of-the-art large language models (LLM) from the GPT series, and their prospective applications across diverse domains. Indeed, key innovations such as large-scale pre-training that captures knowledge across the entire world wide web, instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability and performance. We performed an in-depth analysis of 194 relevant papers on arXiv, encompassing trend analysis, word cloud representation, and distribution analysis across various application domains. The findings reveal a significant and increasing interest in ChatGPT-related research, predominantly centered on direct natural language processing applications, while also demonstrating considerable potential in areas ranging from education and history to mathematics, medicine, and physics. This study endeavors to furnish insights into ChatGPT's capabilities, potential implications, ethical concerns, and offer direction for future advancements in this field. ","[{'version': 'v1', 'created': 'Tue, 4 Apr 2023 15:01:06 GMT'}, {'version': 'v2', 'created': 'Sat, 8 Apr 2023 14:42:40 GMT'}, {'version': 'v3', 'created': 'Thu, 11 May 2023 03:50:53 GMT'}, {'version': 'v4', 'created': 'Tue, 22 Aug 2023 03:18:43 GMT'}]",2023-08-25,"[['Liu', 'Yiheng', ''], ['Han', 'Tianle', ''], ['Ma', 'Siyuan', ''], ['Zhang', 'Jiayue', ''], ['Yang', 'Yuanyuan', ''], ['Tian', 'Jiaming', ''], ['He', 'Hao', ''], ['Li', 'Antong', ''], ['He', 'Mengshen', ''], ['Liu', 'Zhengliang', ''], ['Wu', 'Zihao', ''], ['Zhao', 'Lin', ''], ['Zhu', 'Dajiang', ''], ['Li', 'Xiang', ''], ['Qiang', 'Ning', ''], ['Shen', 'Dingang', ''], ['Liu', 'Tianming', ''], ['Ge', 'Bao', '']]",1,1,2023-04-04,4,18,1,3,0,3,51a0bba0c5fb4257e843040615bb23f712fed4e6,257921533.0,https://www.semanticscholar.org/paper/51a0bba0c5fb4257e843040615bb23f712fed4e6,Meta-Radiology,2023.0,113.0,82.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2107995766', 'name': 'Yi-Hsien Liu'}, {'authorId': '2184719751', 'name': 'Tianle Han'}, {'authorId': '143791100', 'name': 'Siyuan Ma'}, {'authorId': '2144133280', 'name': 'Jia-Yu Zhang'}, {'authorId': '1911009053', 'name': 'Yuanyu Yang'}, {'authorId': '2213753130', 'name': 'Jiaming Tian'}, {'authorId': '2155082967', 'name': 'Haoyang He'}, {'authorId': '2141520266', 'name': 'Antong Li'}, {'authorId': '2165762495', 'name': 'Mengshen He'}, {'authorId': '2145977326', 'name': 'Zheng Liu'}, {'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '2181182', 'name': 'Dajiang Zhu'}, {'authorId': '2144438902', 'name': 'Xiang Li'}, {'authorId': '1500422299', 'name': 'Ning Qiang'}, {'authorId': '2212881824', 'name': 'Dingang Shen'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}, {'authorId': '144691205', 'name': 'Bao Ge'}]","[""Xi'an Jiaotong University"", 'ShanghaiTech University', 'Shanghai Clinical Research Center', 'Massachusetts General Hospital', 'Shaanxi Normal University', 'United Imaging', 'University of Georgia', 'The University of Texas at Arlington']","['China', 'United States']",2023-04
2304.01964,Aditi Mishra,"Aditi Mishra, Utkarsh Soni, Anjana Arunkumar, Jinbin Huang, Bum Chul
  Kwon, Chris Bryan","PromptAid: Prompt Exploration, Perturbation, Testing and Iteration using Visual Analytics for Large Language Models",,,,,cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have gained widespread popularity due to their ability to perform ad-hoc Natural Language Processing (NLP) tasks with a simple natural language prompt. Part of the appeal for LLMs is their approachability to the general public, including individuals with no prior technical experience in NLP techniques. However, natural language prompts can vary significantly in terms of their linguistic structure, context, and other semantics. Modifying one or more of these aspects can result in significant differences in task performance. Non-expert users may find it challenging to identify the changes needed to improve a prompt, especially when they lack domain-specific knowledge and lack appropriate feedback. To address this challenge, we present PromptAid, a visual analytics system designed to interactively create, refine, and test prompts through exploration, perturbation, testing, and iteration. PromptAid uses multiple, coordinated visualizations which allow users to improve prompts by using the three strategies: keyword perturbations, paraphrasing perturbations, and obtaining the best set of in-context few-shot examples. PromptAid was designed through an iterative prototyping process involving NLP experts and was evaluated through quantitative and qualitative assessments for LLMs. Our findings indicate that PromptAid helps users to iterate over prompt template alterations with less cognitive overhead, generate diverse prompts with help of recommendations, and analyze the performance of the generated prompts while surpassing existing state-of-the-art prompting interfaces in performance. ","[{'version': 'v1', 'created': 'Tue, 4 Apr 2023 17:14:54 GMT'}, {'version': 'v2', 'created': 'Sat, 8 Apr 2023 16:25:10 GMT'}]",2023-04-11,"[['Mishra', 'Aditi', ''], ['Soni', 'Utkarsh', ''], ['Arunkumar', 'Anjana', ''], ['Huang', 'Jinbin', ''], ['Kwon', 'Bum Chul', ''], ['Bryan', 'Chris', '']]",0,0,2023-04-04,2,6,1,0,0,0,a2c8d1c5470435176185bf891c76711a9b44808a,257921397.0,https://www.semanticscholar.org/paper/a2c8d1c5470435176185bf891c76711a9b44808a,arXiv.org,2023.0,43.0,10.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2004558290', 'name': 'Aditi Mishra'}, {'authorId': '39181352', 'name': 'Utkarsh Soni'}, {'authorId': '1667817604', 'name': 'Anjana Arunkumar'}, {'authorId': '8669194', 'name': 'Jinbin Huang'}, {'authorId': '145276140', 'name': 'B. Kwon'}, {'authorId': '143922308', 'name': 'Chris Bryan'}]","['Arizona State University', 'IBM Research - China']","['China', 'United States']",2023-04
2304.03022,Chen Li,"Chen Li, Yixiao Ge, Jiayong Mao, Dian Li, Ying Shan",TagGPT: Large Language Models are Zero-shot Multimodal Taggers,"13 pages, 6 figures",,,,cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Tags are pivotal in facilitating the effective distribution of multimedia content in various applications in the contemporary Internet era, such as search engines and recommendation systems. Recently, large language models (LLMs) have demonstrated impressive capabilities across a wide range of tasks. In this work, we propose TagGPT, a fully automated system capable of tag extraction and multimodal tagging in a completely zero-shot fashion. Our core insight is that, through elaborate prompt engineering, LLMs are able to extract and reason about proper tags given textual clues of multimodal data, e.g., OCR, ASR, title, etc. Specifically, to automatically build a high-quality tag set that reflects user intent and interests for a specific application, TagGPT predicts large-scale candidate tags from a series of raw data via prompting LLMs, filtered with frequency and semantics. Given a new entity that needs tagging for distribution, TagGPT introduces two alternative options for zero-shot tagging, i.e., a generative method with late semantic matching with the tag set, and another selective method with early matching in prompts. It is well noticed that TagGPT provides a system-level solution based on a modular framework equipped with a pre-trained LLM (GPT-3.5 used here) and a sentence embedding model (SimCSE used here), which can be seamlessly replaced with any more advanced one you want. TagGPT is applicable for various modalities of data in modern social media and showcases strong generalization ability to a wide range of applications. We evaluate TagGPT on publicly available datasets, i.e., Kuaishou and Food.com, and demonstrate the effectiveness of TagGPT compared to existing hashtags and off-the-shelf taggers. Project page: https://github.com/TencentARC/TagGPT. ","[{'version': 'v1', 'created': 'Thu, 6 Apr 2023 12:17:46 GMT'}]",2023-04-07,"[['Li', 'Chen', ''], ['Ge', 'Yixiao', ''], ['Mao', 'Jiayong', ''], ['Li', 'Dian', ''], ['Shan', 'Ying', '']]",0,1,2023-04-06,1,5,1,1,0,1,4895d443c36bd136a818be2db34442354ba408d1,257984964.0,https://www.semanticscholar.org/paper/4895d443c36bd136a818be2db34442354ba408d1,arXiv.org,2023.0,23.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '40475614', 'name': 'Chen Li'}, {'authorId': '152988335', 'name': 'Yixiao Ge'}, {'authorId': '2213513905', 'name': 'Jiayong Mao'}, {'authorId': '2151495740', 'name': 'Dian Li'}, {'authorId': '1387190008', 'name': 'Ying Shan'}]","['Tencent', 'Proton Collaborative Group']","['China', 'United States']",2023-04
2304.03442,Joon Sung Park,"Joon Sung Park, Joseph C. O'Brien, Carrie J. Cai, Meredith Ringel
  Morris, Percy Liang, Michael S. Bernstein",Generative Agents: Interactive Simulacra of Human Behavior,,,,,cs.HC cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior. ","[{'version': 'v1', 'created': 'Fri, 7 Apr 2023 01:55:19 GMT'}, {'version': 'v2', 'created': 'Sun, 6 Aug 2023 00:21:19 GMT'}]",2023-08-08,"[['Park', 'Joon Sung', ''], [""O'Brien"", 'Joseph C.', ''], ['Cai', 'Carrie J.', ''], ['Morris', 'Meredith Ringel', ''], ['Liang', 'Percy', ''], ['Bernstein', 'Michael S.', '']]",0,0,2023-04-07,2,6,3,0,0,0,5278a8eb2ba2429d4029745caf4e661080073c81,258040990.0,https://www.semanticscholar.org/paper/5278a8eb2ba2429d4029745caf4e661080073c81,ACM Symposium on User Interface Software and Technology,2023.0,112.0,276.0,22.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2116649486', 'name': 'J. Park'}, {'authorId': '2213764034', 'name': ""Joseph C. O'Brien""}, {'authorId': '145941081', 'name': 'Carrie J. Cai'}, {'authorId': '144844426', 'name': 'M. Morris'}, {'authorId': '145419642', 'name': 'Percy Liang'}, {'authorId': '145879842', 'name': 'Michael S. Bernstein'}]","['Stanford University', 'Google', 'Nanjing University of Information Science and Technology']","['China', 'United States']",2023-04
2304.03728,Hongyin Luo,"Tianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell,
  Thomas Hartvigsen, Xixin Wu, Danny Fox, Helen Meng, James Glass",Interpretable Unified Language Checking,10 + 5 pages,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Despite recent concerns about undesirable behaviors generated by large language models (LLMs), including non-factual, biased, and hateful language, we find LLMs are inherent multi-task language checkers based on their latent representations of natural and social knowledge. We present an interpretable, unified, language checking (UniLC) method for both human and machine-generated language that aims to check if language input is factual and fair. While fairness and fact-checking tasks have been handled separately with dedicated models, we find that LLMs can achieve high performance on a combination of fact-checking, stereotype detection, and hate speech detection tasks with a simple, few-shot, unified set of prompts. With the ``1/2-shot'' multi-task language checking method proposed in this work, the GPT3.5-turbo model outperforms fully supervised baselines on several language tasks. The simple approach and results suggest that based on strong latent knowledge representations, an LLM can be an adaptive and explainable tool for detecting misinformation, stereotypes, and hate speech. ","[{'version': 'v1', 'created': 'Fri, 7 Apr 2023 16:47:49 GMT'}]",2023-04-10,"[['Zhang', 'Tianhua', ''], ['Luo', 'Hongyin', ''], ['Chuang', 'Yung-Sung', ''], ['Fang', 'Wei', ''], ['Gaitskell', 'Luc', ''], ['Hartvigsen', 'Thomas', ''], ['Wu', 'Xixin', ''], ['Fox', 'Danny', ''], ['Meng', 'Helen', ''], ['Glass', 'James', '']]",0,1,2023-04-07,1,10,1,1,0,1,3fcc8cb68488cfdfe4c52b81f27a236352fe5582,258041307.0,https://www.semanticscholar.org/paper/3fcc8cb68488cfdfe4c52b81f27a236352fe5582,arXiv.org,2023.0,48.0,12.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2146333115', 'name': 'Tianhua Zhang'}, {'authorId': '1944274', 'name': 'Hongyin Luo'}, {'authorId': '2475831', 'name': 'Yung-Sung Chuang'}, {'authorId': '2117161925', 'name': 'Wei Fang'}, {'authorId': '2214001322', 'name': 'Luc Gaitskell'}, {'authorId': '32452740', 'name': 'Thomas Hartvigsen'}, {'authorId': '1847260', 'name': 'Xixin Wu'}, {'authorId': '31997718', 'name': 'D. Fox'}, {'authorId': '2057833292', 'name': 'Helen M. Meng'}, {'authorId': '145898106', 'name': 'James R. Glass'}]","['MIT Computer Science and Artificial Intelligence Lab, Cambridge MA, USA', 'Massachusetts Institute of Technology', 'Chinese University of Hong Kong']","['China', 'United States']",2023-04
2304.03946,Xiaonan Nie,"Xiaonan Nie, Xupeng Miao, Zilong Wang, Zichao Yang, Jilong Xue,
  Lingxiao Ma, Gang Cao, Bin Cui",FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement,Accepted by SIGMOD 2023,"Proc. ACM Manag. Data, Vol. 1, No. 1, Article 110. Publication
  date: May 2023",10.1145/3588964,,cs.DC cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the increasing data volume, there is a trend of using large-scale pre-trained models to store the knowledge into an enormous number of model parameters. The training of these models is composed of lots of dense algebras, requiring a huge amount of hardware resources. Recently, sparsely-gated Mixture-of-Experts (MoEs) are becoming more popular and have demonstrated impressive pretraining scalability in various downstream tasks. However, such a sparse conditional computation may not be effective as expected in practical systems due to the routing imbalance and fluctuation problems. Generally, MoEs are becoming a new data analytics paradigm in the data life cycle and suffering from unique challenges at scales, complexities, and granularities never before possible.   In this paper, we propose a novel DNN training framework, FlexMoE, which systematically and transparently address the inefficiency caused by dynamic dataflow. We first present an empirical analysis on the problems and opportunities of training MoE models, which motivates us to overcome the routing imbalance and fluctuation problems by a dynamic expert management and device placement mechanism. Then we introduce a novel scheduling module over the existing DNN runtime to monitor the data flow, make the scheduling plans, and dynamically adjust the model-to-hardware mapping guided by the real-time data traffic. A simple but efficient heuristic algorithm is exploited to dynamically optimize the device placement during training. We have conducted experiments on both NLP models (e.g., BERT and GPT) and vision models (e.g., Swin). And results show FlexMoE can achieve superior performance compared with existing systems on real-world workloads -- FlexMoE outperforms DeepSpeed by 1.70x on average and up to 2.10x, and outperforms FasterMoE by 1.30x on average and up to 1.45x. ","[{'version': 'v1', 'created': 'Sat, 8 Apr 2023 07:34:26 GMT'}]",2023-04-11,"[['Nie', 'Xiaonan', ''], ['Miao', 'Xupeng', ''], ['Wang', 'Zilong', ''], ['Yang', 'Zichao', ''], ['Xue', 'Jilong', ''], ['Ma', 'Lingxiao', ''], ['Cao', 'Gang', ''], ['Cui', 'Bin', '']]",0,1,2023-04-08,1,8,2,0,0,0,dbbc5003af690799fa4fe6330fb795311cde106f,258048524.0,https://www.semanticscholar.org/paper/dbbc5003af690799fa4fe6330fb795311cde106f,Proc. ACM Manag. Data,2023.0,48.0,8.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2113588952', 'name': 'Xiaonan Nie'}, {'authorId': '1720763480', 'name': 'Xupeng Miao'}, {'authorId': '2168091655', 'name': 'Zilong Wang'}, {'authorId': '8387085', 'name': 'Zichao Yang'}, {'authorId': '2870618', 'name': 'Jilong Xue'}, {'authorId': '2492241', 'name': 'Lingxiao Ma'}, {'authorId': '2158415558', 'name': 'Gang-Ming Cao'}, {'authorId': '2143385941', 'name': 'Bin Cui'}]","['Peking University', 'Carnegie Mellon University', 'Microsoft', 'Beijing Academy of Artificial Intelligence']","['China', 'United States']",2023-04
2304.04675,Wenhao Zhu,"Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang,
  Lingpeng Kong, Jiajun Chen, Lei Li",Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have demonstrated remarkable potential in handling multilingual machine translation (MMT). In this paper, we systematically investigate the advantages and challenges of LLMs for MMT by answering two questions: 1) How well do LLMs perform in translating a massive number of languages? 2) Which factors affect LLMs' performance in translation? We evaluate popular LLMs, including XGLM, OPT, BLOOMZ, and ChatGPT, on 102 languages. Our empirical results show that even the best model ChatGPT still lags behind the supervised baseline NLLB in 83.33% of translation directions. Through further analysis, we discover that LLMs exhibit new working patterns when used for MMT. First, prompt semantics can surprisingly be ignored when given in-context exemplars, where LLMs still show strong performance even with unreasonable prompts. Second, cross-lingual exemplars can provide better task instruction for low-resource translation than exemplars in the same language pairs. Third, we observe the overestimated performance of BLOOMZ on dataset Flores-101, indicating the potential risk when using public datasets for evaluation. ","[{'version': 'v1', 'created': 'Mon, 10 Apr 2023 15:51:30 GMT'}, {'version': 'v2', 'created': 'Tue, 2 May 2023 02:23:50 GMT'}]",2023-05-03,"[['Zhu', 'Wenhao', ''], ['Liu', 'Hongyi', ''], ['Dong', 'Qingxiu', ''], ['Xu', 'Jingjing', ''], ['Huang', 'Shujian', ''], ['Kong', 'Lingpeng', ''], ['Chen', 'Jiajun', ''], ['Li', 'Lei', '']]",1,1,2023-04-10,2,8,1,4,3,1,dfd8944d39b378489b878d6e105d040fa0e524db,258048937.0,https://www.semanticscholar.org/paper/dfd8944d39b378489b878d6e105d040fa0e524db,arXiv.org,2023.0,53.0,40.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2131383723', 'name': 'Wenhao Zhu'}, {'authorId': '2115669628', 'name': 'Hongyi Liu'}, {'authorId': '2047143813', 'name': 'Qingxiu Dong'}, {'authorId': '47883405', 'name': 'Jingjing Xu'}, {'authorId': '47648549', 'name': 'Lingpeng Kong'}, {'authorId': '1838162', 'name': 'Jiajun Chen'}, {'authorId': '143900005', 'name': 'Lei Li'}, {'authorId': '2046010', 'name': 'Shujian Huang'}]","['Nanjing University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'Peking University', 'University of Hong Kong', 'University of California, Santa Barbara']","['China', 'United States', 'Hong Kong']",2023-04
2304.05197,Haoran Li,"Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng,
  Yangqiu Song",Multi-step Jailbreaking Privacy Attacks on ChatGPT,Updated with more experiments and improved writing,,,,cs.CL cs.CR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the rapid progress of large language models (LLMs), many downstream NLP tasks can be well solved given appropriate prompts. Though model developers and researchers work hard on dialog safety to avoid generating harmful content from LLMs, it is still challenging to steer AI-generated content (AIGC) for the human good. As powerful LLMs are devouring existing text data from various domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether the private information is included in the training data and what privacy threats can these LLMs and their downstream applications bring. In this paper, we study the privacy threats from OpenAI's ChatGPT and the New Bing enhanced by ChatGPT and show that application-integrated LLMs may cause new privacy threats. To this end, we conduct extensive experiments to support our claims and discuss LLMs' privacy implications. ","[{'version': 'v1', 'created': 'Tue, 11 Apr 2023 13:05:04 GMT'}, {'version': 'v2', 'created': 'Thu, 18 May 2023 17:11:40 GMT'}]",2023-05-19,"[['Li', 'Haoran', ''], ['Guo', 'Dadi', ''], ['Fan', 'Wei', ''], ['Xu', 'Mingshi', ''], ['Huang', 'Jie', ''], ['Meng', 'Fanpu', ''], ['Song', 'Yangqiu', '']]",1,1,2023-04-11,2,7,2,2,0,2,025ca4c125d6ecabc816a56f160e5c992abc76d9,258060250.0,https://www.semanticscholar.org/paper/025ca4c125d6ecabc816a56f160e5c992abc76d9,arXiv.org,2023.0,43.0,81.0,6.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Law', 'source': 's2-fos-model'}]","[{'authorId': '2145539796', 'name': 'Haoran Li'}, {'authorId': '2213872863', 'name': 'Dadi Guo'}, {'authorId': '2113533823', 'name': 'Wei Fan'}, {'authorId': '2153556700', 'name': 'Mingshi Xu'}, {'authorId': '1490651934', 'name': 'Jie Huang'}, {'authorId': '1809614', 'name': 'Yangqiu Song'}]","['Peking University', 'University of Illinois Urbana-Champaign', 'Hong Kong University of Science and Technology', 'University of Notre Dame']","['China', 'United States']",2023-04
2304.07493,Cong Guo,"Cong Guo, Jiaming Tang, Weiming Hu, Jingwen Leng, Chen Zhang, Fan
  Yang, Yunxin Liu, Minyi Guo, Yuhao Zhu",OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization,ISCA 2023,,10.1145/3579371.3589038,,cs.AR,http://creativecommons.org/licenses/by/4.0/,"  Transformer-based large language models (LLMs) have achieved great success with the growing model size. LLMs' size grows by $240\times$ every two years, which outpaces the hardware progress and makes model inference increasingly costly. Model quantization is a promising approach to mitigate the widening gap between LLM size and hardware capacity. However, the existence of outliers, values with significant magnitudes, in LLMs makes existing quantization methods less effective. Prior outlier-aware quantization schemes adopt sparsity encoding techniques to separate outliers from normal values where the process requires global coordination (e.g., a global sparsity coordination list). This incurs complex encoding/decoding hardware logics and an extra orchestration controller for the computation between outlier and normal values. As such, it is not hardware-efficient and hence only achieves sub-optimal quantization benefits.   We propose OliVe, an algorithm/architecture co-designed solution that adopts an outlier-victim pair (OVP) quantization and handles outlier values locally with low hardware overheads and high performance gains. The key insight of OliVe is that outliers are important while the normal values next to them are not. Thus those normal values (called victims) can be sacrificed to accommodate outliers. This enables a memory-aligned OVP encoding scheme, which can be efficiently integrated to the existing hardware accelerators like systolic array and tensor core. As a result, OliVe-based accelerator surpasses the existing outlier-aware accelerator, GOBO, by 4.5$\times$ speedup and 4.0$\times$ energy reduction, respectively, with a superior model accuracy. ","[{'version': 'v1', 'created': 'Sat, 15 Apr 2023 07:12:05 GMT'}]",2023-04-18,"[['Guo', 'Cong', ''], ['Tang', 'Jiaming', ''], ['Hu', 'Weiming', ''], ['Leng', 'Jingwen', ''], ['Zhang', 'Chen', ''], ['Yang', 'Fan', ''], ['Liu', 'Yunxin', ''], ['Guo', 'Minyi', ''], ['Zhu', 'Yuhao', '']]",0,0,2023-04-15,1,9,1,0,0,0,e92a5332390f0ba94615935541da4da9bed56512,258179335.0,https://www.semanticscholar.org/paper/e92a5332390f0ba94615935541da4da9bed56512,International Symposium on Computer Architecture,2023.0,95.0,6.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2109865181', 'name': 'Cong Guo'}, {'authorId': '2214687479', 'name': 'Jiaming Tang'}, {'authorId': '2214664410', 'name': 'Weiming Hu'}, {'authorId': '1831521', 'name': 'Jingwen Leng'}, {'authorId': '145107889', 'name': 'Chen Zhang'}, {'authorId': '145338263', 'name': 'Fan Yang'}, {'authorId': '2117415805', 'name': 'Yun-Bo Liu'}, {'authorId': '2151671216', 'name': 'Minyi Guo'}, {'authorId': '2155860957', 'name': 'Yuhao Zhu'}]","['ISCA Technologies (United States)', 'Jiaming Tang *', 'Fan Yang', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'Tsinghua University', 'Microsoft', 'Jingwen Leng ‡', 'University of Rochester', 'Linköping University']","['China', 'United States', 'Sweden']",2023-04
2304.07919,Jiaxin Ge,"Jiaxin Ge, Hongyin Luo, Siyuan Qian, Yulu Gan, Jie Fu, Shanghang Zhang",Chain of Thought Prompt Tuning in Vision Language Models,,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Language-Image Pre-training has demonstrated promising results on zero-shot and few-shot downstream tasks by prompting visual models with natural language prompts. However, most recent studies only use a single prompt for tuning, neglecting the inherent step-to-step cognitive reasoning process that humans conduct in complex task settings, for example, when processing images from unfamiliar domains. Chain of Thought is a simple and effective approximation to human reasoning process and has been proven useful for natural language processing (NLP) tasks. Based on this cognitive intuition, we believe that conducting effective reasoning is also an important problem in visual tasks, and a chain of thought could be a solution to this problem. In this work, we propose a novel chain of thought prompt tuning for vision-language modeling. Extensive experiments show that our method not only generalizes better in image classification tasks, has greater transferability beyond a single dataset, and has stronger domain generalization performance, but also performs much better in imagetext retrieval and visual question answering, which require more reasoning capabilities. We are the first to successfully adapt chain-of-thought prompting that combines visual and textual embeddings. We will release our codes ","[{'version': 'v1', 'created': 'Sun, 16 Apr 2023 23:59:25 GMT'}, {'version': 'v2', 'created': 'Sat, 17 Jun 2023 06:40:27 GMT'}]",2023-06-21,"[['Ge', 'Jiaxin', ''], ['Luo', 'Hongyin', ''], ['Qian', 'Siyuan', ''], ['Gan', 'Yulu', ''], ['Fu', 'Jie', ''], ['Zhang', 'Shanghang', '']]",0,0,2023-04-16,2,6,2,0,0,0,a8680b3419f3cbe6650f72b1023aed0ad0becb9e,258180277.0,https://www.semanticscholar.org/paper/a8680b3419f3cbe6650f72b1023aed0ad0becb9e,,2023.0,46.0,6.0,1.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2214584825', 'name': 'Jiaxin Ge'}, {'authorId': '1944274', 'name': 'Hongyin Luo'}, {'authorId': '1610531198', 'name': 'Siyuan Qian'}, {'authorId': '2142286278', 'name': 'Yulu Gan'}, {'authorId': '2119314983', 'name': 'Jie Fu'}, {'authorId': '2437353', 'name': 'Shanghang Zhang'}]","['China University of Petroleum, Beijing', 'Massachusetts Institute of Technology', 'Peking University']","['China', 'United States']",2023-04
2304.07987,Ruibin Yuan,"Ge Zhang, Yemin Shi, Ruibo Liu, Ruibin Yuan, Yizhi Li, Siwei Dong, Yu
  Shu, Zhaoqun Li, Zekun Wang, Chenghua Lin, Wenhao Huang, Jie Fu",Chinese Open Instruction Generalist: A Preliminary Release,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Instruction tuning is widely recognized as a key technique for building generalist language models, which has attracted the attention of researchers and the public with the release of InstructGPT~\citep{ouyang2022training} and ChatGPT\footnote{\url{https://chat.openai.com/}}. Despite impressive progress in English-oriented large-scale language models (LLMs), it is still under-explored whether English-based foundation LLMs can perform similarly on multilingual tasks compared to English tasks with well-designed instruction tuning and how we can construct the corpora needed for the tuning. To remedy this gap, we propose the project as an attempt to create a Chinese instruction dataset by various methods adapted to the intrinsic characteristics of 4 sub-tasks. We collect around 200k Chinese instruction tuning samples, which have been manually checked to guarantee high quality. We also summarize the existing English and Chinese instruction corpora and briefly describe some potential applications of the newly constructed Chinese instruction corpora. The resulting \textbf{C}hinese \textbf{O}pen \textbf{I}nstruction \textbf{G}eneralist (\textbf{COIG}) corpora are available in Huggingface\footnote{\url{https://huggingface.co/datasets/BAAI/COIG}} and Github\footnote{\url{https://github.com/BAAI-Zlab/COIG}}, and will be continuously updated. ","[{'version': 'v1', 'created': 'Mon, 17 Apr 2023 04:45:06 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Apr 2023 04:46:57 GMT'}, {'version': 'v3', 'created': 'Fri, 21 Apr 2023 03:16:13 GMT'}, {'version': 'v4', 'created': 'Tue, 25 Apr 2023 01:50:19 GMT'}]",2023-04-26,"[['Zhang', 'Ge', ''], ['Shi', 'Yemin', ''], ['Liu', 'Ruibo', ''], ['Yuan', 'Ruibin', ''], ['Li', 'Yizhi', ''], ['Dong', 'Siwei', ''], ['Shu', 'Yu', ''], ['Li', 'Zhaoqun', ''], ['Wang', 'Zekun', ''], ['Lin', 'Chenghua', ''], ['Huang', 'Wenhao', ''], ['Fu', 'Jie', '']]",1,1,2023-04-17,4,12,2,2,0,2,c01e43c65a04d766e429863bdf7cf65b895df20e,258179044.0,https://www.semanticscholar.org/paper/c01e43c65a04d766e429863bdf7cf65b895df20e,arXiv.org,2023.0,52.0,13.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2143853895', 'name': 'Ge Zhang'}, {'authorId': '38179026', 'name': 'Yemin Shi'}, {'authorId': '7247867', 'name': 'Ruibo Liu'}, {'authorId': '2032236274', 'name': 'Ruibin Yuan'}, {'authorId': '2129449392', 'name': 'Yizhi Li'}, {'authorId': '2026180078', 'name': 'Siwei Dong'}, {'authorId': '2066476157', 'name': 'Yu Shu'}, {'authorId': '2214748090', 'name': 'Zhaoqun Li'}, {'authorId': None, 'name': 'Zekun Wang'}, {'authorId': '2268783', 'name': 'Chenghua Lin'}, {'authorId': '1825796760', 'name': 'Wen-Fen Huang'}, {'authorId': '2089772402', 'name': 'Jie Fu'}]","['University of Sheffield', 'Beijing Academy of Artificial Intelligence', 'Zhejiang University', 'Carnegie Mellon University', 'Beihang University', 'Dartmouth College', 'University of Michigan–Ann Arbor']","['China', 'United States', 'United Kingdom']",2023-04
2304.07995,Qian Liu,"Qian Liu, Fan Zhou, Zhengbao Jiang, Longxu Dou, Min Lin",From Zero to Hero: Examining the Power of Symbolic Tasks in Instruction Tuning,"Work in Progress. The code is released at
  https://github.com/sail-sg/symbolic-instruction-tuning",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Fine-tuning language models on tasks with instructions has demonstrated potential in facilitating zero-shot generalization to unseen tasks. In this paper, we introduce a straightforward yet effective method for enhancing instruction tuning by employing symbolic tasks. Compared to crowdsourced human tasks or model-generated tasks, symbolic tasks present a unique advantage as they can be easily generated in vast quantities, theoretically providing an infinite supply of high-quality training instances. To explore the potential of symbolic tasks, we carry out an extensive case study on the representative symbolic task of SQL execution. Empirical results on various benchmarks validate that the integration of SQL execution leads to significant improvements in zero-shot scenarios, particularly in table reasoning. Notably, our 3B model surpasses both the 175B GPT-3 and ChatGPT in zero-shot table reasoning across four benchmarks. Furthermore, experimental results on BBH (27 tasks) and MMLU (57 tasks) reveal that language models can be enhanced through symbolic tasks without compromising their generality. We hope that our paper serves as a catalyst, inspiring increased efforts to incorporate symbolic tasks in instruction tuning. ","[{'version': 'v1', 'created': 'Mon, 17 Apr 2023 05:29:42 GMT'}]",2023-04-18,"[['Liu', 'Qian', ''], ['Zhou', 'Fan', ''], ['Jiang', 'Zhengbao', ''], ['Dou', 'Longxu', ''], ['Lin', 'Min', '']]",1,1,2023-04-17,1,5,2,2,0,2,90dd829f3d64dda19092b6e26909803bea5c37c1,258179750.0,https://www.semanticscholar.org/paper/90dd829f3d64dda19092b6e26909803bea5c37c1,arXiv.org,2023.0,65.0,6.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1409707585', 'name': 'Qian Liu'}, {'authorId': '2153433679', 'name': 'Fan Zhou'}, {'authorId': '2669515', 'name': 'Zhengbao Jiang'}, {'authorId': '49093992', 'name': 'Longxu Dou'}, {'authorId': '1491081747', 'name': 'Min Lin'}]","['National University of Singapore', 'Shanghai Jiao Tong University', 'Carnegie Mellon University', 'Applied Research Laboratory at the University of Hawai‘i']","['China', 'United States', 'Singapore']",2023-04
2304.08448,Chong Ma,"Chong Ma, Zihao Wu, Jiaqi Wang, Shaochen Xu, Yaonai Wei, Zhengliang
  Liu, Xi Jiang, Lei Guo, Xiaoyan Cai, Shu Zhang, Tuo Zhang, Dajiang Zhu,
  Dinggang Shen, Tianming Liu, Xiang Li",ImpressionGPT: An Iterative Optimizing Framework for Radiology Report Summarization with ChatGPT,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  The 'Impression' section of a radiology report is a critical basis for communication between radiologists and other physicians, and it is typically written by radiologists based on the 'Findings' section. However, writing numerous impressions can be laborious and error-prone for radiologists. Although recent studies have achieved promising results in automatic impression generation using large-scale medical text data for pre-training and fine-tuning pre-trained language models, such models often require substantial amounts of medical text data and have poor generalization performance. While large language models (LLMs) like ChatGPT have shown strong generalization capabilities and performance, their performance in specific domains, such as radiology, remains under-investigated and potentially limited. To address this limitation, we propose ImpressionGPT, which leverages the in-context learning capability of LLMs by constructing dynamic contexts using domain-specific, individualized data. This dynamic prompt approach enables the model to learn contextual knowledge from semantically similar examples from existing data. Additionally, we design an iterative optimization algorithm that performs automatic evaluation on the generated impression results and composes the corresponding instruction prompts to further optimize the model. The proposed ImpressionGPT model achieves state-of-the-art performance on both MIMIC-CXR and OpenI datasets without requiring additional training data or fine-tuning the LLMs. This work presents a paradigm for localizing LLMs that can be applied in a wide range of similar application scenarios, bridging the gap between general-purpose LLMs and the specific language processing needs of various domains. ","[{'version': 'v1', 'created': 'Mon, 17 Apr 2023 17:13:42 GMT'}, {'version': 'v2', 'created': 'Wed, 3 May 2023 08:09:53 GMT'}]",2023-05-04,"[['Ma', 'Chong', ''], ['Wu', 'Zihao', ''], ['Wang', 'Jiaqi', ''], ['Xu', 'Shaochen', ''], ['Wei', 'Yaonai', ''], ['Liu', 'Zhengliang', ''], ['Jiang', 'Xi', ''], ['Guo', 'Lei', ''], ['Cai', 'Xiaoyan', ''], ['Zhang', 'Shu', ''], ['Zhang', 'Tuo', ''], ['Zhu', 'Dajiang', ''], ['Shen', 'Dinggang', ''], ['Liu', 'Tianming', ''], ['Li', 'Xiang', '']]",1,1,2023-04-17,2,15,2,1,0,1,a7f8fd45fbcdd81449cb7a1a6a2b2c18b38f8151,258180358.0,https://www.semanticscholar.org/paper/a7f8fd45fbcdd81449cb7a1a6a2b2c18b38f8151,arXiv.org,2023.0,92.0,38.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2132543537', 'name': 'Chong Ma'}, {'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '2143074042', 'name': 'Jiaqi Wang'}, {'authorId': '2211904452', 'name': 'Shaochen Xu'}, {'authorId': '2163675860', 'name': 'Yaonai Wei'}, {'authorId': '2145977326', 'name': 'Zheng Liu'}, {'authorId': '143621713', 'name': 'Lei Guo'}, {'authorId': '2174965546', 'name': 'Xiaoya Cai'}, {'authorId': '2108086798', 'name': 'Shu Zhang'}, {'authorId': '49104946', 'name': 'Tuo Zhang'}, {'authorId': '2181182', 'name': 'Dajiang Zhu'}, {'authorId': '2150038187', 'name': 'Dinggang Shen'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}, {'authorId': '2144438902', 'name': 'Xiang Li'}]","['University of Electronic Science and Technology of China', 'ShanghaiTech University', 'The University of Texas at Arlington', 'Shanghai Clinical Research Center', 'Massachusetts General Hospital', 'United Imaging', 'University of Georgia', 'Northwestern Polytechnical University']","['China', 'United States']",2023-04
2304.08782,Minrui Xu,"Minrui Xu, Dusit Niyato, Hongliang Zhang, Jiawen Kang, Zehui Xiong,
  Shiwen Mao, Zhu Han",Sparks of GPTs in Edge Intelligence for Metaverse: Caching and Inference for Mobile AIGC Services,,,,,cs.NI,http://creativecommons.org/licenses/by/4.0/,"  Aiming at achieving artificial general intelligence (AGI) for Metaverse, pretrained foundation models (PFMs), e.g., generative pretrained transformers (GPTs), can effectively provide various AI services, such as autonomous driving, digital twins, and AI-generated content (AIGC) for extended reality. With the advantages of low latency and privacy-preserving, serving PFMs of mobile AI services in edge intelligence is a viable solution for caching and executing PFMs on edge servers with limited computing resources and GPU memory. However, PFMs typically consist of billions of parameters that are computation and memory-intensive for edge servers during loading and execution. In this article, we investigate edge PFM serving problems for mobile AIGC services of Metaverse. First, we introduce the fundamentals of PFMs and discuss their characteristic fine-tuning and inference methods in edge intelligence. Then, we propose a novel framework of joint model caching and inference for managing models and allocating resources to satisfy users' requests efficiently. Furthermore, considering the in-context learning ability of PFMs, we propose a new metric to evaluate the freshness and relevance between examples in demonstrations and executing tasks, namely the Age of Context (AoC). Finally, we propose a least context algorithm for managing cached models at edge servers by balancing the tradeoff among latency, energy consumption, and accuracy. ","[{'version': 'v1', 'created': 'Tue, 18 Apr 2023 07:34:36 GMT'}, {'version': 'v2', 'created': 'Fri, 28 Apr 2023 05:15:28 GMT'}]",2023-05-01,"[['Xu', 'Minrui', ''], ['Niyato', 'Dusit', ''], ['Zhang', 'Hongliang', ''], ['Kang', 'Jiawen', ''], ['Xiong', 'Zehui', ''], ['Mao', 'Shiwen', ''], ['Han', 'Zhu', '']]",0,1,2023-04-18,2,7,1,0,0,0,5b4fbc167ac3d0045e362282745d298af63ae664,258186961.0,https://www.semanticscholar.org/paper/5b4fbc167ac3d0045e362282745d298af63ae664,arXiv.org,2023.0,22.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1454018677', 'name': 'Minrui Xu'}, {'authorId': '1713586', 'name': 'D. Niyato'}, {'authorId': '46702608', 'name': 'Hongliang Zhang'}, {'authorId': '145993626', 'name': 'Jiawen Kang'}, {'authorId': '2943819', 'name': 'Zehui Xiong'}, {'authorId': '2056609621', 'name': 'Shiwen Mao'}, {'authorId': '2114874032', 'name': 'Zhu Han'}]","['University of Houston', 'Singapore University of Technology and Design', 'Auburn University', 'Nanyang Technological University', 'Guangdong University of Technology', 'Peking University', 'Kyung Hee University']","['South Korea', 'United States', 'China', 'Singapore']",2023-04
2304.09138,Xiang Li,"Zihao Wu, Lu Zhang, Chao Cao, Xiaowei Yu, Haixing Dai, Chong Ma,
  Zhengliang Liu, Lin Zhao, Gang Li, Wei Liu, Quanzheng Li, Dinggang Shen,
  Xiang Li, Dajiang Zhu, Tianming Liu",Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Recently, ChatGPT and GPT-4 have emerged and gained immense global attention due to their unparalleled performance in language processing. Despite demonstrating impressive capability in various open-domain tasks, their adequacy in highly specific fields like radiology remains untested. Radiology presents unique linguistic phenomena distinct from open-domain data due to its specificity and complexity. Assessing the performance of large language models (LLMs) in such specific domains is crucial not only for a thorough evaluation of their overall performance but also for providing valuable insights into future model design directions: whether model design should be generic or domain-specific. To this end, in this study, we evaluate the performance of ChatGPT/GPT-4 on a radiology NLI task and compare it to other models fine-tuned specifically on task-related data samples. We also conduct a comprehensive investigation on ChatGPT/GPT-4's reasoning ability by introducing varying levels of inference difficulty. Our results show that 1) GPT-4 outperforms ChatGPT in the radiology NLI task; 2) other specifically fine-tuned models require significant amounts of data samples to achieve comparable performance to ChatGPT/GPT-4. These findings demonstrate that constructing a generic model that is capable of solving various tasks across different domains is feasible. ","[{'version': 'v1', 'created': 'Tue, 18 Apr 2023 17:21:48 GMT'}]",2023-04-19,"[['Wu', 'Zihao', ''], ['Zhang', 'Lu', ''], ['Cao', 'Chao', ''], ['Yu', 'Xiaowei', ''], ['Dai', 'Haixing', ''], ['Ma', 'Chong', ''], ['Liu', 'Zhengliang', ''], ['Zhao', 'Lin', ''], ['Li', 'Gang', ''], ['Liu', 'Wei', ''], ['Li', 'Quanzheng', ''], ['Shen', 'Dinggang', ''], ['Li', 'Xiang', ''], ['Zhu', 'Dajiang', ''], ['Liu', 'Tianming', '']]",1,1,2023-04-18,1,15,1,2,0,2,258605dc5b00fe66b72091f947642a554e472aee,258187362.0,https://www.semanticscholar.org/paper/258605dc5b00fe66b72091f947642a554e472aee,arXiv.org,2023.0,49.0,24.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '2156146823', 'name': 'Lu Zhang'}, {'authorId': '5762440', 'name': 'Chao-Yang Cao'}, {'authorId': '2139760758', 'name': 'Xiao-Xing Yu'}, {'authorId': '29944950', 'name': 'Haixing Dai'}, {'authorId': '120688117', 'name': 'Chong-Yi Ma'}, {'authorId': '2145977326', 'name': 'Zheng Liu'}, {'authorId': '2111641126', 'name': 'Lin Zhao'}, {'authorId': '143736626', 'name': 'Gang Li'}, {'authorId': None, 'name': 'Wei Liu'}, {'authorId': '1762919', 'name': 'Quanzheng Li'}, {'authorId': '2150038187', 'name': 'Dinggang Shen'}, {'authorId': '113075935', 'name': 'Xiang Li'}, {'authorId': '2181182', 'name': 'Dajiang Zhu'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}]","['Mayo Clinic', 'University of North Carolina at Chapel Hill', 'ShanghaiTech University', 'The University of Texas at Arlington', 'Shanghai Clinical Research Center', 'Massachusetts General Hospital', 'United Imaging', 'University of Georgia', 'Northwestern Polytechnical University']","['China', 'United States']",2023-04
2304.09145,Xiuying Wei,"Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong,
  Jinyang Guo, Xianglong Liu",Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Quantization of transformer language models faces significant challenges due to the existence of detrimental outliers in activations. We observe that these outliers are asymmetric and concentrated in specific channels. To address this issue, we propose the Outlier Suppression+ framework. First, we introduce channel-wise shifting and scaling operations to eliminate asymmetric presentation and scale down problematic channels. We demonstrate that these operations can be seamlessly migrated into subsequent modules while maintaining equivalence. Second, we quantitatively analyze the optimal values for shifting and scaling, taking into account both the asymmetric property and quantization errors of weights in the next layer. Our lightweight framework can incur minimal performance degradation under static and standard post-training quantization settings. Comprehensive results across various tasks and models reveal that our approach achieves near-floating-point performance on both small models, such as BERT, and large language models (LLMs) including OPTs, BLOOM, and BLOOMZ at 8-bit and 6-bit settings. Furthermore, we establish a new state of the art for 4-bit BERT. ","[{'version': 'v1', 'created': 'Tue, 18 Apr 2023 17:34:23 GMT'}]",2023-04-19,"[['Wei', 'Xiuying', ''], ['Zhang', 'Yunchen', ''], ['Li', 'Yuhang', ''], ['Zhang', 'Xiangguo', ''], ['Gong', 'Ruihao', ''], ['Guo', 'Jinyang', ''], ['Liu', 'Xianglong', '']]",0,0,2023-04-18,1,7,1,2,2,0,f3275146eb973c9726dc550a88cc552f0dfa5ea7,264424348.0,https://www.semanticscholar.org/paper/f3275146eb973c9726dc550a88cc552f0dfa5ea7,,2023.0,69.0,0.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '14457026', 'name': 'Xiuying Wei'}, {'authorId': '2129510271', 'name': 'Yunchen Zhang'}, {'authorId': '2110539305', 'name': 'Yuhang Li'}, {'authorId': '2156004638', 'name': 'Xiangguo Zhang'}, {'authorId': '152217579', 'name': 'Ruihao Gong'}, {'authorId': '2261414086', 'name': 'Jinyang Guo'}, {'authorId': '2261367071', 'name': 'Xianglong Liu'}, {'authorId': '2261282667', 'name': 'Tom Brown'}, {'authorId': '2056658938', 'name': 'Benjamin Mann'}, {'authorId': '2260406867', 'name': 'Nick Ryder'}, {'authorId': '2065894334', 'name': 'Melanie Subbiah'}, {'authorId': '2053807409', 'name': 'Jared Kaplan'}, {'authorId': '6515819', 'name': 'Prafulla Dhariwal'}, {'authorId': '2072676', 'name': 'Arvind Neelakantan'}, {'authorId': '67311962', 'name': 'Pranav Shyam'}, {'authorId': '144864359', 'name': 'Girish Sastry'}, {'authorId': '2261276556', 'name': 'Yaohui Cai'}, {'authorId': '9088433', 'name': 'Z. Yao'}, {'authorId': '2257809664', 'name': 'Zhen Dong'}, {'authorId': '10419477', 'name': 'A. Gholami'}, {'authorId': '1717098', 'name': 'Michael W. Mahoney'}, {'authorId': '2242659602', 'name': 'Kurt Keutzer'}, {'authorId': '2261258756', 'name': 'Hawq'}, {'authorId': '2357931', 'name': 'Steven K. Esser'}, {'authorId': '46571359', 'name': 'J. McKinstry'}, {'authorId': '2064431971', 'name': 'Deepika Bablani'}, {'authorId': '2257291831', 'name': 'Edward J. Hu'}, {'authorId': '2261329866', 'name': 'Yelong Shen'}, {'authorId': '2261258092', 'name': 'Zeyuan Phillip Wallis'}, {'authorId': '2072796341', 'name': 'Qing Jin'}, {'authorId': '2258296012', 'name': 'Jian Ren'}, {'authorId': '98168954', 'name': 'Richard Zhuang'}, {'authorId': '2261258088', 'name': 'Sumant Hanu-mante'}, {'authorId': '2145366406', 'name': 'Zhengang Li'}, {'authorId': '2119225630', 'name': 'Zhiyu Chen'}, {'authorId': '2261356529', 'name': 'Yanzhi Wang'}, {'authorId': '2118048312', 'name': 'Kai-Min Yang'}, {'authorId': '2261258409', 'name': 'Sergey Tulyakov. 2022'}, {'authorId': '2261257494', 'name': 'F8net'}, {'authorId': '2109586102', 'name': 'Sehoon Kim'}]","['SenseTime Research', 'Beihang University', 'École Polytechnique Fédérale de Lausanne', 'Yale University']","['China', 'United States', 'Switzerland']",2023-04
2304.09972,David Adelani,"David Ifeoluwa Adelani, Marek Masiak, Israel Abebe Azime, Jesujoba
  Alabi, Atnafu Lambebo Tonja, Christine Mwase, Odunayo Ogundepo, Bonaventure
  F. P. Dossou, Akintunde Oladipo, Doreen Nixdorf, Chris Chinenye Emezue, sana
  al-azzawi, Blessing Sibanda, Davis David, Lolwethu Ndolela, Jonathan Mukiibi,
  Tunde Ajayi, Tatiana Moteu, Brian Odhiambo, Abraham Owodunni, Nnaemeka
  Obiefuna, Muhidin Mohamed, Shamsuddeen Hassan Muhammad, Teshome Mulugeta
  Ababu, Saheed Abdullahi Salahudeen, Mesay Gemeda Yigezu, Tajuddeen Gwadabe,
  Idris Abdulmumin, Mahlet Taye, Oluwabusayo Awoyomi, Iyanuoluwa Shode,
  Tolulope Adelani, Habiba Abdulganiyu, Abdul-Hakeem Omotayo, Adetola Adeeko,
  Abeeb Afolabi, Anuoluwapo Aremu, Olanrewaju Samuel, Clemencia Siro, Wangari
  Kimotho, Onyekachi Ogbu, Chinedu Mbonu, Chiamaka Chukwuneke, Samuel Fanijo,
  Jessica Ojo, Oyinkansola Awosan, Tadesse Kebede, Toadoum Sari Sakayo, Pamela
  Nyatsine, Freedmore Sidume, Oreen Yousuf, Mardiyyah Oduwole, Tshinu Tshinu,
  Ussen Kimanuka, Thina Diko, Siyanda Nxakama, Sinodos Nigusse, Abdulmejid
  Johar, Shafie Mohamed, Fuad Mire Hassan, Moges Ahmed Mehamed, Evrard Ngabire,
  Jules Jules, Ivan Ssenkungu and Pontus Stenetorp",MasakhaNEWS: News Topic Classification for African languages,Accepted to IJCNLP-AACL 2023 (main conference),,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  African languages are severely under-represented in NLP research due to lack of datasets covering several NLP tasks. While there are individual language specific datasets that are being expanded to different tasks, only a handful of NLP tasks (e.g. named entity recognition and machine translation) have standardized benchmark datasets covering several geographical and typologically-diverse African languages. In this paper, we develop MasakhaNEWS -- a new benchmark dataset for news topic classification covering 16 languages widely spoken in Africa. We provide an evaluation of baseline models by training classical machine learning models and fine-tuning several language models. Furthermore, we explore several alternatives to full fine-tuning of language models that are better suited for zero-shot and few-shot learning such as cross-lingual parameter-efficient fine-tuning (like MAD-X), pattern exploiting training (PET), prompting language models (like ChatGPT), and prompt-free sentence transformer fine-tuning (SetFit and Cohere Embedding API). Our evaluation in zero-shot setting shows the potential of prompting ChatGPT for news topic classification in low-resource African languages, achieving an average performance of 70 F1 points without leveraging additional supervision like MAD-X. In few-shot setting, we show that with as little as 10 examples per label, we achieved more than 90\% (i.e. 86.0 F1 points) of the performance of full supervised training (92.6 F1 points) leveraging the PET approach. ","[{'version': 'v1', 'created': 'Wed, 19 Apr 2023 21:12:23 GMT'}, {'version': 'v2', 'created': 'Wed, 20 Sep 2023 17:14:40 GMT'}]",2023-09-21,"[['Adelani', 'David Ifeoluwa', ''], ['Masiak', 'Marek', ''], ['Azime', 'Israel Abebe', ''], ['Alabi', 'Jesujoba', ''], ['Tonja', 'Atnafu Lambebo', ''], ['Mwase', 'Christine', ''], ['Ogundepo', 'Odunayo', ''], ['Dossou', 'Bonaventure F. P.', ''], ['Oladipo', 'Akintunde', ''], ['Nixdorf', 'Doreen', ''], ['Emezue', 'Chris Chinenye', ''], ['al-azzawi', 'sana', ''], ['Sibanda', 'Blessing', ''], ['David', 'Davis', ''], ['Ndolela', 'Lolwethu', ''], ['Mukiibi', 'Jonathan', ''], ['Ajayi', 'Tunde', ''], ['Moteu', 'Tatiana', ''], ['Odhiambo', 'Brian', ''], ['Owodunni', 'Abraham', ''], ['Obiefuna', 'Nnaemeka', ''], ['Mohamed', 'Muhidin', ''], ['Muhammad', 'Shamsuddeen Hassan', ''], ['Ababu', 'Teshome Mulugeta', ''], ['Salahudeen', 'Saheed Abdullahi', ''], ['Yigezu', 'Mesay Gemeda', ''], ['Gwadabe', 'Tajuddeen', ''], ['Abdulmumin', 'Idris', ''], ['Taye', 'Mahlet', ''], ['Awoyomi', 'Oluwabusayo', ''], ['Shode', 'Iyanuoluwa', ''], ['Adelani', 'Tolulope', ''], ['Abdulganiyu', 'Habiba', ''], ['Omotayo', 'Abdul-Hakeem', ''], ['Adeeko', 'Adetola', ''], ['Afolabi', 'Abeeb', ''], ['Aremu', 'Anuoluwapo', ''], ['Samuel', 'Olanrewaju', ''], ['Siro', 'Clemencia', ''], ['Kimotho', 'Wangari', ''], ['Ogbu', 'Onyekachi', ''], ['Mbonu', 'Chinedu', ''], ['Chukwuneke', 'Chiamaka', ''], ['Fanijo', 'Samuel', ''], ['Ojo', 'Jessica', ''], ['Awosan', 'Oyinkansola', ''], ['Kebede', 'Tadesse', ''], ['Sakayo', 'Toadoum Sari', ''], ['Nyatsine', 'Pamela', ''], ['Sidume', 'Freedmore', ''], ['Yousuf', 'Oreen', ''], ['Oduwole', 'Mardiyyah', ''], ['Tshinu', 'Tshinu', ''], ['Kimanuka', 'Ussen', ''], ['Diko', 'Thina', ''], ['Nxakama', 'Siyanda', ''], ['Nigusse', 'Sinodos', ''], ['Johar', 'Abdulmejid', ''], ['Mohamed', 'Shafie', ''], ['Hassan', 'Fuad Mire', ''], ['Mehamed', 'Moges Ahmed', ''], ['Ngabire', 'Evrard', ''], ['Jules', 'Jules', ''], ['Ssenkungu', 'Ivan', ''], ['Stenetorp', 'Pontus', '']]",1,1,2023-04-19,2,65,1,2,0,2,d847ab7f4109d0a4c640d5ee34b510a76002fddb,258236351.0,https://www.semanticscholar.org/paper/d847ab7f4109d0a4c640d5ee34b510a76002fddb,AfricaNLP,2023.0,68.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2518906', 'name': 'David Ifeoluwa Adelani'}, {'authorId': '2091150316', 'name': 'Marek Masiak'}, {'authorId': '2052358080', 'name': 'Israel Abebe Azime'}, {'authorId': '122367036', 'name': 'Jesujoba Oluwadara Alabi'}, {'authorId': '2148631756', 'name': 'A. Tonja'}, {'authorId': '35882251', 'name': 'Christine Mwase'}, {'authorId': '2166106776', 'name': 'Odunayo Ogundepo'}, {'authorId': '1591111757', 'name': 'Bonaventure F. P. Dossou'}, {'authorId': '2175480812', 'name': 'Akintunde Oladipo'}, {'authorId': '2214804027', 'name': 'Doreen Nixdorf'}, {'authorId': '1591176064', 'name': 'Chris C. Emezue'}, {'authorId': '2094432651', 'name': 'Sana Al-Azzawi'}, {'authorId': '1591123568', 'name': 'Blessing K. Sibanda'}, {'authorId': '2058260775', 'name': 'Davis David'}, {'authorId': '2214814174', 'name': 'Lolwethu Ndolela'}, {'authorId': '1412684911', 'name': 'Jonathan Mukiibi'}, {'authorId': '98725872', 'name': 'T. Ajayi'}, {'authorId': '2188733769', 'name': 'Tatiana Moteu Ngoli'}, {'authorId': '98877434', 'name': 'B. Odhiambo'}, {'authorId': '2188832804', 'name': 'A. Owodunni'}, {'authorId': '2214807035', 'name': 'Nnaemeka C. Obiefuna'}, {'authorId': '7744881', 'name': 'Shamsuddeen Hassan Muhammad'}, {'authorId': '1492068759', 'name': 'S. Abdullahi'}, {'authorId': '2148736370', 'name': 'Mesay Gemeda Yigezu'}, {'authorId': '2352354', 'name': 'T. Gwadabe'}, {'authorId': '1429833598', 'name': 'Idris Abdulmumin'}, {'authorId': '2214807032', 'name': 'Mahlet Taye Bame'}, {'authorId': '2190104797', 'name': 'Oluwabusayo Olufunke Awoyomi'}, {'authorId': '2163094493', 'name': 'Iyanuoluwa Shode'}, {'authorId': '2214806255', 'name': 'T. Adelani'}, {'authorId': '2214814170', 'name': 'Habiba Abdulganiy Kailani'}, {'authorId': '2212894510', 'name': 'Abdul-Hakeem Omotayo'}, {'authorId': '2214804172', 'name': 'Adetola Adeeko'}, {'authorId': '2214809191', 'name': 'Afolabi Abeeb'}, {'authorId': '2056773747', 'name': 'Anuoluwapo Aremu'}, {'authorId': '2164156047', 'name': 'Olanrewaju Samuel'}, {'authorId': '2056776870', 'name': 'Clemencia Siro'}, {'authorId': '2214806253', 'name': 'Wangari Kimotho'}, {'authorId': '2214814462', 'name': 'Onyekachi Raphael Ogbu'}, {'authorId': '2158396538', 'name': 'Chinedu E. Mbonu'}, {'authorId': '73054967', 'name': 'C. Chukwuneke'}, {'authorId': '2214305954', 'name': 'Samuel Fanijo'}, {'authorId': '2214806280', 'name': 'Jessica Ojo'}, {'authorId': '2214814443', 'name': 'Oyinkansola F. Awosan'}, {'authorId': '2214804214', 'name': 'Tadesse Kebede Guge'}, {'authorId': '2214809578', 'name': 'Sakayo Toadoum Sari'}, {'authorId': '2214806244', 'name': 'Pamela Nyatsine'}, {'authorId': '146750684', 'name': 'Freedmore Sidume'}, {'authorId': '2164122749', 'name': 'Oreen Yousuf'}, {'authorId': '2214280653', 'name': 'Mardiyyah Oduwole'}, {'authorId': '2038247196', 'name': 'Ussen Kimanuka'}, {'authorId': '2214809343', 'name': 'Kanda Patrick Tshinu'}, {'authorId': '2214809340', 'name': 'Thina Diko'}, {'authorId': '2214809338', 'name': 'Siyanda Nxakama'}, {'authorId': '2214806574', 'name': 'Abdulmejid Tuni Johar'}, {'authorId': '2214809336', 'name': 'Sinodos Gebre'}, {'authorId': '47302320', 'name': 'Muhidin A. Mohamed'}, {'authorId': '2192964958', 'name': 'Shafie Abdi Mohamed'}, {'authorId': '2034285229', 'name': 'Fuad Mire Hassan'}, {'authorId': '2212794028', 'name': 'Moges Ahmed Mehamed'}, {'authorId': '2214804211', 'name': 'Evrard Ngabire'}, {'authorId': '1918552', 'name': 'Pontus Stenetorp'}]","['Insight Centre for Data Analytics, Ireland,', 'Ahmadu Bello University', 'University of California, Davis', 'Somali National University', 'National Open University of Nigeria', 'Jamhuriya University of Science and Technology', 'Technical University of Munich', 'Kaduna State University', 'Paderborn University', 'University of Waterloo', 'Aston University', 'Portuguese Environment Agency', 'College of Saint Rose', 'Lancaster University', 'Botswana International University of Science and Technology', 'Iowa State University', 'Instituto Politécnico Nacional', 'University College London', 'Luleå University of Technology', 'Makerere University', 'Nnamdi Azikiwe University', 'University of Porto', 'Tanzania Data Lab, Tanzania', 'University of Amsterdam', 'Haramaya University', 'Deutschzentrum an der Universität Burundi, Ethiopia', 'Montclair State University', 'Wuhan University of Technology', 'McGill University', 'University of Rwanda', 'Universidad Nacional de La Plata', 'Saarland University', 'Mila Quebec AI Institute, Canada,', 'Fudan University', 'Dire Dawa University', 'Food Research Institute']","['Germany', 'Netherlands', 'Argentina', 'Sweden', 'United Kingdom', 'Canada', 'Nigeria', 'Ethiopia', 'Mexico', 'Uganda', 'Tanzania', 'Botswana', 'Portugal', 'China', 'Ireland', 'Rwanda', 'Somalia', 'United States', 'Japan']",2023-04
2304.11107,Xiang Li,"Tianyang Zhong, Yaonai Wei, Li Yang, Zihao Wu, Zhengliang Liu,
  Xiaozheng Wei, Wenjun Li, Junjie Yao, Chong Ma, Xiang Li, Dajiang Zhu, Xi
  Jiang, Junwei Han, Dinggang Shen, Tianming Liu, Tuo Zhang",ChatABL: Abductive Learning via Natural Language Interaction with ChatGPT,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) such as ChatGPT have recently demonstrated significant potential in mathematical abilities, providing valuable reasoning paradigm consistent with human natural language. However, LLMs currently have difficulty in bridging perception, language understanding and reasoning capabilities due to incompatibility of the underlying information flow among them, making it challenging to accomplish tasks autonomously. On the other hand, abductive learning (ABL) frameworks for integrating the two abilities of perception and reasoning has seen significant success in inverse decipherment of incomplete facts, but it is limited by the lack of semantic understanding of logical reasoning rules and the dependence on complicated domain knowledge representation. This paper presents a novel method (ChatABL) for integrating LLMs into the ABL framework, aiming at unifying the three abilities in a more user-friendly and understandable manner. The proposed method uses the strengths of LLMs' understanding and logical reasoning to correct the incomplete logical facts for optimizing the performance of perceptual module, by summarizing and reorganizing reasoning rules represented in natural language format. Similarly, perceptual module provides necessary reasoning examples for LLMs in natural language format. The variable-length handwritten equation deciphering task, an abstract expression of the Mayan calendar decoding, is used as a testbed to demonstrate that ChatABL has reasoning ability beyond most existing state-of-the-art methods, which has been well supported by comparative studies. To our best knowledge, the proposed ChatABL is the first attempt to explore a new pattern for further approaching human-level cognitive ability via natural language interaction with ChatGPT. ","[{'version': 'v1', 'created': 'Fri, 21 Apr 2023 16:23:47 GMT'}]",2023-04-24,"[['Zhong', 'Tianyang', ''], ['Wei', 'Yaonai', ''], ['Yang', 'Li', ''], ['Wu', 'Zihao', ''], ['Liu', 'Zhengliang', ''], ['Wei', 'Xiaozheng', ''], ['Li', 'Wenjun', ''], ['Yao', 'Junjie', ''], ['Ma', 'Chong', ''], ['Li', 'Xiang', ''], ['Zhu', 'Dajiang', ''], ['Jiang', 'Xi', ''], ['Han', 'Junwei', ''], ['Shen', 'Dinggang', ''], ['Liu', 'Tianming', ''], ['Zhang', 'Tuo', '']]",1,1,2023-04-21,1,16,2,1,0,1,4c8ef2db0c77aba453783f5211ebafc6695d3835,258291600.0,https://www.semanticscholar.org/paper/4c8ef2db0c77aba453783f5211ebafc6695d3835,arXiv.org,2023.0,69.0,18.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2215167446', 'name': 'Tianyang Zhong'}, {'authorId': '2163675860', 'name': 'Yaonai Wei'}, {'authorId': '2153206781', 'name': 'Li Yang'}, {'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '2145977326', 'name': 'Zheng Liu'}, {'authorId': '2202921386', 'name': 'Xiaozheng Wei'}, {'authorId': '2108956293', 'name': 'WenJu Sun'}, {'authorId': '1418690977', 'name': 'Junjie Yao'}, {'authorId': '2108624783', 'name': 'Chongfei Ma'}, {'authorId': '2144438902', 'name': 'Xiang Li'}, {'authorId': '2181182', 'name': 'Dajiang Zhu'}, {'authorId': '143796247', 'name': 'Xi Jiang'}, {'authorId': '2161945158', 'name': 'Jun-Feng Han'}, {'authorId': '2150038187', 'name': 'Dinggang Shen'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}, {'authorId': '49104946', 'name': 'Tuo Zhang'}]","['University of Electronic Science and Technology of China', 'ShanghaiTech University', 'The University of Texas at Arlington', 'Shanghai Clinical Research Center', 'Massachusetts General Hospital', 'United Imaging', 'University of Georgia', 'Northwestern Polytechnical University']","['China', 'United States']",2023-04
2304.11276,Hengky Susanto,"Hengky Susanto, David James Woo, and Kai Guo",The Role of AI in Human-AI Creative Writing for Hong Kong Secondary Students,,"International Council of Teachers of English (ICTE) Newsletter
  (Spring 2023)",,,cs.CL,http://creativecommons.org/publicdomain/zero/1.0/,"  The recent advancement in Natural Language Processing (NLP) capability has led to the development of language models (e.g., ChatGPT) that is capable of generating human-like language. In this study, we explore how language models can be utilized to help the ideation aspect of creative writing. Our empirical findings show that language models play different roles in helping student writers to be more creative, such as the role of a collaborator, a provocateur, etc ","[{'version': 'v1', 'created': 'Fri, 21 Apr 2023 23:50:09 GMT'}]",2023-04-25,"[['Susanto', 'Hengky', ''], ['Woo', 'David James', ''], ['Guo', 'Kai', '']]",1,1,2023-04-21,1,3,1,1,0,1,c1c60be1c4c335564556c25ec901cd7028a880de,258298169.0,https://www.semanticscholar.org/paper/c1c60be1c4c335564556c25ec901cd7028a880de,arXiv.org,2023.0,3.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Education', 'source': 's2-fos-model'}]","[{'authorId': '3353735', 'name': 'Hengky Susanto'}, {'authorId': '12132198', 'name': 'D. Woo'}, {'authorId': '2061502406', 'name': 'Kai Guo'}]","['University of Massachusetts Lowell', 'Precious Blood Secondary School, Hong Kong, China', 'University of Hong Kong']","['China', 'United States', 'Hong Kong']",2023-04
2304.11567,Wenxiong Liao,"Wenxiong Liao, Zhengliang Liu, Haixing Dai, Shaochen Xu, Zihao Wu,
  Yiyang Zhang, Xiaoke Huang, Dajiang Zhu, Hongmin Cai, Tianming Liu, Xiang Li",Differentiate ChatGPT-generated and Human-written Medical Texts,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Background: Large language models such as ChatGPT are capable of generating grammatically perfect and human-like text content, and a large number of ChatGPT-generated texts have appeared on the Internet. However, medical texts such as clinical notes and diagnoses require rigorous validation, and erroneous medical content generated by ChatGPT could potentially lead to disinformation that poses significant harm to healthcare and the general public.   Objective: This research is among the first studies on responsible and ethical AIGC (Artificial Intelligence Generated Content) in medicine. We focus on analyzing the differences between medical texts written by human experts and generated by ChatGPT, and designing machine learning workflows to effectively detect and differentiate medical texts generated by ChatGPT.   Methods: We first construct a suite of datasets containing medical texts written by human experts and generated by ChatGPT. In the next step, we analyze the linguistic features of these two types of content and uncover differences in vocabulary, part-of-speech, dependency, sentiment, perplexity, etc. Finally, we design and implement machine learning methods to detect medical text generated by ChatGPT.   Results: Medical texts written by humans are more concrete, more diverse, and typically contain more useful information, while medical texts generated by ChatGPT pay more attention to fluency and logic, and usually express general terminologies rather than effective information specific to the context of the problem. A BERT-based model can effectively detect medical texts generated by ChatGPT, and the F1 exceeds 95%. ","[{'version': 'v1', 'created': 'Sun, 23 Apr 2023 07:38:07 GMT'}]",2023-04-25,"[['Liao', 'Wenxiong', ''], ['Liu', 'Zhengliang', ''], ['Dai', 'Haixing', ''], ['Xu', 'Shaochen', ''], ['Wu', 'Zihao', ''], ['Zhang', 'Yiyang', ''], ['Huang', 'Xiaoke', ''], ['Zhu', 'Dajiang', ''], ['Cai', 'Hongmin', ''], ['Liu', 'Tianming', ''], ['Li', 'Xiang', '']]",1,1,2023-04-23,1,11,2,1,0,1,286756b2b02d6a7bc49a7ad66686f30831f26c25,258298336.0,https://www.semanticscholar.org/paper/286756b2b02d6a7bc49a7ad66686f30831f26c25,arXiv.org,2023.0,43.0,32.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2036578588', 'name': 'Wenxiong Liao'}, {'authorId': '2145977326', 'name': 'Zheng Liu'}, {'authorId': '29944950', 'name': 'Haixing Dai'}, {'authorId': '2211904452', 'name': 'Shaochen Xu'}, {'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '2108362387', 'name': 'Yiyang Zhang'}, {'authorId': None, 'name': 'Xiaoke Huang'}, {'authorId': '2181182', 'name': 'Dajiang Zhu'}, {'authorId': '2209990831', 'name': 'Hongmin Cai'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}, {'authorId': '2144438902', 'name': 'Xiang Li'}]","['South China University of Technology', 'University of Georgia', 'Massachusetts General Hospital', 'The University of Texas at Arlington']","['China', 'United States']",2023-04
2304.11633,Bo Li,"Bo Li, Gexiang Fang, Yang Yang, Quansen Wang, Wei Ye, Wen Zhao, Shikun
  Zhang","Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The capability of Large Language Models (LLMs) like ChatGPT to comprehend user intent and provide reasonable responses has made them extremely popular lately. In this paper, we focus on assessing the overall ability of ChatGPT using 7 fine-grained information extraction (IE) tasks. Specially, we present the systematically analysis by measuring ChatGPT's performance, explainability, calibration, and faithfulness, and resulting in 15 keys from either the ChatGPT or domain experts. Our findings reveal that ChatGPT's performance in Standard-IE setting is poor, but it surprisingly exhibits excellent performance in the OpenIE setting, as evidenced by human evaluation. In addition, our research indicates that ChatGPT provides high-quality and trustworthy explanations for its decisions. However, there is an issue of ChatGPT being overconfident in its predictions, which resulting in low calibration. Furthermore, ChatGPT demonstrates a high level of faithfulness to the original text in the majority of cases. We manually annotate and release the test sets of 7 fine-grained IE tasks contains 14 datasets to further promote the research. The datasets and code are available at https://github.com/pkuserc/ChatGPT_for_IE. ","[{'version': 'v1', 'created': 'Sun, 23 Apr 2023 12:33:18 GMT'}]",2023-04-25,"[['Li', 'Bo', ''], ['Fang', 'Gexiang', ''], ['Yang', 'Yang', ''], ['Wang', 'Quansen', ''], ['Ye', 'Wei', ''], ['Zhao', 'Wen', ''], ['Zhang', 'Shikun', '']]",1,1,2023-04-23,1,7,1,1,0,1,88abef771472c3aa46c53d5d626a0d0c3b66e8cd,258297899.0,https://www.semanticscholar.org/paper/88abef771472c3aa46c53d5d626a0d0c3b66e8cd,arXiv.org,2023.0,101.0,52.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2485552', 'name': 'Bo Li'}, {'authorId': '2215216578', 'name': 'Gexiang Fang'}, {'authorId': '2152917615', 'name': 'Yang Yang'}, {'authorId': '117898431', 'name': 'Quansen Wang'}, {'authorId': '145235149', 'name': 'Wei Ye'}, {'authorId': '2118223372', 'name': 'Wen Zhao'}, {'authorId': '1705434', 'name': 'Shikun Zhang'}]","['Peking University', 'Boston University']","['China', 'United States']",2023-04
2304.12298,Jiawen Shi,"Jiawen Shi, Yixin Liu, Pan Zhou and Lichao Sun",BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT,This paper is accepted as a poster in NDSS2023,,,,cs.CR cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, ChatGPT has gained significant attention in research due to its ability to interact with humans effectively. The core idea behind this model is reinforcement learning (RL) fine-tuning, a new paradigm that allows language models to align with human preferences, i.e., InstructGPT. In this study, we propose BadGPT, the first backdoor attack against RL fine-tuning in language models. By injecting a backdoor into the reward model, the language model can be compromised during the fine-tuning stage. Our initial experiments on movie reviews, i.e., IMDB, demonstrate that an attacker can manipulate the generated text through BadGPT. ","[{'version': 'v1', 'created': 'Tue, 21 Feb 2023 08:59:22 GMT'}]",2023-04-25,"[['Shi', 'Jiawen', ''], ['Liu', 'Yixin', ''], ['Zhou', 'Pan', ''], ['Sun', 'Lichao', '']]",1,1,2023-02-21,1,4,2,2,0,2,c50b9b97227971f5ed7a8fc0e5a1e45c6b4cb5c0,258298999.0,https://www.semanticscholar.org/paper/c50b9b97227971f5ed7a8fc0e5a1e45c6b4cb5c0,arXiv.org,2023.0,7.0,14.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2117867166', 'name': 'Jiawen Shi'}, {'authorId': None, 'name': 'Yixin Liu'}, {'authorId': '2153245185', 'name': 'Pan Zhou'}, {'authorId': '49755259', 'name': 'Lichao Sun'}]","['Huazhong University of Science and Technology', 'Lehigh University']","['China', 'United States']",2023-02
2304.12562,Zhang Jianzhang,"Jianzhang Zhang, Yiyang Chen, Nan Niu, Yinglin Wang, Chuang Liu",Empirical Evaluation of ChatGPT on Requirements Information Retrieval Under Zero-Shot Setting,,,,,cs.SE cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Recently, various illustrative examples have shown the impressive ability of generative large language models (LLMs) to perform NLP related tasks. ChatGPT undoubtedly is the most representative model. We empirically evaluate ChatGPT's performance on requirements information retrieval (IR) tasks to derive insights into designing or developing more effective requirements retrieval methods or tools based on generative LLMs. We design an evaluation framework considering four different combinations of two popular IR tasks and two common artifact types. Under zero-shot setting, evaluation results reveal ChatGPT's promising ability to retrieve requirements relevant information (high recall) and limited ability to retrieve more specific requirements information (low precision). Our evaluation of ChatGPT on requirements IR under zero-shot setting provides preliminary evidence for designing or developing more effective requirements IR methods or tools based on LLMs. ","[{'version': 'v1', 'created': 'Tue, 25 Apr 2023 04:09:45 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Jul 2023 08:28:45 GMT'}]",2023-07-20,"[['Zhang', 'Jianzhang', ''], ['Chen', 'Yiyang', ''], ['Niu', 'Nan', ''], ['Wang', 'Yinglin', ''], ['Liu', 'Chuang', '']]",1,1,2023-04-25,2,5,2,1,0,1,34dd0b6866b99758b3b5d6f9d1d1c9fcf68d4b91,259983112.0,https://www.semanticscholar.org/paper/34dd0b6866b99758b3b5d6f9d1d1c9fcf68d4b91,,2023.0,66.0,1.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '18148663', 'name': 'Jianzhang Zhang'}, {'authorId': '2190793159', 'name': 'Yiyang Chen'}, {'authorId': '2793310', 'name': 'Nan Niu'}, {'authorId': '2143404491', 'name': 'Yinglin Wang'}, {'authorId': '2188235440', 'name': 'Chuang Liu'}]","['Shanghai University of Finance and Economics', 'Hangzhou Normal University', 'University of Cincinnati']","['China', 'United States']",2023-04
2304.12995,Rongjie Huang,"Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang,
  Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou
  Zhao, Shinji Watanabe","AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head",,,,,cs.CL cs.AI cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. Our system is publicly available at \url{https://github.com/AIGC-Audio/AudioGPT}. ","[{'version': 'v1', 'created': 'Tue, 25 Apr 2023 17:05:38 GMT'}]",2023-04-26,"[['Huang', 'Rongjie', ''], ['Li', 'Mingze', ''], ['Yang', 'Dongchao', ''], ['Shi', 'Jiatong', ''], ['Chang', 'Xuankai', ''], ['Ye', 'Zhenhui', ''], ['Wu', 'Yuning', ''], ['Hong', 'Zhiqing', ''], ['Huang', 'Jiawei', ''], ['Liu', 'Jinglin', ''], ['Ren', 'Yi', ''], ['Zhao', 'Zhou', ''], ['Watanabe', 'Shinji', '']]",1,1,2023-04-25,1,13,4,1,0,1,8bc617c9139648d7a92991d70c671230bac7b2e2,258309430.0,https://www.semanticscholar.org/paper/8bc617c9139648d7a92991d70c671230bac7b2e2,arXiv.org,2023.0,46.0,57.0,7.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2048021099', 'name': 'Rongjie Huang'}, {'authorId': '2112108864', 'name': 'Mingze Li'}, {'authorId': '1752879605', 'name': 'Dongchao Yang'}, {'authorId': '1485531923', 'name': 'Jiatong Shi'}, {'authorId': '8776560', 'name': 'Xuankai Chang'}, {'authorId': '1704406169', 'name': 'Zhenhui Ye'}, {'authorId': '2166452418', 'name': 'Yuning Wu'}, {'authorId': '2215274532', 'name': 'Zhiqing Hong'}, {'authorId': '3068086', 'name': 'Jia-Bin Huang'}, {'authorId': '48211720', 'name': 'Jinglin Liu'}, {'authorId': '2165186676', 'name': 'Yixiang Ren'}, {'authorId': '2156163667', 'name': 'Zhou Zhao'}, {'authorId': '1746678', 'name': 'Shinji Watanabe'}]","['Carnegie Mellon University', 'Zhejiang University']","['China', 'United States']",2023-04
2304.13276,Tianyi Zhou,"Shuai Li, Zhao Song, Yu Xia, Tong Yu, Tianyi Zhou",The Closeness of In-Context Learning and Weight Shifting for Softmax Regression,,,,,cs.CL cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Large language models (LLMs) are known for their exceptional performance in natural language processing, making them highly effective in many human life-related or even job-related tasks. The attention mechanism in the Transformer architecture is a critical component of LLMs, as it allows the model to selectively focus on specific input parts. The softmax unit, which is a key part of the attention mechanism, normalizes the attention scores. Hence, the performance of LLMs in various NLP tasks depends significantly on the crucial role played by the attention mechanism with the softmax unit.   In-context learning, as one of the celebrated abilities of recent LLMs, is an important concept in querying LLMs such as ChatGPT. Without further parameter updates, Transformers can learn to predict based on few in-context examples. However, the reason why Transformers becomes in-context learners is not well understood. Recently, several works [ASA+22,GTLV22,ONR+22] have studied the in-context learning from a mathematical perspective based on a linear regression formulation $\min_x\| Ax - b \|_2$, which show Transformers' capability of learning linear functions in context.   In this work, we study the in-context learning based on a softmax regression formulation $\min_{x} \| \langle \exp(Ax), {\bf 1}_n \rangle^{-1} \exp(Ax) - b \|_2$ of Transformer's attention mechanism. We show the upper bounds of the data transformations induced by a single self-attention layer and by gradient-descent on a $\ell_2$ regression loss for softmax prediction function, which imply that when training self-attention-only Transformers for fundamental regression tasks, the models learned by gradient-descent and Transformers show great similarity. ","[{'version': 'v1', 'created': 'Wed, 26 Apr 2023 04:33:41 GMT'}]",2023-04-27,"[['Li', 'Shuai', ''], ['Song', 'Zhao', ''], ['Xia', 'Yu', ''], ['Yu', 'Tong', ''], ['Zhou', 'Tianyi', '']]",1,1,2023-04-26,1,5,2,1,0,1,a569b9daa3606952dbcfdaa310ddfe6ad4eb95f3,258331729.0,https://www.semanticscholar.org/paper/a569b9daa3606952dbcfdaa310ddfe6ad4eb95f3,arXiv.org,2023.0,60.0,21.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1491648207', 'name': 'Shuai Li'}, {'authorId': '2214956470', 'name': 'Zhao Song'}, {'authorId': '2111273611', 'name': 'Yu Xia'}, {'authorId': '2148832584', 'name': 'Tong Yu'}, {'authorId': '2213956781', 'name': 'Tianyi Zhou'}]","[""Xi'an Jiaotong University"", 'University of California, San Diego']","['China', 'United States']",2023-04
2304.13712,Hongye Jin,"Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng,
  Haoming Jiang, Bing Yin, Xia Hu",Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at \url{https://github.com/Mooler0410/LLMsPracticalGuide}. ","[{'version': 'v1', 'created': 'Wed, 26 Apr 2023 17:52:30 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Apr 2023 17:56:11 GMT'}]",2023-04-28,"[['Yang', 'Jingfeng', ''], ['Jin', 'Hongye', ''], ['Tang', 'Ruixiang', ''], ['Han', 'Xiaotian', ''], ['Feng', 'Qizhang', ''], ['Jiang', 'Haoming', ''], ['Yin', 'Bing', ''], ['Hu', 'Xia', '']]",1,1,2023-04-26,2,8,3,1,0,1,131c6f328c11706de2c43cd16e0b7c5d5e610b6a,258331833.0,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,arXiv.org,2023.0,133.0,92.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '7788583', 'name': 'Jingfeng Yang'}, {'authorId': '1791983892', 'name': 'Hongye Jin'}, {'authorId': '2057059798', 'name': 'Ruixiang Tang'}, {'authorId': '50017230', 'name': 'Xiaotian Han'}, {'authorId': '2151233715', 'name': 'Qizhang Feng'}, {'authorId': '5795999', 'name': 'Haoming Jiang'}, {'authorId': '2021632793', 'name': 'Bing Yin'}, {'authorId': '2193021044', 'name': 'Xia Hu'}]","['Rice University', 'Texas A&M University', 'Amazon', 'USA; Xia Hu,', 'Shanghai University']","['China', 'United States']",2023-04
2305.00201,Yuzhong Chen,"Zhenxiang Xiao, Yuzhong Chen, Lu Zhang, Junjie Yao, Zihao Wu, Xiaowei
  Yu, Yi Pan, Lin Zhao, Chong Ma, Xinyu Liu, Wei Liu, Xiang Li, Yixuan Yuan,
  Dinggang Shen, Dajiang Zhu, Tianming Liu, Xi Jiang",Instruction-ViT: Multi-Modal Prompts for Instruction Learning in ViT,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Prompts have been proven to play a crucial role in large language models, and in recent years, vision models have also been using prompts to improve scalability for multiple downstream tasks. In this paper, we focus on adapting prompt design based on instruction tuning into a visual transformer model for image classification which we called Instruction-ViT. The key idea is to implement multi-modal prompts (text or image prompt) related to category information to guide the fine-tuning of the model. Based on the experiments of several image captionining tasks, the performance and domain adaptability were improved. Our work provided an innovative strategy to fuse multi-modal prompts with better performance and faster adaptability for visual classification models. ","[{'version': 'v1', 'created': 'Sat, 29 Apr 2023 08:59:12 GMT'}]",2023-05-02,"[['Xiao', 'Zhenxiang', ''], ['Chen', 'Yuzhong', ''], ['Zhang', 'Lu', ''], ['Yao', 'Junjie', ''], ['Wu', 'Zihao', ''], ['Yu', 'Xiaowei', ''], ['Pan', 'Yi', ''], ['Zhao', 'Lin', ''], ['Ma', 'Chong', ''], ['Liu', 'Xinyu', ''], ['Liu', 'Wei', ''], ['Li', 'Xiang', ''], ['Yuan', 'Yixuan', ''], ['Shen', 'Dinggang', ''], ['Zhu', 'Dajiang', ''], ['Liu', 'Tianming', ''], ['Jiang', 'Xi', '']]",0,0,2023-04-29,1,17,1,0,0,0,a677938545f63ad44c87d09f85dd231980a8476f,258426716.0,https://www.semanticscholar.org/paper/a677938545f63ad44c87d09f85dd231980a8476f,arXiv.org,2023.0,49.0,6.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2116552919', 'name': 'Zhe Xiao'}, {'authorId': '2121472807', 'name': 'Yuzhong Chen'}, {'authorId': '2156146823', 'name': 'Lu Zhang'}, {'authorId': '145946111', 'name': 'Jun Yao'}, {'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '2139760758', 'name': 'Xiao-Xing Yu'}, {'authorId': '19308245', 'name': 'Yirong Pan'}, {'authorId': '2111641126', 'name': 'Lin Zhao'}, {'authorId': '2116896792', 'name': 'Chonghe Ma'}, {'authorId': '2110789401', 'name': 'Xinyu Liu'}, {'authorId': '46641573', 'name': 'W. Liu'}, {'authorId': '2144438902', 'name': 'Xiang Li'}, {'authorId': '3080513', 'name': 'Yixuan Yuan'}, {'authorId': '2150038187', 'name': 'Dinggang Shen'}, {'authorId': '2181182', 'name': 'Dajiang Zhu'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}, {'authorId': '143796247', 'name': 'Xi Jiang'}]","['University of Electronic Science and Technology of China', 'Shanghai United Imaging Intel-ligence Co., Ltd., Shanghai 200230, China 11 Shanghai Clinical Research and Trial Center, Shanghai, 201210, China.', 'Mayo Clinic in Arizona', 'ShanghaiTech University', 'Chinese University of Hong Kong', 'Massachusetts General Hospital', 'The University of Texas at Arlington', 'University of Georgia', 'Northwestern Polytechnical University']","['China', 'United States']",2023-04
2305.02556,Ruixin Hong,"Ruixin Hong, Hongming Zhang, Hong Zhao, Dong Yu, Changshui Zhang",Faithful Question Answering with Monte-Carlo Planning,ACL 2023 main,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although large language models demonstrate remarkable question-answering performances, revealing the intermediate reasoning steps that the models faithfully follow remains challenging. In this paper, we propose FAME (FAithful question answering with MontE-carlo planning) to answer questions based on faithful reasoning steps. The reasoning steps are organized as a structured entailment tree, which shows how premises are used to produce intermediate conclusions that can prove the correctness of the answer. We formulate the task as a discrete decision-making problem and solve it through the interaction of a reasoning environment and a controller. The environment is modular and contains several basic task-oriented modules, while the controller proposes actions to assemble the modules. Since the search space could be large, we introduce a Monte-Carlo planning algorithm to do a look-ahead search and select actions that will eventually lead to high-quality steps. FAME achieves state-of-the-art performance on the standard benchmark. It can produce valid and faithful reasoning steps compared with large language models with a much smaller model size. ","[{'version': 'v1', 'created': 'Thu, 4 May 2023 05:21:36 GMT'}]",2023-05-05,"[['Hong', 'Ruixin', ''], ['Zhang', 'Hongming', ''], ['Zhao', 'Hong', ''], ['Yu', 'Dong', ''], ['Zhang', 'Changshui', '']]",0,0,2023-05-04,1,5,1,0,0,0,198320a8c26f01e1d48fb1cd385900a7f374d609,258479954.0,https://www.semanticscholar.org/paper/198320a8c26f01e1d48fb1cd385900a7f374d609,Annual Meeting of the Association for Computational Linguistics,2023.0,57.0,2.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2151067405', 'name': 'Ruixin Hong'}, {'authorId': '49723569', 'name': 'Hongming Zhang'}, {'authorId': '2108132432', 'name': 'Honghui Zhao'}, {'authorId': '144580027', 'name': 'Dong Yu'}, {'authorId': '2164309584', 'name': 'Changshui Zhang'}]","['Tencent', 'Tsinghua University', 'Research Organization for Information Science and Technology']","['China', 'United States', 'Japan']",2023-05
2305.03111,Binyuan Hui,"Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li,
  Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo, Xuanhe Zhou,
  Chenhao Ma, Guoliang Li, Kevin C.C. Chang, Fei Huang, Reynold Cheng, Yongbin
  Li",Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text-to-SQL parsing, which aims at converting natural language instructions into executable SQLs, has gained increasing attention in recent years. In particular, Codex and ChatGPT have shown impressive results in this task. However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on database schema with few rows of database contents leaving the gap between academic study and real-world applications. To mitigate this gap, we present Bird, a big benchmark for large-scale database grounded in text-to-SQL tasks, containing 12,751 pairs of text-to-SQL data and 95 databases with a total size of 33.4 GB, spanning 37 professional domains. Our emphasis on database values highlights the new challenges of dirty database contents, external knowledge between NL questions and database contents, and SQL efficiency, particularly in the context of massive databases. To solve these problems, text-to-SQL models must feature database value comprehension in addition to semantic parsing. The experimental results demonstrate the significance of database values in generating accurate text-to-SQLs for big databases. Furthermore, even the most effective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution accuracy, which is still far from the human result of 92.96%, proving that challenges still stand. Besides, we also provide an efficiency analysis to offer insights into generating text-to-efficient-SQLs that are beneficial to industries. We believe that BIRD will contribute to advancing real-world applications of text-to-SQL research. The leaderboard and source code are available: https://bird-bench.github.io/. ","[{'version': 'v1', 'created': 'Thu, 4 May 2023 19:02:29 GMT'}, {'version': 'v2', 'created': 'Tue, 30 May 2023 05:34:57 GMT'}]",2023-05-31,"[['Li', 'Jinyang', ''], ['Hui', 'Binyuan', ''], ['Qu', 'Ge', ''], ['Li', 'Binhua', ''], ['Yang', 'Jiaxi', ''], ['Li', 'Bowen', ''], ['Wang', 'Bailin', ''], ['Qin', 'Bowen', ''], ['Cao', 'Rongyu', ''], ['Geng', 'Ruiying', ''], ['Huo', 'Nan', ''], ['Zhou', 'Xuanhe', ''], ['Ma', 'Chenhao', ''], ['Li', 'Guoliang', ''], ['Chang', 'Kevin C. C.', ''], ['Huang', 'Fei', ''], ['Cheng', 'Reynold', ''], ['Li', 'Yongbin', '']]",1,1,2023-05-04,2,18,1,2,0,2,8f831f341e959955a495730d81996e62c57cc0bd,258547040.0,https://www.semanticscholar.org/paper/8f831f341e959955a495730d81996e62c57cc0bd,arXiv.org,2023.0,71.0,28.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2154860526', 'name': 'Jinyang Li'}, {'authorId': '151471590', 'name': 'Binyuan Hui'}, {'authorId': '2216486244', 'name': 'Ge Qu'}, {'authorId': '66200440', 'name': 'Binhua Li'}, {'authorId': '2135964855', 'name': 'Jiaxi Yang'}, {'authorId': '2132475886', 'name': 'Bowen Li'}, {'authorId': '2118640406', 'name': 'Bailin Wang'}, {'authorId': '50379530', 'name': 'Bowen Qin'}, {'authorId': '5973047', 'name': 'Rongyu Cao'}, {'authorId': '9706609', 'name': 'Ruiying Geng'}, {'authorId': '2147322690', 'name': 'Nan Huo'}, {'authorId': '2108786828', 'name': 'Chenhao Ma'}, {'authorId': '152608657', 'name': 'K. Chang'}, {'authorId': '143857288', 'name': 'Fei Huang'}, {'authorId': '2114454192', 'name': 'Reynold Cheng'}, {'authorId': '1527090216', 'name': 'Yongbin Li'}]","['Alibaba', 'Shanghai Artificial Intelligence Laboratory', 'Chinese University of Hong Kong, Shenzhen', 'Tsinghua University', 'University of Illinois Urbana-Champaign', 'Massachusetts Institute of Technology', 'University of Hong Kong']","['China', 'United States', 'Hong Kong']",2023-05
2305.03513,Yucheng Shi,"Yucheng Shi, Hehuan Ma, Wenliang Zhong, Qiaoyu Tan, Gengchen Mai,
  Xiang Li, Tianming Liu, Junzhou Huang",ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs,"6 pages, 2 figures",,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  ChatGPT, as a recently launched large language model (LLM), has shown superior performance in various natural language processing (NLP) tasks. However, two major limitations hinder its potential applications: (1) the inflexibility of finetuning on downstream tasks and (2) the lack of interpretability in the decision-making process. To tackle these limitations, we propose a novel framework that leverages the power of ChatGPT for specific tasks, such as text classification, while improving its interpretability. The proposed framework conducts a knowledge graph extraction task to extract refined and structural knowledge from the raw data using ChatGPT. The rich knowledge is then converted into a graph, which is further used to train an interpretable linear classifier to make predictions. To evaluate the effectiveness of our proposed method, we conduct experiments on four datasets. The result shows that our method can significantly improve the performance compared to directly utilizing ChatGPT for text classification tasks. And our method provides a more transparent decision-making process compared with previous text classification methods. ","[{'version': 'v1', 'created': 'Wed, 3 May 2023 19:57:43 GMT'}, {'version': 'v2', 'created': 'Tue, 19 Sep 2023 14:26:17 GMT'}]",2023-09-20,"[['Shi', 'Yucheng', ''], ['Ma', 'Hehuan', ''], ['Zhong', 'Wenliang', ''], ['Tan', 'Qiaoyu', ''], ['Mai', 'Gengchen', ''], ['Li', 'Xiang', ''], ['Liu', 'Tianming', ''], ['Huang', 'Junzhou', '']]",1,1,2023-05-03,2,8,3,1,0,1,a49687ee1ce6ace8329cfcb693d8f8198c867bcc,258546734.0,https://www.semanticscholar.org/paper/a49687ee1ce6ace8329cfcb693d8f8198c867bcc,arXiv.org,2023.0,52.0,8.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '46571755', 'name': 'Yucheng Shi'}, {'authorId': '51091763', 'name': 'Hehuan Ma'}, {'authorId': '2100561389', 'name': 'Wenliang Zhong'}, {'authorId': '40626717', 'name': 'Gengchen Mai'}, {'authorId': '2144438902', 'name': 'Xiang Li'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}, {'authorId': '1768190', 'name': 'Junzhou Huang'}]","['New York University Shanghai', 'University of Georgia', 'Massachusetts General Hospital', 'The University of Texas at Arlington']","['China', 'United States']",2023-05
2305.04757,Ziyang Luo,"Ziyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang Tao, Jing Ma,
  Qingwei Lin, Daxin Jiang",Augmented Large Language Models with Parametric Knowledge Guiding,,,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) have significantly advanced natural language processing (NLP) with their impressive language understanding and generation capabilities. However, their performance may be suboptimal for domain-specific tasks that require specialized knowledge due to limited exposure to the related data. Additionally, the lack of transparency of most state-of-the-art (SOTA) LLMs, which can only be accessed via APIs, impedes further fine-tuning with domain custom data. Moreover, providing private data to the LLMs' owner leads to data privacy problems. To address these challenges, we propose the novel Parametric Knowledge Guiding (PKG) framework, which equips LLMs with a knowledge-guiding module to access relevant knowledge without altering the LLMs' parameters. Our PKG is based on open-source ""white-box"" language models, allowing offline memory of any knowledge that LLMs require. We demonstrate that our PKG framework can enhance the performance of ""black-box"" LLMs on a range of domain knowledge-intensive tasks that require factual (+7.9%), tabular (+11.9%), medical (+3.0%), and multimodal (+8.1%) knowledge. ","[{'version': 'v1', 'created': 'Mon, 8 May 2023 15:05:16 GMT'}, {'version': 'v2', 'created': 'Thu, 18 May 2023 08:14:08 GMT'}]",2023-05-19,"[['Luo', 'Ziyang', ''], ['Xu', 'Can', ''], ['Zhao', 'Pu', ''], ['Geng', 'Xiubo', ''], ['Tao', 'Chongyang', ''], ['Ma', 'Jing', ''], ['Lin', 'Qingwei', ''], ['Jiang', 'Daxin', '']]",0,0,2023-05-08,2,8,3,0,0,0,e0dc8e113dbdd2896fb6420ac93e0b976c47f2a2,258556855.0,https://www.semanticscholar.org/paper/e0dc8e113dbdd2896fb6420ac93e0b976c47f2a2,arXiv.org,2023.0,59.0,9.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '23523733', 'name': 'Ziyang Luo'}, {'authorId': '46747953', 'name': 'Can Xu'}, {'authorId': '2007757792', 'name': 'Pu Zhao'}, {'authorId': '2442662', 'name': 'Xiubo Geng'}, {'authorId': '8801869', 'name': 'Chongyang Tao'}, {'authorId': '2157405974', 'name': 'Jing Ma'}, {'authorId': '2793487', 'name': 'Qingwei Lin'}, {'authorId': '2086994543', 'name': 'Daxin Jiang'}]","['Hong Kong Baptist University', 'Microsoft']","['China', 'United States']",2023-05
2305.04835,Shengnan An,"Shengnan An, Zeqi Lin, Qiang Fu, Bei Chen, Nanning Zheng, Jian-Guang
  Lou and Dongmei Zhang",How Do In-Context Examples Affect Compositional Generalization?,"ACL 2023 main conference, long paper",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Compositional generalization--understanding unseen combinations of seen primitives--is an essential reasoning capability in human intelligence. The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning--the prevailing few-shot paradigm based on large language models--exhibits compositional generalization. In this paper, we present CoFe, a test suite to investigate in-context compositional generalization. We find that the compositional generalization performance can be easily affected by the selection of in-context examples, thus raising the research question what the key factors are to make good in-context examples for compositional generalization. We study three potential factors: similarity, diversity and complexity. Our systematic experiments indicate that in-context examples should be structurally similar to the test case, diverse from each other, and individually simple. Furthermore, two strong limitations are observed: in-context compositional generalization on fictional words is much weaker than that on commonly used ones; it is still critical that the in-context examples should cover required linguistic structures, even though the backbone model has been pre-trained on large corpus. We hope our analysis would facilitate the understanding and utilization of in-context learning paradigm. ","[{'version': 'v1', 'created': 'Mon, 8 May 2023 16:32:18 GMT'}, {'version': 'v2', 'created': 'Thu, 25 May 2023 02:34:40 GMT'}, {'version': 'v3', 'created': 'Fri, 9 Jun 2023 02:25:29 GMT'}]",2023-06-12,"[['An', 'Shengnan', ''], ['Lin', 'Zeqi', ''], ['Fu', 'Qiang', ''], ['Chen', 'Bei', ''], ['Zheng', 'Nanning', ''], ['Lou', 'Jian-Guang', ''], ['Zhang', 'Dongmei', '']]",0,0,2023-05-08,3,7,2,0,0,0,0ae12d63f77f40b430f17c791a5191ff5fee5086,258558112.0,https://www.semanticscholar.org/paper/0ae12d63f77f40b430f17c791a5191ff5fee5086,Annual Meeting of the Association for Computational Linguistics,2023.0,55.0,7.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2119217081', 'name': 'Shengnan An'}, {'authorId': '2284174', 'name': 'Zeqi Lin'}, {'authorId': '2113771309', 'name': 'Qiang Fu'}, {'authorId': '143876723', 'name': 'B. Chen'}, {'authorId': '2144620206', 'name': 'Nanning Zheng'}, {'authorId': '153249455', 'name': 'Jian-Guang Lou'}, {'authorId': '46334641', 'name': 'D. Zhang'}]","[""Xi'an Jiaotong University"", 'Microsoft']","['China', 'United States']",2023-05
2305.05138,Wei Qin,"Wei Qin, Zetong Chen, Lei Wang, Yunshi Lan, Weijieying Ren and Richang
  Hong","Read, Diagnose and Chat: Towards Explainable and Interactive LLMs-Augmented Depression Detection in Social Media","8 pages, 5 figures",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  This paper proposes a new depression detection system based on LLMs that is both interpretable and interactive. It not only provides a diagnosis, but also diagnostic evidence and personalized recommendations based on natural language dialogue with the user. We address challenges such as the processing of large amounts of text and integrate professional diagnostic criteria. Our system outperforms traditional methods across various settings and is demonstrated through case studies. ","[{'version': 'v1', 'created': 'Tue, 9 May 2023 02:49:09 GMT'}]",2023-05-10,"[['Qin', 'Wei', ''], ['Chen', 'Zetong', ''], ['Wang', 'Lei', ''], ['Lan', 'Yunshi', ''], ['Ren', 'Weijieying', ''], ['Hong', 'Richang', '']]",0,0,2023-05-09,1,6,1,0,0,0,8efbd687804a13762f135db30b6077a1b171ae01,258564745.0,https://www.semanticscholar.org/paper/8efbd687804a13762f135db30b6077a1b171ae01,arXiv.org,2023.0,48.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1720767577', 'name': 'Wei Qin'}, {'authorId': '2216629182', 'name': 'Zetong Chen'}, {'authorId': '145131956', 'name': 'Lei Wang'}, {'authorId': '3458560', 'name': 'Yunshi Lan'}, {'authorId': '2053308288', 'name': 'Wei Ren'}, {'authorId': '2075339632', 'name': 'Richang Hong'}]","['Singapore Management University', 'Pennsylvania State University', 'Hefei University of Technology', 'University of Science and Technology of China', 'East China Normal University']","['China', 'United States', 'Singapore']",2023-05
2305.06575,Hongyuan Lu,"Hongyuan Lu, Haoyang Huang, Dongdong Zhang, Haoran Yang, Wai Lam, Furu
  Wei",Chain-of-Dictionary Prompting Elicits Translation in Large Language Models,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) have shown surprisingly good performance in multilingual neural machine translation (MNMT) even when trained without parallel data. Yet, despite the fact that the amount of training data is gigantic, they still struggle with translating rare words, particularly for low-resource languages. Even worse, it is usually unrealistic to retrieve relevant demonstrations for in-context learning with low-resource languages on LLMs, which restricts the practical use of LLMs for translation -- how should we mitigate this problem? To this end, we present a novel method, CoD, which augments LLMs with prior knowledge with the chains of multilingual dictionaries for a subset of input words to elicit translation abilities for LLMs. Extensive experiments indicate that augmenting ChatGPT with CoD elicits large gains by up to 13x chrF++ points for MNMT (3.08 to 42.63 for English to Serbian written in Cyrillic script) on FLORES-200 full devtest set. We further demonstrate the importance of chaining the multilingual dictionaries, as well as the superiority of CoD to few-shot demonstration for low-resource languages. ","[{'version': 'v1', 'created': 'Thu, 11 May 2023 05:19:47 GMT'}, {'version': 'v2', 'created': 'Wed, 17 May 2023 05:44:10 GMT'}, {'version': 'v3', 'created': 'Wed, 24 May 2023 05:01:52 GMT'}]",2023-05-25,"[['Lu', 'Hongyuan', ''], ['Huang', 'Haoyang', ''], ['Zhang', 'Dongdong', ''], ['Yang', 'Haoran', ''], ['Lam', 'Wai', ''], ['Wei', 'Furu', '']]",1,1,2023-05-11,3,6,1,1,0,1,97992c13baa6185c03d9e672f53185bc59822596,258615369.0,https://www.semanticscholar.org/paper/97992c13baa6185c03d9e672f53185bc59822596,arXiv.org,2023.0,33.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '2156273800', 'name': 'Hongyuan Lu'}, {'authorId': '15086992', 'name': 'Haoyang Huang'}, {'authorId': '40232931', 'name': 'Dongdong Zhang'}, {'authorId': '2115538342', 'name': 'Haoran Yang'}, {'authorId': '144594306', 'name': 'Wai Lam'}, {'authorId': '49807919', 'name': 'Furu Wei'}]","['Chinese University of Hong Kong', 'Microsoft']","['China', 'United States']",2023-05
2305.07402,Jiazhan Feng,"Jiazhan Feng, Chongyang Tao, Xiubo Geng, Tao Shen, Can Xu, Guodong
  Long, Dongyan Zhao, Daxin Jiang",Knowledge Refinement via Interaction Between Search Engines and Large Language Models,Work in progress. We added BEIR results and released source code,,,,cs.CL cs.IR,http://creativecommons.org/licenses/by/4.0/,"  Information retrieval (IR) plays a crucial role in locating relevant resources from vast amounts of data, and its applications have evolved from traditional knowledge bases to modern search engines (SEs). The emergence of large language models (LLMs) has further revolutionized the IR field by enabling users to interact with search systems in natural language. In this paper, we explore the advantages and disadvantages of LLMs and SEs, highlighting their respective strengths in understanding user-issued queries and retrieving up-to-date information. To leverage the benefits of both paradigms while circumventing their limitations, we propose InteR, a novel framework that facilitates knowledge refinement through interaction between SEs and LLMs. InteR allows SEs to expand knowledge in queries using LLM-generated knowledge collections and enables LLMs to enhance prompt formulation using SE-retrieved documents. This iterative refinement process augments the inputs of SEs and LLMs, leading to more accurate retrieval. Experiments on large-scale retrieval benchmarks involving web search and low-resource retrieval tasks demonstrate that InteR achieves overall superior zero-shot retrieval performance compared to state-of-the-art methods, even those using relevance judgment. Source code is available at https://github.com/Cyril-JZ/InteR ","[{'version': 'v1', 'created': 'Fri, 12 May 2023 11:58:15 GMT'}, {'version': 'v2', 'created': 'Sun, 21 May 2023 14:58:50 GMT'}]",2023-05-23,"[['Feng', 'Jiazhan', ''], ['Tao', 'Chongyang', ''], ['Geng', 'Xiubo', ''], ['Shen', 'Tao', ''], ['Xu', 'Can', ''], ['Long', 'Guodong', ''], ['Zhao', 'Dongyan', ''], ['Jiang', 'Daxin', '']]",0,0,2023-05-12,2,8,2,0,0,0,91f95445eb503b6d710ef4c924e17df70beb19af,258676297.0,https://www.semanticscholar.org/paper/91f95445eb503b6d710ef4c924e17df70beb19af,arXiv.org,2023.0,68.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '147062881', 'name': 'Jiazhan Feng'}, {'authorId': '8801869', 'name': 'Chongyang Tao'}, {'authorId': '2442662', 'name': 'Xiubo Geng'}, {'authorId': '143681703', 'name': 'Tao Shen'}, {'authorId': '46747953', 'name': 'Can Xu'}, {'authorId': '2062835', 'name': 'Guodong Long'}, {'authorId': '144060462', 'name': 'Dongyan Zhao'}, {'authorId': '2086994543', 'name': 'Daxin Jiang'}]","['Peking University', 'Microsoft', 'Finance and Economics Institute of Tajikistan']","['China', 'United States', 'Tajikistan']",2023-05
2305.07667,Iris Berent Dr.,"Iris Berent, Alexzander Sansiveri",Davinci the Dualist: the mind-body divide in large language models and in human learners,,,,,cs.AI cs.HC,http://creativecommons.org/licenses/by/4.0/,"  A large literature suggests that people are intuitive Dualists--they consider the mind ethereal, distinct from the body. Past research also shows that Dualism emerges, in part, via learning (e.g., Barlev & Shtulman, 2021). But whether learning is sufficient to give rise to Dualism is unknown.The evidence from human learners does address this question because humans are endowed not only with general learning capacities but also with core knowledge capacities. And recent results suggest that core knowledge begets Dualism (Berent, Theodore & Valencia, 2021; Berent, 2023). To evaluate the role of learning, here, we probe for a mind-body divide in Davinci--a large language model (LLM) that is devoid of any innate core knowledge. We show that Davinci still leans towards Dualism, and that this bias increases systematically with the learner's inductive potential. Thus, davinci (a GPT-3 model) exhibits mild Dualist tendencies, whereas its descendent, text-davinci-003 (a GPT-3.5 model), shows a full-blown bias. It selectively considers thoughts (epistemic states) as disembodied--as unlikely to show up in the body (in the brain), but not in its absence (after death). While Davinci's performance is constrained by its syntactic limitations, and it differs from humans, its Dualist bias is robust. These results demonstrate that the mind-body divide is partly learnable from experience.They also show how, as LLM's are exposed to human narratives, they induce not only human knowledge but also human biases. ","[{'version': 'v1', 'created': 'Wed, 10 May 2023 12:28:09 GMT'}, {'version': 'v2', 'created': 'Tue, 30 May 2023 21:00:50 GMT'}]",2023-06-01,"[['Berent', 'Iris', ''], ['Sansiveri', 'Alexzander', '']]",0,1,2023-05-10,2,2,2,1,0,1,d083bc4fbf71dc53a37d81cc107748fda9729ab0,258686560.0,https://www.semanticscholar.org/paper/d083bc4fbf71dc53a37d81cc107748fda9729ab0,arXiv.org,2023.0,17.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Biology', 'source': 's2-fos-model'}]","[{'authorId': '2139306', 'name': 'I. Berent'}, {'authorId': '2217228544', 'name': 'Alexzander Sansiveri'}]",['Northeastern University'],"['China', 'United States']",2023-05
2305.08845,Yupeng Hou,"Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian
  McAuley, Wayne Xin Zhao",Large Language Models are Zero-Shot Rankers for Recommender Systems,,,,,cs.IR cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Recently, large language models (LLMs) (e.g. GPT-4) have demonstrated impressive general-purpose task-solving abilities, including the potential to approach recommendation tasks. Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems. To conduct our empirical study, we first formalize the recommendation problem as a conditional ranking task, considering sequential interaction histories as conditions and the items retrieved by the candidate generation model as candidates. We adopt a specific prompting approach to solving the ranking task by LLMs: we carefully design the prompting template by including the sequential interaction history, the candidate items, and the ranking instruction. We conduct extensive experiments on two widely-used datasets for recommender systems and derive several key findings for the use of LLMs in recommender systems. We show that LLMs have promising zero-shot ranking abilities, even competitive to or better than conventional recommendation models on candidates retrieved by multiple candidate generators. We also demonstrate that LLMs struggle to perceive the order of historical interactions and can be affected by biases like position bias, while these issues can be alleviated via specially designed prompting and bootstrapping strategies. The code to reproduce this work is available at https://github.com/RUCAIBox/LLMRank. ","[{'version': 'v1', 'created': 'Mon, 15 May 2023 17:57:39 GMT'}]",2023-05-16,"[['Hou', 'Yupeng', ''], ['Zhang', 'Junjie', ''], ['Lin', 'Zihan', ''], ['Lu', 'Hongyu', ''], ['Xie', 'Ruobing', ''], ['McAuley', 'Julian', ''], ['Zhao', 'Wayne Xin', '']]",0,1,2023-05-15,1,7,2,1,0,1,4683d3d6cb31111cf4499a199c0b036662b3eb32,258686540.0,https://www.semanticscholar.org/paper/4683d3d6cb31111cf4499a199c0b036662b3eb32,arXiv.org,2023.0,46.0,42.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '151472453', 'name': 'Yupeng Hou'}, {'authorId': '2120518257', 'name': 'Junjie Zhang'}, {'authorId': '2112304962', 'name': 'Zihan Lin'}, {'authorId': '2115863242', 'name': 'Hongyu Lu'}, {'authorId': '3360722', 'name': 'Ruobing Xie'}, {'authorId': '35660011', 'name': 'Julian McAuley'}, {'authorId': '2542603', 'name': 'Wayne Xin Zhao'}]","['Tencent', 'Renmin University of China', 'University of California, San Diego']","['China', 'United States']",2023-05
2305.09067,Xiaoying Zhang,"Xiaoying Zhang, Baolin Peng, Kun Li, Jingyan Zhou, Helen Meng",SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting,"21 pages, 13 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Building end-to-end task bots and maintaining their integration with new functionalities using minimal human efforts is a long-standing challenge in dialog research. Recently large language models (LLMs) have demonstrated exceptional proficiency in conversational engagement and adherence to instructions across various downstream tasks. In this work, we introduce SGP-TOD, Schema-Guided Prompting for building Task-Oriented Dialog systems effortlessly based on LLMs. Utilizing the symbolic knowledge -- task schema, we instruct fixed LLMs to generate appropriate responses on novel tasks, circumventing the need for training data. Specifically, SGP-TOD comprises three components: a LLM for engaging with users, a DST Prompter to aid the LLM with dialog state tracking, which is then used to retrieve database items, and a Policy Prompter to elicit proper responses adhering to the provided dialog policy. Experimental results on Multiwoz, RADDLE and STAR datasets show that our training-free strategy SGP-TOD, without any task-specific data, yields state-of-the-art (SOTA) zero-shot performance, greatly surpasses the few-shot approaches. In a domain-extension setting, SGP-TOD aptly adapts to new functionalities by merely adding supplementary schema rules. We make our code and data publicly available. ","[{'version': 'v1', 'created': 'Mon, 15 May 2023 23:29:56 GMT'}]",2023-05-17,"[['Zhang', 'Xiaoying', ''], ['Peng', 'Baolin', ''], ['Li', 'Kun', ''], ['Zhou', 'Jingyan', ''], ['Meng', 'Helen', '']]",0,0,2023-05-15,1,5,1,0,0,0,ec56f49bef8925dc8931cc261ab3aca4dd36ad2d,258715201.0,https://www.semanticscholar.org/paper/ec56f49bef8925dc8931cc261ab3aca4dd36ad2d,arXiv.org,2023.0,56.0,7.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2155167734', 'name': 'Xiaoying Zhang'}, {'authorId': '1780690', 'name': 'Baolin Peng'}, {'authorId': '2185631323', 'name': 'Kun Li'}, {'authorId': '30887444', 'name': 'Jingyan Zhou'}, {'authorId': '1702243', 'name': 'Helen M. Meng'}]","['Tencent', 'Chinese University of Hong Kong', 'Microsoft']","['China', 'United States']",2023-05
2305.10037,Heng Wang,"Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han,
  Yulia Tsvetkov",Can Language Models Solve Graph Problems in Natural Language?,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) are increasingly adopted for a variety of tasks with implicit graphical structures, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these tasks with structure implications, whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph benchmark and find that 1) language models do demonstrate preliminary graph reasoning abilities, 2) the benefit of advanced prompting and in-context learning diminishes on more complex graph problems, while 3) LLMs are also (un)surprisingly brittle in the face of spurious correlations in graph and problem settings. We then propose Build-a-Graph Prompting and Algorithmic Prompting, two instruction-based approaches to enhance LLMs in solving natural language graph problems. Build-a-Graph and Algorithmic prompting improve the performance of LLMs on NLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to solve the most complicated graph reasoning tasks in our setup with language models remains an open research question. The NLGraph benchmark and evaluation code are available at https://github.com/Arthur-Heng/NLGraph. ","[{'version': 'v1', 'created': 'Wed, 17 May 2023 08:29:21 GMT'}]",2023-05-18,"[['Wang', 'Heng', ''], ['Feng', 'Shangbin', ''], ['He', 'Tianxing', ''], ['Tan', 'Zhaoxuan', ''], ['Han', 'Xiaochuang', ''], ['Tsvetkov', 'Yulia', '']]",0,1,2023-05-17,1,6,2,1,0,1,df2beaae63e4d68ef8e762bcd4704c9f11f856d9,258740923.0,https://www.semanticscholar.org/paper/df2beaae63e4d68ef8e762bcd4704c9f11f856d9,arXiv.org,2023.0,40.0,18.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '120241560', 'name': 'Heng Wang'}, {'authorId': '2114887261', 'name': 'Shangbin Feng'}, {'authorId': '3083253', 'name': 'Tianxing He'}, {'authorId': '2093186816', 'name': 'Zhaoxuan Tan'}, {'authorId': '40500540', 'name': 'Xiaochuang Han'}, {'authorId': '2073587169', 'name': 'Yulia Tsvetkov'}]","['University of Washington', ""Xi'an Jiaotong University"", 'University of Notre Dame']","['China', 'United States']",2023-05
2305.11255,Hao Fei,"Hao Fei, Bobo Li, Qian Liu, Lidong Bing, Fei Li, Tat-Seng Chua",Reasoning Implicit Sentiment with Chain-of-Thought Prompting,ACL2023 Short Paper,,,,cs.CL,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  While sentiment analysis systems try to determine the sentiment polarities of given targets based on the key opinion expressions in input texts, in implicit sentiment analysis (ISA) the opinion cues come in an implicit and obscure manner. Thus detecting implicit sentiment requires the common-sense and multi-hop reasoning ability to infer the latent intent of opinion. Inspired by the recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop Reasoning (THOR) CoT framework to mimic the human-like reasoning process for ISA. We design a three-step prompting principle for THOR to step-by-step induce the implicit aspect, opinion, and finally the sentiment polarity. Our THOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6% F1 on supervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50% F1 on zero-shot setting. Our code is open at https://github.com/scofield7419/THOR-ISA. ","[{'version': 'v1', 'created': 'Thu, 18 May 2023 18:38:32 GMT'}, {'version': 'v2', 'created': 'Thu, 25 May 2023 03:57:57 GMT'}, {'version': 'v3', 'created': 'Sat, 3 Jun 2023 03:56:23 GMT'}, {'version': 'v4', 'created': 'Fri, 9 Jun 2023 01:27:58 GMT'}]",2023-06-12,"[['Fei', 'Hao', ''], ['Li', 'Bobo', ''], ['Liu', 'Qian', ''], ['Bing', 'Lidong', ''], ['Li', 'Fei', ''], ['Chua', 'Tat-Seng', '']]",0,0,2023-05-18,4,6,1,4,2,2,fb0474569c14a4af8dc94f231dc6074fb5bec63a,258822870.0,https://www.semanticscholar.org/paper/fb0474569c14a4af8dc94f231dc6074fb5bec63a,Annual Meeting of the Association for Computational Linguistics,2023.0,42.0,13.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '46959445', 'name': 'Hao Fei'}, {'authorId': '2132446579', 'name': 'Bobo Li'}, {'authorId': '2145484051', 'name': 'Qianchu Liu'}, {'authorId': '1996394', 'name': 'Lidong Bing'}, {'authorId': '2109530930', 'name': 'Fei Li'}, {'authorId': '143779329', 'name': 'Tat-seng Chua'}]","['Alibaba', 'Wuhan University', 'National University of Singapore', 'Applied Research Laboratory at the University of Hawai‘i']","['China', 'United States', 'Singapore']",2023-05
2305.11418,Jiayi Fu,"Jiayi Fu, Haoying Han, Xing Su and Chao Fan",Towards Human-AI Collaborative Urban Science Research Enabled by Pre-trained Large Language Models,,,,,cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained large language models (PLMs) have the potential to support urban science research through content creation, information extraction, assisted programming, text classification, and other technical advances. In this research, we explored the opportunities, challenges, and prospects of PLMs in urban science research. Specifically, we discussed potential applications of PLMs to urban institution, urban space, urban information, and citizen behaviors research through seven examples using ChatGPT. We also examined the challenges of PLMs in urban science research from both technical and social perspectives. The prospects of the application of PLMs in urban science research were then proposed. We found that PLMs can effectively aid in understanding complex concepts in urban science, facilitate urban spatial form identification, assist in disaster monitoring, and sense public sentiment. At the same time, however, the applications of PLMs in urban science research face evident threats, such as technical limitations, security, privacy, and social bias. The development of fundamental models based on domain knowledge and human-AI collaboration may help improve PLMs to support urban science research in future. ","[{'version': 'v1', 'created': 'Fri, 19 May 2023 04:04:54 GMT'}]",2023-05-22,"[['Fu', 'Jiayi', ''], ['Han', 'Haoying', ''], ['Su', 'Xing', ''], ['Fan', 'Chao', '']]",1,1,2023-05-19,1,4,1,1,0,1,6f0786b1ee6962d8071d6718bb94f4a935ad8e5f,258822987.0,https://www.semanticscholar.org/paper/6f0786b1ee6962d8071d6718bb94f4a935ad8e5f,arXiv.org,2023.0,61.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2119315206', 'name': 'Jiayi Fu'}, {'authorId': '2716684', 'name': 'Haoying Han'}, {'authorId': '2217993987', 'name': 'Xing Su'}, {'authorId': '2217951835', 'name': 'Chao Fan'}]","['Clemson University', 'Zhejiang University']","['China', 'United States']",2023-05
2305.11499,Tianci Xue,"Tianci Xue, Ziqi Wang, Zhenhailong Wang, Chi Han, Pengfei Yu, Heng Ji",RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought,"24 pages, 21 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language Models (LLMs) have achieved promising performance on arithmetic reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting. However, LLMs face challenges in maintaining factual consistency during reasoning, exhibiting tendencies to condition overlooking, question misinterpretation, and condition hallucination over given problems. Existing methods use coarse-grained feedback (e.g., whether the answer is correct) to improve factual consistency. In this work, we propose RCoT (Reversing Chain-of-Thought), a novel method to improve LLMs' reasoning abilities by automatically detecting and rectifying factual inconsistency in LLMs, generated solutions. To detect factual inconsistency, RCoT first asks LLMs to reconstruct the problem based on generated solutions. Then fine-grained comparisons between the original problem and the reconstructed problem expose the factual inconsistency in the original solutions. To rectify the solution, RCoT formulates detected factual inconsistency into fine-grained feedback to guide LLMs in revising solutions. Experimental results demonstrate improvements of RCoT over standard CoT, Self-Consistency and Self-Refine across seven arithmetic datasets. Moreover, we find that manually written fine-grained feedback can dramatically improve LLMs' reasoning abilities (e.g., ChatGPT reaches 94.6% accuracy on GSM8K), encouraging the community to further explore the fine-grained feedback generation methods. ","[{'version': 'v1', 'created': 'Fri, 19 May 2023 08:02:52 GMT'}, {'version': 'v2', 'created': 'Mon, 2 Oct 2023 03:59:04 GMT'}]",2023-10-03,"[['Xue', 'Tianci', ''], ['Wang', 'Ziqi', ''], ['Wang', 'Zhenhailong', ''], ['Han', 'Chi', ''], ['Yu', 'Pengfei', ''], ['Ji', 'Heng', '']]",1,1,2023-05-19,2,6,1,1,0,1,22d5459d1f47341b355feeb1becc37208d6ec365,258823296.0,https://www.semanticscholar.org/paper/22d5459d1f47341b355feeb1becc37208d6ec365,arXiv.org,2023.0,51.0,11.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2217950757', 'name': 'Tianci Xue'}, {'authorId': '1390880371', 'name': 'Ziqi Wang'}, {'authorId': '2052036545', 'name': 'Zhenhailong Wang'}, {'authorId': '2118642562', 'name': 'Chi Han'}, {'authorId': '144808890', 'name': 'Pengfei Yu'}, {'authorId': '2072975661', 'name': 'Heng Ji'}]","['University of Illinois Urbana-Champaign', 'Nanjing University']","['China', 'United States']",2023-05
2305.11738,Zhibin Gou,"Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan
  Duan, Weizhu Chen",CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing,"add LLaMA-2 7B to 70B results; add more mathematical program
  synthesis datasets",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially ""black boxes"" to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxicity reduction demonstrate that CRITIC consistently enhances the performance of LLMs. Meanwhile, our research highlights the crucial importance of external feedback in promoting the ongoing self-improvement of LLMs. ","[{'version': 'v1', 'created': 'Fri, 19 May 2023 15:19:44 GMT'}, {'version': 'v2', 'created': 'Sat, 30 Sep 2023 08:35:29 GMT'}]",2023-10-03,"[['Gou', 'Zhibin', ''], ['Shao', 'Zhihong', ''], ['Gong', 'Yeyun', ''], ['Shen', 'Yelong', ''], ['Yang', 'Yujiu', ''], ['Duan', 'Nan', ''], ['Chen', 'Weizhu', '']]",0,0,2023-05-19,2,7,2,0,0,0,bcdaf6c98ddbd6809cf6241aa77200d7394db163,258823123.0,https://www.semanticscholar.org/paper/bcdaf6c98ddbd6809cf6241aa77200d7394db163,arXiv.org,2023.0,144.0,41.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1797090', 'name': 'Zhibin Gou'}, {'authorId': '144485528', 'name': 'Zhihong Shao'}, {'authorId': '2171182', 'name': 'Yeyun Gong'}, {'authorId': '1752875', 'name': 'Yelong Shen'}, {'authorId': '2108585311', 'name': 'Yujiu Yang'}, {'authorId': '46429989', 'name': 'Nan Duan'}, {'authorId': '2109136147', 'name': 'Weizhu Chen'}]","['Tsinghua University', 'Microsoft']","['China', 'United States']",2023-05
2305.11828,Hye Sun Yun,"Hye Sun Yun, Iain J. Marshall, Thomas Trikalinos, Byron C. Wallace",Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews,"34 pages, 3 figures, 7 tables",,,,cs.CL cs.AI cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Medical systematic reviews are crucial for informing clinical decision making and healthcare policy. But producing such reviews is onerous and time-consuming. Thus, high-quality evidence synopses are not available for many questions and may be outdated even when they are available. Large language models (LLMs) are now capable of generating long-form texts, suggesting the tantalizing possibility of automatically generating literature reviews on demand. However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucinating or omitting important information. In the healthcare context, this may render LLMs unusable at best and dangerous at worst. Most discussion surrounding the benefits and risks of LLMs have been divorced from specific applications. In this work, we seek to qualitatively characterize the potential utility and risks of LLMs for assisting in production of medical evidence reviews. We conducted 16 semi-structured interviews with international experts in systematic reviews, grounding discussion in the context of generating evidence reviews. Domain experts indicated that LLMs could aid writing reviews, as a tool for drafting or creating plain language summaries, generating templates or suggestions, distilling information, crosschecking, and synthesizing or interpreting text inputs. But they also identified issues with model outputs and expressed concerns about potential downstream harms of confidently composed but inaccurate LLM outputs which might mislead. Other anticipated potential downstream harms included lessened accountability and proliferation of automatically generated reviews that might be of low quality. Informed by this qualitative analysis, we identify criteria for rigorous evaluation of biomedical LLMs aligned with domain expert views. ","[{'version': 'v1', 'created': 'Fri, 19 May 2023 17:09:19 GMT'}, {'version': 'v2', 'created': 'Mon, 22 May 2023 16:17:51 GMT'}]",2023-05-23,"[['Yun', 'Hye Sun', ''], ['Marshall', 'Iain J.', ''], ['Trikalinos', 'Thomas', ''], ['Wallace', 'Byron C.', '']]",0,0,2023-05-19,2,4,3,0,0,0,543345d3f0a0b8bf245a56661a6da5f9ba5295da,264289388.0,https://www.semanticscholar.org/paper/543345d3f0a0b8bf245a56661a6da5f9ba5295da,,2023.0,65.0,0.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Medicine', 'source': 's2-fos-model'}]","[{'authorId': '32401628', 'name': 'Hye Sun Yun'}, {'authorId': '1808775', 'name': 'I. Marshall'}, {'authorId': '2947796', 'name': 'T. Trikalinos'}, {'authorId': '2257308234', 'name': 'Byron C. Wallace'}]","['Northeastern University', 'Brown University']","['China', 'United States']",2023-05
2305.12421,Cunxiang Wang,"Cunxiang Wang, Sirui Cheng, Qipeng Guo, Zhikun Xu, Bowen Ding, Yidong
  Wang, Xiangkun Hu, Zheng Zhang, Yue Zhang",Evaluating Open-QA Evaluation,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  This study focuses on the evaluation of the Open Question Answering (Open-QA) task, which can directly estimate the factuality of large language models (LLMs). Current automatic evaluation methods have shown limitations, indicating that human evaluation still remains the most reliable approach. We introduce a new task, Evaluating QA Evaluation (QA-Eval) and the corresponding dataset EVOUNA, designed to assess the accuracy of AI-generated answers in relation to standard answers within Open-QA. Our evaluation of these methods utilizes human-annotated results to measure their performance. Specifically, the work investigates methods that show high correlation with human evaluations, deeming them more reliable. We also discuss the pitfalls of current methods and methods to improve LLM-based evaluators. We believe this new QA-Eval task and corresponding dataset EVOUNA will facilitate the development of more effective automatic evaluation tools and prove valuable for future research in this area. All resources are available at \url{https://github.com/wangcunxiang/QA-Eval} and it is under the Apache-2.0 License. ","[{'version': 'v1', 'created': 'Sun, 21 May 2023 10:40:55 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Jul 2023 15:41:31 GMT'}, {'version': 'v3', 'created': 'Mon, 28 Aug 2023 16:15:21 GMT'}]",2023-08-29,"[['Wang', 'Cunxiang', ''], ['Cheng', 'Sirui', ''], ['Guo', 'Qipeng', ''], ['Xu', 'Zhikun', ''], ['Ding', 'Bowen', ''], ['Wang', 'Yidong', ''], ['Hu', 'Xiangkun', ''], ['Zhang', 'Zheng', ''], ['Zhang', 'Yue', '']]",0,0,2023-05-21,3,9,2,0,0,0,7abb0536a8faa0f7b40d64d4b1f8a7ee33d9e8fe,259951284.0,https://www.semanticscholar.org/paper/7abb0536a8faa0f7b40d64d4b1f8a7ee33d9e8fe,,2023.0,51.0,1.0,1.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '35504092', 'name': 'Cunxiang Wang'}, {'authorId': '2110845230', 'name': 'Sirui Cheng'}, {'authorId': '3187768', 'name': 'Qipeng Guo'}, {'authorId': '2218316624', 'name': 'Zhikun Xu'}, {'authorId': '2223752132', 'name': 'Bowen Ding'}, {'authorId': '2108024273', 'name': 'Yidong Wang'}, {'authorId': '12040998', 'name': 'Xiangkun Hu'}, {'authorId': '47294621', 'name': 'Zheng Zhang'}, {'authorId': '2211964951', 'name': 'Yue Zhang'}]","['Westlake University', 'Fudan University', 'Amazon', 'Northeastern University']","['China', 'United States']",2023-05
2305.12434,Wenxuan Wang,"Yuxuan Wan, Wenxuan Wang, Pinjia He, Jiazhen Gu, Haonan Bai, Michael
  Lyu",BiasAsker: Measuring the Bias in Conversational AI System,Accepted by FSE 2023,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Powered by advanced Artificial Intelligence (AI) techniques, conversational AI systems, such as ChatGPT and digital assistants like Siri, have been widely deployed in daily life. However, such systems may still produce content containing biases and stereotypes, causing potential social problems. Due to the data-driven, black-box nature of modern AI techniques, comprehensively identifying and measuring biases in conversational systems remains a challenging task. Particularly, it is hard to generate inputs that can comprehensively trigger potential bias due to the lack of data containing both social groups as well as biased properties. In addition, modern conversational systems can produce diverse responses (e.g., chatting and explanation), which makes existing bias detection methods simply based on the sentiment and the toxicity hardly being adopted. In this paper, we propose BiasAsker, an automated framework to identify and measure social bias in conversational AI systems. To obtain social groups and biased properties, we construct a comprehensive social bias dataset, containing a total of 841 groups and 8,110 biased properties. Given the dataset, BiasAsker automatically generates questions and adopts a novel method based on existence measurement to identify two types of biases (i.e., absolute bias and related bias) in conversational systems. Extensive experiments on 8 commercial systems and 2 famous research models, such as ChatGPT and GPT-3, show that 32.83% of the questions generated by BiasAsker can trigger biased behaviors in these widely deployed conversational systems. All the code, data, and experimental results have been released to facilitate future research. ","[{'version': 'v1', 'created': 'Sun, 21 May 2023 11:25:59 GMT'}]",2023-05-23,"[['Wan', 'Yuxuan', ''], ['Wang', 'Wenxuan', ''], ['He', 'Pinjia', ''], ['Gu', 'Jiazhen', ''], ['Bai', 'Haonan', ''], ['Lyu', 'Michael', '']]",1,1,2023-05-21,1,6,2,2,0,2,744a98cc2736fa71d3984602e10b68319a47c65e,258833296.0,https://www.semanticscholar.org/paper/744a98cc2736fa71d3984602e10b68319a47c65e,arXiv.org,2023.0,70.0,12.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2167583580', 'name': 'Yuxuan Wan'}, {'authorId': '2144328160', 'name': 'Wenxuan Wang'}, {'authorId': '40532404', 'name': 'Pinjia He'}, {'authorId': None, 'name': 'Jiazhen Gu'}, {'authorId': '47468054', 'name': 'Haonan Bai'}, {'authorId': '2146840128', 'name': 'Michael R. Lyu'}]","['Chinese University of Hong Kong, Shenzhen', '2023, San Francisco, USA', 'Chinese University of Hong Kong']","['China', 'United States']",2023-05
2305.12463,Renliang Sun,"Renliang Sun, Wei Xu, Xiaojun Wan",Teaching the Pre-trained Model to Generate Simple Texts for Text Simplification,Accepted by ACL Findings: 2023,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-sa/4.0/,"  Randomly masking text spans in ordinary texts in the pre-training stage hardly allows models to acquire the ability to generate simple texts. It can hurt the performance of pre-trained models on text simplification tasks. In this paper, we propose a new continued pre-training strategy to teach the pre-trained model to generate simple texts. We continue pre-training BART, a representative model, to obtain SimpleBART. It consistently and significantly improves the results on lexical simplification, sentence simplification, and document-level simplification tasks over BART. At the end, we compare SimpleBART with several representative large language models (LLMs). ","[{'version': 'v1', 'created': 'Sun, 21 May 2023 14:03:49 GMT'}]",2023-05-23,"[['Sun', 'Renliang', ''], ['Xu', 'Wei', ''], ['Wan', 'Xiaojun', '']]",0,0,2023-05-21,1,3,2,0,0,0,48351e48245c6e19718f0f198f74f24fbb676637,258833544.0,https://www.semanticscholar.org/paper/48351e48245c6e19718f0f198f74f24fbb676637,Annual Meeting of the Association for Computational Linguistics,2023.0,46.0,5.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2068172988', 'name': 'Renliang Sun'}, {'authorId': '145738420', 'name': 'Wei Xu'}, {'authorId': '117908148', 'name': 'Xiaojun Wan'}]","['Peking University', 'Georgia Institute of Technology']","['China', 'United States']",2023-05
2305.12723,Xinlu Zhang,"Xinlu Zhang, Shiyang Li, Xianjun Yang, Chenxin Tian, Yao Qin, Linda
  Ruth Petzold",Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under privacy-restricted scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability. ","[{'version': 'v1', 'created': 'Mon, 22 May 2023 05:14:38 GMT'}]",2023-05-23,"[['Zhang', 'Xinlu', ''], ['Li', 'Shiyang', ''], ['Yang', 'Xianjun', ''], ['Tian', 'Chenxin', ''], ['Qin', 'Yao', ''], ['Petzold', 'Linda Ruth', '']]",0,0,2023-05-22,1,6,2,0,0,0,74b94891f8f7ac8d73d9df817b6720e1cb792bcc,258832501.0,https://www.semanticscholar.org/paper/74b94891f8f7ac8d73d9df817b6720e1cb792bcc,arXiv.org,2023.0,75.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Medicine', 'source': 's2-fos-model'}]","[{'authorId': '2108030191', 'name': 'Xinlu Zhang'}, {'authorId': '50341591', 'name': 'SHIYANG LI'}, {'authorId': '2145170944', 'name': 'Xianjun Yang'}, {'authorId': '2218035941', 'name': 'Chenxin Tian'}, {'authorId': '2219078907', 'name': 'Yao Qin'}, {'authorId': '21038849', 'name': 'Linda Petzold'}]","['Chinese Academy of Medical Sciences & Peking Union Medical College', 'University of California, Santa Barbara']","['China', 'United States']",2023-05
2305.13114,Reza Hadi Mogavi,"Reza Hadi Mogavi, Chao Deng, Justin Juho Kim, Pengyuan Zhou, Young D.
  Kwon, Ahmed Hosny Saleh Metwally, Ahmed Tlili, Simone Bassanelli, Antonio
  Bucchiarone, Sujit Gujar, Lennart E. Nacke, and Pan Hui","Exploring User Perspectives on ChatGPT: Applications, Perceptions, and Implications for AI-Integrated Education",Preprint version,,,,cs.CY cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Understanding user perspectives on Artificial Intelligence (AI) in education is essential for creating pedagogically effective and ethically responsible AI-integrated learning environments. In this paper, we conduct an extensive qualitative content analysis of four major social media platforms (Twitter, Reddit, YouTube, and LinkedIn) to explore the user experience (UX) and perspectives of early adopters toward ChatGPT-an AI Chatbot technology-in various education sectors. We investigate the primary applications of ChatGPT in education (RQ1) and the various perceptions of the technology (RQ2). Our findings indicate that ChatGPT is most popularly used in the contexts of higher education (24.18%), K-12 education (22.09%), and practical-skills learning (15.28%). On social media platforms, the most frequently discussed topics about ChatGPT are productivity, efficiency, and ethics. While some early adopters lean toward seeing ChatGPT as a revolutionary technology with the potential to boost students' self-efficacy and motivation to learn, others express concern that overreliance on the AI system may promote superficial learning habits and erode students' social and critical thinking skills. Our study contributes to the broader discourse on Human-AI Interaction and offers recommendations based on crowd-sourced knowledge for educators and learners interested in incorporating ChatGPT into their educational settings. Furthermore, we propose a research agenda for future studies that sets the foundation for continued investigation into the application of ChatGPT in education. ","[{'version': 'v1', 'created': 'Mon, 22 May 2023 15:13:14 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Jun 2023 09:46:06 GMT'}]",2023-06-14,"[['Mogavi', 'Reza Hadi', ''], ['Deng', 'Chao', ''], ['Kim', 'Justin Juho', ''], ['Zhou', 'Pengyuan', ''], ['Kwon', 'Young D.', ''], ['Metwally', 'Ahmed Hosny Saleh', ''], ['Tlili', 'Ahmed', ''], ['Bassanelli', 'Simone', ''], ['Bucchiarone', 'Antonio', ''], ['Gujar', 'Sujit', ''], ['Nacke', 'Lennart E.', ''], ['Hui', 'Pan', '']]",1,1,2023-05-22,2,12,2,1,0,1,2a350127799a82ce7280b1e95f9ab23c73cfe8f8,258833306.0,https://www.semanticscholar.org/paper/2a350127799a82ce7280b1e95f9ab23c73cfe8f8,arXiv.org,2023.0,89.0,6.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Education', 'source': 's2-fos-model'}]","[{'authorId': '2266408', 'name': 'Reza Hadi Mogavi'}, {'authorId': '40209052', 'name': 'Chaohua Deng'}, {'authorId': '2218347964', 'name': 'Justin Juho Kim'}, {'authorId': '72050450', 'name': 'Pengyuan Zhou'}, {'authorId': '97517796', 'name': 'Young D. Kwon'}, {'authorId': '35391544', 'name': 'A. H. Metwally'}, {'authorId': '27362922', 'name': 'A. Tlili'}, {'authorId': '2146584418', 'name': 'Simone Bassanelli'}, {'authorId': '1680267', 'name': 'A. Bucchiarone'}, {'authorId': '39686395', 'name': 'Sujit Gujar'}, {'authorId': '1953205', 'name': 'L. Nacke'}, {'authorId': '143966169', 'name': 'Pan Hui'}]","['University of Helsinki', 'University of Waterloo', 'Hong Kong University of Science and Technology', 'University of Cambridge', 'International Institute of Information Technology, Hyderabad', 'Beijing Normal University', 'Fondazione Bruno Kessler', 'University of Science and Technology of China', 'Meta']","['Canada', 'India', 'United States', 'United Kingdom', 'China', 'Finland', 'Italy']",2023-05
2305.13592,Yiwen Guo,Jianyu Zhao and Yuyang Rong and Yiwen Guo and Yifeng He and Hao Chen,Understanding Programs by Exploiting (Fuzzing) Test Cases,"Findings of the Association for Computational Linguistics: ACL 2023;
  fix typos and update results to keep the same settings in all experiments",,,,cs.LG cs.AI cs.CL cs.CR cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Semantic understanding of programs has attracted great attention in the community. Inspired by recent successes of large language models (LLMs) in natural language understanding, tremendous progress has been made by treating programming language as another sort of natural language and training LLMs on corpora of program code. However, programs are essentially different from texts after all, in a sense that they are normally heavily structured and syntax-strict. In particular, programs and their basic units (i.e., functions and subroutines) are designed to demonstrate a variety of behaviors and/or provide possible outputs, given different inputs. The relationship between inputs and possible outputs/behaviors represents the functions/subroutines and profiles the program as a whole. Therefore, we propose to incorporate such a relationship into learning, for achieving a deeper semantic understanding of programs. To obtain inputs that are representative enough to trigger the execution of most part of the code, we resort to fuzz testing and propose fuzz tuning to boost the performance of program understanding and code representation learning, given a pre-trained LLM. The effectiveness of the proposed method is verified on two program understanding tasks including code clone detection and code classification, and it outperforms current state-of-the-arts by large margins. Code is available at https://github.com/rabbitjy/FuzzTuning. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 01:51:46 GMT'}, {'version': 'v2', 'created': 'Mon, 12 Jun 2023 00:56:21 GMT'}]",2023-06-13,"[['Zhao', 'Jianyu', ''], ['Rong', 'Yuyang', ''], ['Guo', 'Yiwen', ''], ['He', 'Yifeng', ''], ['Chen', 'Hao', '']]",0,0,2023-05-23,2,5,5,0,0,0,330403b8627ccfd566de439b6126229fde257d49,258841437.0,https://www.semanticscholar.org/paper/330403b8627ccfd566de439b6126229fde257d49,Annual Meeting of the Association for Computational Linguistics,2023.0,40.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2035901', 'name': 'Jianyu Zhao'}, {'authorId': '144278456', 'name': 'Yuyang Rong'}, {'authorId': '2527106', 'name': 'Yiwen Guo'}, {'authorId': '2118918598', 'name': 'Yifeng He'}, {'authorId': '36620472', 'name': 'Hao Chen'}]","['Tencent', 'University of California, Davis', 'Independent Researcher']","['China', 'United States']",2023-05
2305.13661,Yikang Pan,"Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan,
  William Yang Wang",On the Risk of Misinformation Pollution with Large Language Models,Technical Report,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we comprehensively investigate the potential misuse of modern Large Language Models (LLMs) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly Open-Domain Question Answering (ODQA) systems. We establish a threat model and simulate potential misuse scenarios, both unintentional and intentional, to assess the extent to which LLMs can be utilized to produce misinformation. Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation in the performance of ODQA systems. To mitigate the harm caused by LLM-generated misinformation, we explore three defense strategies: prompting, misinformation detection, and majority voting. While initial results show promising trends for these defensive strategies, much more work needs to be done to address the challenge of misinformation pollution. Our work highlights the need for further research and interdisciplinary collaboration to address LLM-generated misinformation and to promote responsible use of LLMs. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 04:10:26 GMT'}]",2023-05-24,"[['Pan', 'Yikang', ''], ['Pan', 'Liangming', ''], ['Chen', 'Wenhu', ''], ['Nakov', 'Preslav', ''], ['Kan', 'Min-Yen', ''], ['Wang', 'William Yang', '']]",0,0,2023-05-23,1,6,2,0,0,0,fef6471c4a2a0e7abc4a2261a6cf916e34091d12,258840876.0,https://www.semanticscholar.org/paper/fef6471c4a2a0e7abc4a2261a6cf916e34091d12,arXiv.org,2023.0,46.0,15.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2218334588', 'name': 'Yikang Pan'}, {'authorId': '3470231', 'name': 'Liangming Pan'}, {'authorId': '2109664620', 'name': 'Wenhu Chen'}, {'authorId': '2026545715', 'name': 'Preslav Nakov'}, {'authorId': '37596605', 'name': 'Min-Yen Kan'}, {'authorId': '152876475', 'name': 'W. Wang'}]","['University of Waterloo', 'National University of Singapore', 'Zhejiang University', 'Equal Contribution.', 'Mohamed bin Zayed University of Artificial Intelligence', 'University of California, Santa Barbara']","['Singapore', 'Canada', 'United States', 'China', 'United Arab Emirates']",2023-05
2305.14091,Ziyin Zhang,"Hai Hu and Ziyin Zhang and Weifang Huang and Jackie Yan-Ki Lai and
  Aini Li and Yina Patterson and Jiahui Huang and Peng Zhang and Chien-Jer
  Charles Lin and Rui Wang",Revisiting Acceptability Judgements,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  In this work, we revisit linguistic acceptability in the context of large language models. We introduce CoLAC - Corpus of Linguistic Acceptability in Chinese, the first large-scale acceptability dataset for a non-Indo-European language. It is verified by native speakers and is the first acceptability dataset that comes with two sets of labels: a linguist label and a crowd label. Our experiments show that even the largest InstructGPT model performs only at chance level on CoLAC, while ChatGPT's performance (48.30 MCC) is also much below supervised models (59.03 MCC) and human (65.11 MCC). Through cross-lingual transfer experiments and fine-grained linguistic analysis, we provide detailed analysis of the model predictions and demonstrate for the first time that knowledge of linguistic acceptability can be transferred across typologically distinct languages, as well as be traced back to pre-training. Our dataset is publicly available at \url{https://github.com/huhailinguist/CoLAC}. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 14:16:22 GMT'}, {'version': 'v2', 'created': 'Wed, 24 May 2023 11:20:46 GMT'}, {'version': 'v3', 'created': 'Thu, 28 Sep 2023 02:58:48 GMT'}]",2023-09-29,"[['Hu', 'Hai', ''], ['Zhang', 'Ziyin', ''], ['Huang', 'Weifang', ''], ['Lai', 'Jackie Yan-Ki', ''], ['Li', 'Aini', ''], ['Patterson', 'Yina', ''], ['Huang', 'Jiahui', ''], ['Zhang', 'Peng', ''], ['Lin', 'Chien-Jer Charles', ''], ['Wang', 'Rui', '']]",1,1,2023-05-23,3,10,2,2,0,2,e9a0b58b498e4d8b16706f04452e854caca1444a,258841631.0,https://www.semanticscholar.org/paper/e9a0b58b498e4d8b16706f04452e854caca1444a,arXiv.org,2023.0,49.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Linguistics', 'source': 's2-fos-model'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '145309512', 'name': 'Hai Hu'}, {'authorId': '2116462959', 'name': 'Ziyin Zhang'}, {'authorId': '2000111554', 'name': 'Wei Huang'}, {'authorId': '8123477', 'name': 'J. Lai'}, {'authorId': '2112544845', 'name': 'Aini Li'}, {'authorId': '2146275527', 'name': 'Yi Ma'}, {'authorId': '2218080662', 'name': 'Jiahui Huang'}, {'authorId': '2151330499', 'name': 'Peng Zhang'}, {'authorId': '2151038626', 'name': 'Rui Wang'}]","['University of Pennsylvania', 'City University of Hong Kong', 'Shanghai Jiao Tong University', 'Indiana University Bloomington', 'University of Washington', 'Brigham Young University']","['China', 'United States']",2023-05
2305.14221,Xinyu Zhu,"Xinyu Zhu, Cheng Yang, Bei Chen, Siheng Li, Jian-Guang Lou, Yujiu Yang",Question Answering as Programming for Solving Time-Sensitive Questions,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work we try to apply Large Language Models (LLMs) to reframe the Question Answering task as Programming (QAaP). Due to the inherent dynamic nature of the real world, factual questions frequently involve a symbolic constraint: time, solving these questions necessitates not only extensive world knowledge, but also advanced reasoning ability to satisfy the temporal constraints. Despite the remarkable intelligence exhibited by LLMs in various NLP tasks, our experiments reveal that the aforementioned problems continue to pose a significant challenge to existing LLMs. To solve these time-sensitive factual questions, considering that modern LLMs possess superior ability in both natural language understanding and programming,we endeavor to leverage LLMs to represent diversely expressed text as well-structured code, and thereby grasp the desired knowledge along with the underlying symbolic constraint. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 16:35:16 GMT'}]",2023-05-24,"[['Zhu', 'Xinyu', ''], ['Yang', 'Cheng', ''], ['Chen', 'Bei', ''], ['Li', 'Siheng', ''], ['Lou', 'Jian-Guang', ''], ['Yang', 'Yujiu', '']]",0,0,2023-05-23,1,6,1,0,0,0,ae80c69872d4af5814d9d7dfa771c794a93697d3,258841488.0,https://www.semanticscholar.org/paper/ae80c69872d4af5814d9d7dfa771c794a93697d3,arXiv.org,2023.0,38.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2116314158', 'name': 'Xinyu Zhu'}, {'authorId': '3443627', 'name': 'Cheng Yang'}, {'authorId': '143876723', 'name': 'B. Chen'}, {'authorId': '47319720', 'name': 'Siheng Li'}, {'authorId': '153249455', 'name': 'Jian-Guang Lou'}, {'authorId': '2108585311', 'name': 'Yujiu Yang'}]","['Tsinghua University', 'Microsoft']","['China', 'United States']",2023-05
2305.14239,Yixin Liu,"Yixin Liu, Alexander R. Fabbri, Pengfei Liu, Dragomir Radev, Arman
  Cohan",On Learning to Summarize with Large Language Models as References,GitHub Repo: https://github.com/yixinL7/SumLLM,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent studies have found that summaries generated by large language models (LLMs) are favored by human annotators over the original reference summaries in commonly used summarization datasets. Therefore, we investigate a new learning paradigm of text summarization models that considers the LLMs as the reference or the gold-standard oracle on commonly used summarization datasets such as the CNN/DailyMail dataset. To examine the standard practices that are aligned with the new learning setting, we propose a novel training method that is based on contrastive learning with LLMs as a summarization quality evaluator. For this reward-based training method, we investigate two different methods of utilizing LLMs for summary quality evaluation, namely GPTScore and GPTRank. Our experiments on the CNN/DailyMail dataset demonstrate that smaller summarization models trained by our proposed method can achieve performance equal to or surpass that of the reference LLMs, as evaluated by the LLMs themselves. This underscores the efficacy of our proposed paradigm in enhancing model performance over the standard maximum likelihood estimation (MLE) training method, and its efficiency since it only requires a small budget to access the LLMs. We release the training scripts, model outputs, and LLM-based evaluation results to facilitate future studies. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 16:56:04 GMT'}]",2023-05-24,"[['Liu', 'Yixin', ''], ['Fabbri', 'Alexander R.', ''], ['Liu', 'Pengfei', ''], ['Radev', 'Dragomir', ''], ['Cohan', 'Arman', '']]",0,1,2023-05-23,1,5,1,0,0,0,d96d6d7f492adb3005aa9371d85bbb882abb6fa4,258841126.0,https://www.semanticscholar.org/paper/d96d6d7f492adb3005aa9371d85bbb882abb6fa4,arXiv.org,2023.0,75.0,14.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108176413', 'name': 'Yixin Liu'}, {'authorId': '46255971', 'name': 'Alexander R. Fabbri'}, {'authorId': '144118452', 'name': 'Pengfei Liu'}, {'authorId': '9215251', 'name': 'Dragomir R. Radev'}, {'authorId': '2527954', 'name': 'Arman Cohan'}]","['Shanghai Jiao Tong University', 'Allen Institute for Artificial Intelligence', 'Yale University']","['China', 'United States']",2023-05
2305.14283,Xinbei Ma,"Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, Nan Duan",Query Rewriting for Retrieval-Augmented Large Language Models,working in progress,,,,cs.CL,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Large Language Models (LLMs) play a powerful \textit{Reader} of the \textit{Retrieve-then-Read} pipeline, making great progress in knowledge-based open-domain tasks. This work introduces a new framework, \textit{Rewrite-Retrieve-Read} that improves the retrieval-augmented method from the perspective of the query rewriting. Prior studies mostly contribute to adapt the retriever or stimulate the reader. Different from them, our approach pay attention of the query adaptation. Because the original query can not be always optimal to retrieve for the LLM, especially in the real world.(1) We first prompt an LLM to rewrite the queries, then conduct retrieval-augmented reading. (2) We further apply a small language model as a trainable rewriter, which rewrite the search query to cater to the frozen retriever and the LLM reader. To fine-tune the rewriter, we first use a pseudo data to conduct supervised warm-up training. Then the \textit{Retrieve-then-Read} pipeline is modeled as a reinforcement learning context. The rewriter is further trained as a policy model by maximize the reward of the pipeline performance. Evaluation is performed on two downstream tasks, open-domain QA and multiple choice. Our framework is proved effective and scalable. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 17:27:50 GMT'}]",2023-05-24,"[['Ma', 'Xinbei', ''], ['Gong', 'Yeyun', ''], ['He', 'Pengcheng', ''], ['Zhao', 'Hai', ''], ['Duan', 'Nan', '']]",0,0,2023-05-23,1,5,1,0,0,0,f743287be3ced6757de7ecb26d03815b22cd737b,258841283.0,https://www.semanticscholar.org/paper/f743287be3ced6757de7ecb26d03815b22cd737b,arXiv.org,2023.0,61.0,3.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2141114505', 'name': 'Xinbei Ma'}, {'authorId': '2171182', 'name': 'Yeyun Gong'}, {'authorId': '50462546', 'name': 'Pengcheng He'}, {'authorId': '2146232510', 'name': 'Hai Zhao'}, {'authorId': '46429989', 'name': 'Nan Duan'}]","['Shanghai Jiao Tong University', 'Microsoft']","['China', 'United States']",2023-05
2305.14303,Yilun Zhao,"Yilun Zhao, Zhenting Qi, Linyong Nan, Boyu Mi, Yixin Liu, Weijin Zou,
  Simeng Han, Xiangru Tang, Yumo Xu, Arman Cohan, Dragomir Radev",QTSumm: A New Benchmark for Query-Focused Table Summarization,work in progress,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  People primarily consult tables to conduct data analysis or answer specific questions. Text generation systems that can provide accurate table summaries tailored to users' information needs can facilitate more efficient access to relevant data insights. However, existing table-to-text generation studies primarily focus on converting tabular data into coherent statements, rather than addressing information-seeking purposes. In this paper, we define a new query-focused table summarization task, where text generation models have to perform human-like reasoning and analysis over the given table to generate a tailored summary, and we introduce a new benchmark named QTSumm for this task. QTSumm consists of 5,625 human-annotated query-summary pairs over 2,437 tables on diverse topics. Moreover, we investigate state-of-the-art models (i.e., text generation, table-to-text generation, and large language models) on the QTSumm dataset. Experimental results and manual analysis reveal that our benchmark presents significant challenges in table-to-text generation for future research. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 17:43:51 GMT'}]",2023-05-24,"[['Zhao', 'Yilun', ''], ['Qi', 'Zhenting', ''], ['Nan', 'Linyong', ''], ['Mi', 'Boyu', ''], ['Liu', 'Yixin', ''], ['Zou', 'Weijin', ''], ['Han', 'Simeng', ''], ['Tang', 'Xiangru', ''], ['Xu', 'Yumo', ''], ['Cohan', 'Arman', ''], ['Radev', 'Dragomir', '']]",0,0,2023-05-23,1,11,1,0,0,0,beeaa3b353f0afeb2387f8336c052042a9b78cc0,258841577.0,https://www.semanticscholar.org/paper/beeaa3b353f0afeb2387f8336c052042a9b78cc0,arXiv.org,2023.0,41.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '46316984', 'name': 'Yilun Zhao'}, {'authorId': '2186056193', 'name': 'Zhenting Qi'}, {'authorId': '51990260', 'name': 'Linyong Nan'}, {'authorId': '2124195718', 'name': 'Boyu Mi'}, {'authorId': '2108176413', 'name': 'Yixin Liu'}, {'authorId': '2166311011', 'name': 'Weijin Zou'}, {'authorId': '3226782', 'name': 'Simeng Han'}, {'authorId': '47274259', 'name': 'Xiangru Tang'}, {'authorId': '115986373', 'name': 'Yumo Xu'}, {'authorId': '2527954', 'name': 'Arman Cohan'}, {'authorId': '9215251', 'name': 'Dragomir R. Radev'}]","['Harvard University', 'Yale University', 'Zhejiang University', 'Allen Institute for Artificial Intelligence', 'University of Edinburgh']","['China', 'United States', 'United Kingdom']",2023-05
2305.14327,Da Yin,"Da Yin, Xiao Liu, Fan Yin, Ming Zhong, Hritik Bansal, Jiawei Han,
  Kai-Wei Chang",Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation,Work in progress,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Instruction tuning has emerged to enhance the capabilities of large language models (LLMs) in providing appropriate outputs based on input instructions. However, existing methods for collecting instruction-tuning data suffer from limitations in scalability and affordability. In this paper, we propose Dynosaur, a dynamic growth paradigm for instruction-tuning data curation. Built upon the metadata of existing NLP datasets, we generate multiple task instructions applicable to various NLP datasets and determine the relevant data fields for constructing instruction-tuning data with LLMs. Dynosaur offers several advantages: 1) lower generation costs (less than $12 for generating 800K instruction-tuning data), 2) good quality of instruction-tuning data (better performance than Alpaca and Instruction GPT-4 on Super-NI with comparable data sizes), and 3) the ability to grow dynamically by incorporating new datasets from Huggingface Datasets Platform. We further investigate continual learning as an approach to learning with the ever-growing instruction-tuning dataset. We demonstrate that replay methods not only help mitigate forgetting issues but help generalize to unseen tasks better. As a novel continual learning scenario for instruction tuning, selecting tasks based on instruction representations can be an effective replaying strategy. Code and data are released at \url{https://github.com/WadeYin9712/Dynosaur}. ","[{'version': 'v1', 'created': 'Tue, 23 May 2023 17:56:26 GMT'}]",2023-05-24,"[['Yin', 'Da', ''], ['Liu', 'Xiao', ''], ['Yin', 'Fan', ''], ['Zhong', 'Ming', ''], ['Bansal', 'Hritik', ''], ['Han', 'Jiawei', ''], ['Chang', 'Kai-Wei', '']]",0,1,2023-05-23,1,7,2,2,0,2,7742233d33da13910d0303e4ec8814a4e26e96e9,258841263.0,https://www.semanticscholar.org/paper/7742233d33da13910d0303e4ec8814a4e26e96e9,arXiv.org,2023.0,39.0,8.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '144508458', 'name': 'Da Yin'}, {'authorId': '49543720', 'name': 'Xiao Liu'}, {'authorId': '2065089223', 'name': 'Fan Yin'}, {'authorId': '1606040932', 'name': 'Ming Zhong'}, {'authorId': '103404553', 'name': 'Hritik Bansal'}, {'authorId': '2111759643', 'name': 'Jiawei Han'}, {'authorId': '2782886', 'name': 'Kai-Wei Chang'}]","['Peking University', 'University of California, Los Angeles', 'University of Illinois Urbana-Champaign']","['China', 'United States']",2023-05
2305.14898,Keming Lu,"Keming Lu, Xiaoman Pan, Kaiqiang Song, Hongming Zhang, Dong Yu,
  Jianshu Chen",PIVOINE: Instruction Tuning for Open-world Information Extraction,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We consider the problem of Open-world Information Extraction (Open-world IE), which extracts comprehensive entity profiles from unstructured texts. Different from the conventional closed-world setting of Information Extraction (IE), Open-world IE considers a more general situation where entities and relations could be beyond a predefined ontology. More importantly, we seek to develop a large language model (LLM) that is able to perform Open-world IE to extract desirable entity profiles characterized by (possibly fine-grained) natural language instructions. We achieve this by finetuning LLMs using instruction tuning. In particular, we construct INSTRUCTOPENWIKI, a substantial instruction tuning dataset for Open-world IE enriched with a comprehensive corpus, extensive annotations, and diverse instructions. We finetune the pretrained BLOOM models on INSTRUCTOPENWIKI and obtain PIVOINE, an LLM for Open-world IE with strong instruction-following capabilities. Our experiments demonstrate that PIVOINE significantly outperforms traditional closed-world methods and other LLM baselines, displaying impressive generalization capabilities on both unseen instructions and out-of-ontology cases. Consequently, PIVOINE emerges as a promising solution to tackle the open-world challenge in IE effectively. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 08:52:08 GMT'}]",2023-05-25,"[['Lu', 'Keming', ''], ['Pan', 'Xiaoman', ''], ['Song', 'Kaiqiang', ''], ['Zhang', 'Hongming', ''], ['Yu', 'Dong', ''], ['Chen', 'Jianshu', '']]",0,0,2023-05-24,1,6,1,1,1,0,0042c4f35d4a3ea947de8374e298099d476dad7f,258866183.0,https://www.semanticscholar.org/paper/0042c4f35d4a3ea947de8374e298099d476dad7f,arXiv.org,2023.0,36.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1515662094', 'name': 'K. Lu'}, {'authorId': '34741133', 'name': 'Xiaoman Pan'}, {'authorId': '50982080', 'name': 'Kaiqiang Song'}, {'authorId': '49723569', 'name': 'Hongming Zhang'}, {'authorId': '144580027', 'name': 'Dong Yu'}, {'authorId': '2108276402', 'name': 'Jianshu Chen'}]","['University of Southern California', 'Tencent']","['China', 'United States']",2023-05
2305.15005,Wenxuan Zhang,"Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, Lidong Bing",Sentiment Analysis in the Era of Large Language Models: A Reality Check,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Sentiment analysis (SA) has been a long-standing research area in natural language processing. It can offer rich insights into human sentiments and opinions and has thus seen considerable interest from both academia and industry. With the advent of large language models (LLMs) such as ChatGPT, there is a great potential for their employment on SA problems. However, the extent to which existing LLMs can be leveraged for different sentiment analysis tasks remains unclear. This paper aims to provide a comprehensive investigation into the capabilities of LLMs in performing various sentiment analysis tasks, from conventional sentiment classification to aspect-based sentiment analysis and multifaceted analysis of subjective texts. We evaluate performance across 13 tasks on 26 datasets and compare the results against small language models (SLMs) trained on domain-specific datasets. Our study reveals that while LLMs demonstrate satisfactory performance in simpler tasks, they lag behind in more complex tasks requiring deeper understanding or structured sentiment information. However, LLMs significantly outperform SLMs in few-shot learning settings, suggesting their potential when annotation resources are limited. We also highlight the limitations of current evaluation practices in assessing LLMs' SA abilities and propose a novel benchmark, \textsc{SentiEval}, for a more comprehensive and realistic evaluation. Data and code during our investigations are available at \url{https://github.com/DAMO-NLP-SG/LLM-Sentiment}. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 10:45:25 GMT'}]",2023-05-25,"[['Zhang', 'Wenxuan', ''], ['Deng', 'Yue', ''], ['Liu', 'Bing', ''], ['Pan', 'Sinno Jialin', ''], ['Bing', 'Lidong', '']]",1,1,2023-05-24,1,5,1,1,0,1,c589ddc6c6fb07189af7c1212f6eb15c5ff72cde,258866189.0,https://www.semanticscholar.org/paper/c589ddc6c6fb07189af7c1212f6eb15c5ff72cde,arXiv.org,2023.0,67.0,30.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '150341144', 'name': 'Wenxuan Zhang'}, {'authorId': '2162024594', 'name': 'Yue Deng'}, {'authorId': '49166962', 'name': 'Bing-Quan Liu'}, {'authorId': '1746914', 'name': 'Sinno Jialin Pan'}, {'authorId': '1996394', 'name': 'Lidong Bing'}]","['Alibaba', 'University of Illinois at Chicago', 'Chinese University of Hong Kong', 'Nanyang Technological University']","['China', 'United States', 'Singapore']",2023-05
2305.15014,Xingxuan Li,"Xingxuan Li, Liying Cheng, Qingyu Tan, Hwee Tou Ng, Shafiq Joty,
  Lidong Bing",Unlocking Temporal Question Answering for Large Language Models Using Code Execution,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have made significant progress in natural language processing (NLP), and are utilized extensively in various applications. Recent works, such as chain-of-thought (CoT), have shown that intermediate reasoning steps can improve the performance of LLMs for complex reasoning tasks, such as math problems and symbolic question-answering tasks. However, we notice the challenge that LLMs face when it comes to temporal reasoning. Our preliminary experiments show that generating intermediate reasoning steps does not always boost the performance of complex temporal question-answering tasks. Therefore, we propose a novel framework that combines the extraction capability of LLMs and the logical reasoning capability of a Python solver to tackle this issue. Extensive experiments and analysis demonstrate the effectiveness of our framework in handling intricate time-bound reasoning tasks. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 10:57:53 GMT'}]",2023-05-25,"[['Li', 'Xingxuan', ''], ['Cheng', 'Liying', ''], ['Tan', 'Qingyu', ''], ['Ng', 'Hwee Tou', ''], ['Joty', 'Shafiq', ''], ['Bing', 'Lidong', '']]",0,0,2023-05-24,1,6,1,0,0,0,220cb18e8e005024a7ed1c1d41b4b6fa4774847f,258865257.0,https://www.semanticscholar.org/paper/220cb18e8e005024a7ed1c1d41b4b6fa4774847f,arXiv.org,2023.0,31.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2155447436', 'name': 'Xingxuan Li'}, {'authorId': '123962152', 'name': 'Liying Cheng'}, {'authorId': '118358816', 'name': 'Qingyu Tan'}, {'authorId': '34789794', 'name': 'H. Ng'}, {'authorId': '2708940', 'name': 'Shafiq R. Joty'}, {'authorId': '1996394', 'name': 'Lidong Bing'}]","['Alibaba', 'National University of Singapore', 'United States Department of State', 'Nanyang Technological University', 'Salesforce AI']","['China', 'United States', 'Singapore']",2023-05
2305.15066,Jiayan Guo,"Jiayan Guo, Lun Du, Hengyu Liu, Mengyu Zhou, Xinyi He, Shi Han",GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking,,,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models~(LLM) like ChatGPT have become indispensable to artificial general intelligence~(AGI), demonstrating excellent performance in various natural language processing tasks. In the real world, graph data is ubiquitous and an essential part of AGI and prevails in domains like social network analysis, bioinformatics and recommender systems. The training corpus of large language models often includes some algorithmic components, which allows them to achieve certain effects on some graph data-related problems. However, there is still little research on their performance on a broader range of graph-structured data. In this study, we conduct an extensive investigation to assess the proficiency of LLMs in comprehending graph data, employing a diverse range of structural and semantic-related tasks. Our analysis encompasses 10 distinct tasks that evaluate the LLMs' capabilities in graph understanding. Through our study, we not only uncover the current limitations of language models in comprehending graph structures and performing associated reasoning tasks but also emphasize the necessity for further advancements and novel approaches to enhance their graph processing capabilities. Our findings contribute valuable insights towards bridging the gap between language models and graph understanding, paving the way for more effective graph mining and knowledge extraction. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 11:53:19 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Jul 2023 15:08:00 GMT'}]",2023-07-12,"[['Guo', 'Jiayan', ''], ['Du', 'Lun', ''], ['Liu', 'Hengyu', ''], ['Zhou', 'Mengyu', ''], ['He', 'Xinyi', ''], ['Han', 'Shi', '']]",1,1,2023-05-24,2,6,2,1,0,1,2b967d82b25088566980aaaf5a7062d90b2fb14f,258865990.0,https://www.semanticscholar.org/paper/2b967d82b25088566980aaaf5a7062d90b2fb14f,arXiv.org,2023.0,44.0,24.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2218911056', 'name': 'Jiayan Guo'}, {'authorId': '12723949', 'name': 'Lun Du'}, {'authorId': '2139794154', 'name': 'Hengyu Liu'}]","['Peking University', ""Xi'an Jiaotong University"", 'Microsoft', 'University of Technology Sydney']","['China', 'United States', 'Australia']",2023-05
2305.15072,Yuxuan Sun,"Yuxuan Sun, Chenglu Zhu, Sunyi Zheng, Kai Zhang, Zhongyi Shui,
  Xiaoxuan Yu, Yizhi Zhao, Honglin Li, Yunlong Zhang, Ruojia Zhao, Xinheng Lyu,
  Lin Yang",PathAsst: Redefining Pathology through Generative Foundation AI Assistant for Pathology,"13 pages, 5 figures, conference",,,,cs.CV cs.MM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, with significant applications in natural image interpretation. However, the field of pathology has largely remained untapped in this regard, despite the growing need for accurate, timely, and personalized diagnostics. To bridge the gap in pathology MLLMs, we present the PathAsst in this study, which is a generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. To develop PathAsst, we collect over 142K high-quality pathology image-text pairs from a variety of reliable sources, including PubMed, comprehensive pathology textbooks, reputable pathology websites, and private data annotated by pathologists. Leveraging the advanced capabilities of ChatGPT/GPT-4, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data, specifically tailored for the invocation of the pathology-specific models, allowing the PathAsst to effectively interact with these models based on the input image and user intent, consequently enhancing the model's diagnostic capabilities. Subsequently, our PathAsst is trained based on Vicuna-13B language model in coordination with the CLIP vision encoder. The results of PathAsst show the potential of harnessing the AI-powered generative foundation model to improve pathology diagnosis and treatment processes. We are committed to open-sourcing our meticulously curated dataset, as well as a comprehensive toolkit designed to aid researchers in the extensive collection and preprocessing of their own datasets. Resources can be obtained at https://github.com/superjamessyx/Generative-Foundation-AI-Assistant-for-Pathology. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 11:55:50 GMT'}]",2023-05-25,"[['Sun', 'Yuxuan', ''], ['Zhu', 'Chenglu', ''], ['Zheng', 'Sunyi', ''], ['Zhang', 'Kai', ''], ['Shui', 'Zhongyi', ''], ['Yu', 'Xiaoxuan', ''], ['Zhao', 'Yizhi', ''], ['Li', 'Honglin', ''], ['Zhang', 'Yunlong', ''], ['Zhao', 'Ruojia', ''], ['Lyu', 'Xinheng', ''], ['Yang', 'Lin', '']]",1,1,2023-05-24,1,12,2,3,1,2,86cbd30d1096b0c7e4ac6b03d97a8df12fd21457,258865639.0,https://www.semanticscholar.org/paper/86cbd30d1096b0c7e4ac6b03d97a8df12fd21457,arXiv.org,2023.0,47.0,7.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2163660875', 'name': 'Yuxuan Sun'}, {'authorId': '50812118', 'name': 'Chenglu Zhu'}, {'authorId': '102500363', 'name': 'S. Zheng'}, {'authorId': '145086492', 'name': 'Kai Zhang'}, {'authorId': '2174668671', 'name': 'Zhongyi Shui'}, {'authorId': '2164321508', 'name': 'Xiaoxuan Yu'}, {'authorId': '2206994095', 'name': 'Yi-Lei Zhao'}, {'authorId': '2115564011', 'name': 'Honglin Li'}, {'authorId': '153533583', 'name': 'Yunlong Zhang'}, {'authorId': '2218101317', 'name': 'Ruojia Zhao'}, {'authorId': '2218462095', 'name': 'Xinheng Lyu'}, {'authorId': '2155558305', 'name': 'Lin Yang'}]","['Westlake University', 'Zhejiang University', 'The Ohio State University']","['China', 'United States']",2023-05
2305.15225,Hongyin Luo,"Hongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim,
  Xixin Wu, Danny Fox, Helen Meng, James Glass",SAIL: Search-Augmented Instruction Learning,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) have been significantly improved by instruction fine-tuning, but still lack transparency and the ability to utilize up-to-date knowledge and information. In this work, we propose search-augmented instruction learning (SAIL), which grounds the language generation and instruction following abilities on complex search results generated by in-house and external search engines. With an instruction tuning corpus, we collect search results for each training case from different search APIs and domains, and construct a new search-grounded training set containing \textit{(instruction, grounding information, response)} triplets. We then fine-tune the LLaMA-7B model on the constructed training set. Since the collected results contain unrelated and disputing languages, the model needs to learn to ground on trustworthy search results, filter out distracting passages, and generate the target response. The search result-denoising process entails explicit trustworthy information selection and multi-hop reasoning, since the retrieved passages might be informative but not contain the instruction-following answer. Experiments show that the fine-tuned SAIL-7B model has a strong instruction-following ability, and it performs significantly better on transparency-sensitive tasks, including open-ended question answering and fact checking. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 15:07:30 GMT'}, {'version': 'v2', 'created': 'Sun, 25 Jun 2023 17:56:37 GMT'}]",2023-06-27,"[['Luo', 'Hongyin', ''], ['Chuang', 'Yung-Sung', ''], ['Gong', 'Yuan', ''], ['Zhang', 'Tianhua', ''], ['Kim', 'Yoon', ''], ['Wu', 'Xixin', ''], ['Fox', 'Danny', ''], ['Meng', 'Helen', ''], ['Glass', 'James', '']]",0,0,2023-05-24,2,9,1,1,1,0,a7977870b58e716cd93f571a3b75a610167a75bd,258865283.0,https://www.semanticscholar.org/paper/a7977870b58e716cd93f571a3b75a610167a75bd,arXiv.org,2023.0,42.0,6.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1944274', 'name': 'Hongyin Luo'}, {'authorId': '2475831', 'name': 'Yung-Sung Chuang'}, {'authorId': '145802952', 'name': 'Yuan Gong'}, {'authorId': '2146333115', 'name': 'Tianhua Zhang'}, {'authorId': '143827730', 'name': 'Yoon Kim'}, {'authorId': '1847260', 'name': 'Xixin Wu'}, {'authorId': '31997718', 'name': 'D. Fox'}, {'authorId': '145199941', 'name': 'H. Meng'}, {'authorId': '145898106', 'name': 'James R. Glass'}]","['MIT Computer Science and Artificial Intelligence Lab, Cambridge MA, USA', 'Massachusetts Institute of Technology', 'Chinese University of Hong Kong']","['China', 'United States']",2023-05
2305.15294,Zhihong Shao,"Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu
  Chen",Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy,Preprint,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which interleaves retrieval with generation when producing an output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 16:17:36 GMT'}]",2023-05-25,"[['Shao', 'Zhihong', ''], ['Gong', 'Yeyun', ''], ['Shen', 'Yelong', ''], ['Huang', 'Minlie', ''], ['Duan', 'Nan', ''], ['Chen', 'Weizhu', '']]",0,0,2023-05-24,1,6,1,0,0,0,a1675f47125aa409525c5f759b5e6bcc1c8831aa,258866037.0,https://www.semanticscholar.org/paper/a1675f47125aa409525c5f759b5e6bcc1c8831aa,arXiv.org,2023.0,40.0,10.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '144485528', 'name': 'Zhihong Shao'}, {'authorId': '2171182', 'name': 'Yeyun Gong'}, {'authorId': '1752875', 'name': 'Yelong Shen'}, {'authorId': '1730108', 'name': 'Minlie Huang'}, {'authorId': '46429989', 'name': 'Nan Duan'}, {'authorId': '2109136147', 'name': 'Weizhu Chen'}]","['State Key Lab of Intelligent Technology and Systems,', 'Artificial Intelligence Research Institute', 'Beijing Information Science & Technology University', 'Tsinghua University', 'Microsoft']","['China', 'United States', 'Spain']",2023-05
2305.15695,Xiaoyu Chen,"Xiaoyu Chen, Shenao Zhang, Pushi Zhang, Li Zhao, Jianyu Chen",Asking Before Action: Gather Information in Embodied Decision Making with Language Models,,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With strong capabilities of reasoning and a generic understanding of the world, Large Language Models (LLMs) have shown great potential in building versatile embodied decision making agents capable of performing diverse tasks. However, when deployed to unfamiliar environments, we show that LLM agents face challenges in efficiently gathering necessary information, leading to suboptimal performance. On the other hand, in unfamiliar scenarios, human individuals often seek additional information from their peers before taking action, leveraging external knowledge to avoid unnecessary trial and error. Building upon this intuition, we propose \textit{Asking Before Action} (ABA), a method that empowers the agent to proactively query external sources for pertinent information using natural language during their interactions in the environment. In this way, the agent is able to enhance its efficiency and performance by mitigating wasteful steps and circumventing the difficulties associated with exploration in unfamiliar environments. We empirically evaluate our method on an embodied decision making benchmark, ALFWorld, and demonstrate that despite modest modifications in prompts, our method exceeds baseline LLM agents by more than $40$%. Further experiments on two variants of ALFWorld illustrate that by imitation learning, ABA effectively retains and reuses queried and known information in subsequent tasks, mitigating the need for repetitive inquiries. Both qualitative and quantitative results exhibit remarkable performance on tasks that previous methods struggle to solve. ","[{'version': 'v1', 'created': 'Thu, 25 May 2023 04:05:08 GMT'}]",2023-05-26,"[['Chen', 'Xiaoyu', ''], ['Zhang', 'Shenao', ''], ['Zhang', 'Pushi', ''], ['Zhao', 'Li', ''], ['Chen', 'Jianyu', '']]",0,0,2023-05-25,1,5,1,0,0,0,d7a74d22139b8ff806f9dbcb5242b216eeac50c7,258887626.0,https://www.semanticscholar.org/paper/d7a74d22139b8ff806f9dbcb5242b216eeac50c7,arXiv.org,2023.0,51.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2218623338', 'name': 'Xiaoyu Chen'}, {'authorId': '2145522248', 'name': 'Shenao Zhang'}, {'authorId': '1570021289', 'name': 'Pushi Zhang'}, {'authorId': '2218154011', 'name': 'Li Zhao'}, {'authorId': '1391201846', 'name': 'Jianyu Chen'}]","['Tsinghua University', 'Northwestern University', 'Microsoft']","['China', 'United States']",2023-05
2305.15808,Yiqi Lin,"Yiqi Lin, Hao Wu, Ruichen Wang, Haonan Lu, Xiaodong Lin, Hui Xiong,
  Lin Wang",Towards Language-guided Interactive 3D Generation: LLMs as Layout Interpreter with Generative Feedback,Preprint. Work in Progres,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generating and editing a 3D scene guided by natural language poses a challenge, primarily due to the complexity of specifying the positional relations and volumetric changes within the 3D space. Recent advancements in Large Language Models (LLMs) have demonstrated impressive reasoning, conversational, and zero-shot generation abilities across various domains. Surprisingly, these models also show great potential in realizing and interpreting the 3D space. In light of this, we propose a novel language-guided interactive 3D generation system, dubbed LI3D, that integrates LLMs as a 3D layout interpreter into the off-the-shelf layout-to-3D generative models, allowing users to flexibly and interactively generate visual content. Specifically, we design a versatile layout structure base on the bounding boxes and semantics to prompt the LLMs to model the spatial generation and reasoning from language. Our system also incorporates LLaVA, a large language and vision assistant, to provide generative feedback from the visual aspect for improving the visual quality of generated content. We validate the effectiveness of LI3D, primarily in 3D generation and editing through multi-round interactions, which can be flexibly extended to 2D generation and editing. Various experiments demonstrate the potential benefits of incorporating LLMs in generative AI for applications, e.g., metaverse. Moreover, we benchmark the layout reasoning performance of LLMs with neural visual artist tasks, revealing their emergent ability in the spatial layout domain. ","[{'version': 'v1', 'created': 'Thu, 25 May 2023 07:43:39 GMT'}]",2023-05-26,"[['Lin', 'Yiqi', ''], ['Wu', 'Hao', ''], ['Wang', 'Ruichen', ''], ['Lu', 'Haonan', ''], ['Lin', 'Xiaodong', ''], ['Xiong', 'Hui', ''], ['Wang', 'Lin', '']]",0,0,2023-05-25,1,7,1,0,0,0,ef8c21e1f574495f0c80b8c1037dbdb886f0808d,258888229.0,https://www.semanticscholar.org/paper/ef8c21e1f574495f0c80b8c1037dbdb886f0808d,arXiv.org,2023.0,42.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '46396519', 'name': 'Yiqi Lin'}, {'authorId': '1664776313', 'name': 'Hao Wu'}, {'authorId': '2213709086', 'name': 'Ruichen Wang'}, {'authorId': '2130373', 'name': 'H. Lu'}, {'authorId': '2117690698', 'name': 'Xiaodong Lin'}, {'authorId': '2217886184', 'name': 'Hui Xiong'}, {'authorId': '2168616303', 'name': 'Lin Wang'}]","['Rutgers, The State University of New Jersey', 'Hong Kong University of Science and Technology']","['China', 'United States']",2023-05
2305.16344,Chongjian Yue,"Chongjian Yue, Xinrun Xu, Xiaojun Ma, Lun Du, Hengyu Liu, Zhiming
  Ding, Yanbing Jiang, Shi Han, Dongmei Zhang",Leveraging LLMs for KPIs Retrieval from Hybrid Long-Document: A Comprehensive Framework and Dataset,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) demonstrate exceptional performance in textual understanding and tabular reasoning tasks. However, their ability to comprehend and analyze hybrid text, containing textual and tabular data, remains underexplored. In this research, we specialize in harnessing the potential of LLMs to comprehend critical information from financial reports, which are hybrid long-documents. We propose an Automated Financial Information Extraction (AFIE) framework that enhances LLMs' ability to comprehend and extract information from financial reports. To evaluate AFIE, we develop a Financial Reports Numerical Extraction (FINE) dataset and conduct an extensive experimental analysis. Our framework is effectively validated on GPT-3.5 and GPT-4, yielding average accuracy increases of 53.94% and 33.77%, respectively, compared to a naive method. These results suggest that the AFIE framework offers accuracy for automated numerical extraction from complex, hybrid documents. ","[{'version': 'v1', 'created': 'Wed, 24 May 2023 10:35:58 GMT'}]",2023-05-29,"[['Yue', 'Chongjian', ''], ['Xu', 'Xinrun', ''], ['Ma', 'Xiaojun', ''], ['Du', 'Lun', ''], ['Liu', 'Hengyu', ''], ['Ding', 'Zhiming', ''], ['Jiang', 'Yanbing', ''], ['Han', 'Shi', ''], ['Zhang', 'Dongmei', '']]",0,1,2023-05-24,1,9,2,2,0,2,520628a4d5b0d609586292c78871ab6b9504a501,258947744.0,https://www.semanticscholar.org/paper/520628a4d5b0d609586292c78871ab6b9504a501,arXiv.org,2023.0,49.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2084608753', 'name': 'C. Yue'}, {'authorId': '2197205915', 'name': 'Xinru Xu'}, {'authorId': '2115573502', 'name': 'Xiao Ma'}, {'authorId': '12723949', 'name': 'Lun Du'}, {'authorId': '2139794154', 'name': 'Hengyu Liu'}, {'authorId': '2112354246', 'name': 'Zhiming Ding'}, {'authorId': '2219041182', 'name': 'Yanbing Jiang'}, {'authorId': '2109750123', 'name': 'Shi Han'}, {'authorId': '2140415600', 'name': 'Dongmei Zhang'}]","['Peking University', 'University of Technology', 'Chinese Academy of Sciences', 'Microsoft']","['China', 'United States', 'Russia']",2023-05
2305.16617,Zhijie Deng,"Zhijie Deng, Hongcheng Gao, Yibo Miao, Hao Zhang",Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model,,,,,cs.LG cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The detection of machine-generated text, especially from large language models (LLMs), is crucial in preventing serious social problems resulting from their misuse. Some methods train dedicated detectors on specific datasets but fall short in generalizing to unseen test data, while other zero-shot ones often yield suboptimal performance. Although the recent DetectGPT has shown promising detection performance, it suffers from significant inefficiency issues, as detecting a single candidate requires scoring hundreds of its perturbations with the source LLM. This paper aims to bridge this gap. Technically, we propose to incorporate a Bayesian surrogate model, which allows us to select typical samples based on Bayesian uncertainty and interpolate scores from typical samples to other ones, to improve query efficiency. Our empirical results demonstrate that our method significantly outperforms existing approaches under a low query budget. Notably, our method achieves similar performance with up to 2 times fewer queries than DetectGPT and 3.7% higher AUROC at a query number of 5. ","[{'version': 'v1', 'created': 'Fri, 26 May 2023 04:23:10 GMT'}]",2023-05-29,"[['Deng', 'Zhijie', ''], ['Gao', 'Hongcheng', ''], ['Miao', 'Yibo', ''], ['Zhang', 'Hao', '']]",0,1,2023-05-26,1,4,3,0,0,0,2eeff1edde4fc66284e7bedeb8eb8878ed4560f4,258947640.0,https://www.semanticscholar.org/paper/2eeff1edde4fc66284e7bedeb8eb8878ed4560f4,arXiv.org,2023.0,36.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '145114723', 'name': 'Zhijie Deng'}, {'authorId': '2162081759', 'name': 'Hongcheng Gao'}, {'authorId': '2188993538', 'name': 'Yibo Miao'}, {'authorId': '46702482', 'name': 'Hao Zhang'}]","['Shanghai Jiao Tong University', 'University of California, San Diego']","['China', 'United States']",2023-05
2305.17147,Zhaowei Zhang,"Zhaowei Zhang, Nian Liu, Siyuan Qi, Ceyao Zhang, Ziqi Rong, Song-Chun
  Zhu, Shuguang Cui, Yaodong Yang",Heterogeneous Value Evaluation for Large Language Models,"Our full prompts are released in the repo:
  https://github.com/zowiezhang/A2EHV",,,,cs.CL cs.AI cs.HC cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The emergent capabilities of Large Language Models (LLMs) have made it crucial to align their values with those of humans. Current methodologies typically attempt alignment with a homogeneous human value and requires human verification, yet lack consensus on the desired aspect and depth of alignment and resulting human biases. In this paper, we propose A2EHV, an Automated Alignment Evaluation with a Heterogeneous Value system that (1) is automated to minimize individual human biases, and (2) allows assessments against various target values to foster heterogeneous agents. Our approach pivots on the concept of value rationality, which represents the ability for agents to execute behaviors that satisfy a target value the most. The quantification of value rationality is facilitated by the Social Value Orientation framework from social psychology, which partitions the value space into four categories to assess social preferences from agents' behaviors. We evaluate the value rationality of eight mainstream LLMs and observe that large models are more inclined to align neutral values compared to those with strong personal values. By examining the behavior of these LLMs, we contribute to a deeper understanding of value alignment within a heterogeneous value system. ","[{'version': 'v1', 'created': 'Fri, 26 May 2023 02:34:20 GMT'}, {'version': 'v2', 'created': 'Thu, 1 Jun 2023 17:00:50 GMT'}]",2023-06-02,"[['Zhang', 'Zhaowei', ''], ['Liu', 'Nian', ''], ['Qi', 'Siyuan', ''], ['Zhang', 'Ceyao', ''], ['Rong', 'Ziqi', ''], ['Zhu', 'Song-Chun', ''], ['Cui', 'Shuguang', ''], ['Yang', 'Yaodong', '']]",0,0,2023-05-26,2,8,4,0,0,0,0cf9616021a281bf6cd4119c5cdd386f5a764481,258959189.0,https://www.semanticscholar.org/paper/0cf9616021a281bf6cd4119c5cdd386f5a764481,arXiv.org,2023.0,30.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Psychology', 'source': 's2-fos-model'}]","[{'authorId': '2174174943', 'name': 'Zhaowei Zhang'}, {'authorId': '2106437037', 'name': 'N. Liu'}, {'authorId': '3390244', 'name': 'Siyuan Qi'}, {'authorId': '2000868582', 'name': 'Ceyao Zhang'}, {'authorId': '2218455070', 'name': 'Ziqi Rong'}, {'authorId': '47796324', 'name': 'Yaodong Yang'}, {'authorId': '1745056', 'name': 'Shuguang Cui'}]","['Peking University', 'National Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI)', 'Chinese University of Hong Kong, Shenzhen', 'University of Michigan–Ann Arbor']","['China', 'United States']",2023-05
2305.17197,Hongyin Luo,"Jiaxin Ge, Hongyin Luo, Yoon Kim, James Glass",Entailment as Robust Self-Learner,Accepted by ACL 2023 main conference,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Entailment has been recognized as an important metric for evaluating natural language understanding (NLU) models, and recent studies have found that entailment pretraining benefits weakly supervised fine-tuning. In this work, we design a prompting strategy that formulates a number of different NLU tasks as contextual entailment. This approach improves the zero-shot adaptation of pretrained entailment models. Secondly, we notice that self-training entailment-based models with unlabeled data can significantly improve the adaptation performance on downstream tasks. To achieve more stable improvement, we propose the Simple Pseudo-Label Editing (SimPLE) algorithm for better pseudo-labeling quality in self-training. We also found that both pretrained entailment-based models and the self-trained models are robust against adversarial evaluation data. Experiments on binary and multi-class classification tasks show that SimPLE leads to more robust self-training results, indicating that the self-trained entailment models are more efficient and trustworthy than large language models on language understanding tasks. ","[{'version': 'v1', 'created': 'Fri, 26 May 2023 18:41:23 GMT'}]",2023-05-30,"[['Ge', 'Jiaxin', ''], ['Luo', 'Hongyin', ''], ['Kim', 'Yoon', ''], ['Glass', 'James', '']]",0,0,2023-05-26,1,4,1,0,0,0,4f2dfe2c224ff00a5ff6f0a852ffedb8d1209fb6,258960342.0,https://www.semanticscholar.org/paper/4f2dfe2c224ff00a5ff6f0a852ffedb8d1209fb6,Annual Meeting of the Association for Computational Linguistics,2023.0,80.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2214584825', 'name': 'Jiaxin Ge'}, {'authorId': '1944274', 'name': 'Hongyin Luo'}, {'authorId': '38367242', 'name': 'Yoon Kim'}, {'authorId': '145898106', 'name': 'James R. Glass'}]","['Peking University', 'Massachusetts Institute of Technology']","['China', 'United States']",2023-05
2305.17331,Zichun Yu,"Zichun Yu, Chenyan Xiong, Shi Yu and Zhiyuan Liu",Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In,Accepted to ACL 2023,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Retrieval augmentation can aid language models (LMs) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target LMs, we propose augmentation-adapted retriever (AAR), which learns LM's preferences obtained from a known source LM. Experiments on the MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM is able to significantly improve the zero-shot generalization of larger target LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates that the preferences of different LMs overlap, enabling AAR trained with a single source LM to serve as a generic plug-in for various target LMs. Our code is open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever. ","[{'version': 'v1', 'created': 'Sat, 27 May 2023 02:26:52 GMT'}]",2023-05-30,"[['Yu', 'Zichun', ''], ['Xiong', 'Chenyan', ''], ['Yu', 'Shi', ''], ['Liu', 'Zhiyuan', '']]",0,1,2023-05-27,1,4,2,4,2,2,24811cadf16519910f643b6084107164e6ca4219,258960666.0,https://www.semanticscholar.org/paper/24811cadf16519910f643b6084107164e6ca4219,Annual Meeting of the Association for Computational Linguistics,2023.0,52.0,5.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '103985655', 'name': 'Zichun Yu'}, {'authorId': '2139787803', 'name': 'Chenyan Xiong'}, {'authorId': '150311558', 'name': 'S. Yu'}, {'authorId': '2109232579', 'name': 'Zhiyuan Liu'}]","['Tsinghua University', 'Microsoft', 'University of Science and Technology Beijing']","['China', 'United States']",2023-05
2305.17455,Dachuan Shi,"Dachuan Shi, Chaofan Tao, Anyi Rao, Zhendong Yang, Chun Yuan, Jiaqi
  Wang",CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers,Technical Report,,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent vision-language models have achieved tremendous progress far beyond what we ever expected. However, their computational costs are also dramatically growing with rapid development, especially for the large models. It makes model acceleration exceedingly critical in a scenario of limited resources. Although extensively studied for unimodal models, the acceleration for multimodal models, especially the vision-language Transformers, is relatively under-explored. To pursue more efficient and accessible vision-language Transformers, this paper introduces \textbf{Cross}-\textbf{G}uided \textbf{E}nsemble of \textbf{T}okens (\textbf{\emph{CrossGET}}), a universal acceleration framework for vision-language Transformers. This framework adaptively combines tokens through real-time, cross-modal guidance, thereby achieving substantial acceleration while keeping high performance. \textit{CrossGET} has two key innovations: 1) \textit{Cross-Guided Matching and Ensemble}. \textit{CrossGET} incorporates cross-modal guided token matching and ensemble to exploit cross-modal information effectively, only introducing cross-modal tokens with negligible extra parameters. 2) \textit{Complete-Graph Soft Matching}. In contrast to the existing bipartite soft matching approach, \textit{CrossGET} introduces a complete-graph soft matching policy to achieve more reliable token-matching results while maintaining parallelizability and high efficiency. Extensive experiments are conducted on various vision-language tasks, including image-text retrieval, visual reasoning, image captioning, and visual question answering. Performance on both classic multimodal architectures and emerging multimodal LLMs demonstrate the effectiveness and versatility of the proposed \textit{CrossGET} framework. The code will be at \url{https://github.com/sdc17/CrossGET}. ","[{'version': 'v1', 'created': 'Sat, 27 May 2023 12:07:21 GMT'}, {'version': 'v2', 'created': 'Wed, 4 Oct 2023 22:11:50 GMT'}]",2023-10-06,"[['Shi', 'Dachuan', ''], ['Tao', 'Chaofan', ''], ['Rao', 'Anyi', ''], ['Yang', 'Zhendong', ''], ['Yuan', 'Chun', ''], ['Wang', 'Jiaqi', '']]",0,0,2023-05-27,2,6,2,0,0,0,1fed19184785b2b50163b3d8ccb7bfaa0321d1aa,258960368.0,https://www.semanticscholar.org/paper/1fed19184785b2b50163b3d8ccb7bfaa0321d1aa,arXiv.org,2023.0,95.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2004872473', 'name': 'Dachuan Shi'}, {'authorId': '144259094', 'name': 'Chaofan Tao'}, {'authorId': '36290866', 'name': 'Anyi Rao'}, {'authorId': '2149234038', 'name': 'Zhendong Yang'}, {'authorId': '2117728946', 'name': 'Chun Yuan'}, {'authorId': '2156546701', 'name': 'Jiaqi Wang'}]","['Shanghai Artificial Intelligence Laboratory', 'Stanford University', 'Tsinghua University', 'University of Hong Kong']","['China', 'United States', 'Hong Kong']",2023-05
2305.18084,Pengxiang Jin,"Pengxiang Jin, Shenglin Zhang, Minghua Ma, Haozhe Li, Yu Kang, Liqun
  Li, Yudong Liu, Bo Qiao, Chaoyun Zhang, Pu Zhao, Shilin He, Federica Sarro,
  Yingnong Dang, Saravan Rajmohan, Qingwei Lin, Dongmei Zhang",Assess and Summarize: Improve Outage Understanding with Large Language Models,,,,,cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Cloud systems have become increasingly popular in recent years due to their flexibility and scalability. Each time cloud computing applications and services hosted on the cloud are affected by a cloud outage, users can experience slow response times, connection issues or total service disruption, resulting in a significant negative business impact. Outages are usually comprised of several concurring events/source causes, and therefore understanding the context of outages is a very challenging yet crucial first step toward mitigating and resolving outages. In current practice, on-call engineers with in-depth domain knowledge, have to manually assess and summarize outages when they happen, which is time-consuming and labor-intensive. In this paper, we first present a large-scale empirical study investigating the way on-call engineers currently deal with cloud outages at Microsoft, and then present and empirically validate a novel approach (dubbed Oasis) to help the engineers in this task. Oasis is able to automatically assess the impact scope of outages as well as to produce human-readable summarization. Specifically, Oasis first assesses the impact scope of an outage by aggregating relevant incidents via multiple techniques. Then, it generates a human-readable summary by leveraging fine-tuned large language models like GPT-3.x. The impact assessment component of Oasis was introduced in Microsoft over three years ago, and it is now widely adopted, while the outage summarization component has been recently introduced, and in this article we present the results of an empirical evaluation we carried out on 18 real-world cloud systems as well as a human-based evaluation with outage owners. The results show that Oasis can effectively and efficiently summarize outages, and lead Microsoft to deploy its first prototype which is currently under experimental adoption by some of the incident teams. ","[{'version': 'v1', 'created': 'Mon, 29 May 2023 13:36:19 GMT'}]",2023-05-30,"[['Jin', 'Pengxiang', ''], ['Zhang', 'Shenglin', ''], ['Ma', 'Minghua', ''], ['Li', 'Haozhe', ''], ['Kang', 'Yu', ''], ['Li', 'Liqun', ''], ['Liu', 'Yudong', ''], ['Qiao', 'Bo', ''], ['Zhang', 'Chaoyun', ''], ['Zhao', 'Pu', ''], ['He', 'Shilin', ''], ['Sarro', 'Federica', ''], ['Dang', 'Yingnong', ''], ['Rajmohan', 'Saravan', ''], ['Lin', 'Qingwei', ''], ['Zhang', 'Dongmei', '']]",0,1,2023-05-29,1,16,1,1,0,1,9943bbb97a48d10b70453e62307c1c797ed64012,258959521.0,https://www.semanticscholar.org/paper/9943bbb97a48d10b70453e62307c1c797ed64012,arXiv.org,2023.0,31.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2450948', 'name': 'Pengxiang Jin'}, {'authorId': '2841424', 'name': 'Shenglin Zhang'}, {'authorId': '3152770', 'name': 'Minghua Ma'}, {'authorId': '2145539091', 'name': 'Haozhe Li'}, {'authorId': '2110042497', 'name': 'Yu Kang'}, {'authorId': '2007715136', 'name': 'Liqun Li'}, {'authorId': '2181323068', 'name': 'Yudong Liu'}, {'authorId': '2007693904', 'name': 'Bo Qiao'}, {'authorId': '3194878', 'name': 'Chaoyun Zhang'}, {'authorId': '2007757792', 'name': 'Pu Zhao'}, {'authorId': '3470504', 'name': 'Shilin He'}, {'authorId': '2103653', 'name': 'Federica Sarro'}, {'authorId': '34402895', 'name': 'Yingnong Dang'}, {'authorId': '148121358', 'name': 'S. Rajmohan'}, {'authorId': '2793487', 'name': 'Qingwei Lin'}, {'authorId': '2109581369', 'name': 'Dongmei Zhang'}]","['University College London', '2023, San Francisco, USA', 'Microsoft', 'Peking University', 'Nankai University']","['China', 'United States', 'United Kingdom']",2023-05
2305.18703,Chen Ling,"Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng,
  Junxiang Wang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, Tianjiao
  Zhao, Amit Panalkar, Wei Cheng, Haoyu Wang, Yanchi Liu, Zhengzhang Chen,
  Haifeng Chen, Chris White, Quanquan Gu, Jian Pei, and Liang Zhao",Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). Domain specification techniques are key to make large language models disruptive in many applications. Specifically, to solve these hurdles, there has been a notable increase in research and practices conducted in recent years on the domain specialization of LLMs. This emerging field of study, with its substantial potential for impact, necessitates a comprehensive and systematic review to better summarize and guide ongoing work in this area. In this article, we present a comprehensive survey on domain specification techniques for large language models, an emerging direction critical for large language model applications. First, we propose a systematic taxonomy that categorizes the LLM domain-specialization techniques based on the accessibility to LLMs and summarizes the framework for all the subcategories as well as their relations and differences to each other. Second, we present an extensive taxonomy of critical application domains that can benefit dramatically from specialized LLMs, discussing their practical significance and open challenges. Last, we offer our insights into the current research status and future trends in this area. ","[{'version': 'v1', 'created': 'Tue, 30 May 2023 03:00:30 GMT'}, {'version': 'v2', 'created': 'Wed, 31 May 2023 00:43:01 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Jul 2023 15:06:21 GMT'}, {'version': 'v4', 'created': 'Tue, 11 Jul 2023 18:34:08 GMT'}, {'version': 'v5', 'created': 'Sat, 26 Aug 2023 02:42:49 GMT'}]",2023-08-29,"[['Ling', 'Chen', ''], ['Zhao', 'Xujiang', ''], ['Lu', 'Jiaying', ''], ['Deng', 'Chengyuan', ''], ['Zheng', 'Can', ''], ['Wang', 'Junxiang', ''], ['Chowdhury', 'Tanmoy', ''], ['Li', 'Yun', ''], ['Cui', 'Hejie', ''], ['Zhang', 'Xuchao', ''], ['Zhao', 'Tianjiao', ''], ['Panalkar', 'Amit', ''], ['Cheng', 'Wei', ''], ['Wang', 'Haoyu', ''], ['Liu', 'Yanchi', ''], ['Chen', 'Zhengzhang', ''], ['Chen', 'Haifeng', ''], ['White', 'Chris', ''], ['Gu', 'Quanquan', ''], ['Pei', 'Jian', ''], ['Zhao', 'Liang', '']]",0,0,2023-05-30,5,21,2,0,0,0,6847b9658f287f430098199cd81bf26308da13f9,259502302.0,https://www.semanticscholar.org/paper/6847b9658f287f430098199cd81bf26308da13f9,,2023.0,331.0,13.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2059988575', 'name': 'Chen Ling'}, {'authorId': '50879401', 'name': 'Xujiang Zhao'}, {'authorId': '2117727751', 'name': 'Jiaying Lu'}, {'authorId': '151483422', 'name': 'Chengyuan Deng'}, {'authorId': '2182238045', 'name': 'Can Zheng'}, {'authorId': '2120473483', 'name': 'Junxiang Wang'}, {'authorId': '2123930262', 'name': 'Tanmoy Chowdhury'}, {'authorId': '2110425042', 'name': 'Yun-Qing Li'}, {'authorId': '2112821580', 'name': 'Hejie Cui'}, {'authorId': '2048981220', 'name': 'Xuchao Zhang'}, {'authorId': '2211987764', 'name': 'Tian-yu Zhao'}, {'authorId': '2218486790', 'name': 'Amit Panalkar'}, {'authorId': '145859270', 'name': 'Wei Cheng'}, {'authorId': '34269118', 'name': 'Haoyu Wang'}, {'authorId': '3215702', 'name': 'Yanchi Liu'}, {'authorId': '1766853', 'name': 'Zhengzhang Chen'}, {'authorId': '2204622281', 'name': 'Haifeng Chen'}, {'authorId': '2218495127', 'name': 'Chris White'}, {'authorId': '144966687', 'name': 'Quanquan Gu'}, {'authorId': '2188744953', 'name': 'Jian Pei'}, {'authorId': '1390553618', 'name': 'Carl Yang'}, {'authorId': '2151579859', 'name': 'Liang Zhao'}]","['Rutgers University', 'Jiaying University', 'Emory University', 'NEC Labs America, USA', 'Liang Zhao,', 'NEC Labs America, Princeton, NJ, USA', 'CHENGYUAN DENG * , NEC Labs America, USA', 'University of South Alabama', 'George Mason University', 'HAIFENG CHEN, CHRIS WHITE, NEC Labs America, USA', 'NEC Labs America, Princeton, NJ, USA; Haifeng Chen, Chris White,', 'NEC Labs America, Princeton, NJ, USA;', 'Duke University', 'Microsoft', 'Xujiang Zhao,', 'and University of Pittsburgh, USA', 'TIANJIAO ZHAO, AMIT PANALKAR, Blackrock, Inc., USA', 'University of California, Los Angeles', 'CAN ZHENG * , NEC Labs America, USA', 'BlackRock', 'and Rutgers University, New Brunswick, NJ, USA;', 'NEC Labs America, Princeton, NJ, USA and University of Pittsburgh, Pittsburgh, PA, USA; Junxiang Wang,']","['China', 'United States']",2023-05
2305.19213,Xiao Liu,"Xiao Liu, Da Yin, Chen Zhang, Yansong Feng, Dongyan Zhao",The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code,"Findings of ACL 2023. Code and data are available at
  https://github.com/xxxiaol/magic-if",,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  Causal reasoning, the ability to identify cause-and-effect relationship, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Given the fact that programming code may express causal relations more often and explicitly with conditional statements like ``if``, we want to explore whether Code-LLMs acquire better causal reasoning abilities. Our experiments show that compared to text-only LLMs, Code-LLMs with code prompts are significantly better in causal reasoning. We further intervene on the prompts from different aspects, and discover that the programming structure is crucial in code prompt design, while Code-LLMs are robust towards format perturbations. ","[{'version': 'v1', 'created': 'Tue, 30 May 2023 17:02:58 GMT'}]",2023-05-31,"[['Liu', 'Xiao', ''], ['Yin', 'Da', ''], ['Zhang', 'Chen', ''], ['Feng', 'Yansong', ''], ['Zhao', 'Dongyan', '']]",0,0,2023-05-30,1,5,1,0,0,0,498d1406fc4cddb05cd46477793f2e726a6fe238,258968140.0,https://www.semanticscholar.org/paper/498d1406fc4cddb05cd46477793f2e726a6fe238,Annual Meeting of the Association for Computational Linguistics,2023.0,52.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '49543720', 'name': 'Xiao Liu'}, {'authorId': '144508458', 'name': 'Da Yin'}, {'authorId': '2111574159', 'name': 'Chen Zhang'}, {'authorId': '2115387922', 'name': 'Yansong Feng'}, {'authorId': '144060462', 'name': 'Dongyan Zhao'}]","['Peking University', 'University of California, Los Angeles', 'State Key Laboratory of Media Convergence Production Technology and Systems', 'Beijing Academy of Artificial Intelligence']","['China', 'United States']",2023-05
2305.19466,Amirhossein Kazemnejad,"Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy,
  Payel Das, Siva Reddy",The Impact of Positional Encoding on Length Generalization in Transformers,,,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Length generalization, the ability to generalize from small training context sizes to larger ones, is a critical challenge in the development of Transformer-based language models. Positional encoding (PE) has been identified as a major factor influencing length generalization, but the exact impact of different PE schemes on extrapolation in downstream tasks remains unclear. In this paper, we conduct a systematic empirical study comparing the length generalization performance of decoder-only Transformers with five different position encoding approaches including Absolute Position Embedding (APE), T5's Relative PE, ALiBi, and Rotary, in addition to Transformers without positional encoding (NoPE). Our evaluation encompasses a battery of reasoning and mathematical tasks. Our findings reveal that the most commonly used positional encoding methods, such as ALiBi, Rotary, and APE, are not well suited for length generalization in downstream tasks. More importantly, NoPE outperforms other explicit positional encoding methods while requiring no additional computation. We theoretically demonstrate that NoPE can represent both absolute and relative PEs, but when trained with SGD, it mostly resembles T5's relative PE attention patterns. Finally, we find that scratchpad is not always helpful to solve length generalization and its format highly impacts the model's performance. Overall, our work suggests that explicit position embeddings are not essential for decoder-only Transformers to generalize well to longer sequences. ","[{'version': 'v1', 'created': 'Wed, 31 May 2023 00:29:55 GMT'}]",2023-06-01,"[['Kazemnejad', 'Amirhossein', ''], ['Padhi', 'Inkit', ''], ['Ramamurthy', 'Karthikeyan Natesan', ''], ['Das', 'Payel', ''], ['Reddy', 'Siva', '']]",0,0,2023-05-31,1,5,3,1,1,0,6f6e2e0311589a9af045f6acd00b7dee6d19fce4,258987259.0,https://www.semanticscholar.org/paper/6f6e2e0311589a9af045f6acd00b7dee6d19fce4,arXiv.org,2023.0,54.0,20.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1754452702', 'name': 'Amirhossein Kazemnejad'}, {'authorId': '8350409', 'name': 'Inkit Padhi'}, {'authorId': '1704263', 'name': 'K. Ramamurthy'}, {'authorId': '1730372', 'name': 'Payel Das'}, {'authorId': '145732771', 'name': 'Siva Reddy'}]","['ServiceNow Research', 'IBM Research - China', 'McGill University', 'Meta']","['Canada', 'United States', 'China']",2023-05
2305.19835,Li Bei,"Bei Li, Rui Wang, Junliang Guo, Kaitao Song, Xu Tan, Hany Hassan, Arul
  Menezes, Tong Xiao, Jiang Bian and JingBo Zhu",Deliberate then Generate: Enhanced Prompting Framework for Text Generation,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have shown remarkable success across a wide range of natural language generation tasks, where proper prompt designs make great impacts. While existing prompting methods are normally restricted to providing correct information, in this paper, we encourage the model to deliberate by proposing a novel Deliberate then Generate (DTG) prompting framework, which consists of error detection instructions and candidates that may contain errors. DTG is a simple yet effective technique that can be applied to various text generation tasks with minimal modifications. We conduct extensive experiments on 20+ datasets across 7 text generation tasks, including summarization, translation, dialogue, and more. We show that DTG consistently outperforms existing prompting methods and achieves state-of-the-art performance on multiple text generation tasks. We also provide in-depth analyses to reveal the underlying mechanisms of DTG, which may inspire future research on prompting for LLMs. ","[{'version': 'v1', 'created': 'Wed, 31 May 2023 13:23:04 GMT'}]",2023-06-01,"[['Li', 'Bei', ''], ['Wang', 'Rui', ''], ['Guo', 'Junliang', ''], ['Song', 'Kaitao', ''], ['Tan', 'Xu', ''], ['Hassan', 'Hany', ''], ['Menezes', 'Arul', ''], ['Xiao', 'Tong', ''], ['Bian', 'Jiang', ''], ['Zhu', 'JingBo', '']]",0,0,2023-05-31,1,10,2,0,0,0,c85c90ef9e9a71efe031c3f7d6e34561f91168fe,258987341.0,https://www.semanticscholar.org/paper/c85c90ef9e9a71efe031c3f7d6e34561f91168fe,arXiv.org,2023.0,48.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '49730090', 'name': 'Bei Li'}, {'authorId': '2151038979', 'name': 'Rui Wang'}, {'authorId': '13838086', 'name': 'Junliang Guo'}, {'authorId': '50982078', 'name': 'Kaitao Song'}, {'authorId': '2112782687', 'name': 'Xuejiao Tan'}, {'authorId': '143925074', 'name': 'Hany Hassan'}, {'authorId': '145280401', 'name': 'Arul Menezes'}, {'authorId': '1391183811', 'name': 'Tong Xiao'}, {'authorId': '2192822005', 'name': 'Jiang Bian'}, {'authorId': '1728004', 'name': 'Jingbo Zhu'}]","['Northeastern University', 'NiuTrans Research', 'Microsoft']","['China', 'United States']",2023-05
2306.00014,Zhuocheng Gong,"Zhuocheng Gong, Jiahao Liu, Qifan Wang, Yang Yang, Jingang Wang, Wei
  Wu, Yunsen Xian, Dongyan Zhao, Rui Yan",PreQuant: A Task-agnostic Quantization Approach for Pre-trained Language Models,Findings of ACL2023,,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  While transformer-based pre-trained language models (PLMs) have dominated a number of NLP applications, these models are heavy to deploy and expensive to use. Therefore, effectively compressing large-scale PLMs becomes an increasingly important problem. Quantization, which represents high-precision tensors with low-bit fix-point format, is a viable solution. However, most existing quantization methods are task-specific, requiring customized training and quantization with a large number of trainable parameters on each individual task. Inspired by the observation that the over-parameterization nature of PLMs makes it possible to freeze most of the parameters during the fine-tuning stage, in this work, we propose a novel ``quantize before fine-tuning'' framework, PreQuant, that differs from both quantization-aware training and post-training quantization. PreQuant is compatible with various quantization strategies, with outlier-aware parameter-efficient fine-tuning incorporated to correct the induced quantization error. We demonstrate the effectiveness of PreQuant on the GLUE benchmark using BERT, RoBERTa, and T5. We also provide an empirical investigation into the workflow of PreQuant, which sheds light on its efficacy. ","[{'version': 'v1', 'created': 'Tue, 30 May 2023 08:41:33 GMT'}]",2023-06-02,"[['Gong', 'Zhuocheng', ''], ['Liu', 'Jiahao', ''], ['Wang', 'Qifan', ''], ['Yang', 'Yang', ''], ['Wang', 'Jingang', ''], ['Wu', 'Wei', ''], ['Xian', 'Yunsen', ''], ['Zhao', 'Dongyan', ''], ['Yan', 'Rui', '']]",0,0,2023-05-30,1,9,2,1,1,0,0967134fb5c778d73003dabc9ccaa5841f09372b,258999233.0,https://www.semanticscholar.org/paper/0967134fb5c778d73003dabc9ccaa5841f09372b,Annual Meeting of the Association for Computational Linguistics,2023.0,43.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2165228008', 'name': 'Zhuocheng Gong'}, {'authorId': '2108421184', 'name': 'Jiahao Liu'}, {'authorId': '2145778781', 'name': 'Qifan Wang'}, {'authorId': '2152915671', 'name': 'Yang Yang'}, {'authorId': '2109593338', 'name': 'Jingang Wang'}, {'authorId': '2118256028', 'name': 'Wei Wu'}, {'authorId': '2069503881', 'name': 'Yunsen Xian'}, {'authorId': '144060462', 'name': 'Dongyan Zhao'}, {'authorId': '144539156', 'name': 'Rui Yan'}]","['Ministry of Education', 'Beijing Academy of Artificial Intelligence', 'Renmin University of China', 'National Key Laboratory of General Artificial Intelligence', 'Peking University', 'Meituan', 'Meta']","['China', 'United States', 'Thailand']",2023-05
2306.03799,Dong Yang,"Fobo Shi, Peijun Qing, Dong Yang, Nan Wang, Youbo Lei, Haonan Lu,
  Xiaodong Lin",Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models,Natural language processing (NLP),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Prompt engineering is an essential technique for enhancing the abilities of large language models (LLMs) by providing explicit and specific instructions. It enables LLMs to excel in various tasks, such as arithmetic reasoning, question answering, summarization, relation extraction, machine translation, and sentiment analysis. Researchers have been actively exploring different prompt engineering strategies, such as Chain of Thought (CoT), Zero-CoT, and In-context learning. However, an unresolved problem arises from the fact that current approaches lack a solid theoretical foundation for determining optimal prompts. To address this issue in prompt engineering, we propose a new and effective approach called Prompt Space. Our methodology utilizes text embeddings to obtain basis vectors by matrix decomposition, and then constructs a space for representing all prompts. Prompt Space significantly outperforms state-of-the-art prompt paradigms on ten public reasoning benchmarks. Notably, without the help of the CoT method and the prompt ""Let's think step by step"", Prompt Space shows superior performance over the few-shot method. Overall, our approach provides a robust and fundamental theoretical framework for selecting simple and effective prompts. This advancement marks a significant step towards improving prompt engineering for a wide variety of applications in LLMs. ","[{'version': 'v1', 'created': 'Tue, 6 Jun 2023 15:43:16 GMT'}]",2023-06-07,"[['Shi', 'Fobo', ''], ['Qing', 'Peijun', ''], ['Yang', 'Dong', ''], ['Wang', 'Nan', ''], ['Lei', 'Youbo', ''], ['Lu', 'Haonan', ''], ['Lin', 'Xiaodong', '']]",0,0,2023-06-06,1,7,1,0,0,0,2d338cdd12091814dec11155d3f6f848d7bab4d8,259088958.0,https://www.semanticscholar.org/paper/2d338cdd12091814dec11155d3f6f848d7bab4d8,arXiv.org,2023.0,42.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2212609873', 'name': 'Fobo Shi'}, {'authorId': '2188984699', 'name': 'Peijun Qing'}, {'authorId': '144041880', 'name': 'D. Yang'}, {'authorId': '144457723', 'name': 'Nan Wang'}, {'authorId': '2181902670', 'name': 'Youbo Lei'}, {'authorId': '2130373', 'name': 'H. Lu'}, {'authorId': '2117690698', 'name': 'Xiaodong Lin'}]","[""Xi'an Jiaotong University"", 'OPPO', 'Dartmouth College', 'Central China Normal University', 'Rugster University']","['China', 'United States']",2023-06
2306.03823,Sukhpal Singh Gill,"Sukhpal Singh Gill, Minxian Xu, Panos Patros, Huaming Wu, Rupinder
  Kaur, Kamalpreet Kaur, Stephanie Fuller, Manmeet Singh, Priyansh Arora, Ajith
  Kumar Parlikad, Vlado Stankovski, Ajith Abraham, Soumya K. Ghosh, Hanan
  Lutfiyya, Salil S. Kanhere, Rami Bahsoon, Omer Rana, Schahram Dustdar, Rizos
  Sakellariou, Steve Uhlig, Rajkumar Buyya",Transformative Effects of ChatGPT on Modern Education: Emerging Era of AI Chatbots,Preprint submitted to IoTCPS Elsevier (2023),"Internet of Things and Cyber-Physical Systems (Elsevier), Volume
  4, 2024, Pages 19-23",10.1016/j.iotcps.2023.06.002,,cs.CY cs.AI cs.CL,http://creativecommons.org/publicdomain/zero/1.0/,"  ChatGPT, an AI-based chatbot, was released to provide coherent and useful replies based on analysis of large volumes of data. In this article, leading scientists, researchers and engineers discuss the transformative effects of ChatGPT on modern education. This research seeks to improve our knowledge of ChatGPT capabilities and its use in the education sector, identifying potential concerns and challenges. Our preliminary evaluation concludes that ChatGPT performed differently in each subject area including finance, coding and maths. While ChatGPT has the ability to help educators by creating instructional content, offering suggestions and acting as an online educator to learners by answering questions and promoting group work, there are clear drawbacks in its use, such as the possibility of producing inaccurate or false data and circumventing duplicate content (plagiarism) detectors where originality is essential. The often reported hallucinations within Generative AI in general, and also relevant for ChatGPT, can render its use of limited benefit where accuracy is essential. What ChatGPT lacks is a stochastic measure to help provide sincere and sensitive communication with its users. Academic regulations and evaluation practices used in educational institutions need to be updated, should ChatGPT be used as a tool in education. To address the transformative effects of ChatGPT on the learning environment, educating teachers and students alike about its capabilities and limitations will be crucial. ","[{'version': 'v1', 'created': 'Thu, 25 May 2023 17:35:57 GMT'}]",2023-07-18,"[['Gill', 'Sukhpal Singh', ''], ['Xu', 'Minxian', ''], ['Patros', 'Panos', ''], ['Wu', 'Huaming', ''], ['Kaur', 'Rupinder', ''], ['Kaur', 'Kamalpreet', ''], ['Fuller', 'Stephanie', ''], ['Singh', 'Manmeet', ''], ['Arora', 'Priyansh', ''], ['Parlikad', 'Ajith Kumar', ''], ['Stankovski', 'Vlado', ''], ['Abraham', 'Ajith', ''], ['Ghosh', 'Soumya K.', ''], ['Lutfiyya', 'Hanan', ''], ['Kanhere', 'Salil S.', ''], ['Bahsoon', 'Rami', ''], ['Rana', 'Omer', ''], ['Dustdar', 'Schahram', ''], ['Sakellariou', 'Rizos', ''], ['Uhlig', 'Steve', ''], ['Buyya', 'Rajkumar', '']]",1,1,2023-05-25,1,21,3,1,0,1,95f0884c30bdc83551fe3de67a6bb473cc38f893,259088562.0,https://www.semanticscholar.org/paper/95f0884c30bdc83551fe3de67a6bb473cc38f893,Internet of Things and Cyber-Physical Systems,2023.0,39.0,14.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Education', 'source': 's2-fos-model'}]","[{'authorId': '31043248', 'name': 'S. S. Gill'}, {'authorId': '2889350', 'name': 'Minxian Xu'}, {'authorId': '151473641', 'name': 'Panos Patros'}, {'authorId': '47987518', 'name': 'Huaming Wu'}, {'authorId': '2130749816', 'name': 'Rupinder Kaur'}, {'authorId': '1751000', 'name': 'K. Kaur'}, {'authorId': '2014841369', 'name': 'Stephanie Fuller'}, {'authorId': '2110677193', 'name': 'Manmeet Singh'}, {'authorId': '2219452948', 'name': 'Priyansh Arora'}, {'authorId': '52050652', 'name': 'A. Parlikad'}, {'authorId': '3034012', 'name': 'V. Stankovski'}, {'authorId': '2064038663', 'name': 'Ajith Abraham'}, {'authorId': '2155615327', 'name': 'Soumya K. Ghosh'}, {'authorId': '1769104', 'name': 'H. Lutfiyya'}, {'authorId': '1733096', 'name': 'S. Kanhere'}, {'authorId': '1804288', 'name': 'R. Bahsoon'}, {'authorId': '2099659875', 'name': 'O. Rana'}, {'authorId': '1691109', 'name': 'S. Dustdar'}, {'authorId': '1741531', 'name': 'R. Sakellariou'}, {'authorId': '2157668837', 'name': 'Steve Uhlig'}, {'authorId': '1709598', 'name': 'R. Buyya'}]","['Queen Mary University of London', 'TU Wien', 'Western University', 'Raygun Performance Monitoring, Wellington, New Zealand', 'University of Melbourne', 'The University of Texas at Austin', 'University of Ljubljana', 'Shenzhen Institutes of Advanced Technology', 'Microsoft', 'University of Cambridge', 'University of Manchester', 'Machine Intelligence Research Labs', 'UNSW Sydney', 'University of Birmingham', ""King's College Hospital"", 'Cymax Group Technologies, British Columbia, Canada', 'Cardiff University', 'Indian Institute of Tropical Meteorology', 'Indian Institute of Technology Kharagpur', 'Tianjin University']","['Canada', 'Slovenia', 'New Zealand', 'United States', 'India', 'United Kingdom', 'China', 'Austria', 'Australia']",2023-05
2306.04528,Jindong Wang,"Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong
  Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, Xing Xie",PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts,"Technical report; updated with new experiments and related work; 27
  pages; code is at: https://github.com/microsoft/promptbench",,,,cs.CL cs.CR cs.LG,http://creativecommons.org/licenses/by/4.0/,"  The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptBench, a robustness benchmark designed to measure LLMs' resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. These prompts are then employed in diverse tasks, such as sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4,032 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets, with 567,084 test samples in total. Our findings demonstrate that contemporary LLMs are vulnerable to adversarial prompts. Furthermore, we present comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users. We make our code, prompts, and methodologies to generate adversarial prompts publicly accessible, thereby enabling and encouraging collaborative exploration in this pivotal field: https://github.com/microsoft/promptbench. ","[{'version': 'v1', 'created': 'Wed, 7 Jun 2023 15:37:00 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Jun 2023 12:44:10 GMT'}, {'version': 'v3', 'created': 'Thu, 24 Aug 2023 07:09:25 GMT'}]",2023-08-25,"[['Zhu', 'Kaijie', ''], ['Wang', 'Jindong', ''], ['Zhou', 'Jiaheng', ''], ['Wang', 'Zichen', ''], ['Chen', 'Hao', ''], ['Wang', 'Yidong', ''], ['Yang', 'Linyi', ''], ['Ye', 'Wei', ''], ['Gong', 'Neil Zhenqiang', ''], ['Zhang', 'Yue', ''], ['Xie', 'Xing', '']]",0,0,2023-06-07,3,11,3,0,0,0,77d6d7482d1a32ad147c39993758b6c63816f5c0,259095572.0,https://www.semanticscholar.org/paper/77d6d7482d1a32ad147c39993758b6c63816f5c0,arXiv.org,2023.0,123.0,46.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2219270546', 'name': 'Kaijie Zhu'}, {'authorId': '1519290245', 'name': 'Jindong Wang'}, {'authorId': '2188574368', 'name': 'Jiaheng Zhou'}, {'authorId': '47196709', 'name': 'Zichen Wang'}, {'authorId': '2051536212', 'name': 'Hao Chen'}, {'authorId': '2108024273', 'name': 'Yidong Wang'}, {'authorId': '2145500840', 'name': 'Linyi Yang'}, {'authorId': '2147205193', 'name': 'Weirong Ye'}, {'authorId': '144516687', 'name': 'N. Gong'}, {'authorId': '2211964951', 'name': 'Yue Zhang'}, {'authorId': '1576441343', 'name': 'Xingxu Xie'}]","['Duke University', 'Chinese Academy of Sciences', 'Microsoft']","['China', 'United States']",2023-06
2306.04556,Arjun Guha,"Hannah McLean Babe, Sydney Nguyen, Yangtian Zi, Arjun Guha, Molly Q
  Feldman, Carolyn Jane Anderson",StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code,,,,,cs.LG cs.HC cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Code LLMs are being rapidly deployed and there is evidence that they can make professional programmers more productive. Current benchmarks for code generation measure whether models generate correct programs given an expert prompt. In this paper, we present a new benchmark containing multiple prompts per problem, written by a specific population of non-expert prompters: beginning programmers. StudentEval contains 1,749 prompts for 48 problems, written by 80 students who have only completed one semester of Python programming. Our students wrote these prompts while working interactively with a Code LLM, and we observed very mixed success rates. We use StudentEval to evaluate 5 Code LLMs and find that StudentEval is a better discriminator of model performance than existing benchmarks. We analyze the prompts and find significant variation in students' prompting techniques. We also find that nondeterministic LLM sampling could mislead students into thinking that their prompts are more (or less) effective than they actually are, which has implications for how to teach with Code LLMs. ","[{'version': 'v1', 'created': 'Wed, 7 Jun 2023 16:03:55 GMT'}]",2023-06-08,"[['Babe', 'Hannah McLean', ''], ['Nguyen', 'Sydney', ''], ['Zi', 'Yangtian', ''], ['Guha', 'Arjun', ''], ['Feldman', 'Molly Q', ''], ['Anderson', 'Carolyn Jane', '']]",0,0,2023-06-07,1,6,3,0,0,0,a4929de687f3c6937dabbf733258af635781d3c4,259095478.0,https://www.semanticscholar.org/paper/a4929de687f3c6937dabbf733258af635781d3c4,arXiv.org,2023.0,27.0,6.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2219267303', 'name': 'Hannah McLean Babe'}, {'authorId': '2181287609', 'name': 'S. Nguyen'}, {'authorId': '2181729017', 'name': 'Yangtian Zi'}, {'authorId': '2100712', 'name': 'Arjun Guha'}, {'authorId': '47637572', 'name': 'Molly Q. Feldman'}, {'authorId': '144901955', 'name': 'Carolyn Jane Anderson'}]","['Wellesley College', 'Oberlin College', 'Northeastern University']","['China', 'United States']",2023-06
2306.04618,Yangyi Chen,"Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Fangyuan Zou,
  Xingyi Cheng, Heng Ji, Zhiyuan Liu, Maosong Sun","Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations",Code is available at \url{https://github.com/lifan-yuan/OOD_NLP},,,,cs.CL cs.CR cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  This paper reexamines the research on out-of-distribution (OOD) robustness in the field of NLP. We find that the distribution shift settings in previous studies commonly lack adequate challenges, hindering the accurate evaluation of OOD robustness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we conduct a series of experiments on pre-trained language models for analysis and evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the relationship between in-distribution (ID) and OOD performance. We identify three typical types that unveil the inner learning mechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and find that, despite exhibiting some effectiveness in specific cases, they do not offer significant improvement compared to vanilla fine-tuning. Further, we evaluate 5 LLMs with various adaptation paradigms and find that when sufficient ID data is available, fine-tuning domain-specific models outperform LLMs on ID examples significantly. However, in the case of OOD instances, prioritizing LLMs with in-context learning yields better results. We identify that both fine-tuned small models and LLMs face challenges in effectively addressing downstream tasks. The code is public at \url{https://github.com/lifan-yuan/OOD_NLP}. ","[{'version': 'v1', 'created': 'Wed, 7 Jun 2023 17:47:03 GMT'}]",2023-06-08,"[['Yuan', 'Lifan', ''], ['Chen', 'Yangyi', ''], ['Cui', 'Ganqu', ''], ['Gao', 'Hongcheng', ''], ['Zou', 'Fangyuan', ''], ['Cheng', 'Xingyi', ''], ['Ji', 'Heng', ''], ['Liu', 'Zhiyuan', ''], ['Sun', 'Maosong', '']]",0,0,2023-06-07,1,9,3,0,0,0,1a55d16c14587edda62dc9c9ff09e0b531dd169c,259096157.0,https://www.semanticscholar.org/paper/1a55d16c14587edda62dc9c9ff09e0b531dd169c,arXiv.org,2023.0,114.0,6.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2152195191', 'name': 'Lifan Yuan'}, {'authorId': '123331686', 'name': 'Yangyi Chen'}, {'authorId': '52297757', 'name': 'Ganqu Cui'}, {'authorId': '2162081759', 'name': 'Hongcheng Gao'}, {'authorId': '2073382741', 'name': 'Fangyuan Zou'}, {'authorId': '26382255', 'name': 'Xingyi Cheng'}, {'authorId': '2072975663', 'name': 'Heng Ji'}, {'authorId': '2141313179', 'name': 'Zhiyuan Liu'}, {'authorId': '1753344', 'name': 'Maosong Sun'}]","['University of Illinois Urbana-Champaign', 'Tsinghua University', 'University of Chinese Academy of Sciences']","['China', 'United States']",2023-06
2306.05443,Qianqian Xie,"Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng,
  Alejandro Lopez-Lira, Jimin Huang","PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance","12 pages, 1 figures",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although large language models (LLMs) has shown great performance on natural language processing (NLP) in the financial domain, there are no publicly available financial tailtored LLMs, instruction tuning datasets, and evaluation benchmarks, which is critical for continually pushing forward the open-source development of financial artificial intelligence (AI). This paper introduces PIXIU, a comprehensive framework including the first financial LLM based on fine-tuning LLaMA with instruction data, the first instruction data with 136K data samples to support the fine-tuning, and an evaluation benchmark with 5 tasks and 9 datasets. We first construct the large-scale multi-task instruction data considering a variety of financial tasks, financial document types, and financial data modalities. We then propose a financial LLM called FinMA by fine-tuning LLaMA with the constructed dataset to be able to follow instructions for various financial tasks. To support the evaluation of financial LLMs, we propose a standardized benchmark that covers a set of critical financial tasks, including five financial NLP tasks and one financial prediction task. With this benchmark, we conduct a detailed analysis of FinMA and several existing LLMs, uncovering their strengths and weaknesses in handling critical financial tasks. The model, datasets, benchmark, and experimental results are open-sourced to facilitate future research in financial AI. ","[{'version': 'v1', 'created': 'Thu, 8 Jun 2023 14:20:29 GMT'}]",2023-06-12,"[['Xie', 'Qianqian', ''], ['Han', 'Weiguang', ''], ['Zhang', 'Xiao', ''], ['Lai', 'Yanzhao', ''], ['Peng', 'Min', ''], ['Lopez-Lira', 'Alejandro', ''], ['Huang', 'Jimin', '']]",0,0,2023-06-08,1,7,2,1,1,0,109929be7890ef982fb3b6be0d78609cfab1ea13,259129602.0,https://www.semanticscholar.org/paper/109929be7890ef982fb3b6be0d78609cfab1ea13,arXiv.org,2023.0,38.0,19.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '145229872', 'name': 'Qianqian Xie'}, {'authorId': '104843747', 'name': 'Weiguang Han'}, {'authorId': '2177329782', 'name': 'Xiao Zhang'}, {'authorId': '2202405535', 'name': 'Yanzhao Lai'}, {'authorId': '47278503', 'name': 'Min Peng'}, {'authorId': '1411351217', 'name': 'Alejandro Lopez-Lira'}, {'authorId': '2555230', 'name': 'Jimin Huang'}]","['University of Florida', 'Wuhan University', 'Sun Yat-sen University', 'ChanceFocus AMC. Shanghai, China', 'Southwest Jiaotong University']","['China', 'United States']",2023-06
2306.06211,Chaoning Zhang,"Chaoning Zhang, Fachrina Dewi Puspitasari, Sheng Zheng, Chenghao Li,
  Yu Qiao, Taegoo Kang, Xinru Shan, Chenshuang Zhang, Caiyan Qin, Francois
  Rameau, Lik-Hang Lee, Sung-Ho Bae, Choong Seon Hong",A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering,"First survey on Segment Anything Model (SAM), work under progress",,,,cs.CV,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Segment anything model (SAM) developed by Meta AI Research has recently attracted significant attention. Trained on a large segmentation dataset of over 1 billion masks, SAM is capable of segmenting any object on a certain image. In the original SAM work, the authors turned to zero-short transfer tasks (like edge detection) for evaluating the performance of SAM. Recently, numerous works have attempted to investigate the performance of SAM in various scenarios to recognize and segment objects. Moreover, numerous projects have emerged to show the versatility of SAM as a foundation model by combining it with other models, like Grounding DINO, Stable Diffusion, ChatGPT, etc. With the relevant papers and projects increasing exponentially, it is challenging for the readers to catch up with the development of SAM. To this end, this work conducts the first yet comprehensive survey on SAM. This is an ongoing project and we intend to update the manuscript on a regular basis. Therefore, readers are welcome to contact us if they complete new works related to SAM so that we can include them in our next version. ","[{'version': 'v1', 'created': 'Fri, 12 May 2023 07:21:59 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Jun 2023 01:12:29 GMT'}, {'version': 'v3', 'created': 'Mon, 3 Jul 2023 08:35:14 GMT'}]",2023-07-04,"[['Zhang', 'Chaoning', ''], ['Puspitasari', 'Fachrina Dewi', ''], ['Zheng', 'Sheng', ''], ['Li', 'Chenghao', ''], ['Qiao', 'Yu', ''], ['Kang', 'Taegoo', ''], ['Shan', 'Xinru', ''], ['Zhang', 'Chenshuang', ''], ['Qin', 'Caiyan', ''], ['Rameau', 'Francois', ''], ['Lee', 'Lik-Hang', ''], ['Bae', 'Sung-Ho', ''], ['Hong', 'Choong Seon', '']]",1,1,2023-05-12,3,13,1,1,0,1,42219b26a503d03bf70e9953edc3af94c255cb2a,259138732.0,https://www.semanticscholar.org/paper/42219b26a503d03bf70e9953edc3af94c255cb2a,arXiv.org,2023.0,150.0,19.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '31044159', 'name': 'Chaoning Zhang'}, {'authorId': '2211960167', 'name': 'Sheng Zheng'}, {'authorId': '2144124301', 'name': 'Chenghao Li'}, {'authorId': '2209185953', 'name': 'Yu Qiao'}, {'authorId': '2213065322', 'name': 'Taegoo Kang'}, {'authorId': '2036753387', 'name': 'Xinru Shan'}, {'authorId': '48934876', 'name': 'Chenshuang Zhang'}, {'authorId': '2219915744', 'name': 'Caiyan Qin'}, {'authorId': '2771818', 'name': 'François Rameau'}, {'authorId': '40547898', 'name': 'S. Bae'}, {'authorId': '2159807650', 'name': 'Choong-Seon Hong'}]","['Beijing Institute of Technology', 'Harbin Institute of Technology', 'Microsoft', 'Hong Kong Polytechnic University', 'Kyung Hee University']","['China', 'United States', 'South Korea', 'Hong Kong']",2023-05
2306.06615,Jiatong Li,"Jiatong Li, Yunqing Liu, Wenqi Fan, Xiao-Yong Wei, Hui Liu, Jiliang
  Tang, and Qing Li",Empowering Molecule Discovery for Molecule-Caption Translation with Large Language Models: A ChatGPT Perspective,"Our implementation is available at:
  https://github.com/phenixace/MolReGPT",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Molecule discovery plays a crucial role in various scientific fields, advancing the design of tailored materials and drugs. Traditional methods for molecule discovery follow a trial-and-error process, which are both time-consuming and costly, while computational approaches such as artificial intelligence (AI) have emerged as revolutionary tools to expedite various tasks, like molecule-caption translation. Despite the importance of molecule-caption translation for molecule discovery, most of the existing methods heavily rely on domain experts, require excessive computational cost, and suffer from poor performance. On the other hand, Large Language Models (LLMs), like ChatGPT, have shown remarkable performance in various cross-modal tasks due to their great powerful capabilities in natural language understanding, generalization, and reasoning, which provides unprecedented opportunities to advance molecule discovery. To address the above limitations, in this work, we propose a novel LLMs-based framework (\textbf{MolReGPT}) for molecule-caption translation, where a retrieval-based prompt paradigm is introduced to empower molecule discovery with LLMs like ChatGPT without fine-tuning. More specifically, MolReGPT leverages the principle of molecular similarity to retrieve similar molecules and their text descriptions from a local database to ground the generation of LLMs through in-context few-shot molecule learning. We evaluate the effectiveness of MolReGPT via molecule-caption translation, which includes molecule understanding and text-based molecule generation. Experimental results show that MolReGPT outperforms fine-tuned models like MolT5-base without any additional training. To the best of our knowledge, MolReGPT is the first work to leverage LLMs in molecule-caption translation for advancing molecule discovery. ","[{'version': 'v1', 'created': 'Sun, 11 Jun 2023 08:16:25 GMT'}]",2023-06-13,"[['Li', 'Jiatong', ''], ['Liu', 'Yunqing', ''], ['Fan', 'Wenqi', ''], ['Wei', 'Xiao-Yong', ''], ['Liu', 'Hui', ''], ['Tang', 'Jiliang', ''], ['Li', 'Qing', '']]",1,1,2023-06-11,1,7,2,1,0,1,073e4f0c3a66b7557abd053301b5104cdc582636,259137456.0,https://www.semanticscholar.org/paper/073e4f0c3a66b7557abd053301b5104cdc582636,arXiv.org,2023.0,65.0,15.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2109018826', 'name': 'Jiatong Li'}, {'authorId': '2208630682', 'name': 'Yunqing Liu'}, {'authorId': '41031455', 'name': 'Wenqi Fan'}, {'authorId': '2115493866', 'name': 'Xiao Wei'}, {'authorId': '2146672392', 'name': 'Hui Liu'}, {'authorId': '1736632', 'name': 'Jiliang Tang'}, {'authorId': '2117897052', 'name': 'Qing Li'}]","['Michigan State University', 'Sichuan University', 'Hong Kong Polytechnic University']","['China', 'United States', 'Hong Kong']",2023-06
2306.07906,Xiao Liu,"Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng
  Zhang, Yuxiao Dong, Jie Tang",WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences,Accepted to KDD 2023,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  We present WebGLM, a web-enhanced question-answering system based on the General Language Model (GLM). Its goal is to augment a pre-trained large language model (LLM) with web search and retrieval capabilities while being efficient for real-world deployments. To achieve this, we develop WebGLM with strategies for the LLM-augmented retriever, bootstrapped generator, and human preference-aware scorer. Specifically, we identify and address the limitations of WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency, and cost-effectiveness advantages. In addition, we propose systematic criteria for evaluating web-enhanced QA systems. We conduct multi-dimensional human evaluation and quantitative ablation studies, which suggest the outperformance of the proposed WebGLM designs over existing systems. WebGLM with the 10-billion-parameter GLM (10B) is shown to perform better than the similar-sized WebGPT (13B) and even comparably to WebGPT (175B) in human evaluation. The code, demo, and data are at \url{https://github.com/THUDM/WebGLM}. ","[{'version': 'v1', 'created': 'Tue, 13 Jun 2023 16:57:53 GMT'}]",2023-06-14,"[['Liu', 'Xiao', ''], ['Lai', 'Hanyu', ''], ['Yu', 'Hao', ''], ['Xu', 'Yifan', ''], ['Zeng', 'Aohan', ''], ['Du', 'Zhengxiao', ''], ['Zhang', 'Peng', ''], ['Dong', 'Yuxiao', ''], ['Tang', 'Jie', '']]",0,1,2023-06-13,1,9,2,2,1,1,07bfba1087176d862c953a55df389e3d5b3d38ac,259144903.0,https://www.semanticscholar.org/paper/07bfba1087176d862c953a55df389e3d5b3d38ac,Knowledge Discovery and Data Mining,2023.0,43.0,15.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2111312892', 'name': 'Xiao Liu'}, {'authorId': '2051311700', 'name': 'Hanyu Lai'}, {'authorId': '2110750027', 'name': 'Hao Yu'}, {'authorId': '2125063007', 'name': 'Yifan Xu'}, {'authorId': '2051712753', 'name': 'Aohan Zeng'}, {'authorId': '66395694', 'name': 'Zhengxiao Du'}, {'authorId': '47243067', 'name': 'P. Zhang'}, {'authorId': '2047998', 'name': 'Yuxiao Dong'}, {'authorId': '2148911956', 'name': 'Jie Tang'}]","['Jie Tang †', 'Zhipu.AI Beijing, China', 'Beihang University', 'Tsinghua University', 'Peng Zhang', ""KDD '23, August 6-10, 2023, Long Beach, CA, USA.""]","['China', 'United States']",2023-06
2306.08666,Zhengliang Liu,"Zhengliang Liu, Aoxiao Zhong, Yiwei Li, Longtao Yang, Chao Ju, Zihao
  Wu, Chong Ma, Peng Shu, Cheng Chen, Sekeun Kim, Haixing Dai, Lin Zhao,
  Dajiang Zhu, Jun Liu, Wei Liu, Dinggang Shen, Xiang Li, Quanzheng Li,
  Tianming Liu",Radiology-GPT: A Large Language Model for Radiology,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  We introduce Radiology-GPT, a large language model for radiology. Using an instruction tuning approach on an extensive dataset of radiology domain knowledge, Radiology-GPT demonstrates superior performance compared to general language models such as StableLM, Dolly and LLaMA. It exhibits significant versatility in radiological diagnosis, research, and communication. This work serves as a catalyst for future developments in clinical NLP. The successful implementation of Radiology-GPT is indicative of the potential of localizing generative large language models, specifically tailored for distinctive medical specialties, while ensuring adherence to privacy standards such as HIPAA. The prospect of developing individualized, large-scale language models that cater to specific needs of various hospitals presents a promising direction. The fusion of conversational competence and domain-specific knowledge in these models is set to foster future development in healthcare AI. A demo of Radiology-GPT is available at https://huggingface.co/spaces/allen-eric/radiology-gpt. ","[{'version': 'v1', 'created': 'Wed, 14 Jun 2023 17:57:24 GMT'}]",2023-06-16,"[['Liu', 'Zhengliang', ''], ['Zhong', 'Aoxiao', ''], ['Li', 'Yiwei', ''], ['Yang', 'Longtao', ''], ['Ju', 'Chao', ''], ['Wu', 'Zihao', ''], ['Ma', 'Chong', ''], ['Shu', 'Peng', ''], ['Chen', 'Cheng', ''], ['Kim', 'Sekeun', ''], ['Dai', 'Haixing', ''], ['Zhao', 'Lin', ''], ['Zhu', 'Dajiang', ''], ['Liu', 'Jun', ''], ['Liu', 'Wei', ''], ['Shen', 'Dinggang', ''], ['Li', 'Xiang', ''], ['Li', 'Quanzheng', ''], ['Liu', 'Tianming', '']]",0,1,2023-06-14,1,19,2,1,1,0,a4f16dda8d25bdc0f93d2deb3b0876d278de9284,259164788.0,https://www.semanticscholar.org/paper/a4f16dda8d25bdc0f93d2deb3b0876d278de9284,arXiv.org,2023.0,51.0,23.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2145977326', 'name': 'Zheng Liu'}, {'authorId': '40153228', 'name': 'Aoxiao Zhong'}, {'authorId': '2111161355', 'name': 'Yiwei Li'}, {'authorId': '6553816', 'name': 'Longtao Yang'}, {'authorId': '2220096793', 'name': 'Chao Ju'}, {'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '2132543537', 'name': 'Chong Ma'}, {'authorId': '2220096705', 'name': 'Peng Shu'}, {'authorId': '1390805683', 'name': 'Cheng Chen'}, {'authorId': '40991527', 'name': 'Sekeun Kim'}, {'authorId': '29944950', 'name': 'Haixing Dai'}, {'authorId': '2111641126', 'name': 'Lin Zhao'}, {'authorId': '2181182', 'name': 'Dajiang Zhu'}, {'authorId': '2157176152', 'name': 'Jun Liu'}, {'authorId': '46641573', 'name': 'W. Liu'}, {'authorId': '2150038187', 'name': 'Dinggang Shen'}, {'authorId': '113075935', 'name': 'Xiang Li'}, {'authorId': '1762919', 'name': 'Quanzheng Li'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}]","['Northwestern Polytechnical University', 'Second Xiangya Hospital of Central South University', 'Mayo Clinic', 'Harvard University', 'Shanghai United Imaging Intelligence Co., Ltd.', 'ShanghaiTech University', 'Shanghai Clinical Research Center', 'Massachusetts General Hospital', 'University of Georgia', 'The University of Texas at Arlington']","['China', 'United States']",2023-06
2306.08891,Zihui Gu,"Zihui Gu, Ju Fan, Nan Tang, Songyue Zhang, Yuxin Zhang, Zui Chen, Lei
  Cao, Guoliang Li, Sam Madden, Xiaoyong Du",Interleaving Pre-Trained Language Models and Large Language Models for Zero-Shot NL2SQL Generation,Working in progress,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Zero-shot NL2SQL is crucial in achieving natural language to SQL that is adaptive to new environments (e.g., new databases, new linguistic phenomena or SQL structures) with zero annotated NL2SQL samples from such environments. Existing approaches either fine-tune pre-trained language models (PLMs) based on annotated data or use prompts to guide fixed large language models (LLMs) such as ChatGPT. PLMs can perform well in schema alignment but struggle to achieve complex reasoning, while LLMs is superior in complex reasoning tasks but cannot achieve precise schema alignment. In this paper, we propose a ZeroNL2SQL framework that combines the complementary advantages of PLMs and LLMs for supporting zero-shot NL2SQL. ZeroNL2SQL first uses PLMs to generate an SQL sketch via schema alignment, then uses LLMs to fill the missing information via complex reasoning. Moreover, in order to better align the generated SQL queries with values in the given database instances, we design a predicate calibration method to guide the LLM in completing the SQL sketches based on the database instances and select the optimal SQL query via an execution-based strategy. Comprehensive experiments show that ZeroNL2SQL can achieve the best zero-shot NL2SQL performance on real-world benchmarks. Specifically, ZeroNL2SQL outperforms the state-of-the-art PLM-based methods by 3.2% to 13% and exceeds LLM-based methods by 10% to 20% on execution accuracy. ","[{'version': 'v1', 'created': 'Thu, 15 Jun 2023 06:50:51 GMT'}]",2023-06-16,"[['Gu', 'Zihui', ''], ['Fan', 'Ju', ''], ['Tang', 'Nan', ''], ['Zhang', 'Songyue', ''], ['Zhang', 'Yuxin', ''], ['Chen', 'Zui', ''], ['Cao', 'Lei', ''], ['Li', 'Guoliang', ''], ['Madden', 'Sam', ''], ['Du', 'Xiaoyong', '']]",1,1,2023-06-15,1,10,1,1,0,1,71e996ff55b972946b9fe0f88394c19425f5a3ab,259165667.0,https://www.semanticscholar.org/paper/71e996ff55b972946b9fe0f88394c19425f5a3ab,arXiv.org,2023.0,52.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2082344591', 'name': 'Zihui Gu'}, {'authorId': '1704755170', 'name': 'Ju Fan'}, {'authorId': '2213989628', 'name': 'Nan Tang'}, {'authorId': '151226330', 'name': 'Songyue Zhang'}, {'authorId': '2220137935', 'name': 'Yuxin Zhang'}, {'authorId': '48354042', 'name': 'Zui Chen'}, {'authorId': '50260667', 'name': 'Lei Cao'}, {'authorId': '2108491555', 'name': 'Guoliang Li'}, {'authorId': '2053630301', 'name': 'Sam Madden'}, {'authorId': '2152944669', 'name': 'Xiaoyong Du'}]","['University of Arizona', 'Tsinghua University', 'Qatar Computing Research Institute', 'Massachusetts Institute of Technology', 'Renmin University of China']","['China', 'United States', 'Qatar']",2023-06
2306.09212,Haonan Li,"Haonan Li and Yixuan Zhang and Fajri Koto and Yifei Yang and Hai Zhao
  and Yeyun Gong and Nan Duan and Timothy Baldwin",CMMLU: Measuring massive multitask language understanding in Chinese,,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  As the capabilities of large language models (LLMs) continue to advance, evaluating their performance becomes increasingly crucial and challenging. This paper aims to bridge this gap by introducing CMMLU, a comprehensive Chinese benchmark that covers various subjects, including natural science, social sciences, engineering, and humanities. We conduct a thorough evaluation of 18 advanced multilingual- and Chinese-oriented LLMs, assessing their performance across different subjects and settings. The results reveal that most existing LLMs struggle to achieve an average accuracy of 50%, even when provided with in-context examples and chain-of-thought prompts, whereas the random baseline stands at 25%. This highlights significant room for improvement in LLMs. Additionally, we conduct extensive experiments to identify factors impacting the models' performance and propose directions for enhancing LLMs. CMMLU fills the gap in evaluating the knowledge and reasoning capabilities of large language models within the Chinese context. ","[{'version': 'v1', 'created': 'Thu, 15 Jun 2023 15:49:51 GMT'}]",2023-06-16,"[['Li', 'Haonan', ''], ['Zhang', 'Yixuan', ''], ['Koto', 'Fajri', ''], ['Yang', 'Yifei', ''], ['Zhao', 'Hai', ''], ['Gong', 'Yeyun', ''], ['Duan', 'Nan', ''], ['Baldwin', 'Timothy', '']]",0,0,2023-06-15,1,8,1,0,0,0,2ed07f3574fde65772b625153e853d716c2a6e14,259164635.0,https://www.semanticscholar.org/paper/2ed07f3574fde65772b625153e853d716c2a6e14,arXiv.org,2023.0,43.0,27.0,5.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '49404498', 'name': 'Haonan Li'}, {'authorId': '2211919724', 'name': 'Yixuan Zhang'}, {'authorId': '2789148', 'name': 'Fajri Koto'}, {'authorId': '2108989265', 'name': 'Yifei Yang'}, {'authorId': '2146232510', 'name': 'Hai Zhao'}, {'authorId': '2171182', 'name': 'Yeyun Gong'}, {'authorId': '46429989', 'name': 'Nan Duan'}, {'authorId': '123917295', 'name': 'Tim Baldwin'}]","['University of Melbourne', 'Shanghai Jiao Tong University', 'Microsoft', 'Mohamed bin Zayed University of Artificial Intelligence', 'Laureate Institute for Brain Research']","['China', 'United States', 'United Arab Emirates', 'Australia']",2023-06
2306.09649,Jackie (Junrui) Yang,"Jackie (Junrui) Yang, Karina Li, Daniel Wan Rosli, Shuning Zhang,
  Yuhan Zhang, Monica S. Lam, James A. Landay",ReactGenie: An Object-Oriented State Abstraction for Complex Multimodal Interactions Using Large Language Models,,,,,cs.HC cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal interactions have been shown to be more flexible, efficient, and adaptable for diverse users and tasks than traditional graphical interfaces. However, existing multimodal development frameworks either do not handle the complexity and compositionality of multimodal commands well or require developers to write a substantial amount of code to support these multimodal interactions. In this paper, we present ReactGenie, a programming framework that uses a shared object-oriented state abstraction to support building complex multimodal mobile applications. Having different modalities share the same state abstraction allows developers using ReactGenie to seamlessly integrate and compose these modalities to deliver multimodal interaction.   ReactGenie is a natural extension to the existing workflow of building a graphical app, like the workflow with React-Redux. Developers only have to add a few annotations and examples to indicate how natural language is mapped to the user-accessible functions in the program. ReactGenie automatically handles the complex problem of understanding natural language by generating a parser that leverages large language models.   We evaluated the ReactGenie framework by using it to build three demo apps. We evaluated the accuracy of the language parser using elicited commands from crowd workers and evaluated the usability of the generated multimodal app with 16 participants. Our results show that ReactGenie can be used to build versatile multimodal applications with highly accurate language parsers, and the multimodal app can lower users' cognitive load and task completion time. ","[{'version': 'v1', 'created': 'Fri, 16 Jun 2023 06:53:26 GMT'}]",2023-06-19,"[['Jackie', '', '', 'Junrui'], ['Yang', '', ''], ['Li', 'Karina', ''], ['Rosli', 'Daniel Wan', ''], ['Zhang', 'Shuning', ''], ['Zhang', 'Yuhan', ''], ['Lam', 'Monica S.', ''], ['Landay', 'James A.', '']]",0,0,2023-06-16,1,8,2,0,0,0,372c2123d813685527387aa71865fcd2f2464ea2,259187980.0,https://www.semanticscholar.org/paper/372c2123d813685527387aa71865fcd2f2464ea2,arXiv.org,2023.0,54.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '3886207', 'name': 'Jackie Yang'}, {'authorId': '2220791894', 'name': 'Karina Li'}, {'authorId': '2220218129', 'name': 'Daniel Wan Rosli'}, {'authorId': '2167035973', 'name': 'Shuning Zhang'}, {'authorId': '2220237818', 'name': 'Yuhan Zhang'}, {'authorId': '39682108', 'name': 'M. Lam'}, {'authorId': '9522307', 'name': 'J. Landay'}]","['Stanford University', 'Tsinghua University']","['China', 'United States']",2023-06
2306.09776,Ze Gao Mr,"Yuqian Sun, Xingyu Li, Ze Gao",Inspire creativity with ORIBA: Transform Artists' Original Characters into Chatbots through Large Language Model,"5 pages, 2 figures, 1 table",,,,cs.MM cs.AI cs.HC,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  This research delves into the intersection of illustration art and artificial intelligence (AI), focusing on how illustrators engage with AI agents that embody their original characters (OCs). We introduce 'ORIBA', a customizable AI chatbot that enables illustrators to converse with their OCs. This approach allows artists to not only receive responses from their OCs but also to observe their inner monologues and behavior. Despite the existing tension between artists and AI, our study explores innovative collaboration methods that are inspiring to illustrators. By examining the impact of AI on the creative process and the boundaries of authorship, we aim to enhance human-AI interactions in creative fields, with potential applications extending beyond illustration to interactive storytelling and more. ","[{'version': 'v1', 'created': 'Fri, 16 Jun 2023 11:25:44 GMT'}]",2023-06-19,"[['Sun', 'Yuqian', ''], ['Li', 'Xingyu', ''], ['Gao', 'Ze', '']]",0,0,2023-06-16,1,3,3,0,0,0,69412cba7097994617a8e008ea71cb91eb8d6dee,259188107.0,https://www.semanticscholar.org/paper/69412cba7097994617a8e008ea71cb91eb8d6dee,UbiComp/ISWC Adjunct,2023.0,27.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Art', 'source': 's2-fos-model'}]","[{'authorId': '1666629650', 'name': 'Yuqian Sun'}, {'authorId': '2155446933', 'name': 'Xingyu Li'}, {'authorId': '2256489428', 'name': 'Jun Peng'}, {'authorId': '2181257140', 'name': 'Ze Gao'}]","['Royal College of Art', 'Georgia Institute of Technology', 'Hong Kong University of Science and Technology']","['China', 'United States', 'United Kingdom']",2023-06
2306.10933,Yunjia Xi,"Yunjia Xi, Weiwen Liu, Jianghao Lin, Jieming Zhu, Bo Chen, Ruiming
  Tang, Weinan Zhang, Rui Zhang, Yong Yu",Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models,,,,,cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recommender systems play a vital role in various online services. However, the insulated nature of training and deploying separately within a specific domain limits their access to open-world knowledge. Recently, the emergence of large language models (LLMs) has shown promise in bridging this gap by encoding extensive world knowledge and demonstrating reasoning capability. Nevertheless, previous attempts to directly use LLMs as recommenders have not achieved satisfactory results. In this work, we propose an Open-World Knowledge Augmented Recommendation Framework with Large Language Models, dubbed KAR, to acquire two types of external knowledge from LLMs -- the reasoning knowledge on user preferences and the factual knowledge on items. We introduce factorization prompting to elicit accurate reasoning on user preferences. The generated reasoning and factual knowledge are effectively transformed and condensed into augmented vectors by a hybrid-expert adaptor in order to be compatible with the recommendation task. The obtained vectors can then be directly used to enhance the performance of any recommendation model. We also ensure efficient inference by preprocessing and prestoring the knowledge from the LLM. Extensive experiments show that KAR significantly outperforms the state-of-the-art baselines and is compatible with a wide range of recommendation algorithms. ","[{'version': 'v1', 'created': 'Mon, 19 Jun 2023 13:44:48 GMT'}, {'version': 'v2', 'created': 'Sun, 25 Jun 2023 07:05:30 GMT'}]",2023-06-27,"[['Xi', 'Yunjia', ''], ['Liu', 'Weiwen', ''], ['Lin', 'Jianghao', ''], ['Zhu', 'Jieming', ''], ['Chen', 'Bo', ''], ['Tang', 'Ruiming', ''], ['Zhang', 'Weinan', ''], ['Zhang', 'Rui', ''], ['Yu', 'Yong', '']]",0,0,2023-06-19,2,9,1,0,0,0,c5481668f78ab0c8ef2de9230f2fc1ce27eea6e4,259202547.0,https://www.semanticscholar.org/paper/c5481668f78ab0c8ef2de9230f2fc1ce27eea6e4,arXiv.org,2023.0,63.0,12.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2056826850', 'name': 'Yunjia Xi'}, {'authorId': '2130051800', 'name': 'Weiwen Liu'}, {'authorId': '2144908858', 'name': 'Jianghao Lin'}, {'authorId': '2108997533', 'name': 'Jieming Zhu'}, {'authorId': '92633145', 'name': 'Bo Chen'}, {'authorId': '2824766', 'name': 'Ruiming Tang'}, {'authorId': '2108309275', 'name': 'Weinan Zhang'}, {'authorId': '144142354', 'name': 'Rui Zhang'}, {'authorId': '2156098229', 'name': 'Yong Yu'}]","['Shanghai Jiao Tong University', 'Shenzhen, China Yong Yu', 'Huawei Technologies (China)', 'Cicatelli Associates']","['China', 'United States']",2023-06
2306.11027,Kun Zhou,"Wayne Xin Zhao, Kun Zhou, Beichen Zhang, Zheng Gong, Zhipeng Chen,
  Yuanhang Zhou, Ji-Rong Wen, Jing Sha, Shijin Wang, Cong Liu, Guoping Hu",JiuZhang 2.0: A Unified Chinese Pre-trained Language Model for Multi-task Mathematical Problem Solving,"Accepted by KDD 2023 ADS track, the 2.0 version of JiuZhang
  (arxiv:2206.06315v1)",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although pre-trained language models~(PLMs) have recently advanced the research progress in mathematical reasoning, they are not specially designed as a capable multi-task solver, suffering from high cost for multi-task deployment (\eg a model copy for a task) and inferior performance on complex mathematical problems in practical applications. To address these issues, in this paper, we propose \textbf{JiuZhang~2.0}, a unified Chinese PLM specially for multi-task mathematical problem solving. Our idea is to maintain a moderate-sized model and employ the \emph{cross-task knowledge sharing} to improve the model capacity in a multi-task setting. Specially, we construct a Mixture-of-Experts~(MoE) architecture for modeling mathematical text, so as to capture the common mathematical knowledge across tasks. For optimizing the MoE architecture, we design \emph{multi-task continual pre-training} and \emph{multi-task fine-tuning} strategies for multi-task adaptation. These training strategies can effectively decompose the knowledge from the task data and establish the cross-task sharing via expert networks. In order to further improve the general capacity of solving different complex tasks, we leverage large language models~(LLMs) as complementary models to iteratively refine the generated solution by our PLM, via in-context learning. Extensive experiments have demonstrated the effectiveness of our model. ","[{'version': 'v1', 'created': 'Mon, 19 Jun 2023 15:45:36 GMT'}]",2023-06-21,"[['Zhao', 'Wayne Xin', ''], ['Zhou', 'Kun', ''], ['Zhang', 'Beichen', ''], ['Gong', 'Zheng', ''], ['Chen', 'Zhipeng', ''], ['Zhou', 'Yuanhang', ''], ['Wen', 'Ji-Rong', ''], ['Sha', 'Jing', ''], ['Wang', 'Shijin', ''], ['Liu', 'Cong', ''], ['Hu', 'Guoping', '']]",0,0,2023-06-19,1,11,2,0,0,0,fae57797d357bfa3b39b220336d1a2e8deba5318,259203134.0,https://www.semanticscholar.org/paper/fae57797d357bfa3b39b220336d1a2e8deba5318,Knowledge Discovery and Data Mining,2023.0,59.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2542603', 'name': 'Wayne Xin Zhao'}, {'authorId': '1423651904', 'name': 'Kun Zhou'}, {'authorId': '2107926615', 'name': 'Beichen Zhang'}, {'authorId': '2164092564', 'name': 'Zheng Gong'}, {'authorId': '2111335050', 'name': 'Zhipeng Chen'}, {'authorId': '2116568362', 'name': 'Yuanhang Zhou'}, {'authorId': '153693432', 'name': 'Ji-rong Wen'}, {'authorId': '2165225571', 'name': 'Jing Sha'}, {'authorId': '2108620507', 'name': 'Shijin Wang'}, {'authorId': '2155398718', 'name': 'Cong Liu'}, {'authorId': '2090465180', 'name': 'Guoping Hu'}]","['iFLYTEK Research State Key Laboratory of Cognitive Intelligence Hefei, China iFLYTEK AI Research (Central China) Wuhan, China', 'Shijin Wang, Cong Liu, Guoping Hu. 2023. JiuZhang', ""KDD '23, August 6-10, 2023, Long Beach, CA, USA"", 'Renmin University of China']","['China', 'United States']",2023-06
2306.11507,Yue Huang,Yue Huang and Qihui Zhang and Philip S. Y and Lichao Sun,TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models,We are currently expanding this work and welcome collaborators!,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) such as ChatGPT, have gained significant attention due to their impressive natural language processing capabilities. It is crucial to prioritize human-centered principles when utilizing these models. Safeguarding the ethical and moral compliance of LLMs is of utmost importance. However, individual ethical issues have not been well studied on the latest LLMs. Therefore, this study aims to address these gaps by introducing a new benchmark -- TrustGPT. TrustGPT provides a comprehensive evaluation of LLMs in three crucial areas: toxicity, bias, and value-alignment. Initially, TrustGPT examines toxicity in language models by employing toxic prompt templates derived from social norms. It then quantifies the extent of bias in models by measuring quantifiable toxicity values across different groups. Lastly, TrustGPT assesses the value of conversation generation models from both active value-alignment and passive value-alignment tasks. Through the implementation of TrustGPT, this research aims to enhance our understanding of the performance of conversation generation models and promote the development of language models that are more ethical and socially responsible. ","[{'version': 'v1', 'created': 'Tue, 20 Jun 2023 12:53:39 GMT'}]",2023-06-21,"[['Huang', 'Yue', ''], ['Zhang', 'Qihui', ''], ['Y', 'Philip S.', ''], ['Sun', 'Lichao', '']]",1,1,2023-06-20,1,4,2,1,0,1,9d81ec931b85d6c6cf3453126670cd7a30a689e7,259202452.0,https://www.semanticscholar.org/paper/9d81ec931b85d6c6cf3453126670cd7a30a689e7,arXiv.org,2023.0,61.0,8.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2108715615', 'name': 'Yue Huang'}, {'authorId': '46324457', 'name': 'Qihui Zhang'}, {'authorId': '2191036692', 'name': 'Philip S. Yu'}, {'authorId': '46732871', 'name': 'Lichao Sun'}]","['Lehigh University', 'University of Illinois at Chicago', 'Sichuan University']","['China', 'United States']",2023-06
2306.11585,Minghua He,"Minghua He, Nanfei Gu, Yuntao Shi, Qionghui Zhang, Yaying Chen",FAIR: A Causal Framework for Accurately Inferring Judgments Reversals,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Artificial intelligence researchers have made significant advances in legal intelligence in recent years. However, the existing studies have not focused on the important value embedded in judgments reversals, which limits the improvement of the efficiency of legal intelligence. In this paper, we propose a causal Framework for Accurately Inferring case Reversals (FAIR), which models the problem of judgments reversals based on real Chinese judgments. We mine the causes of judgments reversals by causal inference methods and inject the obtained causal relationships into the neural network as a priori knowledge. And then, our framework is validated on a challenging dataset as a legal judgment prediction task. The experimental results show that our framework can tap the most critical factors in judgments reversal, and the obtained causal relationships can effectively improve the neural network's performance. In addition, we discuss the generalization ability of large language models for legal intelligence tasks using ChatGPT as an example. Our experiment has found that the generalization ability of large language models still has defects, and mining causal relationships can effectively improve the accuracy and explain ability of model predictions. ","[{'version': 'v1', 'created': 'Tue, 20 Jun 2023 15:02:25 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Jul 2023 14:31:10 GMT'}]",2023-07-24,"[['He', 'Minghua', ''], ['Gu', 'Nanfei', ''], ['Shi', 'Yuntao', ''], ['Zhang', 'Qionghui', ''], ['Chen', 'Yaying', '']]",1,1,2023-06-20,2,5,1,1,0,1,e73692f030adc112c10cc365cf584ebdf8fc7899,259203304.0,https://www.semanticscholar.org/paper/e73692f030adc112c10cc365cf584ebdf8fc7899,arXiv.org,2023.0,24.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2163015691', 'name': 'Minghua He'}, {'authorId': '2220302508', 'name': 'Nanfei Gu'}, {'authorId': '2155598287', 'name': 'Yuntao Shi'}, {'authorId': '2220327158', 'name': 'Qionghui Zhang'}, {'authorId': '2220322546', 'name': 'Yaying Chen'}]","['University of Southern California', 'Jilin University']","['China', 'United States']",2023-06
2306.11698,Boxin Wang,"Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang,
  Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T.
  Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng,
  Sanmi Koyejo, Dawn Song, Bo Li",DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,,,,,cs.CL cs.AI cs.CR,http://creativecommons.org/licenses/by-sa/4.0/,"  Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in capabilities, capturing the interest of practitioners and the public alike. Yet, while the literature on the trustworthiness of GPT models remains limited, practitioners have proposed employing capable GPT models for sensitive applications to healthcare and finance - where mistakes can be costly. To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives - including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history. We also find that although GPT-4 is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more vulnerable given jailbreaking system or user prompts, potentially due to the reason that GPT-4 follows the (misleading) instructions more precisely. Our work illustrates a comprehensive trustworthiness evaluation of GPT models and sheds light on the trustworthiness gaps. Our benchmark is publicly available at https://decodingtrust.github.io/. ","[{'version': 'v1', 'created': 'Tue, 20 Jun 2023 17:24:23 GMT'}]",2023-06-21,"[['Wang', 'Boxin', ''], ['Chen', 'Weixin', ''], ['Pei', 'Hengzhi', ''], ['Xie', 'Chulin', ''], ['Kang', 'Mintong', ''], ['Zhang', 'Chenhui', ''], ['Xu', 'Chejian', ''], ['Xiong', 'Zidi', ''], ['Dutta', 'Ritik', ''], ['Schaeffer', 'Rylan', ''], ['Truong', 'Sang T.', ''], ['Arora', 'Simran', ''], ['Mazeika', 'Mantas', ''], ['Hendrycks', 'Dan', ''], ['Lin', 'Zinan', ''], ['Cheng', 'Yu', ''], ['Koyejo', 'Sanmi', ''], ['Song', 'Dawn', ''], ['Li', 'Bo', '']]",0,1,2023-06-20,1,19,3,2,0,2,e5d5d32b098671cf71dcb4ca2039524154183db8,259202782.0,https://www.semanticscholar.org/paper/e5d5d32b098671cf71dcb4ca2039524154183db8,arXiv.org,2023.0,0.0,28.0,4.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '51454501', 'name': 'Boxin Wang'}, {'authorId': '2108947078', 'name': 'Weixin Chen'}, {'authorId': '146922081', 'name': 'Hengzhi Pei'}, {'authorId': '150961077', 'name': 'Chulin Xie'}, {'authorId': '2153110066', 'name': 'Mintong Kang'}, {'authorId': '2146063748', 'name': 'Chenhui Zhang'}, {'authorId': '2153079868', 'name': 'Chejian Xu'}, {'authorId': '2155965725', 'name': 'Zidi Xiong'}, {'authorId': '151183175', 'name': 'Ritik Dutta'}, {'authorId': '1749176844', 'name': 'Rylan Schaeffer'}, {'authorId': '2127191901', 'name': 'Sang Truong'}, {'authorId': '2220301960', 'name': 'Simran Arora'}, {'authorId': '16787428', 'name': 'Mantas Mazeika'}, {'authorId': '3422872', 'name': 'Dan Hendrycks'}, {'authorId': '2695029', 'name': 'Zi-Han Lin'}, {'authorId': '98742322', 'name': 'Yuk-Kit Cheng'}, {'authorId': '123593472', 'name': 'Sanmi Koyejo'}, {'authorId': '143711382', 'name': 'D. Song'}, {'authorId': '2165245120', 'name': 'Bo Li'}]","['Shanghai FRP Research Institute (China)', 'University of California, Berkeley', 'Stanford University', 'Chinese University of Hong Kong', 'Microsoft', 'Center for AI Safety', 'University of Illinois Urbana-Champaign']","['China', 'United States']",2023-06
2306.11702,Chen Zui,"Zui Chen, Lei Cao, Sam Madden",Lingua Manga: A Generic Large Language Model Centric System for Data Curation,"4 pages, 6 figures, VLDB 2023 Demo paper",,,,cs.DB cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Data curation is a wide-ranging area which contains many critical but time-consuming data processing tasks. However, the diversity of such tasks makes it challenging to develop a general-purpose data curation system. To address this issue, we present Lingua Manga, a user-friendly and versatile system that utilizes pre-trained large language models. Lingua Manga offers automatic optimization for achieving high performance and label efficiency while facilitating flexible and rapid development. Through three example applications with distinct objectives and users of varying levels of technical proficiency, we demonstrate that Lingua Manga can effectively assist both skilled programmers and low-code or even no-code users in addressing data curation challenges. ","[{'version': 'v1', 'created': 'Tue, 20 Jun 2023 17:30:02 GMT'}, {'version': 'v2', 'created': 'Fri, 1 Sep 2023 15:40:40 GMT'}]",2023-09-04,"[['Chen', 'Zui', ''], ['Cao', 'Lei', ''], ['Madden', 'Sam', '']]",0,0,2023-06-20,2,3,2,0,0,0,d0cfff2d0187dbe6829a8b2a785e3bc22595cf72,259203938.0,https://www.semanticscholar.org/paper/d0cfff2d0187dbe6829a8b2a785e3bc22595cf72,Proceedings of the VLDB Endowment,2023.0,20.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '48354042', 'name': 'Zui Chen'}, {'authorId': '50260667', 'name': 'Lei Cao'}, {'authorId': '144478906', 'name': 'S. Madden'}]","['Massachusetts Institute of Technology', 'Tsinghua University', 'U of Arizona/MIT Tucson, Arizona']","['China', 'United States']",2023-06
2306.11731,Huiguo He,"Huiguo He, Tianfu Wang, Huan Yang, Jianlong Fu, Nicholas Jing Yuan,
  Jian Yin, Hongyang Chao, Qi Zhang",Learning Profitable NFT Image Diffusions via Multiple Visual-Policy Guided Reinforcement Learning,Accepted by ACM-MM 2023,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We study the task of generating profitable Non-Fungible Token (NFT) images from user-input texts. Recent advances in diffusion models have shown great potential for image generation. However, existing works can fall short in generating visually-pleasing and highly-profitable NFT images, mainly due to the lack of 1) plentiful and fine-grained visual attribute prompts for an NFT image, and 2) effective optimization metrics for generating high-quality NFT images. To solve these challenges, we propose a Diffusion-based generation framework with Multiple Visual-Policies as rewards (i.e., Diffusion-MVP) for NFT images. The proposed framework consists of a large language model (LLM), a diffusion-based image generator, and a series of visual rewards by design. First, the LLM enhances a basic human input (such as ""panda"") by generating more comprehensive NFT-style prompts that include specific visual attributes, such as ""panda with Ninja style and green background."" Second, the diffusion-based image generator is fine-tuned using a large-scale NFT dataset to capture fine-grained image styles and accessory compositions of popular NFT elements. Third, we further propose to utilize multiple visual-policies as optimization goals, including visual rarity levels, visual aesthetic scores, and CLIP-based text-image relevances. This design ensures that our proposed Diffusion-MVP is capable of minting NFT images with high visual quality and market value. To facilitate this research, we have collected the largest publicly available NFT image dataset to date, consisting of 1.5 million high-quality images with corresponding texts and market values. Extensive experiments including objective evaluations and user studies demonstrate that our framework can generate NFT images showing more visually engaging elements and higher market value, compared with SOTA approaches. ","[{'version': 'v1', 'created': 'Tue, 20 Jun 2023 17:59:46 GMT'}, {'version': 'v2', 'created': 'Thu, 17 Aug 2023 17:57:26 GMT'}]",2023-08-21,"[['He', 'Huiguo', ''], ['Wang', 'Tianfu', ''], ['Yang', 'Huan', ''], ['Fu', 'Jianlong', ''], ['Yuan', 'Nicholas Jing', ''], ['Yin', 'Jian', ''], ['Chao', 'Hongyang', ''], ['Zhang', 'Qi', '']]",0,0,2023-06-20,2,8,1,0,0,0,240fd6b32c48dcca9d847cdf9f9930b3e717709e,259203620.0,https://www.semanticscholar.org/paper/240fd6b32c48dcca9d847cdf9f9930b3e717709e,ACM Multimedia,2023.0,82.0,0.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1643688444', 'name': 'Huiguo He'}, {'authorId': '2155412982', 'name': 'Tianfu Wang'}, {'authorId': '46402216', 'name': 'Huan Yang'}, {'authorId': '3247966', 'name': 'Jianlong Fu'}, {'authorId': '1677643972', 'name': 'N. Yuan'}, {'authorId': '2152938031', 'name': 'Jian Yin'}, {'authorId': '47636228', 'name': 'Hongyang Chao'}, {'authorId': '2145908108', 'name': 'Qi Zhang'}]","['Sun Yat-sen University', 'Microsoft', 'University of Science and Technology of China', 'Sun Yat-Sun University Tianfu Wang', 'Sun Yat-Sun University Qi Zhang']","['China', 'United States']",2023-06
2306.14321,Yilun Zhao,"Yilun Zhao, Chen Zhao, Linyong Nan, Zhenting Qi, Wenlin Zhang, Xiangru
  Tang, Boyu Mi, Dragomir Radev",RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations,Accepted at ACL 2023,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Despite significant progress having been made in question answering on tabular data (Table QA), it's unclear whether, and to what extent existing Table QA models are robust to task-specific perturbations, e.g., replacing key question entities or shuffling table columns. To systematically study the robustness of Table QA models, we propose a benchmark called RobuT, which builds upon existing Table QA datasets (WTQ, WikiSQL-Weak, and SQA) and includes human-annotated adversarial perturbations in terms of table header, table content, and question. Our results indicate that both state-of-the-art Table QA models and large language models (e.g., GPT-3) with few-shot learning falter in these adversarial sets. We propose to address this problem by using large language models to generate adversarial examples to enhance training, which significantly improves the robustness of Table QA models. Our data and code is publicly available at https://github.com/yilunzhao/RobuT. ","[{'version': 'v1', 'created': 'Sun, 25 Jun 2023 19:23:21 GMT'}]",2023-06-27,"[['Zhao', 'Yilun', ''], ['Zhao', 'Chen', ''], ['Nan', 'Linyong', ''], ['Qi', 'Zhenting', ''], ['Zhang', 'Wenlin', ''], ['Tang', 'Xiangru', ''], ['Mi', 'Boyu', ''], ['Radev', 'Dragomir', '']]",0,1,2023-06-25,1,8,2,1,0,1,341d516cc858dc92ba14a788ef40d0559b5a2b26,259252406.0,https://www.semanticscholar.org/paper/341d516cc858dc92ba14a788ef40d0559b5a2b26,Annual Meeting of the Association for Computational Linguistics,2023.0,59.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '46316984', 'name': 'Yilun Zhao'}, {'authorId': '145756130', 'name': 'Chen Zhao'}, {'authorId': '51990260', 'name': 'Linyong Nan'}, {'authorId': '2186056193', 'name': 'Zhenting Qi'}, {'authorId': '2220571367', 'name': 'Wenlin Zhang'}, {'authorId': '47274259', 'name': 'Xiangru Tang'}, {'authorId': '2124195718', 'name': 'Boyu Mi'}, {'authorId': '9215251', 'name': 'Dragomir R. Radev'}]","['New York University', 'Zhejiang University', 'Yale University']","['China', 'United States']",2023-06
2306.17103,Le Zhuo,"Le Zhuo, Ruibin Yuan, Jiahao Pan, Yinghao Ma, Yizhi LI, Ge Zhang, Si
  Liu, Roger Dannenberg, Jie Fu, Chenghua Lin, Emmanouil Benetos, Wenhu Chen,
  Wei Xue, Yike Guo",LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by Whispering to ChatGPT,"9 pages, 2 figures, 5 tables, accepted by ISMIR 2023",,,,cs.CL cs.SD eess.AS,http://creativecommons.org/licenses/by-sa/4.0/,"  We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic lyrics transcription method achieving state-of-the-art performance on various lyrics transcription datasets, even in challenging genres such as rock and metal. Our novel, training-free approach utilizes Whisper, a weakly supervised robust speech recognition model, and GPT-4, today's most performant chat-based large language model. In the proposed method, Whisper functions as the ""ear"" by transcribing the audio, while GPT-4 serves as the ""brain,"" acting as an annotator with a strong performance for contextualized output selection and correction. Our experiments show that LyricWhiz significantly reduces Word Error Rate compared to existing methods in English and can effectively transcribe lyrics across multiple languages. Furthermore, we use LyricWhiz to create the first publicly available, large-scale, multilingual lyrics transcription dataset with a CC-BY-NC-SA copyright license, based on MTG-Jamendo, and offer a human-annotated subset for noise level estimation and evaluation. We anticipate that our proposed method and dataset will advance the development of multilingual lyrics transcription, a challenging and emerging task. ","[{'version': 'v1', 'created': 'Thu, 29 Jun 2023 17:01:51 GMT'}, {'version': 'v2', 'created': 'Fri, 7 Jul 2023 16:32:26 GMT'}]",2023-07-10,"[['Zhuo', 'Le', ''], ['Yuan', 'Ruibin', ''], ['Pan', 'Jiahao', ''], ['Ma', 'Yinghao', ''], ['LI', 'Yizhi', ''], ['Zhang', 'Ge', ''], ['Liu', 'Si', ''], ['Dannenberg', 'Roger', ''], ['Fu', 'Jie', ''], ['Lin', 'Chenghua', ''], ['Benetos', 'Emmanouil', ''], ['Chen', 'Wenhu', ''], ['Xue', 'Wei', ''], ['Guo', 'Yike', '']]",1,1,2023-06-29,2,14,3,2,0,2,19363e9fa47f02744d49f24b8e94c8cdf19bc4af,259287024.0,https://www.semanticscholar.org/paper/19363e9fa47f02744d49f24b8e94c8cdf19bc4af,arXiv.org,2023.0,48.0,1.0,0.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2191619409', 'name': 'Le Zhuo'}, {'authorId': '2032236274', 'name': 'Ruibin Yuan'}, {'authorId': '1944785400', 'name': 'Jiahao Pan'}, {'authorId': '2146277129', 'name': 'Yi Ma'}, {'authorId': '2129449392', 'name': 'Yizhi Li'}, {'authorId': '2143853895', 'name': 'Ge Zhang'}, {'authorId': '2220617810', 'name': 'Si Liu'}, {'authorId': '1697732', 'name': 'R. Dannenberg'}, {'authorId': '49252800', 'name': 'Jie Fu'}, {'authorId': '2268783', 'name': 'Chenghua Lin'}, {'authorId': '2109397', 'name': 'Emmanouil Benetos'}, {'authorId': '2928777', 'name': 'Wenhu Chen'}, {'authorId': '2195745651', 'name': 'Wei Xue'}, {'authorId': '2118270918', 'name': 'Yi-Ting Guo'}]","['University of Sheffield', 'Beijing Academy of Artificial Intelligence', 'University of Waterloo', 'Hong Kong University of Science and Technology', 'Queen Mary University of London', 'Carnegie Mellon University', 'Beihang University']","['Canada', 'China', 'United Kingdom', 'United States']",2023-06
2307.01379,Jinhao Duan,"Jinhao Duan, Hao Cheng, Shiqi Wang, Chenan Wang, Alex Zavalny, Renjing
  Xu, Bhavya Kailkhura, Kaidi Xu",Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although Large Language Models (LLMs) have shown great potential in Natural Language Generation, it is still challenging to characterize the uncertainty of model generations, i.e., when users could trust model outputs. Our research is derived from the heuristic facts that tokens are created unequally in reflecting the meaning of generations by auto-regressive LLMs, i.e., some tokens are more relevant (or representative) than others, yet all the tokens are equally valued when estimating uncertainty. It is because of the linguistic redundancy where mostly a few keywords are sufficient to convey the meaning of a long sentence. We name these inequalities as generative inequalities and investigate how they affect uncertainty estimation. Our results reveal that considerable tokens and sentences containing limited semantics are weighted equally or even heavily when estimating uncertainty. To tackle these biases posed by generative inequalities, we propose to jointly Shifting Attention to more Relevant (SAR) components from both the token level and the sentence level while estimating uncertainty. We conduct experiments over popular ""off-the-shelf"" LLMs (e.g., OPT, LLaMA) with model sizes up to 30B and powerful commercial LLMs (e.g., Davinci from OpenAI), across various free-form question-answering tasks. Experimental results and detailed demographic analysis indicate the superior performance of SAR. Code is available at https://github.com/jinhaoduan/shifting-attention-to-relevance. ","[{'version': 'v1', 'created': 'Mon, 3 Jul 2023 22:17:16 GMT'}]",2023-07-06,"[['Duan', 'Jinhao', ''], ['Cheng', 'Hao', ''], ['Wang', 'Shiqi', ''], ['Wang', 'Chenan', ''], ['Zavalny', 'Alex', ''], ['Xu', 'Renjing', ''], ['Kailkhura', 'Bhavya', ''], ['Xu', 'Kaidi', '']]",0,0,2023-07-03,1,8,3,2,2,0,5424e311319c58847b4c690d5c91090e3b6a4ac3,259342406.0,https://www.semanticscholar.org/paper/5424e311319c58847b4c690d5c91090e3b6a4ac3,arXiv.org,2023.0,47.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2004228925', 'name': 'Jinhao Duan'}, {'authorId': '2117983322', 'name': 'Hao Cheng'}, {'authorId': '2108622760', 'name': 'Shiqi Wang'}, {'authorId': '2108811793', 'name': 'Chenan Wang'}, {'authorId': '2162860875', 'name': 'Alex Zavalny'}, {'authorId': '2200295329', 'name': 'Renjing Xu'}, {'authorId': '1749353', 'name': 'B. Kailkhura'}, {'authorId': '46321210', 'name': 'Kaidi Xu'}]","['Drexel University', 'American Welding Society', 'Lawrence Livermore National Laboratory', 'Hong Kong University of Science and Technology']","['China', 'United States']",2023-07
2307.01981,Jiaxiang Liu,"Jiaxiang Liu, Tianxiang Hu, Yan Zhang, Xiaotang Gai, Yang Feng, Zuozhu
  Liu",A ChatGPT Aided Explainable Framework for Zero-Shot Medical Image Diagnosis,"Workshop on Interpretable ML in Healthcare at International
  Conference on Machine Learning (ICML) 2023",,,,eess.IV cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Zero-shot medical image classification is a critical process in real-world scenarios where we have limited access to all possible diseases or large-scale annotated data. It involves computing similarity scores between a query medical image and possible disease categories to determine the diagnostic result. Recent advances in pretrained vision-language models (VLMs) such as CLIP have shown great performance for zero-shot natural image recognition and exhibit benefits in medical applications. However, an explainable zero-shot medical image recognition framework with promising performance is yet under development. In this paper, we propose a novel CLIP-based zero-shot medical image classification framework supplemented with ChatGPT for explainable diagnosis, mimicking the diagnostic process performed by human experts. The key idea is to query large language models (LLMs) with category names to automatically generate additional cues and knowledge, such as disease symptoms or descriptions other than a single category name, to help provide more accurate and explainable diagnosis in CLIP. We further design specific prompts to enhance the quality of generated texts by ChatGPT that describe visual medical features. Extensive results on one private dataset and four public datasets along with detailed analysis demonstrate the effectiveness and explainability of our training-free zero-shot diagnosis pipeline, corroborating the great potential of VLMs and LLMs for medical applications. ","[{'version': 'v1', 'created': 'Wed, 5 Jul 2023 01:45:19 GMT'}]",2023-07-06,"[['Liu', 'Jiaxiang', ''], ['Hu', 'Tianxiang', ''], ['Zhang', 'Yan', ''], ['Gai', 'Xiaotang', ''], ['Feng', 'Yang', ''], ['Liu', 'Zuozhu', '']]",1,1,2023-07-05,1,6,3,1,0,1,f89b8e79a1b4b9a2febd9b8ab3f7933c89e1c3e0,259342254.0,https://www.semanticscholar.org/paper/f89b8e79a1b4b9a2febd9b8ab3f7933c89e1c3e0,arXiv.org,2023.0,33.0,3.0,0.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2144130913', 'name': 'Jiaxiang Liu'}, {'authorId': '2112911910', 'name': 'Tianxiang Hu'}, {'authorId': '39831806', 'name': 'Yan Zhang'}, {'authorId': '2221127244', 'name': 'Xiaotang Gai'}, {'authorId': '2157371366', 'name': 'Yang Feng'}, {'authorId': '2390951', 'name': 'Zuozhu Liu'}]","['National University of Singapore', 'Workshop on Interpretable ML in Healthcare at International Con-ference on Machine Learning (ICML), Honolulu, Hawaii, USA.', 'Zhejiang University', 'Angelalign Inc., Shanghai, China.', 'University of Illinois Urbana-Champaign']","['China', 'United States', 'Singapore']",2023-07
2307.02485,Hongxin Zhang,"Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua
  B. Tenenbaum, Tianmin Shu, Chuang Gan",Building Cooperative Embodied Agents Modularly with Large Language Models,Project page: https://vis-www.cs.umass.edu/Co-LLM-Agents/,,,,cs.AI cs.CL cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have demonstrated impressive planning abilities in single-agent embodied tasks across various domains. However, their capacity for planning and communication in multi-agent cooperation remains unclear, even though these are crucial skills for intelligent embodied agents. In this paper, we present a novel framework that utilizes LLMs for multi-agent cooperation and tests it in various embodied environments. Our framework enables embodied agents to plan, communicate, and cooperate with other embodied agents or humans to accomplish long-horizon tasks efficiently. We demonstrate that recent LLMs, such as GPT-4, can surpass strong planning-based methods and exhibit emergent effective communication using our framework without requiring fine-tuning or few-shot prompting. We also discover that LLM-based agents that communicate in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for embodied AI and lays the foundation for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/. ","[{'version': 'v1', 'created': 'Wed, 5 Jul 2023 17:59:27 GMT'}]",2023-07-06,"[['Zhang', 'Hongxin', ''], ['Du', 'Weihua', ''], ['Shan', 'Jiaming', ''], ['Zhou', 'Qinhong', ''], ['Du', 'Yilun', ''], ['Tenenbaum', 'Joshua B.', ''], ['Shu', 'Tianmin', ''], ['Gan', 'Chuang', '']]",0,1,2023-07-05,1,8,3,1,0,1,587352c3b95c90de6d37f061c8e117f42be0b575,259342833.0,https://www.semanticscholar.org/paper/587352c3b95c90de6d37f061c8e117f42be0b575,arXiv.org,2023.0,58.0,15.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2118083343', 'name': 'Hongxin Zhang'}, {'authorId': '2214830561', 'name': 'Weihua Du'}, {'authorId': '2193050211', 'name': 'Jiaming Shan'}, {'authorId': '2107604346', 'name': 'Qinhong Zhou'}, {'authorId': '15394275', 'name': 'Yilun Du'}, {'authorId': '1763295', 'name': 'J. Tenenbaum'}, {'authorId': '1844358', 'name': 'Tianmin Shu'}, {'authorId': '2056157586', 'name': 'Chuang Gan'}]","['Shanghai Jiao Tong University', 'Tsinghua University', 'University of Massachusetts Amherst', 'MIT-IBM Watson AI Lab']","['China', 'United States']",2023-07
2307.02514,Xiang Li,"Hongmin Cai, Xiaoke Huang, Zhengliang Liu, Wenxiong Liao, Haixing Dai,
  Zihao Wu, Dajiang Zhu, Hui Ren, Quanzheng Li, Tianming Liu, and Xiang Li",Exploring Multimodal Approaches for Alzheimer's Disease Detection Using Patient Speech Transcript and Audio Data,,,,,eess.AS cs.AI cs.SD,http://creativecommons.org/licenses/by/4.0/,"  Alzheimer's disease (AD) is a common form of dementia that severely impacts patient health. As AD impairs the patient's language understanding and expression ability, the speech of AD patients can serve as an indicator of this disease. This study investigates various methods for detecting AD using patients' speech and transcripts data from the DementiaBank Pitt database. The proposed approach involves pre-trained language models and Graph Neural Network (GNN) that constructs a graph from the speech transcript, and extracts features using GNN for AD detection. Data augmentation techniques, including synonym replacement, GPT-based augmenter, and so on, were used to address the small dataset size. Audio data was also introduced, and WavLM model was used to extract audio features. These features were then fused with text features using various methods. Finally, a contrastive learning approach was attempted by converting speech transcripts back to audio and using it for contrastive learning with the original audio. We conducted intensive experiments and analysis on the above methods. Our findings shed light on the challenges and potential solutions in AD detection using speech and audio data. ","[{'version': 'v1', 'created': 'Wed, 5 Jul 2023 12:40:11 GMT'}]",2023-07-07,"[['Cai', 'Hongmin', ''], ['Huang', 'Xiaoke', ''], ['Liu', 'Zhengliang', ''], ['Liao', 'Wenxiong', ''], ['Dai', 'Haixing', ''], ['Wu', 'Zihao', ''], ['Zhu', 'Dajiang', ''], ['Ren', 'Hui', ''], ['Li', 'Quanzheng', ''], ['Liu', 'Tianming', ''], ['Li', 'Xiang', '']]",0,1,2023-07-05,1,11,3,0,0,0,0590ec99d2b36b8922139078ac1a91fd62eeda61,259360421.0,https://www.semanticscholar.org/paper/0590ec99d2b36b8922139078ac1a91fd62eeda61,arXiv.org,2023.0,38.0,5.0,0.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2209990831', 'name': 'Hongmin Cai'}, {'authorId': None, 'name': 'Xiaoke Huang'}, {'authorId': '2145977326', 'name': 'Zheng Liu'}, {'authorId': '2036578588', 'name': 'Wenxiong Liao'}, {'authorId': '29944950', 'name': 'Haixing Dai'}, {'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '2181182', 'name': 'Dajiang Zhu'}, {'authorId': '1485746845', 'name': 'Hui Ren'}, {'authorId': '1762919', 'name': 'Quanzheng Li'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}, {'authorId': '113075935', 'name': 'Xiang Li'}]","['South China University of Technology', 'University of Georgia', 'Massachusetts General Hospital', 'The University of Texas at Arlington']","['China', 'United States']",2023-07
2307.03109,Jindong Wang,"Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu,
  Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi
  Chang, Philip S. Yu, Qiang Yang, Xing Xie",A Survey on Evaluation of Large Language Models,"26 pages; a major update to include more recent works;
  https://llm-eval.github.io/",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey. ","[{'version': 'v1', 'created': 'Thu, 6 Jul 2023 16:28:35 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Jul 2023 12:31:50 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Jul 2023 15:43:03 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Jul 2023 12:33:20 GMT'}, {'version': 'v5', 'created': 'Tue, 18 Jul 2023 08:11:21 GMT'}, {'version': 'v6', 'created': 'Wed, 2 Aug 2023 07:39:17 GMT'}, {'version': 'v7', 'created': 'Mon, 28 Aug 2023 05:50:53 GMT'}]",2023-08-29,"[['Chang', 'Yupeng', ''], ['Wang', 'Xu', ''], ['Wang', 'Jindong', ''], ['Wu', 'Yuan', ''], ['Yang', 'Linyi', ''], ['Zhu', 'Kaijie', ''], ['Chen', 'Hao', ''], ['Yi', 'Xiaoyuan', ''], ['Wang', 'Cunxiang', ''], ['Wang', 'Yidong', ''], ['Ye', 'Wei', ''], ['Zhang', 'Yue', ''], ['Chang', 'Yi', ''], ['Yu', 'Philip S.', ''], ['Yang', 'Qiang', ''], ['Xie', 'Xing', '']]",0,0,2023-07-06,7,16,2,0,0,0,888728745dbb769e29ed475d4f7661eebe1a71cf,259360395.0,https://www.semanticscholar.org/paper/888728745dbb769e29ed475d4f7661eebe1a71cf,arXiv.org,2023.0,280.0,81.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","[{'authorId': '2140050490', 'name': 'Yu-Chu Chang'}, {'authorId': '2206577797', 'name': 'Xu Wang'}, {'authorId': '1519290245', 'name': 'Jindong Wang'}, {'authorId': '2157349111', 'name': 'Yuanyi Wu'}, {'authorId': '2219270546', 'name': 'Kaijie Zhu'}, {'authorId': '2051536212', 'name': 'Hao Chen'}, {'authorId': '2145500840', 'name': 'Linyi Yang'}, {'authorId': '3393196', 'name': 'Xiaoyuan Yi'}, {'authorId': '35504092', 'name': 'Cunxiang Wang'}, {'authorId': '2108024273', 'name': 'Yidong Wang'}, {'authorId': '2147205193', 'name': 'Weirong Ye'}, {'authorId': '2211964951', 'name': 'Yue Zhang'}, {'authorId': '2131636065', 'name': 'Yi Chang'}, {'authorId': '2191036692', 'name': 'Philip S. Yu'}, {'authorId': '2158406244', 'name': 'Qian Yang'}, {'authorId': '1576441343', 'name': 'Xingxu Xie'}]","['Westlake University', ""Xi'an University of Science and Technology"", 'Hong Kong University of Science and Technology', 'Jilin University', 'Chinese Academy of Sciences', 'Carnegie Mellon University', 'University of Illinois at Chicago', 'Microsoft', 'Peking University']","['China', 'United States']",2023-07
2307.03393,Zhikai Chen,"Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei,
  Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, Jiliang Tang",Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs,add code,,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct comprehensive and systematical studies on these two pipelines under various settings. From comprehensive empirical results, we make original observations and find new insights that open new possibilities and suggest promising directions to leverage LLMs for learning on graphs. Our codes and datasets are available at https://github.com/CurryTang/Graph-LLM. ","[{'version': 'v1', 'created': 'Fri, 7 Jul 2023 05:31:31 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Jul 2023 06:06:04 GMT'}, {'version': 'v3', 'created': 'Wed, 2 Aug 2023 07:33:22 GMT'}]",2023-08-03,"[['Chen', 'Zhikai', ''], ['Mao', 'Haitao', ''], ['Li', 'Hang', ''], ['Jin', 'Wei', ''], ['Wen', 'Hongzhi', ''], ['Wei', 'Xiaochi', ''], ['Wang', 'Shuaiqiang', ''], ['Yin', 'Dawei', ''], ['Fan', 'Wenqi', ''], ['Liu', 'Hui', ''], ['Tang', 'Jiliang', '']]",0,0,2023-07-07,3,11,2,0,0,0,105669ec59a58fb2d4dd3021a984af33c227c5ab,259375824.0,https://www.semanticscholar.org/paper/105669ec59a58fb2d4dd3021a984af33c227c5ab,arXiv.org,2023.0,89.0,31.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2109393101', 'name': 'Zhikai Chen'}, {'authorId': '2125202063', 'name': 'Haitao Mao'}, {'authorId': '2145571830', 'name': 'Hang Li'}, {'authorId': '144767914', 'name': 'Wei Jin'}, {'authorId': '30580446', 'name': 'Haifang Wen'}, {'authorId': '7621447', 'name': 'Xiaochi Wei'}, {'authorId': '2386396', 'name': 'Shuaiqiang Wang'}, {'authorId': '2136400100', 'name': 'Dawei Yin'}, {'authorId': '41031455', 'name': 'Wenqi Fan'}, {'authorId': '2146672392', 'name': 'Hui Liu'}, {'authorId': '1736632', 'name': 'Jiliang Tang'}]","['Emory University', 'Michigan State University', 'Hong Kong Polytechnic University', 'Baidu']","['China', 'United States', 'Hong Kong']",2023-07
2307.03637,Nikhil Prakash,"Xander Davies, Max Nadeau, Nikhil Prakash, Tamar Rott Shaham, David
  Bau",Discovering Variable Binding Circuitry with Desiderata,,,,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Recent work has shown that computation in language models may be human-understandable, with successful efforts to localize and intervene on both single-unit features and input-output circuits. Here, we introduce an approach which extends causal mediation experiments to automatically identify model components responsible for performing a specific subtask by solely specifying a set of \textit{desiderata}, or causal attributes of the model components executing that subtask. As a proof of concept, we apply our method to automatically discover shared \textit{variable binding circuitry} in LLaMA-13B, which retrieves variable values for multiple arithmetic tasks. Our method successfully localizes variable binding to only 9 attention heads (of the 1.6k) and one MLP in the final token's residual stream. ","[{'version': 'v1', 'created': 'Fri, 7 Jul 2023 14:51:30 GMT'}]",2023-07-10,"[['Davies', 'Xander', ''], ['Nadeau', 'Max', ''], ['Prakash', 'Nikhil', ''], ['Shaham', 'Tamar Rott', ''], ['Bau', 'David', '']]",0,0,2023-07-07,1,5,1,1,1,0,b5131c07be779d90af946e6f370156b9506b7c0d,259375455.0,https://www.semanticscholar.org/paper/b5131c07be779d90af946e6f370156b9506b7c0d,arXiv.org,2023.0,18.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2212371252', 'name': 'Xander Davies'}, {'authorId': '2131005702', 'name': 'Max Nadeau'}, {'authorId': '2104923802', 'name': 'N. Prakash'}, {'authorId': '3459255', 'name': 'Tamar Rott Shaham'}, {'authorId': '144159726', 'name': 'David Bau'}]","['Northeastern University', 'Max Nadeau', 'Harvard University']","['China', 'United States']",2023-07
2307.03838,Pin-Yu Chen,Xiaomeng Hu and Pin-Yu Chen and Tsung-Yi Ho,RADAR: Robust AI-Text Detection via Adversarial Learning,Preprint. Project page and demos: https://radar.vizhub.ai,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusation of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a Robust AI-text Detector via Adversarial leaRning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic contents to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets, experimental results show that RADAR significantly outperforms existing AI-text detection methods, especially when paraphrasing is in place. We also identify the strong transferability of RADAR from instruction-tuned LLMs to other LLMs, and evaluate the improved capability of RADAR via GPT-3.5. ","[{'version': 'v1', 'created': 'Fri, 7 Jul 2023 21:13:27 GMT'}]",2023-07-11,"[['Hu', 'Xiaomeng', ''], ['Chen', 'Pin-Yu', ''], ['Ho', 'Tsung-Yi', '']]",1,1,2023-07-07,1,3,3,5,3,2,ef3bdb9e805887a1a14e1e4cdb27145135591305,259501842.0,https://www.semanticscholar.org/paper/ef3bdb9e805887a1a14e1e4cdb27145135591305,arXiv.org,2023.0,36.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2199229406', 'name': 'Xiao-bing Hu'}, {'authorId': '2158177948', 'name': 'Pin-Yu Chen'}, {'authorId': '2103197703', 'name': 'Tsung-Yi Ho'}]","['IBM (United States)', 'Chinese University of Hong Kong']","['China', 'United States']",2023-07
2307.05493,David Woo,"David James Woo, Kai Guo and Hengky Susanto",Cases of EFL Secondary Students' Prompt Engineering Pathways to Complete a Writing Task with ChatGPT,"41 pages, 6 figures",,10.13140/RG.2.2.31464.85762,,cs.HC cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  ChatGPT is a state-of-the-art (SOTA) chatbot. Although it has potential to support English as a foreign language (EFL) students' writing, to effectively collaborate with it, a student must learn to engineer prompts, that is, the skill of crafting appropriate instructions so that ChatGPT produces desired outputs. However, writing an appropriate prompt for ChatGPT is not straightforward for non-technical users who suffer a trial-and-error process. This paper examines the content of EFL students' ChatGPT prompts when completing a writing task and explores patterns in the quality and quantity of the prompts. The data come from iPad screen recordings of secondary school EFL students who used ChatGPT and other SOTA chatbots for the first time to complete the same writing task. The paper presents a case study of four distinct pathways that illustrate the trial-and-error process and show different combinations of prompt content and quantity. The cases contribute evidence for the need to provide prompt engineering education in the context of the EFL writing classroom, if students are to move beyond an individual trial-and-error process, learning a greater variety of prompt content and more sophisticated prompts to support their writing. ","[{'version': 'v1', 'created': 'Mon, 19 Jun 2023 06:45:04 GMT'}]",2023-07-13,"[['Woo', 'David James', ''], ['Guo', 'Kai', ''], ['Susanto', 'Hengky', '']]",1,1,2023-06-19,1,3,3,1,0,1,344f801663a76aa15e0dd13344261d8648c382a2,259836994.0,https://www.semanticscholar.org/paper/344f801663a76aa15e0dd13344261d8648c382a2,arXiv.org,2023.0,36.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Education', 'source': 's2-fos-model'}]","[{'authorId': '12132198', 'name': 'D. Woo'}, {'authorId': '2061502406', 'name': 'Kai Guo'}, {'authorId': '3353735', 'name': 'Hengky Susanto'}]","['University of Massachusetts Lowell', 'Huawei Technologies (China)', 'Precious Blood Secondary School, 338 San Ha Street, Chai Wan, Hong Kong, China', 'University of Hong Kong']","['China', 'United States', 'Hong Kong']",2023-06
2307.05628,Daoan Zhang,"Daoan Zhang, Weitong Zhang, Yu Zhao, Jianguo Zhang, Bing He, Chenchen
  Qin, Jianhua Yao",DNAGPT: A Generalized Pre-trained Tool for Versatile DNA Sequence Analysis Tasks,,,,,q-bio.GN cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained large language models demonstrate potential in extracting information from DNA sequences, yet adapting to a variety of tasks and data modalities remains a challenge. To address this, we propose DNAGPT, a generalized DNA pre-training model trained on over 200 billion base pairs from all mammals. By enhancing the classic GPT model with a binary classification task (DNA sequence order), a numerical regression task (guanine-cytosine content prediction), and a comprehensive token language, DNAGPT can handle versatile DNA analysis tasks while processing both sequence and numerical data. Our evaluation of genomic signal and region recognition, mRNA abundance regression, and artificial genomes generation tasks demonstrates DNAGPT's superior performance compared to existing models designed for specific downstream tasks, benefiting from pre-training using the newly designed model structure. ","[{'version': 'v1', 'created': 'Tue, 11 Jul 2023 06:30:43 GMT'}, {'version': 'v2', 'created': 'Mon, 7 Aug 2023 07:41:47 GMT'}, {'version': 'v3', 'created': 'Wed, 30 Aug 2023 20:16:55 GMT'}]",2023-09-01,"[['Zhang', 'Daoan', ''], ['Zhang', 'Weitong', ''], ['Zhao', 'Yu', ''], ['Zhang', 'Jianguo', ''], ['He', 'Bing', ''], ['Qin', 'Chenchen', ''], ['Yao', 'Jianhua', '']]",0,1,2023-07-11,3,7,2,0,0,0,5bfce5239842726bcb8b51261630538a0cbe3ca1,259837062.0,https://www.semanticscholar.org/paper/5bfce5239842726bcb8b51261630538a0cbe3ca1,bioRxiv,2023.0,56.0,0.0,0.0,True,"['Biology', 'Computer Science']","[{'category': 'Biology', 'source': 'external'}, {'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Biology', 'source': 's2-fos-model'}]","[{'authorId': '2188765758', 'name': 'Daoan Zhang'}, {'authorId': '2108190244', 'name': 'Weitong Zhang'}, {'authorId': '2186372946', 'name': 'Bing He'}, {'authorId': '97522134', 'name': 'Yu Zhao'}, {'authorId': '2108313681', 'name': 'Jiang Zhang'}, {'authorId': '1919324982', 'name': 'Chenchen Qin'}, {'authorId': '2200180645', 'name': 'Jianhua Yao'}]","['City University of Hong Kong', 'Tencent', 'University of Rochester', 'Southern University of Science and Technology']","['China', 'United States']",2023-07
2307.07697,Gasol Sun,"Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin,
  Yeyun Gong, Lionel M. Ni, Heung-Yeung Shum, Jian Guo",Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph,"30 pages, 13 figures, 20 tables",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although large language models (LLMs) have achieved significant success in various tasks, they often struggle with hallucination problems, especially in scenarios requiring deep and responsible reasoning. These issues could be partially addressed by introducing external knowledge graphs (KG) in LLM reasoning. In this paper, we propose a new LLM-KG integrating paradigm ``$\hbox{LLM}\otimes\hbox{KG}$'' which treats the LLM as an agent to interactively explore related entities and relations on KGs and perform reasoning based on the retrieved knowledge. We further implement this paradigm by introducing a new approach called Think-on-Graph (ToG), in which the LLM agent iteratively executes beam search on KG, discovers the most promising reasoning paths, and returns the most likely reasoning results. We use a number of well-designed experiments to examine and illustrate the following advantages of ToG: 1) compared with LLMs, ToG has better deep reasoning power; 2) ToG has the ability of knowledge traceability and knowledge correctability by leveraging LLMs reasoning and expert feedback; 3) ToG provides a flexible plug-and-play framework for different LLMs, KGs and prompting strategies without any additional training cost; 4) the performance of ToG with small LLM models could exceed large LLM such as GPT-4 in certain scenarios and this reduces the cost of LLM deployment and application. As a training-free method with lower computational cost and better generality, ToG achieves overall SOTA in 6 out of 9 datasets where most previous SOTAs rely on additional training. ","[{'version': 'v1', 'created': 'Sat, 15 Jul 2023 03:31:38 GMT'}, {'version': 'v2', 'created': 'Tue, 19 Sep 2023 08:39:04 GMT'}, {'version': 'v3', 'created': 'Sat, 30 Sep 2023 05:00:58 GMT'}]",2023-10-03,"[['Sun', 'Jiashuo', ''], ['Xu', 'Chengjin', ''], ['Tang', 'Lumingyuan', ''], ['Wang', 'Saizhuo', ''], ['Lin', 'Chen', ''], ['Gong', 'Yeyun', ''], ['Ni', 'Lionel M.', ''], ['Shum', 'Heung-Yeung', ''], ['Guo', 'Jian', '']]",0,1,2023-07-15,3,9,1,1,0,1,c7184f9a914dbfbad59faa5aeaf9ba7019dfcf74,263333907.0,https://www.semanticscholar.org/paper/c7184f9a914dbfbad59faa5aeaf9ba7019dfcf74,,2023.0,78.0,1.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2202454665', 'name': 'Jiashuo Sun'}, {'authorId': '2250617116', 'name': 'Chengjin Xu'}, {'authorId': '2253747495', 'name': 'Lumingyuan Tang'}, {'authorId': '2211588034', 'name': 'Sai Wang'}, {'authorId': '2249759096', 'name': 'Chen Lin'}, {'authorId': '2254121688', 'name': 'Yeyun Gong'}, {'authorId': '2249757348', 'name': 'Lionel M. Ni'}, {'authorId': '93596028', 'name': 'H. Shum'}, {'authorId': '2188226506', 'name': 'Jian Guo'}]","['University of Southern California', 'Xiamen University', 'Hong Kong University of Science and Technology', 'IDEA Research, International Digital Economy Academy', 'Microsoft']","['China', 'United States']",2023-07
2307.07909,Yao Wei,"Yao Wei and Yanchao Sun and Ruijie Zheng and Sai Vemprala and Rogerio
  Bonatti and Shuhang Chen and Ratnesh Madaan and Zhongjie Ba and Ashish Kapoor
  and Shuang Ma",Is Imitation All You Need? Generalized Decision-Making with Dual-Phase Training,,,,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  We introduce DualMind, a generalist agent designed to tackle various decision-making tasks that addresses challenges posed by current methods, such as overfitting behaviors and dependence on task-specific fine-tuning. DualMind uses a novel ""Dual-phase"" training strategy that emulates how humans learn to act in the world. The model first learns fundamental common knowledge through a self-supervised objective tailored for control tasks and then learns how to make decisions based on different contexts through imitating behaviors conditioned on given prompts. DualMind can handle tasks across domains, scenes, and embodiments using just a single set of model weights and can execute zero-shot prompting without requiring task-specific fine-tuning. We evaluate DualMind on MetaWorld and Habitat through extensive experiments and demonstrate its superior generalizability compared to previous techniques, outperforming other generalist agents by over 50$\%$ and 70$\%$ on Habitat and MetaWorld, respectively. On the 45 tasks in MetaWorld, DualMind achieves over 30 tasks at a 90$\%$ success rate. ","[{'version': 'v1', 'created': 'Sun, 16 Jul 2023 00:34:12 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Jul 2023 16:05:00 GMT'}]",2023-07-19,"[['Wei', 'Yao', ''], ['Sun', 'Yanchao', ''], ['Zheng', 'Ruijie', ''], ['Vemprala', 'Sai', ''], ['Bonatti', 'Rogerio', ''], ['Chen', 'Shuhang', ''], ['Madaan', 'Ratnesh', ''], ['Ba', 'Zhongjie', ''], ['Kapoor', 'Ashish', ''], ['Ma', 'Shuang', '']]",0,0,2023-07-16,2,10,1,0,0,0,283ab0486c77c9c9fccb060704fcdc559cae24ce,259937676.0,https://www.semanticscholar.org/paper/283ab0486c77c9c9fccb060704fcdc559cae24ce,arXiv.org,2023.0,65.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2211584665', 'name': 'Yao Wei'}, {'authorId': '120738683', 'name': 'Yanchao Sun'}, {'authorId': '2108815599', 'name': 'Ruijie Zheng'}, {'authorId': '8355354', 'name': 'Sai Vemprala'}, {'authorId': '3444893', 'name': 'Rogerio Bonatti'}, {'authorId': '122665192', 'name': 'Shuhang Chen'}, {'authorId': '31549065', 'name': 'Ratnesh Madaan'}, {'authorId': '36890675', 'name': 'Zhongjie Ba'}, {'authorId': '2189118', 'name': 'Ashish Kapoor'}, {'authorId': '2118867113', 'name': 'Shuang Ma'}]","['Scaled Foundations', 'University of Maryland, College Park', 'Zhejiang University']","['China', 'United States']",2023-07
2307.07951,Zhenwen Liang,"Zhenwen Liang, Dian Yu, Xiaoman Pan, Wenlin Yao, Qingkai Zeng,
  Xiangliang Zhang, Dong Yu",MinT: Boosting Generalization in Mathematical Reasoning via Multi-View Fine-Tuning,,,,,cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Reasoning in mathematical domains remains a significant challenge for relatively small language models (LMs). Many current methods focus on specializing LMs in mathematical reasoning and rely heavily on knowledge distillation from powerful but inefficient large LMs (LLMs). In this work, we explore a new direction that avoids over-reliance on LLM teachers, introducing a multi-view fine-tuning method that efficiently exploits existing mathematical problem datasets with diverse annotation styles. Our approach uniquely considers the various annotation formats as different ""views"" and leverages them in training the model. By postpending distinct instructions to input questions, models can learn to generate solutions in diverse formats in a flexible manner. Experimental results show that our strategy enables a LLaMA-7B model to outperform prior approaches that utilize knowledge distillation, as well as carefully established baselines. Additionally, the proposed method grants the models promising generalization ability across various views and datasets, and the capability to learn from inaccurate or incomplete noisy data. We hope our multi-view training paradigm could inspire future studies in other machine reasoning domains. ","[{'version': 'v1', 'created': 'Sun, 16 Jul 2023 05:41:53 GMT'}]",2023-07-18,"[['Liang', 'Zhenwen', ''], ['Yu', 'Dian', ''], ['Pan', 'Xiaoman', ''], ['Yao', 'Wenlin', ''], ['Zeng', 'Qingkai', ''], ['Zhang', 'Xiangliang', ''], ['Yu', 'Dong', '']]",0,0,2023-07-16,1,7,2,1,1,0,c1c93b4916aa5b8ce1d99a9e59c700e3d13ada36,259937550.0,https://www.semanticscholar.org/paper/c1c93b4916aa5b8ce1d99a9e59c700e3d13ada36,arXiv.org,2023.0,45.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '151474408', 'name': 'Zhenwen Liang'}, {'authorId': '41190054', 'name': 'Dian Yu'}, {'authorId': '34741133', 'name': 'Xiaoman Pan'}, {'authorId': '2087264100', 'name': 'Wenlin Yao'}, {'authorId': '1694209', 'name': 'Qingkai Zeng'}, {'authorId': '2928371', 'name': 'Xiangliang Zhang'}, {'authorId': '144580027', 'name': 'Dong Yu'}]","['Tencent', 'University of Notre Dame']","['China', 'United States']",2023-07
2307.08152,Jingqing Zhang,"Jingqing Zhang, Kai Sun, Akshay Jagadeesh, Mahta Ghahfarokhi, Deepa
  Gupta, Ashok Gupta, Vibhor Gupta, Yike Guo",The Potential and Pitfalls of using a Large Language Model such as ChatGPT or GPT-4 as a Clinical Assistant,"This manuscript is pre-print and in peer review. Supplementary
  materials will be published later",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Recent studies have demonstrated promising performance of ChatGPT and GPT-4 on several medical domain tasks. However, none have assessed its performance using a large-scale real-world electronic health record database, nor have evaluated its utility in providing clinical diagnostic assistance for patients across a full range of disease presentation. We performed two analyses using ChatGPT and GPT-4, one to identify patients with specific medical diagnoses using a real-world large electronic health record database and the other, in providing diagnostic assistance to healthcare workers in the prospective evaluation of hypothetical patients. Our results show that GPT-4 across disease classification tasks with chain of thought and few-shot prompting can achieve performance as high as 96% F1 scores. For patient assessment, GPT-4 can accurately diagnose three out of four times. However, there were mentions of factually incorrect statements, overlooking crucial medical findings, recommendations for unnecessary investigations and overtreatment. These issues coupled with privacy concerns, make these models currently inadequate for real world clinical use. However, limited data and time needed for prompt engineering in comparison to configuration of conventional machine learning workflows highlight their potential for scalability across healthcare applications. ","[{'version': 'v1', 'created': 'Sun, 16 Jul 2023 21:19:47 GMT'}]",2023-07-18,"[['Zhang', 'Jingqing', ''], ['Sun', 'Kai', ''], ['Jagadeesh', 'Akshay', ''], ['Ghahfarokhi', 'Mahta', ''], ['Gupta', 'Deepa', ''], ['Gupta', 'Ashok', ''], ['Gupta', 'Vibhor', ''], ['Guo', 'Yike', '']]",1,1,2023-07-16,1,8,1,2,0,2,b3d6fec3f1a878b0c612f0ffed820b045c2c46d8,259937819.0,https://www.semanticscholar.org/paper/b3d6fec3f1a878b0c612f0ffed820b045c2c46d8,arXiv.org,2023.0,33.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Medicine', 'source': 's2-fos-model'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '47540100', 'name': 'Jingqing Zhang'}, {'authorId': '145932999', 'name': 'K. Sun'}, {'authorId': '66890764', 'name': 'A. Jagadeesh'}, {'authorId': '2223592134', 'name': 'Mahta Ghahfarokhi'}, {'authorId': '143615116', 'name': 'Deepa Gupta'}, {'authorId': '2223694395', 'name': 'Ashok Gupta'}, {'authorId': '2110652718', 'name': 'Vibhor Gupta'}, {'authorId': '2118269437', 'name': 'Yike Guo'}]","['Imperial College London', 'Pangaea Data Limited, UK, USA', 'Hong Kong University of Science and Technology']","['China', 'United States', 'United Kingdom']",2023-07
2307.09765,Bowen Xu,"Bowen Xu, Thanh-Dat Nguyen, Thanh Le-Cong, Thong Hoang, Jiakun Liu,
  Kisub Kim, Chen Gong, Changan Niu, Chenyu Wang, Bach Le, David Lo",Are We Ready to Embrace Generative AI for Software Q&A?,"Accepted by the New Ideas and Emerging Results (NIER) track at The
  IEEE/ACM Automated Software Engineering (ASE) Conference",,,,cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Stack Overflow, the world's largest software Q&A (SQA) website, is facing a significant traffic drop due to the emergence of generative AI techniques. ChatGPT is banned by Stack Overflow after only 6 days from its release. The main reason provided by the official Stack Overflow is that the answers generated by ChatGPT are of low quality. To verify this, we conduct a comparative evaluation of human-written and ChatGPT-generated answers. Our methodology employs both automatic comparison and a manual study. Our results suggest that human-written and ChatGPT-generated answers are semantically similar, however, human-written answers outperform ChatGPT-generated ones consistently across multiple aspects, specifically by 10% on the overall score. We release the data, analysis scripts, and detailed results at https://anonymous.4open.science/r/GAI4SQA-FD5C. ","[{'version': 'v1', 'created': 'Wed, 19 Jul 2023 05:54:43 GMT'}, {'version': 'v2', 'created': 'Sat, 12 Aug 2023 13:10:02 GMT'}]",2023-08-15,"[['Xu', 'Bowen', ''], ['Nguyen', 'Thanh-Dat', ''], ['Le-Cong', 'Thanh', ''], ['Hoang', 'Thong', ''], ['Liu', 'Jiakun', ''], ['Kim', 'Kisub', ''], ['Gong', 'Chen', ''], ['Niu', 'Changan', ''], ['Wang', 'Chenyu', ''], ['Le', 'Bach', ''], ['Lo', 'David', '']]",1,1,2023-07-19,2,11,1,1,0,1,43633a1a90a3991ffcf108859bb25720330c145f,259982596.0,https://www.semanticscholar.org/paper/43633a1a90a3991ffcf108859bb25720330c145f,International Conference on Automated Software Engineering,2023.0,19.0,2.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2203459', 'name': 'Bowen Xu'}, {'authorId': '2174165242', 'name': 'Thanh-Dat Nguyen'}, {'authorId': '1811413300', 'name': 'Thanh Le-Cong'}, {'authorId': '9719656', 'name': 'Thong Hoang'}, {'authorId': '120809900', 'name': 'Jiakun Liu'}, {'authorId': '35276441', 'name': 'Kisub Kim'}, {'authorId': '2223951228', 'name': 'Chen Gong'}, {'authorId': '2069657833', 'name': 'Changan Niu'}, {'authorId': '2211991264', 'name': 'Chenyu Wang'}, {'authorId': '2248993736', 'name': 'Xuan-Bach Dinh Le'}, {'authorId': '2150912791', 'name': 'David Lo'}]","['Nanjing University', 'Data61', 'University of Virginia', 'Singapore Management University', 'University of Melbourne', 'North Carolina State University']","['China', 'United States', 'Singapore', 'Australia']",2023-07
2307.10432,Zhengliang Liu,"Zhengliang Liu, Zihao Wu, Mengxuan Hu, Bokai Zhao, Lin Zhao, Tianyi
  Zhang, Haixing Dai, Xianyan Chen, Ye Shen, Sheng Li, Brian Murray, Tianming
  Liu, Andrea Sikora",PharmacyGPT: The AI Pharmacist,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  In this study, we introduce PharmacyGPT, a novel framework to assess the capabilities of large language models (LLMs) such as ChatGPT and GPT-4 in emulating the role of clinical pharmacists. Our methodology encompasses the utilization of LLMs to generate comprehensible patient clusters, formulate medication plans, and forecast patient outcomes. We conduct our investigation using real data acquired from the intensive care unit (ICU) at the University of North Carolina Chapel Hill (UNC) Hospital. Our analysis offers valuable insights into the potential applications and limitations of LLMs in the field of clinical pharmacy, with implications for both patient care and the development of future AI-driven healthcare solutions. By evaluating the performance of PharmacyGPT, we aim to contribute to the ongoing discourse surrounding the integration of artificial intelligence in healthcare settings, ultimately promoting the responsible and efficacious use of such technologies. ","[{'version': 'v1', 'created': 'Wed, 19 Jul 2023 19:40:34 GMT'}, {'version': 'v2', 'created': 'Fri, 21 Jul 2023 02:22:14 GMT'}]",2023-07-24,"[['Liu', 'Zhengliang', ''], ['Wu', 'Zihao', ''], ['Hu', 'Mengxuan', ''], ['Zhao', 'Bokai', ''], ['Zhao', 'Lin', ''], ['Zhang', 'Tianyi', ''], ['Dai', 'Haixing', ''], ['Chen', 'Xianyan', ''], ['Shen', 'Ye', ''], ['Li', 'Sheng', ''], ['Murray', 'Brian', ''], ['Liu', 'Tianming', ''], ['Sikora', 'Andrea', '']]",1,1,2023-07-19,2,13,1,2,0,2,8ef83aec0d772ebb9fb22e49d15aa979d38d3ef4,259991555.0,https://www.semanticscholar.org/paper/8ef83aec0d772ebb9fb22e49d15aa979d38d3ef4,arXiv.org,2023.0,74.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Medicine', 'source': 's2-fos-model'}]","[{'authorId': '2145977326', 'name': 'Zheng Liu'}, {'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '2215174877', 'name': 'Mengxuan Hu'}, {'authorId': '2112525162', 'name': 'Bokai Zhao'}, {'authorId': '2111641126', 'name': 'Lin Zhao'}, {'authorId': '2221113406', 'name': 'Tianyi Zhang'}, {'authorId': '29944950', 'name': 'Haixing Dai'}, {'authorId': '2167589230', 'name': 'Xianyan Chen'}, {'authorId': '2196205696', 'name': 'Ye Shen'}, {'authorId': '2153702893', 'name': 'Sheng Li'}, {'authorId': '2156692365', 'name': 'Brian Murray'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}, {'authorId': '84406057', 'name': 'A. Sikora'}]","['University of Virginia', 'University of Georgia', 'Northwest University']","['China', 'United States']",2023-07
2307.13693,Zhengliang Liu,"Zhengliang Liu, Tianyang Zhong, Yiwei Li, Yutong Zhang, Yi Pan, Zihao
  Zhao, Peixin Dong, Chao Cao, Yuxiao Liu, Peng Shu, Yaonai Wei, Zihao Wu,
  Chong Ma, Jiaqi Wang, Sheng Wang, Mengyue Zhou, Zuowei Jiang, Chunlin Li,
  Jason Holmes, Shaochen Xu, Lu Zhang, Haixing Dai, Kai Zhang, Lin Zhao,
  Yuanhao Chen, Xu Liu, Peilong Wang, Pingkun Yan, Jun Liu, Bao Ge, Lichao Sun,
  Dajiang Zhu, Xiang Li, Wei Liu, Xiaoyan Cai, Xintao Hu, Xi Jiang, Shu Zhang,
  Xin Zhang, Tuo Zhang, Shijie Zhao, Quanzheng Li, Hongtu Zhu, Dinggang Shen,
  Tianming Liu",Evaluating Large Language Models for Radiology Natural Language Processing,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The rise of large language models (LLMs) has marked a pivotal shift in the field of natural language processing (NLP). LLMs have revolutionized a multitude of domains, and they have made a significant impact in the medical field. Large language models are now more abundant than ever, and many of these models exhibit bilingual capabilities, proficient in both English and Chinese. However, a comprehensive evaluation of these models remains to be conducted. This lack of assessment is especially apparent within the context of radiology NLP. This study seeks to bridge this gap by critically evaluating thirty two LLMs in interpreting radiology reports, a crucial component of radiology NLP. Specifically, the ability to derive impressions from radiologic findings is assessed. The outcomes of this evaluation provide key insights into the performance, strengths, and weaknesses of these LLMs, informing their practical applications within the medical domain. ","[{'version': 'v1', 'created': 'Tue, 25 Jul 2023 17:57:18 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Jul 2023 12:58:59 GMT'}]",2023-07-28,"[['Liu', 'Zhengliang', ''], ['Zhong', 'Tianyang', ''], ['Li', 'Yiwei', ''], ['Zhang', 'Yutong', ''], ['Pan', 'Yi', ''], ['Zhao', 'Zihao', ''], ['Dong', 'Peixin', ''], ['Cao', 'Chao', ''], ['Liu', 'Yuxiao', ''], ['Shu', 'Peng', ''], ['Wei', 'Yaonai', ''], ['Wu', 'Zihao', ''], ['Ma', 'Chong', ''], ['Wang', 'Jiaqi', ''], ['Wang', 'Sheng', ''], ['Zhou', 'Mengyue', ''], ['Jiang', 'Zuowei', ''], ['Li', 'Chunlin', ''], ['Holmes', 'Jason', ''], ['Xu', 'Shaochen', ''], ['Zhang', 'Lu', ''], ['Dai', 'Haixing', ''], ['Zhang', 'Kai', ''], ['Zhao', 'Lin', ''], ['Chen', 'Yuanhao', ''], ['Liu', 'Xu', ''], ['Wang', 'Peilong', ''], ['Yan', 'Pingkun', ''], ['Liu', 'Jun', ''], ['Ge', 'Bao', ''], ['Sun', 'Lichao', ''], ['Zhu', 'Dajiang', ''], ['Li', 'Xiang', ''], ['Liu', 'Wei', ''], ['Cai', 'Xiaoyan', ''], ['Hu', 'Xintao', ''], ['Jiang', 'Xi', ''], ['Zhang', 'Shu', ''], ['Zhang', 'Xin', ''], ['Zhang', 'Tuo', ''], ['Zhao', 'Shijie', ''], ['Li', 'Quanzheng', ''], ['Zhu', 'Hongtu', ''], ['Shen', 'Dinggang', ''], ['Liu', 'Tianming', '']]",0,0,2023-07-25,2,45,1,0,0,0,2525f8185ff4eee23dea96f2a820714a1619fb89,260209167.0,https://www.semanticscholar.org/paper/2525f8185ff4eee23dea96f2a820714a1619fb89,,2023.0,135.0,3.0,0.0,False,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Medicine', 'source': 's2-fos-model'}]","[{'authorId': '2145977326', 'name': 'Zheng Liu'}, {'authorId': '2215167446', 'name': 'Tianyang Zhong'}, {'authorId': '2111161355', 'name': 'Yiwei Li'}, {'authorId': '2224856036', 'name': 'Yutong Zhang'}, {'authorId': '19308245', 'name': 'Yirong Pan'}, {'authorId': '15594254', 'name': 'Zihao Zhao'}, {'authorId': '11628039', 'name': 'Pei Dong'}, {'authorId': '5762440', 'name': 'Chao-Yang Cao'}, {'authorId': '2108135727', 'name': 'Yu‐Xin Liu'}, {'authorId': '2220096705', 'name': 'Peng Shu'}, {'authorId': '2163675860', 'name': 'Yaonai Wei'}, {'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '120688117', 'name': 'Chong-Yi Ma'}, {'authorId': '2136025369', 'name': 'Jiaqi Wang'}, {'authorId': '2151485716', 'name': 'Shengming Wang'}, {'authorId': '2212701018', 'name': 'Mengyue Zhou'}, {'authorId': '2184750732', 'name': 'Zuowei Jiang'}, {'authorId': '48162177', 'name': 'Chunlin Li'}, {'authorId': '2177342883', 'name': 'J. Holmes'}, {'authorId': '2211904452', 'name': 'Shaochen Xu'}, {'authorId': '2156146823', 'name': 'Lu Zhang'}, {'authorId': '29944950', 'name': 'Haixing Dai'}, {'authorId': '2158521525', 'name': 'Kailiang Zhang'}, {'authorId': '2111641126', 'name': 'Lin Zhao'}, {'authorId': '84735131', 'name': 'Yuanhao'}, {'authorId': '2117444859', 'name': 'Chen'}, {'authorId': '40913460', 'name': 'X. Liu'}, {'authorId': '2118952169', 'name': 'Pei Wang'}, {'authorId': '1690097', 'name': 'Pingkun Yan'}, {'authorId': '2157176152', 'name': 'Jun Liu'}, {'authorId': '144691205', 'name': 'Bao Ge'}, {'authorId': '2219704055', 'name': 'Lichao Sun'}, {'authorId': '2081561953', 'name': 'Dajiang'}, {'authorId': '2054765028', 'name': 'Zhu'}, {'authorId': '113075935', 'name': 'Xiang Li'}, {'authorId': '46641573', 'name': 'W. Liu'}, {'authorId': '2149789733', 'name': 'Xiaoyan Cai'}, {'authorId': '1742535', 'name': 'Xintao Hu'}, {'authorId': '143796247', 'name': 'Xi Jiang'}, {'authorId': '2108086798', 'name': 'Shu Zhang'}, {'authorId': '2149174952', 'name': 'Xin Zhang'}, {'authorId': '49104946', 'name': 'Tuo Zhang'}, {'authorId': '2946035', 'name': 'Shijie Zhao'}, {'authorId': '1762919', 'name': 'Quanzheng Li'}, {'authorId': '3280236', 'name': 'Hongtu Zhu'}, {'authorId': '2150038187', 'name': 'Dinggang Shen'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}]","['University of Electronic Science and Technology of China', 'Lingang Laboratory, Shanghai, 200031, China', 'Second Xiangya Hospital of Central South University', 'Mayo Clinic', 'University of North Carolina at Chapel Hill', 'Shanghai United Imaging Intelligence Co., Ltd.', 'Lehigh University', 'ShanghaiTech University', 'Shanghai Jiao Tong University', 'Dartmouth College', 'Shanghai Clinical Research Center', 'Massachusetts General Hospital', 'Shaanxi Normal University', 'The University of Texas at Arlington', 'University of Georgia', 'Rensselaer Polytechnic Institute', 'Northwestern Polytechnical University']","['China', 'United States']",2023-07
2308.10025,Kaihang Pan,"Kaihang Pan, Juncheng Li, Hongye Song, Hao Fei, Wei Ji, Shuo Zhang,
  Jun Lin, Xiaozhong Liu, Siliang Tang",ControlRetriever: Harnessing the Power of Instructions for Controllable Retrieval,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent studies have shown that dense retrieval models, lacking dedicated training data, struggle to perform well across diverse retrieval tasks, as different retrieval tasks often entail distinct search intents. To address this challenge, in this work we introduce ControlRetriever, a generic and efficient approach with a parameter isolated architecture, capable of controlling dense retrieval models to directly perform varied retrieval tasks, harnessing the power of instructions that explicitly describe retrieval intents in natural language. Leveraging the foundation of ControlNet, which has proven powerful in text-to-image generation, ControlRetriever imbues different retrieval models with the new capacity of controllable retrieval, all while being guided by task-specific instructions. Furthermore, we propose a novel LLM guided Instruction Synthesizing and Iterative Training strategy, which iteratively tunes ControlRetriever based on extensive automatically-generated retrieval data with diverse instructions by capitalizing the advancement of large language models. Extensive experiments show that in the BEIR benchmark, with only natural language descriptions of specific retrieval intent for each task, ControlRetriever, as a unified multi-task retrieval system without task-specific tuning, significantly outperforms baseline methods designed with task-specific retrievers and also achieves state-of-the-art zero-shot performance. ","[{'version': 'v1', 'created': 'Sat, 19 Aug 2023 14:17:57 GMT'}]",2023-08-22,"[['Pan', 'Kaihang', ''], ['Li', 'Juncheng', ''], ['Song', 'Hongye', ''], ['Fei', 'Hao', ''], ['Ji', 'Wei', ''], ['Zhang', 'Shuo', ''], ['Lin', 'Jun', ''], ['Liu', 'Xiaozhong', ''], ['Tang', 'Siliang', '']]",0,0,2023-08-19,1,9,1,0,0,0,ab3f7615d9ed62483f98048b62ba1e4cbf0fada2,261048776.0,https://www.semanticscholar.org/paper/ab3f7615d9ed62483f98048b62ba1e4cbf0fada2,arXiv.org,2023.0,52.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2212175601', 'name': 'Kaihang Pan'}, {'authorId': '2108998895', 'name': 'Juncheng Li'}, {'authorId': '2122383983', 'name': 'Hongye Song'}, {'authorId': '2142672912', 'name': 'Hao Fei'}, {'authorId': '2150078659', 'name': 'Wei Ji'}, {'authorId': '2144324539', 'name': 'Shuo Zhang'}, {'authorId': '2110808728', 'name': 'Jun Lin'}, {'authorId': '1713802', 'name': 'Xiaozhong Liu'}, {'authorId': '2118071462', 'name': 'Siliang Tang'}]","['Alibaba', 'National University of Singapore', 'Worcester Polytechnic Institute', 'Zhejiang University']","['China', 'United States', 'Singapore']",2023-08
2308.10149,Yingji Li,"Yingji Li, Mengnan Du, Rui Song, Xin Wang, Ying Wang",A Survey on Fairness in Large Language Models,"12 pages, 2 figures, 101 references",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have shown powerful performance and development prospect and are widely deployed in the real world. However, LLMs can capture social biases from unprocessed training data and propagate the biases to downstream tasks. Unfair LLM systems have undesirable social impacts and potential harms. In this paper, we provide a comprehensive review of related research on fairness in LLMs. First, for medium-scale LLMs, we introduce evaluation metrics and debiasing methods from the perspectives of intrinsic bias and extrinsic bias, respectively. Then, for large-scale LLMs, we introduce recent fairness research, including fairness evaluation, reasons for bias, and debiasing methods. Finally, we discuss and provide insight on the challenges and future directions for the development of fairness in LLMs. ","[{'version': 'v1', 'created': 'Sun, 20 Aug 2023 03:30:22 GMT'}]",2023-08-22,"[['Li', 'Yingji', ''], ['Du', 'Mengnan', ''], ['Song', 'Rui', ''], ['Wang', 'Xin', ''], ['Wang', 'Ying', '']]",0,0,2023-08-20,1,5,2,0,0,0,03bf28df6e282a7e36e1686edeb9c624e6ffb13b,261049466.0,https://www.semanticscholar.org/paper/03bf28df6e282a7e36e1686edeb9c624e6ffb13b,arXiv.org,2023.0,104.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2116202874', 'name': 'Yingji Li'}, {'authorId': '3432460', 'name': 'Mengnan Du'}, {'authorId': '145401373', 'name': 'Rui Song'}, {'authorId': '2153687737', 'name': 'Xin Wang'}, {'authorId': '49416173', 'name': 'Y. Wang'}]","['Jilin University', 'New Jersey Institute of Technology']","['China', 'United States']",2023-08
2308.12032,Ming Li,"Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng,
  Jianzong Wang, Tianyi Zhou, Jing Xiao",From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the realm of Large Language Models, the balance between instruction data quality and quantity has become a focal point. Recognizing this, we introduce a self-guided methodology for LLMs to autonomously discern and select cherry samples from vast open-source datasets, effectively minimizing manual curation and potential cost for instruction tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD) metric, emerges as a pivotal tool to identify discrepancies between a model's expected responses and its autonomous generation prowess. Through the adept application of IFD, cherry samples are pinpointed, leading to a marked uptick in model training efficiency. Empirical validations on renowned datasets like Alpaca and WizardLM underpin our findings; with a mere 10% of conventional data input, our strategy showcases improved results. This synthesis of self-guided cherry-picking and the IFD metric signifies a transformative leap in the optimization of LLMs, promising both efficiency and resource-conscious advancements. Codes, data, and models are available: https://github.com/MingLiiii/Cherry_LLM ","[{'version': 'v1', 'created': 'Wed, 23 Aug 2023 09:45:29 GMT'}, {'version': 'v2', 'created': 'Fri, 8 Sep 2023 05:11:01 GMT'}, {'version': 'v3', 'created': 'Fri, 15 Sep 2023 20:33:44 GMT'}]",2023-09-19,"[['Li', 'Ming', ''], ['Zhang', 'Yong', ''], ['Li', 'Zhitao', ''], ['Chen', 'Jiuhai', ''], ['Chen', 'Lichang', ''], ['Cheng', 'Ning', ''], ['Wang', 'Jianzong', ''], ['Zhou', 'Tianyi', ''], ['Xiao', 'Jing', '']]",0,0,2023-08-23,3,9,1,1,0,1,e3052ebca5eeae6a8a73e44517903d39746f5f3a,261076515.0,https://www.semanticscholar.org/paper/e3052ebca5eeae6a8a73e44517903d39746f5f3a,arXiv.org,2023.0,37.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Business', 'source': 's2-fos-model'}]","[{'authorId': '2150655891', 'name': 'Ming Li'}, {'authorId': '2144289768', 'name': 'Yong Zhang'}, {'authorId': '2111336489', 'name': 'Zhitao Li'}, {'authorId': '1391200710', 'name': 'Jiuhai Chen'}, {'authorId': '2108451006', 'name': 'Lichang Chen'}, {'authorId': '145292435', 'name': 'Ning Cheng'}, {'authorId': '66063851', 'name': 'Jianzong Wang'}, {'authorId': '2213956781', 'name': 'Tianyi Zhou'}, {'authorId': '91353860', 'name': 'Jing Xiao'}]","['https://github.com/MingLiiii/Cherry LLM', 'Ping An Technology (Shenzhen) Co., Ltd., China', 'University of Maryland, College Park']","['China', 'United States']",2023-08
2308.12067,Lai Wei,"Lai Wei, Zihao Jiang, Weiran Huang, Lichao Sun",InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4,,,,,cs.LG cs.AI cs.CL cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal large language models acquire their instruction-following capabilities through a two-stage training process: pre-training on image-text pairs and fine-tuning on supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6% of the instruction-following data used in the alignment dataset for MiniGPT-4. We first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present a simple and effective data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations (e.g., visual question answering, GPT-4 preference). Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient to enable multimodal large language models to generate better output. ","[{'version': 'v1', 'created': 'Wed, 23 Aug 2023 11:27:30 GMT'}]",2023-08-24,"[['Wei', 'Lai', ''], ['Jiang', 'Zihao', ''], ['Huang', 'Weiran', ''], ['Sun', 'Lichao', '']]",0,1,2023-08-23,1,4,4,1,0,1,d7e92d03dfa5427c0c5ef2b59de54733e0589606,261075922.0,https://www.semanticscholar.org/paper/d7e92d03dfa5427c0c5ef2b59de54733e0589606,arXiv.org,2023.0,44.0,8.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2110973198', 'name': 'Lai Wei'}, {'authorId': '31089377', 'name': 'Zihao Jiang'}, {'authorId': '8007867', 'name': 'Weiran Huang'}, {'authorId': '2219704055', 'name': 'Lichao Sun'}]","['Shanghai Jiao Tong University', 'Lehigh University']","['China', 'United States']",2023-08
2308.12241,Junling Liu,"Junling Liu, Chao Liu, Peilin Zhou, Qichen Ye, Dading Chong, Kang
  Zhou, Yueqi Xie, Yuwei Cao, Shoujin Wang, Chenyu You, Philip S.Yu",LLMRec: Benchmarking Large Language Models on Recommendation Task,,,,,cs.IR cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, the fast development of Large Language Models (LLMs) such as ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. However, the application of LLMs in the recommendation domain has not been thoroughly investigated. To bridge this gap, we propose LLMRec, a LLM-based recommender system designed for benchmarking LLMs on various recommendation tasks. Specifically, we benchmark several popular off-the-shelf LLMs, such as ChatGPT, LLaMA, ChatGLM, on five recommendation tasks, including rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. Furthermore, we investigate the effectiveness of supervised finetuning to improve LLMs' instruction compliance ability. The benchmark results indicate that LLMs displayed only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation. However, they demonstrated comparable performance to state-of-the-art methods in explainability-based tasks. We also conduct qualitative evaluations to further evaluate the quality of contents generated by different models, and the results show that LLMs can truly understand the provided information and generate clearer and more reasonable results. We aspire that this benchmark will serve as an inspiration for researchers to delve deeper into the potential of LLMs in enhancing recommendation performance. Our codes, processed data and benchmark results are available at https://github.com/williamliujl/LLMRec. ","[{'version': 'v1', 'created': 'Wed, 23 Aug 2023 16:32:54 GMT'}]",2023-08-24,"[['Liu', 'Junling', ''], ['Liu', 'Chao', ''], ['Zhou', 'Peilin', ''], ['Ye', 'Qichen', ''], ['Chong', 'Dading', ''], ['Zhou', 'Kang', ''], ['Xie', 'Yueqi', ''], ['Cao', 'Yuwei', ''], ['Wang', 'Shoujin', ''], ['You', 'Chenyu', ''], ['Yu', 'Philip S.', '']]",1,1,2023-08-23,1,11,2,3,2,1,85722b13631d9846866d45ff2bfc2a2fe1026ac8,261076297.0,https://www.semanticscholar.org/paper/85722b13631d9846866d45ff2bfc2a2fe1026ac8,arXiv.org,2023.0,60.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2218869839', 'name': 'Junling Liu'}, {'authorId': '3741691', 'name': 'Chao-Hong Liu'}, {'authorId': '1800462890', 'name': 'Peilin Zhou'}, {'authorId': '2190432576', 'name': 'Qichen Ye'}, {'authorId': '52290752', 'name': 'Dading Chong'}, {'authorId': '2165702320', 'name': 'Kangan Zhou'}, {'authorId': '2154871075', 'name': 'Yueqi Xie'}, {'authorId': '150346771', 'name': 'Yuwei Cao'}, {'authorId': '2116951322', 'name': 'Shoujin Wang'}, {'authorId': '2061592207', 'name': 'Chenyu You'}, {'authorId': '2233087809', 'name': 'Philip S.Yu'}]","['Hong Kong University of Science and Technology', 'Yale University', 'University of Illinois at Chicago', 'University of Technology Sydney', 'Peking University']","['China', 'United States', 'Australia']",2023-08
2308.12261,Vijay Viswanathan,"Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Wu,
  Graham Neubig",Prompt2Model: Generating Deployable Models from Natural Language Instructions,8 pages,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model. ","[{'version': 'v1', 'created': 'Wed, 23 Aug 2023 17:28:21 GMT'}]",2023-08-24,"[['Viswanathan', 'Vijay', ''], ['Zhao', 'Chenyang', ''], ['Bertsch', 'Amanda', ''], ['Wu', 'Tongshuang', ''], ['Neubig', 'Graham', '']]",0,1,2023-08-23,1,5,1,1,0,1,e69684fb06a7b1fe621d7ef0c97fc2ca0e122c43,261075905.0,https://www.semanticscholar.org/paper/e69684fb06a7b1fe621d7ef0c97fc2ca0e122c43,arXiv.org,2023.0,44.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2061499362', 'name': 'Vijay Viswanathan'}, {'authorId': '2023526', 'name': 'Chenyang Zhao'}, {'authorId': '2138301112', 'name': 'Amanda Bertsch'}, {'authorId': '35232494', 'name': 'Tongshuang Sherry Wu'}, {'authorId': '2075395906', 'name': 'Graham Neubig'}]","['Carnegie Mellon University', 'Tsinghua University']","['China', 'United States']",2023-08
2308.13416,Ensheng Shi,"Ensheng Shi, Fengji Zhang, Yanlin Wang, Bei Chen, Lun Du, Hongyu
  Zhang, Shi Han, Dongmei Zhang, Hongbin Sun",SoTaNa: The Open-Source Software Development Assistant,,,,,cs.SE cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Software development plays a crucial role in driving innovation and efficiency across modern societies. To meet the demands of this dynamic field, there is a growing need for an effective software development assistant. However, existing large language models represented by ChatGPT suffer from limited accessibility, including training data and model weights. Although other large open-source models like LLaMA have shown promise, they still struggle with understanding human intent. In this paper, we present SoTaNa, an open-source software development assistant. SoTaNa utilizes ChatGPT to generate high-quality instruction-based data for the domain of software engineering and employs a parameter-efficient fine-tuning approach to enhance the open-source foundation model, LLaMA. We evaluate the effectiveness of \our{} in answering Stack Overflow questions and demonstrate its capabilities. Additionally, we discuss its capabilities in code summarization and generation, as well as the impact of varying the volume of generated data on model performance. Notably, SoTaNa can run on a single GPU, making it accessible to a broader range of researchers. Our code, model weights, and data are public at \url{https://github.com/DeepSoftwareAnalytics/SoTaNa}. ","[{'version': 'v1', 'created': 'Fri, 25 Aug 2023 14:56:21 GMT'}]",2023-08-28,"[['Shi', 'Ensheng', ''], ['Zhang', 'Fengji', ''], ['Wang', 'Yanlin', ''], ['Chen', 'Bei', ''], ['Du', 'Lun', ''], ['Zhang', 'Hongyu', ''], ['Han', 'Shi', ''], ['Zhang', 'Dongmei', ''], ['Sun', 'Hongbin', '']]",1,1,2023-08-25,1,9,2,2,1,1,7ee2841cced52dc38d5f334d1939c205c1cad716,261214379.0,https://www.semanticscholar.org/paper/7ee2841cced52dc38d5f334d1939c205c1cad716,arXiv.org,2023.0,58.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2006371687', 'name': 'Ensheng Shi'}, {'authorId': '2158120018', 'name': 'Fengji Zhang'}, {'authorId': '2108975906', 'name': 'Yanlin Wang'}, {'authorId': '143876723', 'name': 'B. Chen'}, {'authorId': '12723949', 'name': 'Lun Du'}, {'authorId': '2108881199', 'name': 'Hongyu Zhang'}, {'authorId': '2109750123', 'name': 'Shi Han'}, {'authorId': '2140415600', 'name': 'Dongmei Zhang'}, {'authorId': '2152992779', 'name': 'Hongbin Sun'}]","['Sun Yat-sen University', ""Xi'an Jiaotong University"", 'Microsoft']","['China', 'United States']",2023-08
2308.13785,Minheng Ni,"Minheng Ni, Chenfei Wu, Xiaodong Wang, Shengming Yin, Lijuan Wang,
  Zicheng Liu, Nan Duan",ORES: Open-vocabulary Responsible Visual Synthesis,,,,,cs.CV,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Avoiding synthesizing specific visual concepts is an essential challenge in responsible visual synthesis. However, the visual concept that needs to be avoided for responsible visual synthesis tends to be diverse, depending on the region, context, and usage scenarios. In this work, we formalize a new task, Open-vocabulary Responsible Visual Synthesis (ORES), where the synthesis model is able to avoid forbidden visual concepts while allowing users to input any desired content. To address this problem, we present a Two-stage Intervention (TIN) framework. By introducing 1) rewriting with learnable instruction through a large-scale language model (LLM) and 2) synthesizing with prompt intervention on a diffusion synthesis model, it can effectively synthesize images avoiding any concepts but following the user's query as much as possible. To evaluate on ORES, we provide a publicly available dataset, baseline models, and benchmark. Experimental results demonstrate the effectiveness of our method in reducing risks of image generation. Our work highlights the potential of LLMs in responsible visual synthesis. Our code and dataset is public available. ","[{'version': 'v1', 'created': 'Sat, 26 Aug 2023 06:47:34 GMT'}]",2023-08-29,"[['Ni', 'Minheng', ''], ['Wu', 'Chenfei', ''], ['Wang', 'Xiaodong', ''], ['Yin', 'Shengming', ''], ['Wang', 'Lijuan', ''], ['Liu', 'Zicheng', ''], ['Duan', 'Nan', '']]",0,0,2023-08-26,1,7,1,0,0,0,95be6a38f9b3ca3b7a9d81215e52cdcf545d554a,261243073.0,https://www.semanticscholar.org/paper/95be6a38f9b3ca3b7a9d81215e52cdcf545d554a,arXiv.org,2023.0,26.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1576502392', 'name': 'Minheng Ni'}, {'authorId': '2151101534', 'name': 'Chenfei Wu'}, {'authorId': '2211071131', 'name': 'Xiaodong Wang'}, {'authorId': '2008158083', 'name': 'Sheng-Siang Yin'}, {'authorId': '29957038', 'name': 'Lijuan Wang'}, {'authorId': '2145253136', 'name': 'Zicheng Liu'}, {'authorId': '2072609829', 'name': 'Nan Duan'}]",['Microsoft'],"['China', 'United States']",2023-08
2308.13916,Liang Yao,"Liang Yao, Jiazhen Peng, Chengsheng Mao, Yuan Luo",Exploring Large Language Models for Knowledge Graph Completion,Work in progress,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge graphs play a vital role in numerous artificial intelligence tasks, yet they frequently face the issue of incompleteness. In this study, we explore utilizing Large Language Models (LLM) for knowledge graph completion. We consider triples in knowledge graphs as text sequences and introduce an innovative framework called Knowledge Graph LLM (KG-LLM) to model these triples. Our technique employs entity and relation descriptions of a triple as prompts and utilizes the response for predictions. Experiments on various benchmark knowledge graphs demonstrate that our method attains state-of-the-art performance in tasks such as triple classification and relation prediction. We also find that fine-tuning relatively smaller models (e.g., LLaMA-7B, ChatGLM-6B) outperforms recent ChatGPT and GPT-4. ","[{'version': 'v1', 'created': 'Sat, 26 Aug 2023 16:51:17 GMT'}, {'version': 'v2', 'created': 'Thu, 31 Aug 2023 08:53:34 GMT'}, {'version': 'v3', 'created': 'Sun, 10 Sep 2023 17:42:37 GMT'}]",2023-09-12,"[['Yao', 'Liang', ''], ['Peng', 'Jiazhen', ''], ['Mao', 'Chengsheng', ''], ['Luo', 'Yuan', '']]",1,1,2023-08-26,3,4,2,4,2,2,450c65dce00b05bdf5d58e48289acd066aca8383,261242758.0,https://www.semanticscholar.org/paper/450c65dce00b05bdf5d58e48289acd066aca8383,arXiv.org,2023.0,41.0,3.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '100680875', 'name': 'Liang Yao'}, {'authorId': '2208655669', 'name': 'Jiazhen Peng'}, {'authorId': '145449667', 'name': 'Chengsheng Mao'}, {'authorId': '1830568527', 'name': 'Yuan Luo'}]","['Tencent', 'Northwestern University']","['China', 'United States']",2023-08
2308.14284,Longchao Da,"Longchao Da, Minchiuan Gao, Hao Mei, Hua Wei",LLM Powered Sim-to-real Transfer for Traffic Signal Control,"9 pages, 7 figures",,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Numerous solutions are proposed for the Traffic Signal Control (TSC) tasks aiming to provide efficient transportation and mitigate congestion waste. In recent, promising results have been attained by Reinforcement Learning (RL) methods through trial and error in simulators, bringing confidence in solving cities' congestion headaches. However, there still exist performance gaps when simulator-trained policies are deployed to the real world. This issue is mainly introduced by the system dynamic difference between the training simulator and the real-world environments. The Large Language Models (LLMs) are trained on mass knowledge and proved to be equipped with astonishing inference abilities. In this work, we leverage LLMs to understand and profile the system dynamics by a prompt-based grounded action transformation. Accepting the cloze prompt template, and then filling in the answer based on accessible context, the pre-trained LLM's inference ability is exploited and applied to understand how weather conditions, traffic states, and road types influence traffic dynamics, being aware of this, the policies' action is taken and grounded based on realistic dynamics, thus help the agent learn a more realistic policy. We conduct experiments using DQN to show the effectiveness of the proposed PromptGAT's ability in mitigating the performance gap from simulation to reality (sim-to-real). ","[{'version': 'v1', 'created': 'Mon, 28 Aug 2023 03:49:13 GMT'}, {'version': 'v2', 'created': 'Mon, 4 Sep 2023 22:31:44 GMT'}]",2023-09-06,"[['Da', 'Longchao', ''], ['Gao', 'Minchiuan', ''], ['Mei', 'Hao', ''], ['Wei', 'Hua', '']]",0,0,2023-08-28,2,4,1,0,0,0,d40430275383ef8a453eefb693c44cbc686008e0,261245212.0,https://www.semanticscholar.org/paper/d40430275383ef8a453eefb693c44cbc686008e0,arXiv.org,2023.0,43.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1387492282', 'name': 'Longchao Da'}, {'authorId': '39703662', 'name': 'Mingchen Gao'}, {'authorId': '2176403267', 'name': 'Hao Mei'}, {'authorId': '1474226770', 'name': 'Hua Wei'}]","['Arizona State University', 'Zhejiang University']","['China', 'United States']",2023-08
2308.14536,Baorian Nuchged,"Linkai Peng, Baorian Nuchged and Yingming Gao",Spoken Language Intelligence of Large Language Models for Language Learning,"28 pages, 7 figures, Preprint",,,,cs.CL cs.AI cs.LG cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  People have long hoped for a conversational system that can assist in real-life situations, and recent progress on large language models (LLMs) is bringing this idea closer to reality. While LLMs are often impressive in performance, their efficacy in real-world scenarios that demand expert knowledge remains unclear. LLMs are believed to hold the most potential and value in education, especially in the development of Artificial intelligence (AI) based virtual teachers capable of facilitating language learning. Our focus is centered on evaluating the efficacy of LLMs in the realm of education, specifically in the areas of spoken language learning which encompass phonetics, phonology, and second language acquisition. We introduce a new multiple-choice question dataset to evaluate the effectiveness of LLMs in the aforementioned scenarios, including understanding and application of spoken language knowledge. In addition, we investigate the influence of various prompting techniques such as zero- and few-shot method (prepending the question with question-answer exemplars), chain-of-thought (CoT, think step-by-step), in-domain exampler and external tools (Google, Wikipedia). We conducted large-scale evaluation on popular LLMs (20 distinct models) using these methods. We achieved significant performance improvements compared to the zero-shot baseline in the practical questions reasoning (GPT-3.5, 49.1% -> 63.1%; LLaMA2-70B-Chat, 42.2% -> 48.6%). We found that models of different sizes have good understanding of concepts in phonetics, phonology, and second language acquisition, but show limitations in reasoning for real-world problems. Additionally, we also explore preliminary findings on conversational communication. ","[{'version': 'v1', 'created': 'Mon, 28 Aug 2023 12:47:41 GMT'}]",2023-08-29,"[['Peng', 'Linkai', ''], ['Nuchged', 'Baorian', ''], ['Gao', 'Yingming', '']]",0,1,2023-08-28,1,3,5,1,0,1,19b43ff57e5d8f8a99da4110fbc30b4ecc39a527,261243763.0,https://www.semanticscholar.org/paper/19b43ff57e5d8f8a99da4110fbc30b4ecc39a527,arXiv.org,2023.0,59.0,1.0,0.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2117699319', 'name': 'Linkai Peng'}, {'authorId': '2234373508', 'name': 'Baorian Nuchged'}, {'authorId': '2118543175', 'name': 'Yingming Gao'}]","['Beijing University of Posts and Telecommunications', 'NetEase', 'The University of Texas at Austin']","['China', 'United States']",2023-08
2308.15272,Hao Wen,"Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun
  Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, Yunxin Liu",Empowering LLM to use Smartphone for Intelligent Task Automation,,,,,cs.AI cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Mobile task automation is an attractive technique that aims to enable voice-based hands-free user interaction with smartphones. However, existing approaches suffer from poor scalability due to the limited language understanding ability and the non-trivial manual efforts required from developers or end-users. The recent advance of large language models (LLMs) in language understanding and reasoning inspires us to rethink the problem from a model-centric perspective, where task preparation, comprehension, and execution are handled by a unified language model. In this work, we introduce AutoDroid, a mobile task automation system that can handle arbitrary tasks on any Android application without manual efforts. The key insight is to combine the commonsense knowledge of LLMs and domain-specific knowledge of apps through automated dynamic analysis. The main components include a functionality-aware UI representation method that bridges the UI with the LLM, exploration-based memory injection techniques that augment the app-specific domain knowledge of LLM, and a multi-granularity query optimization module that reduces the cost of model inference. We integrate AutoDroid with off-the-shelf LLMs including online GPT-4/GPT-3.5 and on-device Vicuna, and evaluate its performance on a new benchmark for memory-augmented Android task automation with 158 common tasks. The results demonstrated that AutoDroid is able to precisely generate actions with an accuracy of 90.9%, and complete tasks with a success rate of 71.3%, outperforming the GPT-4-powered baselines by 36.4% and 39.7%. The demo, benchmark suites, and source code of AutoDroid will be released at url{https://autodroid-sys.github.io/}. ","[{'version': 'v1', 'created': 'Tue, 29 Aug 2023 13:02:30 GMT'}, {'version': 'v2', 'created': 'Wed, 6 Sep 2023 14:14:04 GMT'}, {'version': 'v3', 'created': 'Sat, 9 Sep 2023 14:08:32 GMT'}]",2023-09-12,"[['Wen', 'Hao', ''], ['Li', 'Yuanchun', ''], ['Liu', 'Guohong', ''], ['Zhao', 'Shanhui', ''], ['Yu', 'Tao', ''], ['Li', 'Toby Jia-Jun', ''], ['Jiang', 'Shiqi', ''], ['Liu', 'Yunhao', ''], ['Zhang', 'Yaqin', ''], ['Liu', 'Yunxin', '']]",0,1,2023-08-29,3,10,2,3,1,2,cba18e04e8474b21838b076626486a6b02a68406,261277501.0,https://www.semanticscholar.org/paper/cba18e04e8474b21838b076626486a6b02a68406,arXiv.org,2023.0,69.0,6.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Engineering', 'source': 's2-fos-model'}]","[{'authorId': '2188861830', 'name': 'Hao Wen'}, {'authorId': '2144416765', 'name': 'Yuanchun Li'}, {'authorId': '2235520970', 'name': 'Guohong Liu'}, {'authorId': '92736469', 'name': 'Shanhui Zhao'}, {'authorId': '2256865742', 'name': 'Tao Yu'}, {'authorId': '34997918', 'name': 'Toby Jia-Jun Li'}, {'authorId': '2170664295', 'name': 'Shiqi Jiang'}, {'authorId': '2235192744', 'name': 'Yunhao Liu'}, {'authorId': '2108201285', 'name': 'Yaqin Zhang'}, {'authorId': '2161440197', 'name': 'Yunxin Liu'}]","['Tsinghua University', 'Microsoft', 'University of Notre Dame']","['China', 'United States']",2023-08
2308.15727,Jiuyang Xiang,"Zhenhong Zhou, Jiuyang Xiang, Chaomeng Chen, Sen Su",Quantifying and Analyzing Entity-level Memorization in Large Language Models,"9 pages, 7 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have been proven capable of memorizing their training data, which can be extracted through specifically designed prompts. As the scale of datasets continues to grow, privacy risks arising from memorization have attracted increasing attention. Quantifying language model memorization helps evaluate potential privacy risks. However, prior works on quantifying memorization require access to the precise original data or incur substantial computational overhead, making it difficult for applications in real-world language models. To this end, we propose a fine-grained, entity-level definition to quantify memorization with conditions and metrics closer to real-world scenarios. In addition, we also present an approach for efficiently extracting sensitive entities from autoregressive language models. We conduct extensive experiments based on the proposed, probing language models' ability to reconstruct sensitive entities under different settings. We find that language models have strong memorization at the entity level and are able to reproduce the training data even with partial leakages. The results demonstrate that LLMs not only memorize their training data but also understand associations between entities. These findings necessitate that trainers of LLMs exercise greater prudence regarding model memorization, adopting memorization mitigation techniques to preclude privacy violations. ","[{'version': 'v1', 'created': 'Wed, 30 Aug 2023 03:06:47 GMT'}]",2023-08-31,"[['Zhou', 'Zhenhong', ''], ['Xiang', 'Jiuyang', ''], ['Chen', 'Chaomeng', ''], ['Su', 'Sen', '']]",0,0,2023-08-30,1,4,1,0,0,0,99c8724fd43e9ed6c7d759e420021950e984aa95,261339641.0,https://www.semanticscholar.org/paper/99c8724fd43e9ed6c7d759e420021950e984aa95,arXiv.org,2023.0,32.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2236011027', 'name': 'Zhenhong Zhou'}, {'authorId': '2235836076', 'name': 'Jiuyang Xiang'}, {'authorId': '2145765162', 'name': 'Chao-Yi Chen'}, {'authorId': '2150496984', 'name': 'Sen Su'}]","['Beijing University of Posts and Telecommunications', 'University of Michigan–Ann Arbor']","['China', 'United States']",2023-08
2309.01029,Haiyan Zhao,"Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi
  Cai, Shuaiqiang Wang, Dawei Yin, Mengnan Du",Explainability for Large Language Models: A Survey,,,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional machine learning models. ","[{'version': 'v1', 'created': 'Sat, 2 Sep 2023 22:14:26 GMT'}, {'version': 'v2', 'created': 'Sun, 17 Sep 2023 02:24:30 GMT'}]",2023-09-19,"[['Zhao', 'Haiyan', ''], ['Chen', 'Hanjie', ''], ['Yang', 'Fan', ''], ['Liu', 'Ninghao', ''], ['Deng', 'Huiqi', ''], ['Cai', 'Hengyi', ''], ['Wang', 'Shuaiqiang', ''], ['Yin', 'Dawei', ''], ['Du', 'Mengnan', '']]",0,0,2023-09-02,2,9,3,0,0,0,26089bdfdbca1e6eaaceca71e3116b715bec6d47,261530292.0,https://www.semanticscholar.org/paper/26089bdfdbca1e6eaaceca71e3116b715bec6d47,arXiv.org,2023.0,169.0,7.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2237987232', 'name': 'Haiyan Zhao'}, {'authorId': '2237948828', 'name': 'Hanjie Chen'}, {'authorId': '145338224', 'name': 'F. Yang'}, {'authorId': '47717322', 'name': 'Ninghao Liu'}, {'authorId': '13689700', 'name': 'Huiqi Deng'}, {'authorId': '22561596', 'name': 'Hengyi Cai'}, {'authorId': '2237948548', 'name': 'Shuaiqiang Wang'}, {'authorId': '2136400100', 'name': 'Dawei Yin'}, {'authorId': '2237804196', 'name': 'Mengnan Du'}]","['Rice University', 'New Jersey Institute of Technology', 'Baidu', 'Wake Forest University', 'Shanghai Jiao Tong University', 'Institute of Computing Technology', 'Johns Hopkins University', 'University of Georgia']","['China', 'United States']",2023-09
2309.01157,Lei Li,"Lei Li, Yongfeng Zhang, Dugang Liu, Li Chen",Large Language Models for Generative Recommendation: A Survey and Visionary Discussions,,,,,cs.IR cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Recent years have witnessed the wide adoption of large language models (LLM) in different fields, especially natural language processing and computer vision. Such a trend can also be observed in recommender systems (RS). However, most of related work treat LLM as a component of the conventional recommendation pipeline (e.g., as a feature extractor) which may not be able to fully leverage the generative power of LLM. Instead of separating the recommendation process into multiple stages such as score computation and re-ranking, this process can be simplified to one stage with LLM: directly generating recommendations from the complete pool of items. This survey reviews the progress, methods and future directions of LLM-based generative recommendation by examining three questions: 1) What generative recommendation is, 2) Why RS should advance to generative recommendation, and 3) How to implement LLM-based generative recommendation for various RS tasks. We hope that the survey can provide the context and guidance needed to explore this interesting and emerging topic. ","[{'version': 'v1', 'created': 'Sun, 3 Sep 2023 12:33:47 GMT'}]",2023-09-06,"[['Li', 'Lei', ''], ['Zhang', 'Yongfeng', ''], ['Liu', 'Dugang', ''], ['Chen', 'Li', '']]",0,0,2023-09-03,1,4,3,0,0,0,a1081c6fc6921d6b76d9ebda4d712333fd7bbbf5,261531422.0,https://www.semanticscholar.org/paper/a1081c6fc6921d6b76d9ebda4d712333fd7bbbf5,arXiv.org,2023.0,85.0,6.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2151529879', 'name': 'Lei Li'}, {'authorId': '1739818', 'name': 'Yongfeng Zhang'}, {'authorId': '2237956346', 'name': 'Dugang Liu'}, {'authorId': '152875291', 'name': 'L. Chen'}]","['Rutgers, The State University of New Jersey', 'Hong Kong Baptist University', 'Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), Shenzhen, China']","['China', 'United States']",2023-09
2309.01219,Yue Zhang,"Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu,
  Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu,
  Wei Bi, Freda Shi, Shuming Shi",Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models,work in progress; 32 pages,,,,cs.CL cs.AI cs.CY cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research. ","[{'version': 'v1', 'created': 'Sun, 3 Sep 2023 16:56:48 GMT'}, {'version': 'v2', 'created': 'Sun, 24 Sep 2023 16:03:24 GMT'}]",2023-09-26,"[['Zhang', 'Yue', ''], ['Li', 'Yafu', ''], ['Cui', 'Leyang', ''], ['Cai', 'Deng', ''], ['Liu', 'Lemao', ''], ['Fu', 'Tingchen', ''], ['Huang', 'Xinting', ''], ['Zhao', 'Enbo', ''], ['Zhang', 'Yu', ''], ['Chen', 'Yulong', ''], ['Wang', 'Longyue', ''], ['Luu', 'Anh Tuan', ''], ['Bi', 'Wei', ''], ['Shi', 'Freda', ''], ['Shi', 'Shuming', '']]",0,0,2023-09-03,2,15,4,0,0,0,669441cb46666036f663f19def44bec2a838a518,261530162.0,https://www.semanticscholar.org/paper/669441cb46666036f663f19def44bec2a838a518,arXiv.org,2023.0,215.0,37.0,3.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1895977079', 'name': 'Yue Zhang'}, {'authorId': '2110450452', 'name': 'Yafu Li'}, {'authorId': '152496687', 'name': 'Leyang Cui'}, {'authorId': '1724421', 'name': 'Deng Cai'}, {'authorId': '2978364', 'name': 'Lemao Liu'}, {'authorId': '2156525869', 'name': 'Tingchen Fu'}, {'authorId': '14799547', 'name': 'Xinting Huang'}, {'authorId': '2065703096', 'name': 'Enbo Zhao'}, {'authorId': '2257439415', 'name': 'Yu Zhang'}, {'authorId': '2109404730', 'name': 'Yulong Chen'}, {'authorId': '1800190', 'name': 'Longyue Wang'}, {'authorId': '1755919', 'name': 'A. Luu'}, {'authorId': '2237804371', 'name': 'Wei Bi'}, {'authorId': '2237805147', 'name': 'Freda Shi'}, {'authorId': '34720053', 'name': 'Shuming Shi'}]","['Zhejiang University', 'Soochow University', 'Tencent', 'Nanyang Technological University', 'Toyota Technological Institute at Chicago', 'Renmin University of China']","['China', 'United States', 'Singapore']",2023-09
2309.01686,Zihao Zhou,"Zihao Zhou and Qiufeng Wang and Mingyu Jin and Jie Yao and Jianan Ye
  and Wei Liu and Wei Wang and Xiaowei Huang and Kaizhu Huang",MathAttack: Attacking Large Language Models Towards Math Solving Ability,"11 pages, 6 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the boom of Large Language Models (LLMs), the research of solving Math Word Problem (MWP) has recently made great progress. However, there are few studies to examine the security of LLMs in math solving ability. Instead of attacking prompts in the use of LLMs, we propose a MathAttack model to attack MWP samples which are closer to the essence of security in solving math problems. Compared to traditional text adversarial attack, it is essential to preserve the mathematical logic of original MWPs during the attacking. To this end, we propose logical entity recognition to identify logical entries which are then frozen. Subsequently, the remaining text are attacked by adopting a word-level attacker. Furthermore, we propose a new dataset RobustMath to evaluate the robustness of LLMs in math solving ability. Extensive experiments on our RobustMath and two another math benchmark datasets GSM8K and MultiAirth show that MathAttack could effectively attack the math solving ability of LLMs. In the experiments, we observe that (1) Our adversarial samples from higher-accuracy LLMs are also effective for attacking LLMs with lower accuracy (e.g., transfer from larger to smaller-size LLMs, or from few-shot to zero-shot prompts); (2) Complex MWPs (such as more solving steps, longer text, more numbers) are more vulnerable to attack; (3) We can improve the robustness of LLMs by using our adversarial samples in few-shot prompts. Finally, we hope our practice and observation can serve as an important attempt towards enhancing the robustness of LLMs in math solving ability. We will release our code and dataset. ","[{'version': 'v1', 'created': 'Mon, 4 Sep 2023 16:02:23 GMT'}]",2023-09-06,"[['Zhou', 'Zihao', ''], ['Wang', 'Qiufeng', ''], ['Jin', 'Mingyu', ''], ['Yao', 'Jie', ''], ['Ye', 'Jianan', ''], ['Liu', 'Wei', ''], ['Wang', 'Wei', ''], ['Huang', 'Xiaowei', ''], ['Huang', 'Kaizhu', '']]",0,0,2023-09-04,1,9,1,0,0,0,3886f3bd2a0af9e75bf9fa5b7db4224969dbf346,261531153.0,https://www.semanticscholar.org/paper/3886f3bd2a0af9e75bf9fa5b7db4224969dbf346,arXiv.org,2023.0,32.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2117793393', 'name': 'Zihao Zhou'}, {'authorId': '1500377117', 'name': 'Qiufeng Wang'}, {'authorId': '2237809092', 'name': 'Mingyu Jin'}, {'authorId': '2220808197', 'name': 'Jie Yao'}, {'authorId': '2153258359', 'name': 'Jianan Ye'}, {'authorId': '2157222998', 'name': 'Wei Liu'}, {'authorId': '145200778', 'name': 'Wei Wang'}, {'authorId': '2145052820', 'name': 'Xiaowei Huang'}, {'authorId': '2238398725', 'name': 'Kaizhu Huang'}]","['Xi’an Jiaotong-Liverpool University', 'Northwestern University', 'ShanghaiTech University', 'University of Liverpool', 'Duke Kunshan University']","['China', 'United States', 'United Kingdom']",2023-09
2309.01957,Ankita Sharma,"Ankita Sharma, Xuanmao Li, Hong Guan, Guoxin Sun, Liang Zhang, Lanjun
  Wang, Kesheng Wu, Lei Cao, Erkang Zhu, Alexander Sim, Teresa Wu, Jia Zou",Automatic Data Transformation Using Large Language Model: An Experimental Study on Building Energy Data,"10 pages, 7 figures",,,,cs.DB,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing approaches to automatic data transformation are insufficient to meet the requirements in many real-world scenarios, such as the building sector. First, there is no convenient interface for domain experts to provide domain knowledge easily. Second, they require significant training data collection overheads. Third, the accuracy suffers from complicated schema changes. To bridge this gap, we present a novel approach that leverages the unique capabilities of large language models (LLMs) in coding, complex reasoning, and zero-shot learning to generate SQL code that transforms the source datasets into the target datasets. We demonstrate the viability of this approach by designing an LLM-based framework, termed SQLMorpher, which comprises a prompt generator that integrates the initial prompt with optional domain knowledge and historical patterns in external databases. It also implements an iterative prompt optimization mechanism that automatically improves the prompt based on flaw detection. The key contributions of this work include (1) pioneering an end-to-end LLM-based solution for data transformation, (2) developing a benchmark dataset of 105 real-world building energy data transformation problems, and (3) conducting an extensive empirical evaluation where our approach achieved 96% accuracy in all 105 problems. SQLMorpher demonstrates the effectiveness of utilizing LLMs in complex, domain-specific challenges, highlighting the potential of their potential to drive sustainable solutions. ","[{'version': 'v1', 'created': 'Tue, 5 Sep 2023 05:19:35 GMT'}, {'version': 'v2', 'created': 'Wed, 6 Sep 2023 04:25:02 GMT'}]",2023-09-07,"[['Sharma', 'Ankita', ''], ['Li', 'Xuanmao', ''], ['Guan', 'Hong', ''], ['Sun', 'Guoxin', ''], ['Zhang', 'Liang', ''], ['Wang', 'Lanjun', ''], ['Wu', 'Kesheng', ''], ['Cao', 'Lei', ''], ['Zhu', 'Erkang', ''], ['Sim', 'Alexander', ''], ['Wu', 'Teresa', ''], ['Zou', 'Jia', '']]",0,0,2023-09-05,2,12,1,0,0,0,3120c2763edab339b937ddbe76991ebdfe0e01e6,261530167.0,https://www.semanticscholar.org/paper/3120c2763edab339b937ddbe76991ebdfe0e01e6,arXiv.org,2023.0,32.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2237952927', 'name': 'Ankita Sharma'}, {'authorId': '2237949801', 'name': 'Xuanmao Li'}, {'authorId': '2237800042', 'name': 'Hong Guan'}, {'authorId': '2237955180', 'name': 'Guoxin Sun'}, {'authorId': '2237796000', 'name': 'Liang Zhang'}, {'authorId': '2237944344', 'name': 'Lanjun Wang'}, {'authorId': '2238407326', 'name': 'Kesheng Wu'}, {'authorId': '2237951882', 'name': 'Lei Cao'}, {'authorId': '2237798544', 'name': 'Erkang Zhu'}, {'authorId': '2165305224', 'name': 'Alexander Sim'}, {'authorId': '2237957088', 'name': 'Teresa Wu'}, {'authorId': '2237786723', 'name': 'Jia Zou'}]","['Lawrence Berkeley National Laboratory', 'Huazhong University of Science and Technology', 'National Renewable Energy Laboratory', 'Microsoft']","['China', 'United States']",2023-09
2309.02726,Zonglin Yang,"Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, Erik
  Cambria",Large Language Models for Automated Open-domain Scientific Hypotheses Discovery,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Hypothetical induction is recognized as the main reasoning type when scientists make observations about the world and try to propose hypotheses to explain those observations. Past research on hypothetical induction has a limited setting that (1) the observation annotations of the dataset are not raw web corpus but are manually selected sentences (resulting in a close-domain setting); and (2) the ground truth hypotheses annotations are mostly commonsense knowledge, making the task less challenging. In this work, we propose the first NLP dataset for social science academic hypotheses discovery, consisting of 50 recent papers published in top social science journals. Raw web corpora that are necessary for developing hypotheses in the published papers are also collected in the dataset, with the final goal of creating a system that automatically generates valid, novel, and helpful (to human researchers) hypotheses, given only a pile of raw web corpora. The new dataset can tackle the previous problems because it requires to (1) use raw web corpora as observations; and (2) propose hypotheses even new to humanity. A multi-module framework is developed for the task, as well as three different feedback mechanisms that empirically show performance gain over the base framework. Finally, our framework exhibits high performance in terms of both GPT-4 based evaluation and social science expert evaluation. ","[{'version': 'v1', 'created': 'Wed, 6 Sep 2023 05:19:41 GMT'}]",2023-09-07,"[['Yang', 'Zonglin', ''], ['Du', 'Xinya', ''], ['Li', 'Junxian', ''], ['Zheng', 'Jie', ''], ['Poria', 'Soujanya', ''], ['Cambria', 'Erik', '']]",0,1,2023-09-06,1,6,2,1,0,1,5aea5c8b536380c5ad1d42108c2c6767622318ee,261557055.0,https://www.semanticscholar.org/paper/5aea5c8b536380c5ad1d42108c2c6767622318ee,arXiv.org,2023.0,29.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2124477940', 'name': 'Zonglin Yang'}, {'authorId': '13728923', 'name': 'Xinya Du'}, {'authorId': '2238138967', 'name': 'Junxian Li'}, {'authorId': '2237998834', 'name': 'Jie Zheng'}, {'authorId': '1746416', 'name': 'Soujanya Poria'}, {'authorId': '49943757', 'name': 'E. Cambria'}]","['Huazhong University of Science and Technology', 'Singapore University of Technology and Design', 'The University of Texas at Dallas', 'Nanyang Technological University']","['China', 'United States', 'Singapore']",2023-09
2309.03126,Pengyu Cheng,"Pengyu Cheng, Jiawen Xie, Ke Bai, Yong Dai, Nan Du",Everyone Deserves A Reward: Learning Customized Human Preferences,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Reward models (RMs) are essential for aligning large language models (LLMs) with human preferences to improve interaction quality. However, the real world is pluralistic, which leads to diversified human preferences with respect to different religions, politics, cultures, etc. Moreover, each individual can have their unique preferences on various topics. Neglecting the diversity of human preferences, current human feedback aligning methods only consider a general reward model, which is below satisfaction for customized or personalized application scenarios. To explore customized preference learning, we collect a domain-specific preference (DSP) dataset, which includes preferred responses for each given query from four practical domains. Besides, from the perspective of data efficiency, we propose a three-stage customized RM learning scheme, then empirically verify its effectiveness on both general preference datasets and our DSP set. Furthermore, we test multiple training and data strategies on the three learning stages. We find several ways to better preserve the general preferring ability while training the customized RMs, especially general preference enrichment, and customized preference imitation learning. The DSP dataset and code are available at https://github.com/Linear95/DSP. ","[{'version': 'v1', 'created': 'Wed, 6 Sep 2023 16:03:59 GMT'}, {'version': 'v2', 'created': 'Fri, 15 Sep 2023 09:24:30 GMT'}]",2023-09-18,"[['Cheng', 'Pengyu', ''], ['Xie', 'Jiawen', ''], ['Bai', 'Ke', ''], ['Dai', 'Yong', ''], ['Du', 'Nan', '']]",0,0,2023-09-06,2,5,1,0,0,0,e0e106fc72458cd46b42d6341f809b4474b6226b,261557043.0,https://www.semanticscholar.org/paper/e0e106fc72458cd46b42d6341f809b4474b6226b,arXiv.org,2023.0,33.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '144533942', 'name': 'Pengyu Cheng'}, {'authorId': '2238052559', 'name': 'Jiawen Xie'}, {'authorId': '2146069687', 'name': 'Ke Bai'}, {'authorId': '2238187314', 'name': 'Yong Dai'}, {'authorId': '2237986798', 'name': 'Nan Du'}]","['Duke University', 'Shanghai Jiao Tong University', 'Tencent']","['China', 'United States']",2023-09
2309.05021,Yaonai Wei Dr.,"Yaonai Wei, Tuo Zhang, Han Zhang, Tianyang Zhong, Lin Zhao, Zhengliang
  Liu, Chong Ma, Songyao Zhang, Muheng Shang, Lei Du, Xiao Li, Tianming Liu and
  Junwei Han",Chat2Brain: A Method for Mapping Open-Ended Semantic Queries to Brain Activation Maps,"8 pages, 4 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Over decades, neuroscience has accumulated a wealth of research results in the text modality that can be used to explore cognitive processes. Meta-analysis is a typical method that successfully establishes a link from text queries to brain activation maps using these research results, but it still relies on an ideal query environment. In practical applications, text queries used for meta-analyses may encounter issues such as semantic redundancy and ambiguity, resulting in an inaccurate mapping to brain images. On the other hand, large language models (LLMs) like ChatGPT have shown great potential in tasks such as context understanding and reasoning, displaying a high degree of consistency with human natural language. Hence, LLMs could improve the connection between text modality and neuroscience, resolving existing challenges of meta-analyses. In this study, we propose a method called Chat2Brain that combines LLMs to basic text-2-image model, known as Text2Brain, to map open-ended semantic queries to brain activation maps in data-scarce and complex query environments. By utilizing the understanding and reasoning capabilities of LLMs, the performance of the mapping model is optimized by transferring text queries to semantic queries. We demonstrate that Chat2Brain can synthesize anatomically plausible neural activation patterns for more complex tasks of text queries. ","[{'version': 'v1', 'created': 'Sun, 10 Sep 2023 13:06:45 GMT'}]",2023-09-12,"[['Wei', 'Yaonai', ''], ['Zhang', 'Tuo', ''], ['Zhang', 'Han', ''], ['Zhong', 'Tianyang', ''], ['Zhao', 'Lin', ''], ['Liu', 'Zhengliang', ''], ['Ma', 'Chong', ''], ['Zhang', 'Songyao', ''], ['Shang', 'Muheng', ''], ['Du', 'Lei', ''], ['Li', 'Xiao', ''], ['Liu', 'Tianming', ''], ['Han', 'Junwei', '']]",1,1,2023-09-10,1,13,1,1,0,1,dd61de46eb2d5681004f8219ec39a0c65eb9170c,261682053.0,https://www.semanticscholar.org/paper/dd61de46eb2d5681004f8219ec39a0c65eb9170c,arXiv.org,2023.0,31.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Psychology', 'source': 's2-fos-model'}]","[{'authorId': '2163675860', 'name': 'Yaonai Wei'}, {'authorId': '2239052067', 'name': 'Tuo Zhang'}, {'authorId': '2239125771', 'name': 'Han Zhang'}, {'authorId': '2215167446', 'name': 'Tianyang Zhong'}, {'authorId': '46586837', 'name': 'Lin Zhao'}, {'authorId': '2145977326', 'name': 'Zheng Liu'}, {'authorId': '120688117', 'name': 'Chong-Yi Ma'}, {'authorId': '2108965026', 'name': 'Songyao Zhang'}, {'authorId': '2144460985', 'name': 'Muheng Shang'}, {'authorId': '2238954278', 'name': 'Lei Du'}, {'authorId': '2108786618', 'name': 'Xiao Li'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}, {'authorId': '2161945158', 'name': 'Jun-Feng Han'}]","['University of Georgia', 'Northwest University', 'Northwestern Polytechnical University']","['China', 'United States']",2023-09
2309.05274,Dongyu Yao,"Dongyu Yao, Jianshu Zhang, Ian G. Harris, Marcel Carlsson",FuzzLLM: A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabilities in Large Language Models,"In submission, a preprint version",,,,cs.CR,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Jailbreak vulnerabilities in Large Language Models (LLMs), which exploit meticulously crafted prompts to elicit content that violates service guidelines, have captured the attention of research communities. While model owners can defend against individual jailbreak prompts through safety training strategies, this relatively passive approach struggles to handle the broader category of similar jailbreaks. To tackle this issue, we introduce FuzzLLM, an automated fuzzing framework designed to proactively test and discover jailbreak vulnerabilities in LLMs. We utilize templates to capture the structural integrity of a prompt and isolate key features of a jailbreak class as constraints. By integrating different base classes into powerful combo attacks and varying the elements of constraints and prohibited questions, FuzzLLM enables efficient testing with reduced manual effort. Extensive experiments demonstrate FuzzLLM's effectiveness and comprehensiveness in vulnerability discovery across various LLMs. ","[{'version': 'v1', 'created': 'Mon, 11 Sep 2023 07:15:02 GMT'}]",2023-09-12,"[['Yao', 'Dongyu', ''], ['Zhang', 'Jianshu', ''], ['Harris', 'Ian G.', ''], ['Carlsson', 'Marcel', '']]",0,0,2023-09-11,1,4,1,0,0,0,3c784cd3150a359e269c70cfbadd18774d66055d,261681918.0,https://www.semanticscholar.org/paper/3c784cd3150a359e269c70cfbadd18774d66055d,arXiv.org,2023.0,23.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2238951719', 'name': 'Dongyu Yao'}, {'authorId': '2239054292', 'name': 'Jianshu Zhang'}, {'authorId': '2238951230', 'name': 'Ian G. Harris'}, {'authorId': '145488218', 'name': 'Marcel Carlsson'}]","['Wuhan University', 'University of California, Riverside']","['China', 'United States']",2023-09
2309.06126,Yuan-Sen Ting Assoc. Prof.,"Tuan Dung Nguyen, Yuan-Sen Ting, Ioana Ciuc\u{a}, Charlie O'Neill,
  Ze-Chang Sun, Maja Jab{\l}o\'nska, Sandor Kruk, Ernest Perkowski, Jack
  Miller, Jason Li, Josh Peek, Kartheik Iyer, Tomasz R\'o\.za\'nski, Pranav
  Khetarpal, Sharaf Zaman, David Brodrick, Sergio J. Rodr\'iguez M\'endez,
  Thang Bui, Alyssa Goodman, Alberto Accomazzi, Jill Naiman, Jesse Cranney,
  Kevin Schawinski, UniverseTBD",AstroLLaMA: Towards Specialized Foundation Models in Astronomy,"6 pages, 3 figures, submitted to IJCNLP-AACL 2023. Comments are
  welcome. The model can be found on Hugging Face -
  https://huggingface.co/universeTBD/astrollama",,,,astro-ph.IM astro-ph.CO astro-ph.GA astro-ph.HE cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models excel in many human-language tasks but often falter in highly specialized domains like scholarly astronomy. To bridge this gap, we introduce AstroLLaMA, a 7-billion-parameter model fine-tuned from LLaMA-2 using over 300,000 astronomy abstracts from arXiv. Optimized for traditional causal language modeling, AstroLLaMA achieves a 30% lower perplexity than Llama-2, showing marked domain adaptation. Our model generates more insightful and scientifically relevant text completions and embedding extraction than state-of-the-arts foundation models despite having significantly fewer parameters. AstroLLaMA serves as a robust, domain-specific model with broad fine-tuning potential. Its public release aims to spur astronomy-focused research, including automatic paper summarization and conversational agent development. ","[{'version': 'v1', 'created': 'Tue, 12 Sep 2023 11:02:27 GMT'}]",2023-09-13,"[['Nguyen', 'Tuan Dung', ''], ['Ting', 'Yuan-Sen', ''], ['Ciucă', 'Ioana', ''], [""O'Neill"", 'Charlie', ''], ['Sun', 'Ze-Chang', ''], ['Jabłońska', 'Maja', ''], ['Kruk', 'Sandor', ''], ['Perkowski', 'Ernest', ''], ['Miller', 'Jack', ''], ['Li', 'Jason', ''], ['Peek', 'Josh', ''], ['Iyer', 'Kartheik', ''], ['Różański', 'Tomasz', ''], ['Khetarpal', 'Pranav', ''], ['Zaman', 'Sharaf', ''], ['Brodrick', 'David', ''], ['Méndez', 'Sergio J. Rodríguez', ''], ['Bui', 'Thang', ''], ['Goodman', 'Alyssa', ''], ['Accomazzi', 'Alberto', ''], ['Naiman', 'Jill', ''], ['Cranney', 'Jesse', ''], ['Schawinski', 'Kevin', ''], ['UniverseTBD', '', '']]",0,0,2023-09-12,1,24,6,1,1,0,e33a538e80d1877782df26e1493f5adc661ceec4,261696577.0,https://www.semanticscholar.org/paper/e33a538e80d1877782df26e1493f5adc661ceec4,arXiv.org,2023.0,17.0,1.0,0.0,True,"['Computer Science', 'Physics']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Physics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Physics', 'source': 's2-fos-model'}]","[{'authorId': '2116225572', 'name': 'Tuan Dung Nguyen'}, {'authorId': '2239095445', 'name': 'Yuan-Sen Ting'}, {'authorId': '50062876', 'name': 'I. Ciucă'}, {'authorId': '2239094893', 'name': ""Charlie O'Neill""}, {'authorId': '2239167308', 'name': 'Ze-Chang Sun'}, {'authorId': '2239094677', 'name': ""Maja Jablo'nska""}, {'authorId': '143919054', 'name': 'S. Kruk'}, {'authorId': '2239094950', 'name': 'Ernest Perkowski'}, {'authorId': '2229023715', 'name': 'Jack W. Miller'}, {'authorId': '2239158695', 'name': 'Jason Li'}, {'authorId': '2239094501', 'name': 'Josh Peek'}, {'authorId': '102239912', 'name': 'K. Iyer'}, {'authorId': '2142546267', 'name': ""Tomasz R'o.za'nski""}, {'authorId': '2239095448', 'name': 'P. Khetarpal'}, {'authorId': '2239094542', 'name': 'Sharaf Zaman'}, {'authorId': '101726491', 'name': 'D. Brodrick'}, {'authorId': '2125183970', 'name': ""Sergio J. Rodr'iguez M'endez""}, {'authorId': '2229238231', 'name': 'Thang Bui'}, {'authorId': '2239094775', 'name': 'Alyssa Goodman'}, {'authorId': '2751757', 'name': 'A. Accomazzi'}, {'authorId': '2239094393', 'name': 'Jill P. Naiman'}, {'authorId': '2401845', 'name': 'Jesse Cranney'}, {'authorId': '6552704', 'name': 'K. Schawinski'}, {'authorId': '2239094383', 'name': 'UniverseTBD'}]","['University of Pennsylvania', 'Learning Machines, Australia', 'Harvard University', 'The Ohio State University', 'Indian Institute of Technology Delhi', 'Columbia University', 'Australian National University', 'Tsinghua University', 'Center for Astrophysics Harvard & Smithsonian', 'University of Illinois Urbana-Champaign', 'University of Wrocław', 'European Space Astronomy Centre', 'Space Telescope Science Institute']","['Poland', 'Spain', 'United States', 'India', 'China', 'Australia']",2023-09
2309.06256,Yong Lin,"Yong Lin, Lu Tan, Hangyu Lin, Zeming Zheng, Renjie Pi, Jipeng Zhang,
  Shizhe Diao, Haoxiang Wang, Han Zhao, Yuan Yao, and Tong Zhang",Speciality vs Generality: An Empirical Study on Catastrophic Forgetting in Fine-tuning Foundation Models,30 Pages,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Foundation models, including Vision Language Models (VLMs) and Large Language Models (LLMs), possess the $generality$ to handle diverse distributions and tasks, which stems from their extensive pre-training datasets. The fine-tuning of foundation models is a common practice to enhance task performance or align the model's behavior with human expectations, allowing them to gain $speciality$. However, the small datasets used for fine-tuning may not adequately cover the diverse distributions and tasks encountered during pre-training. Consequently, the pursuit of speciality during fine-tuning can lead to a loss of {generality} in the model, which is related to catastrophic forgetting (CF) in deep learning. In this study, we demonstrate this phenomenon in both VLMs and LLMs. For instance, fine-tuning VLMs like CLIP on ImageNet results in a loss of generality in handling diverse distributions, and fine-tuning LLMs like Galactica in the medical domain leads to a loss in following instructions and common sense.   To address the trade-off between the speciality and generality, we investigate multiple regularization methods from continual learning, the weight averaging method (Wise-FT) from out-of-distributional (OOD) generalization, which interpolates parameters between pre-trained and fine-tuned models, and parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA). Our findings show that both continual learning and Wise-ft methods effectively mitigate the loss of generality, with Wise-FT exhibiting the strongest performance in balancing speciality and generality. ","[{'version': 'v1', 'created': 'Tue, 12 Sep 2023 14:16:54 GMT'}, {'version': 'v2', 'created': 'Wed, 4 Oct 2023 13:26:06 GMT'}]",2023-10-05,"[['Lin', 'Yong', ''], ['Tan', 'Lu', ''], ['Lin', 'Hangyu', ''], ['Zheng', 'Zeming', ''], ['Pi', 'Renjie', ''], ['Zhang', 'Jipeng', ''], ['Diao', 'Shizhe', ''], ['Wang', 'Haoxiang', ''], ['Zhao', 'Han', ''], ['Yao', 'Yuan', ''], ['Zhang', 'Tong', '']]",0,0,2023-09-12,2,11,1,0,0,0,3789c115f71886a47feb421f45d4903082a29b22,261697277.0,https://www.semanticscholar.org/paper/3789c115f71886a47feb421f45d4903082a29b22,arXiv.org,2023.0,90.0,5.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2238123947', 'name': 'Yong Lin'}, {'authorId': '2239156696', 'name': 'Lu Tan'}, {'authorId': '2239167492', 'name': 'Hangyu Lin'}, {'authorId': '2239197570', 'name': 'Zeming Zheng'}, {'authorId': '2066420772', 'name': 'Renjie Pi'}, {'authorId': '50561049', 'name': 'Jipeng Zhang'}, {'authorId': '50826757', 'name': 'Shizhe Diao'}, {'authorId': '2266194421', 'name': 'Haoxiang Wang'}, {'authorId': '2188204871', 'name': 'Han Zhao'}, {'authorId': '2238126734', 'name': 'Yuan Yao'}, {'authorId': '38144094', 'name': 'T. Zhang'}]","['University of Illinois Urbana-Champaign', 'Tsinghua University', 'Hong Kong University of Science and Technology']","['China', 'United States']",2023-09
2309.06419,Zhengliang Liu,"Zhengliang Liu, Yiwei Li, Peng Shu, Aoxiao Zhong, Longtao Yang, Chao
  Ju, Zihao Wu, Chong Ma, Jie Luo, Cheng Chen, Sekeun Kim, Jiang Hu, Haixing
  Dai, Lin Zhao, Dajiang Zhu, Jun Liu, Wei Liu, Dinggang Shen, Tianming Liu,
  Quanzheng Li, and Xiang Li",Radiology-Llama2: Best-in-Class Large Language Model for Radiology,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  This paper introduces Radiology-Llama2, a large language model specialized for radiology through a process known as instruction tuning. Radiology-Llama2 is based on the Llama2 architecture and further trained on a large dataset of radiology reports to generate coherent and clinically useful impressions from radiological findings. Quantitative evaluations using ROUGE metrics on the MIMIC-CXR and OpenI datasets demonstrate that Radiology-Llama2 achieves state-of-the-art performance compared to other generative language models, with a Rouge-1 score of 0.4834 on MIMIC-CXR and 0.4185 on OpenI. Additional assessments by radiology experts highlight the model's strengths in understandability, coherence, relevance, conciseness, and clinical utility. The work illustrates the potential of localized language models designed and tuned for specialized domains like radiology. When properly evaluated and deployed, such models can transform fields like radiology by automating rote tasks and enhancing human expertise. ","[{'version': 'v1', 'created': 'Tue, 29 Aug 2023 17:44:28 GMT'}]",2023-09-13,"[['Liu', 'Zhengliang', ''], ['Li', 'Yiwei', ''], ['Shu', 'Peng', ''], ['Zhong', 'Aoxiao', ''], ['Yang', 'Longtao', ''], ['Ju', 'Chao', ''], ['Wu', 'Zihao', ''], ['Ma', 'Chong', ''], ['Luo', 'Jie', ''], ['Chen', 'Cheng', ''], ['Kim', 'Sekeun', ''], ['Hu', 'Jiang', ''], ['Dai', 'Haixing', ''], ['Zhao', 'Lin', ''], ['Zhu', 'Dajiang', ''], ['Liu', 'Jun', ''], ['Liu', 'Wei', ''], ['Shen', 'Dinggang', ''], ['Liu', 'Tianming', ''], ['Li', 'Quanzheng', ''], ['Li', 'Xiang', '']]",0,0,2023-08-29,1,21,1,0,0,0,420d6754315ac5db8a040386245cd15b9fe5b459,261696494.0,https://www.semanticscholar.org/paper/420d6754315ac5db8a040386245cd15b9fe5b459,arXiv.org,2023.0,66.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2145977326', 'name': 'Zheng Liu'}, {'authorId': '2111161355', 'name': 'Yiwei Li'}, {'authorId': '2220096705', 'name': 'Peng Shu'}, {'authorId': '40153228', 'name': 'Aoxiao Zhong'}, {'authorId': '2237381357', 'name': 'Longtao Yang'}, {'authorId': '2220096793', 'name': 'Chao Ju'}, {'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '120688117', 'name': 'Chong-Yi Ma'}, {'authorId': '2239186794', 'name': 'Jie Luo'}, {'authorId': '1390805683', 'name': 'Cheng Chen'}, {'authorId': '2239159354', 'name': 'Sekeun Kim'}, {'authorId': '2239161307', 'name': 'Jiang Hu'}, {'authorId': '29944950', 'name': 'Haixing Dai'}, {'authorId': '2111641126', 'name': 'Lin Zhao'}, {'authorId': '2181182', 'name': 'Dajiang Zhu'}, {'authorId': '2157176152', 'name': 'Jun Liu'}, {'authorId': '46641573', 'name': 'W. Liu'}, {'authorId': '2150038187', 'name': 'Dinggang Shen'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}, {'authorId': '2210054417', 'name': 'Quanzheng Li'}, {'authorId': '113075935', 'name': 'Xiang Li'}]","['Northwestern Polytechnical University', 'Second Xiangya Hospital of Central South University', 'Mayo Clinic', 'Harvard University', 'Shanghai United Imaging Intelligence Co., Ltd.', 'ShanghaiTech University', 'Shanghai Clinical Research Center', 'Massachusetts General Hospital', 'University of Georgia', 'The University of Texas at Arlington']","['China', 'United States']",2023-08
2309.07081,Siyin Wang,"Siyin Wang, Chao-Han Huck Yang, Ji Wu, Chao Zhang",Can Whisper perform speech-based in-context learning,Submitted to ICASSP 2024,,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper investigates the in-context learning abilities of the Whisper automatic speech recognition (ASR) models released by OpenAI. A novel speech-based in-context learning (SICL) approach is proposed for test-time adaptation, which can reduce the word error rates (WERs) with only a small number of labelled speech samples without gradient descent. Language-level adaptation experiments using Chinese dialects showed that when applying SICL to isolated word ASR, consistent and considerable relative WER reductions can be achieved using Whisper models of any size on two dialects, which is on average 32.3%. A k-nearest-neighbours-based in-context example selection technique can be applied to further improve the efficiency of SICL, which can increase the average relative WER reduction to 36.4%. The findings are verified using speaker adaptation or continuous speech recognition tasks, and both achieved considerable relative WER reductions. Detailed quantitative analyses are also provided to shed light on SICL's adaptability to phonological variances and dialect-specific lexical nuances. ","[{'version': 'v1', 'created': 'Wed, 13 Sep 2023 16:46:27 GMT'}]",2023-09-14,"[['Wang', 'Siyin', ''], ['Yang', 'Chao-Han Huck', ''], ['Wu', 'Ji', ''], ['Zhang', 'Chao', '']]",0,0,2023-09-13,1,4,3,0,0,0,3a944ddba8b6fbaaac36126fc955f181f8b8b06a,261706064.0,https://www.semanticscholar.org/paper/3a944ddba8b6fbaaac36126fc955f181f8b8b06a,arXiv.org,2023.0,37.0,3.0,0.0,True,"['Engineering', 'Computer Science']","[{'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2239283908', 'name': 'Siyin Wang'}, {'authorId': '2240094771', 'name': 'Chao-Han Huck Yang'}, {'authorId': '2239514842', 'name': 'Ji Wu'}, {'authorId': '2256776223', 'name': 'Chao Zhang'}]","['Tsinghua University', 'Georgia Institute of Technology']","['China', 'United States']",2023-09
2309.07694,Shentong Mo,"Shentong Mo, Miao Xin",Tree of Uncertain Thoughts Reasoning for Large Language Models,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While the recently introduced Tree of Thoughts (ToT) has heralded advancements in allowing Large Language Models (LLMs) to reason through foresight and backtracking for global decision-making, it has overlooked the inherent local uncertainties in intermediate decision points or ""thoughts"". These local uncertainties, intrinsic to LLMs given their potential for diverse responses, remain a significant concern in the reasoning process. Addressing this pivotal gap, we introduce the Tree of Uncertain Thoughts (TouT) - a reasoning framework tailored for LLMs. Our TouT effectively leverages Monte Carlo Dropout to quantify uncertainty scores associated with LLMs' diverse local responses at these intermediate steps. By marrying this local uncertainty quantification with global search algorithms, TouT enhances the model's precision in response generation. We substantiate our approach with rigorous experiments on two demanding planning tasks: Game of 24 and Mini Crosswords. The empirical evidence underscores TouT's superiority over both ToT and chain-of-thought prompting methods. ","[{'version': 'v1', 'created': 'Thu, 14 Sep 2023 13:14:51 GMT'}]",2023-09-15,"[['Mo', 'Shentong', ''], ['Xin', 'Miao', '']]",0,0,2023-09-14,1,2,3,0,0,0,875d71bae61a66f7e65a2b6d363b7a0a27a6ed25,261822536.0,https://www.semanticscholar.org/paper/875d71bae61a66f7e65a2b6d363b7a0a27a6ed25,arXiv.org,2023.0,16.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2066123456', 'name': 'Shentong Mo'}, {'authorId': '1888678', 'name': 'Miao Xin'}]","['Carnegie Mellon University', 'Chinese Academy of Sciences', 'Mohamed bin Zayed University of Artificial Intelligence']","['China', 'United States', 'United Arab Emirates']",2023-09
2309.07915,HaoZhe Zhao,"Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang
  Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, Baobao Chang",MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning,"Code, dataset, checkpoints, and demos are available at
  https://github.com/PKUnlp-icler/MIC",,,,cs.CL cs.AI cs.CV,http://creativecommons.org/licenses/by-sa/4.0/,"  Since the resurgence of deep learning, vision-language models (VLMs) enhanced by large language models (LLMs) have grown exponentially in popularity. However, while LLMs can utilize extensive background knowledge and task information with in-context learning, most VLMs still struggle with understanding complex multi-modal prompts with multiple images, making VLMs less effective in downstream vision-language tasks. In this paper, we address the limitation above by 1) introducing MMICL, a new approach to allow the VLM to deal with multi-modal inputs efficiently; 2) proposing a novel context scheme to augment the in-context learning ability of the VLM; 3) constructing the Multi-modal In-Context Learning (MIC) dataset, designed to enhance the VLM's ability to understand complex multi-modal prompts. Our experiments confirm that MMICL achieves new state-of-the-art zero-shot performance on a wide range of general vision-language tasks, especially for complex benchmarks, including MME and MMBench. Our analysis demonstrates that MMICL effectively tackles the challenge of complex multi-modal prompt understanding and emerges the impressive ICL ability. Furthermore, we observe that MMICL successfully alleviates language bias in VLMs, a common issue for VLMs that often leads to hallucination when faced with extensive textual context. ","[{'version': 'v1', 'created': 'Thu, 14 Sep 2023 17:59:17 GMT'}, {'version': 'v2', 'created': 'Mon, 2 Oct 2023 14:46:01 GMT'}]",2023-10-03,"[['Zhao', 'Haozhe', ''], ['Cai', 'Zefan', ''], ['Si', 'Shuzheng', ''], ['Ma', 'Xiaojian', ''], ['An', 'Kaikai', ''], ['Chen', 'Liang', ''], ['Liu', 'Zixuan', ''], ['Wang', 'Sheng', ''], ['Han', 'Wenjuan', ''], ['Chang', 'Baobao', '']]",0,0,2023-09-14,2,10,3,0,0,0,3803d1f291e162bdaa4678a2c5a2bbcf63c050f4,261823391.0,https://www.semanticscholar.org/paper/3803d1f291e162bdaa4678a2c5a2bbcf63c050f4,arXiv.org,2023.0,120.0,10.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2112675144', 'name': 'Haozhe Zhao'}, {'authorId': '2117632647', 'name': 'Zefan Cai'}, {'authorId': '2053739525', 'name': 'Shuzheng Si'}, {'authorId': '2241105586', 'name': 'Xiaojian Ma'}, {'authorId': '2240549884', 'name': 'Kaikai An'}, {'authorId': '2240946204', 'name': 'Liang Chen'}, {'authorId': '46271003', 'name': 'Zixuan Liu'}, {'authorId': '47673176', 'name': 'Sheng Wang'}, {'authorId': '2240787926', 'name': 'Wenjuan Han'}, {'authorId': '39488576', 'name': 'Baobao Chang'}]","['Peking University', 'University of Washington', 'Beijing Jiaotong University', 'The University of Tokyo']","['China', 'United States', 'Japan']",2023-09
2309.07918,Zeqi Xiao,"Zeqi Xiao, Tai Wang, Jingbo Wang, Jinkun Cao, Wenwei Zhang, Bo Dai,
  Dahua Lin, Jiangmiao Pang",Unified Human-Scene Interaction via Prompted Chain-of-Contacts,"A unified Human-Scene Interaction framework that supports versatile
  interactions through language commands.Project URL:
  https://xizaoqu.github.io/unihsi/ . Code:
  https://github.com/OpenRobotLab/UniHSI",,,,cs.CV,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Human-Scene Interaction (HSI) is a vital component of fields like embodied AI and virtual reality. Despite advancements in motion quality and physical plausibility, two pivotal factors, versatile interaction control and the development of a user-friendly interface, require further exploration before the practical application of HSI. This paper presents a unified HSI framework, UniHSI, which supports unified control of diverse interactions through language commands. This framework is built upon the definition of interaction as Chain of Contacts (CoC): steps of human joint-object part pairs, which is inspired by the strong correlation between interaction types and human-object contact regions. Based on the definition, UniHSI constitutes a Large Language Model (LLM) Planner to translate language prompts into task plans in the form of CoC, and a Unified Controller that turns CoC into uniform task execution. To facilitate training and evaluation, we collect a new dataset named ScenePlan that encompasses thousands of task plans generated by LLMs based on diverse scenarios. Comprehensive experiments demonstrate the effectiveness of our framework in versatile task execution and generalizability to real scanned scenes. The project page is at https://github.com/OpenRobotLab/UniHSI . ","[{'version': 'v1', 'created': 'Thu, 14 Sep 2023 17:59:49 GMT'}, {'version': 'v2', 'created': 'Sun, 17 Sep 2023 14:43:09 GMT'}]",2023-09-19,"[['Xiao', 'Zeqi', ''], ['Wang', 'Tai', ''], ['Wang', 'Jingbo', ''], ['Cao', 'Jinkun', ''], ['Zhang', 'Wenwei', ''], ['Dai', 'Bo', ''], ['Lin', 'Dahua', ''], ['Pang', 'Jiangmiao', '']]",0,0,2023-09-14,2,8,1,0,0,0,dca4ed6d4db18216796336d647f8d4bdf197f039,261822943.0,https://www.semanticscholar.org/paper/dca4ed6d4db18216796336d647f8d4bdf197f039,arXiv.org,2023.0,43.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2241095971', 'name': 'Zeqi Xiao'}, {'authorId': '2257008641', 'name': 'Tai Wang'}, {'authorId': '2240784290', 'name': 'Jingbo Wang'}, {'authorId': '23590685', 'name': 'Jinkun Cao'}, {'authorId': '2240771481', 'name': 'Wenwei Zhang'}, {'authorId': '144445937', 'name': 'Bo Dai'}, {'authorId': '2237091231', 'name': 'Dahua Lin'}, {'authorId': '49968574', 'name': 'Jiangmiao Pang'}]","['Shanghai Artificial Intelligence Laboratory', 'Carnegie Mellon University', 'Nanyang Technological University']","['China', 'United States', 'Singapore']",2023-09
2309.08168,Jun Zhang,"Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, Sharad
  Mehrotra",Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  We present a novel inference scheme, self-speculative decoding, for accelerating Large Language Models (LLMs) without the need for an auxiliary model. This approach is characterized by a two-stage process: drafting and verification. The drafting stage generates draft tokens at a slightly lower quality but more quickly, which is achieved by selectively skipping certain intermediate layers during drafting Subsequently, the verification stage employs the original LLM to validate those draft output tokens in one forward pass. This process ensures the final output remains identical to that produced by the unaltered LLM, thereby maintaining output quality. The proposed method requires no additional neural network training and no extra memory footprint, making it a plug-and-play and cost-effective solution for inference acceleration. Benchmarks with LLaMA-2 and its fine-tuned models demonstrated a speedup up to 1.73$\times$. ","[{'version': 'v1', 'created': 'Fri, 15 Sep 2023 05:34:32 GMT'}]",2023-09-18,"[['Zhang', 'Jun', ''], ['Wang', 'Jue', ''], ['Li', 'Huan', ''], ['Shou', 'Lidan', ''], ['Chen', 'Ke', ''], ['Chen', 'Gang', ''], ['Mehrotra', 'Sharad', '']]",0,0,2023-09-15,1,7,1,1,1,0,8df524e0c50903d0b2c4be338081906d13ea42af,262013673.0,https://www.semanticscholar.org/paper/8df524e0c50903d0b2c4be338081906d13ea42af,arXiv.org,2023.0,45.0,3.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '27672597', 'name': 'Jinchao Zhang'}, {'authorId': '39597242', 'name': 'Jue Wang'}, {'authorId': '2242812507', 'name': 'Huan Li'}, {'authorId': '144128216', 'name': 'L. Shou'}, {'authorId': '32811782', 'name': 'Ke Chen'}, {'authorId': '2196572147', 'name': 'Gang Chen'}, {'authorId': '2242716456', 'name': 'Sharad Mehrotra'}]","['University of California, Irvine', 'Zhejiang University']","['China', 'United States']",2023-09
2309.08182,Jingzhe Ding,"Jingzhe Ding, Yan Cen, Xinyuan Wei",Using Large Language Model to Solve and Explain Physics Word Problems Approaching Human Level,"9 pages, 6 figures",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Our work demonstrates that large language model (LLM) pre-trained on texts can not only solve pure math word problems, but also physics word problems, whose solution requires calculation and inference based on prior physical knowledge. We collect and annotate the first physics word problem dataset-PhysQA, which contains over 1000 junior high school physics word problems (covering Kinematics, Mass&Density, Mechanics, Heat, Electricity). Then we use OpenAI' s GPT3.5 to generate the answer of these problems and found that GPT3.5 could automatically solve 49.3% of the problems through zero-shot learning and 73.2% through few-shot learning. This result demonstrates that by using similar problems and their answers as prompt, LLM could solve elementary physics word problems approaching human level performance. In addition to solving problems, GPT3.5 can also summarize the knowledge or topics covered by the problems, provide relevant explanations, and generate new physics word problems based on the input. Our work is the first research to focus on the automatic solving, explanation, and generation of physics word problems across various types and scenarios, and we achieve an acceptable and state-of-the-art accuracy. This underscores the potential of LLMs for further applications in secondary education. ","[{'version': 'v1', 'created': 'Fri, 15 Sep 2023 06:13:06 GMT'}, {'version': 'v2', 'created': 'Wed, 20 Sep 2023 07:08:53 GMT'}]",2023-09-21,"[['Ding', 'Jingzhe', ''], ['Cen', 'Yan', ''], ['Wei', 'Xinyuan', '']]",0,1,2023-09-15,2,3,2,1,0,1,1f8d74abcb89ee21bf01e7133cea503d8c99fef7,261960080.0,https://www.semanticscholar.org/paper/1f8d74abcb89ee21bf01e7133cea503d8c99fef7,arXiv.org,2023.0,24.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Physics', 'source': 's2-fos-model'}, {'category': 'Education', 'source': 's2-fos-model'}]","[{'authorId': '2244168236', 'name': 'Jingzhe Ding'}, {'authorId': '2242254524', 'name': 'Yan Cen'}, {'authorId': '2243749651', 'name': 'Xinyuan Wei'}]","['Fudan University', 'Columbia University']","['China', 'United States']",2023-09
2309.08594,Cheng Qian,"Cheng Qian, Xinran Zhao, Sherry Tongshuang Wu","""Merge Conflicts!"" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) acquire extensive knowledge during pre-training, known as their parametric knowledge. However, in order to remain up-to-date and align with human instructions, LLMs inevitably require external knowledge during their interactions with users. This raises a crucial question: How will LLMs respond when external knowledge interferes with their parametric knowledge? To investigate this question, we propose a framework that systematically elicits LLM parametric knowledge and introduces external knowledge. Specifically, we uncover the impacts by constructing a parametric knowledge graph to reveal the different knowledge structures of LLMs, and introduce external knowledge through distractors of varying degrees, methods, positions, and formats. Our experiments on both black-box and open-source models demonstrate that LLMs tend to produce responses that deviate from their parametric knowledge, particularly when they encounter direct conflicts or confounding changes of information within detailed contexts. We also find that while LLMs are sensitive to the veracity of external knowledge, they can still be distracted by unrelated information. These findings highlight the risk of hallucination when integrating external knowledge, even indirectly, during interactions with current LLMs. All the data and results are publicly available. ","[{'version': 'v1', 'created': 'Fri, 15 Sep 2023 17:47:59 GMT'}]",2023-09-18,"[['Qian', 'Cheng', ''], ['Zhao', 'Xinran', ''], ['Wu', 'Sherry Tongshuang', '']]",0,0,2023-09-15,1,3,1,0,0,0,7bf607c643a1f7f4607db84dfc0f4b63ff0bde70,261875641.0,https://www.semanticscholar.org/paper/7bf607c643a1f7f4607db84dfc0f4b63ff0bde70,arXiv.org,2023.0,36.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2241207237', 'name': 'Cheng Qian'}, {'authorId': '2241234006', 'name': 'Xinran Zhao'}, {'authorId': '2241209809', 'name': 'Sherry Tongshuang Wu'}]","['Carnegie Mellon University', 'Tsinghua University']","['China', 'United States']",2023-09
2309.09825,Minjia Mao,"Xiao Fang, Shangkun Che, Minjia Mao, Hongzhe Zhang, Ming Zhao,
  Xiaohang Zhao",Bias of AI-Generated Content: An Examination of News Produced by Large Language Models,,,,,cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Large language models (LLMs) have the potential to transform our lives and work through the content they generate, known as AI-Generated Content (AIGC). To harness this transformation, we need to understand the limitations of LLMs. Here, we investigate the bias of AIGC produced by seven representative LLMs, including ChatGPT and LLaMA. We collect news articles from The New York Times and Reuters, both known for their dedication to provide unbiased news. We then apply each examined LLM to generate news content with headlines of these news articles as prompts, and evaluate the gender and racial biases of the AIGC produced by the LLM by comparing the AIGC and the original news articles. We further analyze the gender bias of each LLM under biased prompts by adding gender-biased messages to prompts constructed from these news headlines. Our study reveals that the AIGC produced by each examined LLM demonstrates substantial gender and racial biases. Moreover, the AIGC generated by each LLM exhibits notable discrimination against females and individuals of the Black race. Among the LLMs, the AIGC generated by ChatGPT demonstrates the lowest level of bias, and ChatGPT is the sole model capable of declining content generation when provided with biased prompts. ","[{'version': 'v1', 'created': 'Mon, 18 Sep 2023 14:47:24 GMT'}, {'version': 'v2', 'created': 'Tue, 19 Sep 2023 01:13:22 GMT'}]",2023-09-20,"[['Fang', 'Xiao', ''], ['Che', 'Shangkun', ''], ['Mao', 'Minjia', ''], ['Zhang', 'Hongzhe', ''], ['Zhao', 'Ming', ''], ['Zhao', 'Xiaohang', '']]",1,1,2023-09-18,2,6,1,2,1,1,da0b5feee06ae1564dd03623741273bb0ab939e2,261898112.0,https://www.semanticscholar.org/paper/da0b5feee06ae1564dd03623741273bb0ab939e2,Social Science Research Network,2023.0,35.0,1.0,1.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Sociology', 'source': 's2-fos-model'}]","[{'authorId': '2087230367', 'name': 'Xiao Fang'}, {'authorId': '2241599054', 'name': 'Shangkun Che'}, {'authorId': '2241598392', 'name': 'Minjia Mao'}, {'authorId': '2241774909', 'name': 'Hongzhe Zhang'}, {'authorId': '2242386091', 'name': 'Ming Zhao'}, {'authorId': '2108811727', 'name': 'Xiaohang Zhao'}]","['Shanghai University of Finance and Economics', 'Chinese University of Hong Kong, Shenzhen', 'Tsinghua University', 'University of Delaware']","['China', 'United States']",2023-09
2309.10238,Chenhao Tang,"Chenhao Tang, Zhengliang Liu, Chong Ma, Zihao Wu, Yiwei Li, Wei Liu,
  Dajiang Zhu, Quanzheng Li, Xiang Li, Tianming Liu, Lei Fan",PolicyGPT: Automated Analysis of Privacy Policies with Large Language Models,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Privacy policies serve as the primary conduit through which online service providers inform users about their data collection and usage procedures. However, in a bid to be comprehensive and mitigate legal risks, these policy documents are often quite verbose. In practical use, users tend to click the Agree button directly rather than reading them carefully. This practice exposes users to risks of privacy leakage and legal issues. Recently, the advent of Large Language Models (LLM) such as ChatGPT and GPT-4 has opened new possibilities for text analysis, especially for lengthy documents like privacy policies. In this study, we investigate a privacy policy text analysis framework PolicyGPT based on the LLM. This framework was tested using two datasets. The first dataset comprises of privacy policies from 115 websites, which were meticulously annotated by legal experts, categorizing each segment into one of 10 classes. The second dataset consists of privacy policies from 304 popular mobile applications, with each sentence manually annotated and classified into one of another 10 categories. Under zero-shot learning conditions, PolicyGPT demonstrated robust performance. For the first dataset, it achieved an accuracy rate of 97%, while for the second dataset, it attained an 87% accuracy rate, surpassing that of the baseline machine learning and neural network models. ","[{'version': 'v1', 'created': 'Tue, 19 Sep 2023 01:22:42 GMT'}]",2023-09-20,"[['Tang', 'Chenhao', ''], ['Liu', 'Zhengliang', ''], ['Ma', 'Chong', ''], ['Wu', 'Zihao', ''], ['Li', 'Yiwei', ''], ['Liu', 'Wei', ''], ['Zhu', 'Dajiang', ''], ['Li', 'Quanzheng', ''], ['Li', 'Xiang', ''], ['Liu', 'Tianming', ''], ['Fan', 'Lei', '']]",1,1,2023-09-19,1,11,1,2,0,2,437a386f3fe4b8c7449a37e2364412a26e0a478c,262054160.0,https://www.semanticscholar.org/paper/437a386f3fe4b8c7449a37e2364412a26e0a478c,arXiv.org,2023.0,32.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2243996656', 'name': 'Chenhao Tang'}, {'authorId': '2145977326', 'name': 'Zheng Liu'}, {'authorId': '120688117', 'name': 'Chong-Yi Ma'}, {'authorId': '47039788', 'name': 'Zihao Wu'}, {'authorId': '2111161355', 'name': 'Yiwei Li'}, {'authorId': '46641573', 'name': 'W. Liu'}, {'authorId': '2181182', 'name': 'Dajiang Zhu'}, {'authorId': '2210054417', 'name': 'Quanzheng Li'}, {'authorId': '113075935', 'name': 'Xiang Li'}, {'authorId': '2115345993', 'name': 'Tianming Liu'}, {'authorId': '2244143682', 'name': 'Lei Fan'}]","['Mayo Clinic', 'Shanghai Jiao Tong University', 'Massachusetts General Hospital', 'The University of Texas at Arlington', 'University of Georgia', 'Northwestern Polytechnical University']","['China', 'United States']",2023-09
2309.10691,Xingyao Wang,"Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao
  Peng, Heng Ji",MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback,Code will be available at https://xingyaoww.github.io/mint-bench,,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools. However, current evaluation paradigms often focus solely on benchmark performance with single-turn exchanges, neglecting the intricate interactions among the user, LLMs, and external tools, creating a discrepancy between benchmark evaluation and real-world use cases. We introduce MINT benchmark to evaluate LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback. To ensure reproducibility, we provide an evaluation framework where LLMs can access tools by executing Python code and receive natural language feedback from the user simulated with GPT-4. We repurpose a diverse set of established datasets and tasks focusing on reasoning, coding, and decision-making and carefully curate them into a compact subset of instances for efficient evaluation. Our analysis of 20 open- and closed-source LLMs offers intriguing findings. (1) LLMs generally benefit from tool interactions and language feedback, with performance gains (absolute, same below) of 1--8% per additional turn with tool use and 2--17% with natural language feedback. (2) Better single-turn performance does not guarantee better multi-turn performance. (3) Surprisingly, on LLMs we evaluated, we found supervised instruction-finetuning (SIFT) and reinforcement learning from human feedback (RLHF) generally hurt multi-turn capabilities. We hope MINT can help measure progress and incentivize research in improving LLMs' capabilities in multi-turn interactions, especially for open-source communities where multi-turn human evaluation has been less accessible compared to commercial LLMs with a larger user base. ","[{'version': 'v1', 'created': 'Tue, 19 Sep 2023 15:25:42 GMT'}]",2023-09-20,"[['Wang', 'Xingyao', ''], ['Wang', 'Zihan', ''], ['Liu', 'Jiateng', ''], ['Chen', 'Yangyi', ''], ['Yuan', 'Lifan', ''], ['Peng', 'Hao', ''], ['Ji', 'Heng', '']]",0,1,2023-09-19,1,7,3,1,0,1,12b233752c7097ea6525622bed238ae2d2193c5a,262053695.0,https://www.semanticscholar.org/paper/12b233752c7097ea6525622bed238ae2d2193c5a,arXiv.org,2023.0,69.0,14.0,2.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2144803999', 'name': 'Xingyao Wang'}, {'authorId': '2243360876', 'name': 'Zihan Wang'}, {'authorId': '33456794', 'name': 'Jiateng Liu'}, {'authorId': '123331686', 'name': 'Yangyi Chen'}, {'authorId': '2152195191', 'name': 'Lifan Yuan'}, {'authorId': '1818378366', 'name': 'Hao Peng'}, {'authorId': '2243197103', 'name': 'Heng Ji'}]","['University of Illinois Urbana-Champaign', 'Renmin University of China']","['China', 'United States']",2023-09
2309.10929,Ruiqi Xu,"Ruiqi Xu, Yongfeng Huang, Xin Chen, Lin Zhang",Specializing Small Language Models towards Complex Style Transfer via Latent Attribute Pre-Training,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  In this work, we introduce the concept of complex text style transfer tasks, and constructed complex text datasets based on two widely applicable scenarios. Our dataset is the first large-scale data set of its kind, with 700 rephrased sentences and 1,000 sentences from the game Genshin Impact. While large language models (LLM) have shown promise in complex text style transfer, they have drawbacks such as data privacy concerns, network instability, and high deployment costs. To address these issues, we explore the effectiveness of small models (less than T5-3B) with implicit style pre-training through contrastive learning. We also propose a method for automated evaluation of text generation quality based on alignment with human evaluations using ChatGPT. Finally, we compare our approach with existing methods and show that our model achieves state-of-art performances of few-shot text style transfer models. ","[{'version': 'v1', 'created': 'Tue, 19 Sep 2023 21:01:40 GMT'}]",2023-09-21,"[['Xu', 'Ruiqi', ''], ['Huang', 'Yongfeng', ''], ['Chen', 'Xin', ''], ['Zhang', 'Lin', '']]",1,1,2023-09-19,1,4,1,2,1,1,02f17e5918e4a803f81410901c1375fc323eccd2,262064711.0,https://www.semanticscholar.org/paper/02f17e5918e4a803f81410901c1375fc323eccd2,European Conference on Artificial Intelligence,2023.0,26.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2243405915', 'name': 'Ruiqi Xu'}, {'authorId': '2192183418', 'name': 'Y. Huang'}, {'authorId': '48283677', 'name': 'Xin Chen'}, {'authorId': '2243033035', 'name': 'Lin Zhang'}]","['Symbiotic Matrix', 'ORCID', 'Platinum AI Inc., China']","['China', 'United States']",2023-09
2309.11087,Pavan Holur,"Pavan Holur, K. C. Enevoldsen, Lajoyce Mboning, Thalia Georgiou,
  Louis-S. Bouchard, Matteo Pellegrini and Vwani Roychowdhury",Embed-Search-Align: DNA Sequence Alignment using Transformer Models,"17 pages, Tables 5, Figures 5, Under review, ICLR",,,,q-bio.GN cs.AI,http://creativecommons.org/licenses/by/4.0/,"  DNA sequence alignment involves assigning short DNA reads to the most probable locations on an extensive reference genome. This process is crucial for various genomic analyses, including variant calling, transcriptomics, and epigenomics. Conventional methods, refined over decades, tackle this challenge in two steps: genome indexing followed by efficient search to locate likely positions for given reads. Building on the success of Large Language Models (LLM) in encoding text into embeddings, where the distance metric captures semantic similarity, recent efforts have explored whether the same Transformer architecture can produce numerical representations for DNA sequences. Such models have shown early promise in tasks involving classification of short DNA sequences, such as the detection of coding vs non-coding regions, as well as the identification of enhancer and promoter sequences. Performance at sequence classification tasks does not, however, translate to sequence alignment, where it is necessary to conduct a genome-wide search to successfully align every read. We address this open problem by framing it as an Embed-Search-Align task. In this framework, a novel encoder model DNA-ESA generates representations of reads and fragments of the reference, which are projected into a shared vector space where the read-fragment distance is used as surrogate for alignment. In particular, DNA-ESA introduces: (1) Contrastive loss for self-supervised training of DNA sequence representations, facilitating rich sequence-level embeddings, and (2) a DNA vector store to enable search across fragments on a global scale. DNA-ESA is >97% accurate when aligning 250-length reads onto a human reference genome of 3 gigabases (single-haploid), far exceeds the performance of 6 recent DNA-Transformer model baselines and shows task transfer across chromosomes and species. ","[{'version': 'v1', 'created': 'Wed, 20 Sep 2023 06:30:39 GMT'}]",2023-09-21,"[['Holur', 'Pavan', ''], ['Enevoldsen', 'K. C.', ''], ['Mboning', 'Lajoyce', ''], ['Georgiou', 'Thalia', ''], ['Bouchard', 'Louis-S.', ''], ['Pellegrini', 'Matteo', ''], ['Roychowdhury', 'Vwani', '']]",0,0,2023-09-20,1,7,2,0,0,0,b8d14f9f62d91411add3911d7f52c0a20f542918,262065499.0,https://www.semanticscholar.org/paper/b8d14f9f62d91411add3911d7f52c0a20f542918,arXiv.org,2023.0,51.0,0.0,0.0,True,"['Biology', 'Computer Science']","[{'category': 'Biology', 'source': 'external'}, {'category': 'Computer Science', 'source': 'external'}, {'category': 'Biology', 'source': 's2-fos-model'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1645199194', 'name': 'Pavan Holur'}, {'authorId': '120365560', 'name': 'K. Enevoldsen'}, {'authorId': '2243288713', 'name': 'Lajoyce Mboning'}, {'authorId': '2243133195', 'name': 'Thalia Georgiou'}, {'authorId': '2243229561', 'name': 'Louis-S. Bouchard'}, {'authorId': '2243263422', 'name': 'Matteo Pellegrini'}, {'authorId': '1686063', 'name': 'V. Roychowdhury'}]","['University of California, Los Angeles', 'Center for Excellence in Molecular Cell Science', 'Aarhus University']","['China', 'United States', 'Denmark']",2023-09
2309.11166,Haoyu Wang,"Haoyu Wang, Guozheng Ma, Cong Yu, Ning Gui, Linrui Zhang, Zhiqi Huang,
  Suwei Ma, Yongzhe Chang, Sen Zhang, Li Shen, Xueqian Wang, Peilin Zhao,
  Dacheng Tao",Are Large Language Models Really Robust to Word-Level Perturbations?,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The swift advancement in the scales and capabilities of Large Language Models (LLMs) positions them as promising tools for a variety of downstream tasks. In addition to the pursuit of better performance and the avoidance of violent feedback on a certain prompt, to ensure the responsibility of the LLM, much attention is drawn to the robustness of LLMs. However, existing evaluation methods mostly rely on traditional question answering datasets with predefined supervised labels, which do not align with the superior generation capabilities of contemporary LLMs. To address this issue, we propose a novel rational evaluation approach that leverages pre-trained reward models as diagnostic tools to evaluate the longer conversation generated from more challenging open questions by LLMs, which we refer to as the Reward Model for Reasonable Robustness Evaluation (TREvaL). Longer conversations manifest the comprehensive grasp of language models in terms of their proficiency in understanding questions, a capability not entirely encompassed by individual words or letters, which may exhibit oversimplification and inherent biases. Our extensive empirical experiments demonstrate that TREvaL provides an innovative method for evaluating the robustness of an LLM. Furthermore, our results demonstrate that LLMs frequently exhibit vulnerability to word-level perturbations that are commonplace in daily language usage. Notably, we are surprised to discover that robustness tends to decrease as fine-tuning (SFT and RLHF) is conducted. The code of TREval is available in https://github.com/Harry-mic/TREvaL. ","[{'version': 'v1', 'created': 'Wed, 20 Sep 2023 09:23:46 GMT'}, {'version': 'v2', 'created': 'Wed, 27 Sep 2023 09:53:16 GMT'}]",2023-09-28,"[['Wang', 'Haoyu', ''], ['Ma', 'Guozheng', ''], ['Yu', 'Cong', ''], ['Gui', 'Ning', ''], ['Zhang', 'Linrui', ''], ['Huang', 'Zhiqi', ''], ['Ma', 'Suwei', ''], ['Chang', 'Yongzhe', ''], ['Zhang', 'Sen', ''], ['Shen', 'Li', ''], ['Wang', 'Xueqian', ''], ['Zhao', 'Peilin', ''], ['Tao', 'Dacheng', '']]",0,0,2023-09-20,2,13,2,0,0,0,57207b935fc3484d175f5e9e2980d73ca793f994,262064288.0,https://www.semanticscholar.org/paper/57207b935fc3484d175f5e9e2980d73ca793f994,arXiv.org,2023.0,44.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2256769475', 'name': 'Haoyu Wang'}, {'authorId': '2171444073', 'name': 'Guozheng Ma'}, {'authorId': '2243365066', 'name': 'Cong Yu'}, {'authorId': '2243275935', 'name': 'Ning Gui'}, {'authorId': '2243296871', 'name': 'Linrui Zhang'}, {'authorId': '2243609675', 'name': 'Zhiqi Huang'}, {'authorId': '2243370574', 'name': 'Suwei Ma'}, {'authorId': '3837093', 'name': 'Yongzhe Chang'}, {'authorId': '2243287503', 'name': 'Sen Zhang'}, {'authorId': '2144035454', 'name': 'Li Shen'}, {'authorId': '2243105430', 'name': 'Xueqian Wang'}, {'authorId': '2243527845', 'name': 'Peilin Zhao'}, {'authorId': '2135519749', 'name': 'Dacheng Tao'}]","['University of Sydney', 'Columbia University', 'Tencent', 'Tsinghua University', 'Jingdong']","['China', 'United States', 'Australia']",2023-09
2309.11478,Hanyi Wang,"Yuqian Sun, Hanyi Wang, Pok Man Chan, Morteza Tabibi, Yan Zhang, Huan
  Lu, Yuheng Chen, Chang Hee Lee, Ali Asadipour","Fictional Worlds, Real Connections: Developing Community Storytelling Social Chatbots through LLMs",,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We address the integration of storytelling and Large Language Models (LLMs) to develop engaging and believable Social Chatbots (SCs) in community settings. Motivated by the potential of fictional characters to enhance social interactions, we introduce Storytelling Social Chatbots (SSCs) and the concept of story engineering to transform fictional game characters into ""live"" social entities within player communities. Our story engineering process includes three steps: (1) Character and story creation, defining the SC's personality and worldview, (2) Presenting Live Stories to the Community, allowing the SC to recount challenges and seek suggestions, and (3) Communication with community members, enabling interaction between the SC and users. We employed the LLM GPT-3 to drive our SSC prototypes, ""David"" and ""Catherine,"" and evaluated their performance in an online gaming community, ""DE (Alias),"" on Discord. Our mixed-method analysis, based on questionnaires (N=15) and interviews (N=8) with community members, reveals that storytelling significantly enhances the engagement and believability of SCs in community settings. ","[{'version': 'v1', 'created': 'Wed, 20 Sep 2023 17:23:05 GMT'}]",2023-09-21,"[['Sun', 'Yuqian', ''], ['Wang', 'Hanyi', ''], ['Chan', 'Pok Man', ''], ['Tabibi', 'Morteza', ''], ['Zhang', 'Yan', ''], ['Lu', 'Huan', ''], ['Chen', 'Yuheng', ''], ['Lee', 'Chang Hee', ''], ['Asadipour', 'Ali', '']]",0,1,2023-09-20,1,9,1,1,0,1,09d8b3e9172ddd08ab6c1a6c7c8217cec438b6d5,262065244.0,https://www.semanticscholar.org/paper/09d8b3e9172ddd08ab6c1a6c7c8217cec438b6d5,arXiv.org,2023.0,25.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1666629650', 'name': 'Yuqian Sun'}, {'authorId': '2243661143', 'name': 'Hanyi Wang'}, {'authorId': '2242918586', 'name': 'Pok Man Chan'}, {'authorId': '2242892622', 'name': 'Morteza Tabibi'}, {'authorId': '2256617256', 'name': 'Yan Zhang'}, {'authorId': '2243367755', 'name': 'Huan Lu'}, {'authorId': '2047427816', 'name': 'Yuheng Chen'}, {'authorId': '2187449929', 'name': 'Chang Hee Lee'}, {'authorId': '2024403', 'name': 'A. Asadipour'}]","['Royal College of Art', 'Independent University', 'Independent, Hong Kong 4 rct.ai, United States', 'Independent, China', 'Korea Advanced Institute of Science and Technology']","['South Korea', 'Bangladesh', 'United States', 'United Kingdom', 'China']",2023-09
2309.11489,Tianbao Xie,"Tianbao Xie and Siheng Zhao and Chen Henry Wu and Yitao Liu and Qian
  Luo and Victor Zhong and Yanchao Yang and Tao Yu",Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning,"23 pages, 10 figures, update",,,,cs.LG cs.AI cs.CL cs.RO,http://creativecommons.org/licenses/by/4.0/,"  Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better task success rates and convergence speed than expert-written reward codes. For locomotion tasks, our method learns six novel locomotion behaviors with a success rate exceeding 94%. Furthermore, we show that the policies trained in the simulator with our method can be deployed in the real world. Finally, Text2Reward further improves the policies by refining their reward functions with human feedback. Video results are available at https://text-to-reward.github.io ","[{'version': 'v1', 'created': 'Wed, 20 Sep 2023 17:39:13 GMT'}, {'version': 'v2', 'created': 'Thu, 21 Sep 2023 15:17:09 GMT'}]",2023-09-22,"[['Xie', 'Tianbao', ''], ['Zhao', 'Siheng', ''], ['Wu', 'Chen Henry', ''], ['Liu', 'Yitao', ''], ['Luo', 'Qian', ''], ['Zhong', 'Victor', ''], ['Yang', 'Yanchao', ''], ['Yu', 'Tao', '']]",0,0,2023-09-20,2,8,4,0,0,0,2d8713cbccfed20bb250ebda6bd0643707c12f91,262053612.0,https://www.semanticscholar.org/paper/2d8713cbccfed20bb250ebda6bd0643707c12f91,arXiv.org,2023.0,61.0,4.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2057038673', 'name': 'Tianbao Xie'}, {'authorId': '2243033718', 'name': 'Siheng Zhao'}, {'authorId': '114621402', 'name': 'Chen Henry Wu'}, {'authorId': '2243302973', 'name': 'Yitao Liu'}, {'authorId': '2242962339', 'name': 'Qian Luo'}, {'authorId': '2243226284', 'name': 'Victor Zhong'}, {'authorId': '2243300847', 'name': 'Yanchao Yang'}, {'authorId': '2256865348', 'name': 'Tao Yu'}]","['Nanjing University', 'University of Waterloo', 'Carnegie Mellon University', 'Microsoft', 'University of Hong Kong']","['Canada', 'United States', 'China', 'Hong Kong']",2023-09
2309.13192,Wei Gao,"Kai Huang, Hanyun Yin, Heng Huang, Wei Gao",Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation,14 pages,,,,cs.LG cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Fine-tuning is the most effective way of adapting pre-trained large language models (LLMs) to downstream applications. With the fast growth of LLM-enabled AI applications and democratization of open-souced LLMs, fine-tuning has become possible for non-expert individuals, but intensively performed LLM fine-tuning worldwide could result in significantly high energy consumption and carbon footprint, which may bring large environmental impact. Mitigating such environmental impact towards Green AI directly correlates to reducing the FLOPs of fine-tuning, but existing techniques on efficient LLM fine-tuning can only achieve limited reduction of such FLOPs, due to their ignorance of the backpropagation cost in fine-tuning. To address this limitation, in this paper we present GreenTrainer, a new LLM fine-tuning technique that adaptively evaluates different tensors' backpropagation costs and contributions to the fine-tuned model accuracy, to minimize the fine-tuning cost by selecting the most appropriate set of tensors in training. Such selection in GreenTrainer is made based on a given objective of FLOPs reduction, which can flexibly adapt to the carbon footprint in energy supply and the need in Green AI. Experiment results over multiple open-sourced LLM models and abstractive summarization datasets show that, compared to fine-tuning the whole LLM model, GreenTrainer can save up to 64% FLOPs in fine-tuning without any noticeable model accuracy loss. Compared to the existing fine-tuning techniques such as LoRa, GreenTrainer can achieve up to 4% improvement on model accuracy with on-par FLOPs reduction. ","[{'version': 'v1', 'created': 'Fri, 22 Sep 2023 21:55:18 GMT'}]",2023-09-26,"[['Huang', 'Kai', ''], ['Yin', 'Hanyun', ''], ['Huang', 'Heng', ''], ['Gao', 'Wei', '']]",0,0,2023-09-22,1,4,2,0,0,0,33f5c02e55b556ce88c4121be12b6958bc093a63,262460575.0,https://www.semanticscholar.org/paper/33f5c02e55b556ce88c4121be12b6958bc093a63,arXiv.org,2023.0,45.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2112769493', 'name': 'Kai Huang'}, {'authorId': '30974319', 'name': 'Hanyu Yin'}, {'authorId': '2115528613', 'name': 'Heng Huang'}, {'authorId': '48385754', 'name': 'W. Gao'}]","['University of Science and Technology of China', 'University of Pittsburgh', 'University of Maryland, College Park']","['China', 'United States']",2023-09
2309.13308,Yuxuan Liu,"Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang,
  Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang",Calibrating LLM-Based Evaluator,"22 pages,11 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advancements in large language models (LLMs) on language modeling and emergent capabilities make them a promising reference-free evaluator of natural language generation quality, and a competent alternative to human evaluation. However, hindered by the closed-source or high computational demand to host and tune, there is a lack of practice to further calibrate an off-the-shelf LLM-based evaluator towards better human alignment. In this work, we propose AutoCalibrate, a multi-stage, gradient-free approach to automatically calibrate and align an LLM-based evaluator toward human preference. Instead of explicitly modeling human preferences, we first implicitly encompass them within a set of human labels. Then, an initial set of scoring criteria is drafted by the language model itself, leveraging in-context learning on different few-shot examples. To further calibrate this set of criteria, we select the best performers and re-draft them with self-refinement. Our experiments on multiple text quality evaluation datasets illustrate a significant improvement in correlation with expert evaluation through calibration. Our comprehensive qualitative analysis conveys insightful intuitions and observations on the essence of effective scoring criteria. ","[{'version': 'v1', 'created': 'Sat, 23 Sep 2023 08:46:11 GMT'}]",2023-09-26,"[['Liu', 'Yuxuan', ''], ['Yang', 'Tianchi', ''], ['Huang', 'Shaohan', ''], ['Zhang', 'Zihan', ''], ['Huang', 'Haizhen', ''], ['Wei', 'Furu', ''], ['Deng', 'Weiwei', ''], ['Sun', 'Feng', ''], ['Zhang', 'Qi', '']]",0,0,2023-09-23,1,9,1,0,0,0,2d12f95dd521101f3092cc3bb04e7e88aba8f562,262464745.0,https://www.semanticscholar.org/paper/2d12f95dd521101f3092cc3bb04e7e88aba8f562,arXiv.org,2023.0,38.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2116696995', 'name': 'Yuxuan Liu'}, {'authorId': '2122813607', 'name': 'Tianchi Yang'}, {'authorId': '3110003', 'name': 'Shaohan Huang'}, {'authorId': '2144371301', 'name': 'Zihan Zhang'}, {'authorId': '2146285313', 'name': 'Haizhen Huang'}, {'authorId': '49807919', 'name': 'Furu Wei'}, {'authorId': '2066621592', 'name': 'Weiwei Deng'}, {'authorId': '2247156451', 'name': 'Feng Sun'}, {'authorId': '2256972722', 'name': 'Qi Zhang'}]","['Peking University', 'Microsoft']","['China', 'United States']",2023-09
2309.13876,Yifan Peng,"Yifan Peng, Jinchuan Tian, Brian Yan, Dan Berrebbi, Xuankai Chang,
  Xinjian Li, Jiatong Shi, Siddhant Arora, William Chen, Roshan Sharma, Wangyou
  Zhang, Yui Sudo, Muhammad Shakeel, Jee-weon Jung, Soumi Maiti, Shinji
  Watanabe",Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data,Accepted at ASRU 2023,,,,cs.CL cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisper-style training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pre-trained models and training logs to promote open science. ","[{'version': 'v1', 'created': 'Mon, 25 Sep 2023 05:01:34 GMT'}, {'version': 'v2', 'created': 'Mon, 2 Oct 2023 01:10:01 GMT'}]",2023-10-03,"[['Peng', 'Yifan', ''], ['Tian', 'Jinchuan', ''], ['Yan', 'Brian', ''], ['Berrebbi', 'Dan', ''], ['Chang', 'Xuankai', ''], ['Li', 'Xinjian', ''], ['Shi', 'Jiatong', ''], ['Arora', 'Siddhant', ''], ['Chen', 'William', ''], ['Sharma', 'Roshan', ''], ['Zhang', 'Wangyou', ''], ['Sudo', 'Yui', ''], ['Shakeel', 'Muhammad', ''], ['Jung', 'Jee-weon', ''], ['Maiti', 'Soumi', ''], ['Watanabe', 'Shinji', '']]",0,0,2023-09-25,2,16,3,0,0,0,d43338451cd8676548811e1ff8f9c92ea987c5bd,262465225.0,https://www.semanticscholar.org/paper/d43338451cd8676548811e1ff8f9c92ea987c5bd,arXiv.org,2023.0,68.0,0.0,0.0,True,"['Computer Science', 'Engineering']","[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Engineering', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2111014429', 'name': 'Yifan Peng'}, {'authorId': '2087131860', 'name': 'Jinchuan Tian'}, {'authorId': '2087059555', 'name': 'Brian Yan'}, {'authorId': '2142561945', 'name': 'Dan Berrebbi'}, {'authorId': '8776560', 'name': 'Xuankai Chang'}, {'authorId': '2108191722', 'name': 'Xinjian Li'}, {'authorId': '1485531923', 'name': 'Jiatong Shi'}, {'authorId': '72401599', 'name': 'Siddhant Arora'}, {'authorId': '2144302389', 'name': 'William Chen'}, {'authorId': '145521253', 'name': 'Roshan Sharma'}, {'authorId': '1390725481', 'name': 'Wangyou Zhang'}, {'authorId': '151265408', 'name': 'Yui Sudo'}, {'authorId': '47551878', 'name': 'M. Shakeel'}, {'authorId': '31930118', 'name': 'Jee-weon Jung'}, {'authorId': '31949212', 'name': 'Soumi Maiti'}, {'authorId': '1746678', 'name': 'Shinji Watanabe'}]","['Carnegie Mellon University', 'Shanghai Jiao Tong University', 'Honda (Japan)']","['China', 'United States', 'Japan']",2023-09
2309.14623,Xu Chen,"Jiayi Liao, Xu Chen, Qiang Fu, Lun Du, Xiangnan He, Xiang Wang, Shi
  Han, Dongmei Zhang",Text-to-Image Generation for Abstract Concepts,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent years have witnessed the substantial progress of large-scale models across various domains, such as natural language processing and computer vision, facilitating the expression of concrete concepts. Unlike concrete concepts that are usually directly associated with physical objects, expressing abstract concepts through natural language requires considerable effort, which results from their intricate semantics and connotations. An alternative approach is to leverage images to convey rich visual information as a supplement. Nevertheless, existing Text-to-Image (T2I) models are primarily trained on concrete physical objects and tend to fail to visualize abstract concepts. Inspired by the three-layer artwork theory that identifies critical factors, intent, object and form during artistic creation, we propose a framework of Text-to-Image generation for Abstract Concepts (TIAC). The abstract concept is clarified into a clear intent with a detailed definition to avoid ambiguity. LLMs then transform it into semantic-related physical objects, and the concept-dependent form is retrieved from an LLM-extracted form pattern set. Information from these three aspects will be integrated to generate prompts for T2I models via LLM. Evaluation results from human assessments and our newly designed metric concept score demonstrate the effectiveness of our framework in creating images that can sufficiently express abstract concepts. ","[{'version': 'v1', 'created': 'Tue, 26 Sep 2023 02:22:39 GMT'}, {'version': 'v2', 'created': 'Wed, 27 Sep 2023 05:34:17 GMT'}]",2023-09-28,"[['Liao', 'Jiayi', ''], ['Chen', 'Xu', ''], ['Fu', 'Qiang', ''], ['Du', 'Lun', ''], ['He', 'Xiangnan', ''], ['Wang', 'Xiang', ''], ['Han', 'Shi', ''], ['Zhang', 'Dongmei', '']]",0,0,2023-09-26,2,8,1,0,0,0,0d38f1edac66b4645cf5fa05abaf9d92cba5d5d3,262824861.0,https://www.semanticscholar.org/paper/0d38f1edac66b4645cf5fa05abaf9d92cba5d5d3,arXiv.org,2023.0,49.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2212781448', 'name': 'Jiayi Liao'}, {'authorId': '2118183722', 'name': 'Xu Chen'}, {'authorId': '2089209772', 'name': 'Qiang Fu'}, {'authorId': '12723949', 'name': 'Lun Du'}, {'authorId': '2240825631', 'name': 'Xiangnan He'}, {'authorId': '2240694093', 'name': 'Xiang Wang'}, {'authorId': '2109750123', 'name': 'Shi Han'}, {'authorId': '2140415600', 'name': 'Dongmei Zhang'}]","['University of Science and Technology of China', 'Microsoft']","['China', 'United States']",2023-09
2309.15461,Mengyuan Liu,"June M. Liu, Donghao Li, He Cao, Tianhe Ren, Zeyi Liao and Jiamin Wu",ChatCounselor: A Large Language Models for Mental Health Support,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents ChatCounselor, a large language model (LLM) solution designed to provide mental health support. Unlike generic chatbots, ChatCounselor is distinguished by its foundation in real conversations between consulting clients and professional psychologists, enabling it to possess specialized knowledge and counseling skills in the field of psychology. The training dataset, Psych8k, was constructed from 260 in-depth interviews, each spanning an hour. To assess the quality of counseling responses, the counseling Bench was devised. Leveraging GPT-4 and meticulously crafted prompts based on seven metrics of psychological counseling assessment, the model underwent evaluation using a set of real-world counseling questions. Impressively, ChatCounselor surpasses existing open-source models in the counseling Bench and approaches the performance level of ChatGPT, showcasing the remarkable enhancement in model capability attained through high-quality domain-specific data. ","[{'version': 'v1', 'created': 'Wed, 27 Sep 2023 07:57:21 GMT'}]",2023-09-28,"[['Liu', 'June M.', ''], ['Li', 'Donghao', ''], ['Cao', 'He', ''], ['Ren', 'Tianhe', ''], ['Liao', 'Zeyi', ''], ['Wu', 'Jiamin', '']]",1,1,2023-09-27,1,6,1,2,0,2,d030361469baabf5acbaee1623ea495e70591bae,262943261.0,https://www.semanticscholar.org/paper/d030361469baabf5acbaee1623ea495e70591bae,arXiv.org,2023.0,42.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Psychology', 'source': 's2-fos-model'}]","[{'authorId': '2247666353', 'name': 'June M. Liu'}, {'authorId': '2248406777', 'name': 'Donghao Li'}, {'authorId': '2153244338', 'name': 'He Cao'}, {'authorId': '2247536954', 'name': 'Tianhe Ren'}, {'authorId': '2247514935', 'name': 'Zeyi Liao'}, {'authorId': '2247759048', 'name': 'Jiamin Wu'}]","['University of Hong Kong', 'The International Digital Economy Academy (IDEA) Shenzhen, China', 'Hong Kong University of Science and Technology', 'The Ohio State University']","['China', 'United States', 'Hong Kong']",2023-09
2309.16289,Zhiwei Fei,"Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Songyang
  Zhang, Kai Chen, Zongwen Shen, Jidong Ge",LawBench: Benchmarking Legal Knowledge of Large Language Models,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have demonstrated strong capabilities in various aspects. However, when applying them to the highly specialized, safe-critical legal domain, it is unclear how much legal knowledge they possess and whether they can reliably perform legal-related tasks. To address this gap, we propose a comprehensive evaluation benchmark LawBench. LawBench has been meticulously crafted to have precise assessment of the LLMs' legal capabilities from three cognitive levels: (1) Legal knowledge memorization: whether LLMs can memorize needed legal concepts, articles and facts; (2) Legal knowledge understanding: whether LLMs can comprehend entities, events and relationships within legal text; (3) Legal knowledge applying: whether LLMs can properly utilize their legal knowledge and make necessary reasoning steps to solve realistic legal tasks. LawBench contains 20 diverse tasks covering 5 task types: single-label classification (SLC), multi-label classification (MLC), regression, extraction and generation. We perform extensive evaluations of 51 LLMs on LawBench, including 20 multilingual LLMs, 22 Chinese-oriented LLMs and 9 legal specific LLMs. The results show that GPT-4 remains the best-performing LLM in the legal domain, surpassing the others by a significant margin. While fine-tuning LLMs on legal specific text brings certain improvements, we are still a long way from obtaining usable and reliable LLMs in legal tasks. All data, model predictions and evaluation code are released in https://github.com/open-compass/LawBench/. We hope this benchmark provides in-depth understanding of the LLMs' domain-specified capabilities and speed up the development of LLMs in the legal domain. ","[{'version': 'v1', 'created': 'Thu, 28 Sep 2023 09:35:59 GMT'}]",2023-09-29,"[['Fei', 'Zhiwei', ''], ['Shen', 'Xiaoyu', ''], ['Zhu', 'Dawei', ''], ['Zhou', 'Fengzhe', ''], ['Han', 'Zhuo', ''], ['Zhang', 'Songyang', ''], ['Chen', 'Kai', ''], ['Shen', 'Zongwen', ''], ['Ge', 'Jidong', '']]",0,1,2023-09-28,1,9,3,1,0,1,9099ee08e59cc33ed1c88d4708cf5c931bf46dc4,263134950.0,https://www.semanticscholar.org/paper/9099ee08e59cc33ed1c88d4708cf5c931bf46dc4,arXiv.org,2023.0,89.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Law', 'source': 's2-fos-model'}]","[{'authorId': '2248062587', 'name': 'Zhiwei Fei'}, {'authorId': '2249736444', 'name': 'Xiaoyu Shen'}, {'authorId': '47770349', 'name': 'D. Zhu'}, {'authorId': '2248972766', 'name': 'Fengzhe Zhou'}, {'authorId': '2247916656', 'name': 'Zhuo Han'}, {'authorId': '1734973476', 'name': 'Songyang Zhang'}, {'authorId': '2261273470', 'name': 'Kai Chen'}, {'authorId': '2249070589', 'name': 'Zongwen Shen'}, {'authorId': '2248015856', 'name': 'Jidong Ge'}]","['Shanghai Artificial Intelligence Laboratory', 'Nanjing University', 'Saarland University', 'Amazon']","['China', 'Germany', 'United States']",2023-09
2309.16639,Xuhai Xu,"Ruolan Wu, Chun Yu, Xiaole Pan, Yujia Liu, Ningning Zhang, Yue Fu,
  Yuhan Wang, Zhi Zheng, Li Chen, Qiaolei Jiang, Xuhai Xu, Yuanchun Shi",MindShift: Leveraging Large Language Models for Mental-States-Based Problematic Smartphone Use Intervention,,,,,cs.CL cs.AI cs.HC,http://creativecommons.org/licenses/by/4.0/,"  Problematic smartphone use negatively affects physical and mental health. Despite the wide range of prior research, existing persuasive techniques are not flexible enough to provide dynamic persuasion content based on users' physical contexts and mental states. We first conduct a Wizard-of-Oz study (N=12) and an interview study (N=10) to summarize the mental states behind problematic smartphone use: boredom, stress, and inertia. This informs our design of four persuasion strategies: understanding, comforting, evoking, and scaffolding habits. We leverage large language models (LLMs) to enable the automatic and dynamic generation of effective persuasion content. We develop MindShift, a novel LLM-powered problematic smartphone use intervention technique. MindShift takes users' in-the-moment physical contexts, mental states, app usage behaviors, users' goals & habits as input, and generates high-quality and flexible persuasive content with appropriate persuasion strategies. We conduct a 5-week field experiment (N=25) to compare MindShift with baseline techniques. The results show that MindShift significantly improves intervention acceptance rates by 17.8-22.5% and reduces smartphone use frequency by 12.1-14.4%. Moreover, users have a significant drop in smartphone addiction scale scores and a rise in self-efficacy. Our study sheds light on the potential of leveraging LLMs for context-aware persuasion in other behavior change domains. ","[{'version': 'v1', 'created': 'Thu, 28 Sep 2023 17:49:03 GMT'}]",2023-09-29,"[['Wu', 'Ruolan', ''], ['Yu', 'Chun', ''], ['Pan', 'Xiaole', ''], ['Liu', 'Yujia', ''], ['Zhang', 'Ningning', ''], ['Fu', 'Yue', ''], ['Wang', 'Yuhan', ''], ['Zheng', 'Zhi', ''], ['Chen', 'Li', ''], ['Jiang', 'Qiaolei', ''], ['Xu', 'Xuhai', ''], ['Shi', 'Yuanchun', '']]",0,0,2023-09-28,1,12,3,0,0,0,2b35579ecf8b487ed7e24bab4fc53941051f4dd0,263134184.0,https://www.semanticscholar.org/paper/2b35579ecf8b487ed7e24bab4fc53941051f4dd0,arXiv.org,2023.0,92.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Psychology', 'source': 's2-fos-model'}]","[{'authorId': '2256330034', 'name': 'Ruolan Wu'}, {'authorId': '2265952293', 'name': 'Chun Yu'}, {'authorId': '2248834172', 'name': 'Xiaole Pan'}, {'authorId': '2247775088', 'name': 'Yujia Liu'}, {'authorId': '2247972147', 'name': 'Ningning Zhang'}, {'authorId': '2257139682', 'name': 'Yue Fu'}, {'authorId': '2247789527', 'name': 'Yuhan Wang'}, {'authorId': '2256386334', 'name': 'Zhi Zheng'}, {'authorId': '2247970182', 'name': 'Li Chen'}, {'authorId': '2249057460', 'name': 'Qiaolei Jiang'}, {'authorId': '2247941519', 'name': 'Xuhai Xu'}, {'authorId': '2152861676', 'name': 'Yuanchun Shi'}]","['Beijing University of Posts and Telecommunications', 'China XUHAI XU,', 'Tsinghua University', 'University of Washington', 'Massachusetts Institute of Technology']","['China', 'United States']",2023-09
2309.16804,Yu Li,"Yu Li, Shang Qu, Jili Shen, Shangchao Min and Zhou Yu",Curriculum-Driven Edubot: A Framework for Developing Language Learning Chatbots Through Synthesizing Conversational Data,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Chatbots have become popular in educational settings, revolutionizing how students interact with material and how teachers teach. We present Curriculum-Driven EduBot, a framework for developing a chatbot that combines the interactive features of chatbots with the systematic material of English textbooks to assist students in enhancing their conversational skills. We begin by extracting pertinent topics from textbooks and then using large language models to generate dialogues related to these topics. We then fine-tune an open-source LLM using our generated conversational data to create our curriculum-driven chatbot. User studies demonstrate that our chatbot outperforms ChatGPT in leading curriculum-based dialogues and adapting its dialogue to match the user's English proficiency level. By combining traditional textbook methodologies with conversational AI, our approach offers learners an interactive tool that aligns with their curriculum and provides user-tailored conversation practice. This facilitates meaningful student-bot dialogues and enriches the overall learning experience within the curriculum's pedagogical framework. ","[{'version': 'v1', 'created': 'Thu, 28 Sep 2023 19:14:18 GMT'}]",2023-10-02,"[['Li', 'Yu', ''], ['Qu', 'Shang', ''], ['Shen', 'Jili', ''], ['Min', 'Shangchao', ''], ['Yu', 'Zhou', '']]",1,1,2023-09-28,1,5,1,1,0,1,6bde8138560851914a9a9426c2a1ec3f11c6509f,263310830.0,https://www.semanticscholar.org/paper/6bde8138560851914a9a9426c2a1ec3f11c6509f,arXiv.org,2023.0,44.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Education', 'source': 's2-fos-model'}]","[{'authorId': '2116612786', 'name': 'Yu Li'}, {'authorId': '2249537855', 'name': 'Shang Qu'}, {'authorId': '2251087810', 'name': 'Jili Shen'}, {'authorId': '2249538586', 'name': 'Shangchao Min'}, {'authorId': '2249545171', 'name': 'Zhou Yu'}]","['University of Science and Technology of China', 'Columbia University', 'Zhejiang University']","['China', 'United States']",2023-09
2309.17061,Xin Cheng,"Xin Cheng and Xun Wang and Tao Ge and Si-Qing Chen and Furu Wei and
  Dongyan Zhao and Rui Yan",SCALE: Synergized Collaboration of Asymmetric Language Translation Engines,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we introduce SCALE, a collaborative framework that connects compact Specialized Translation Models (STMs) and general-purpose Large Language Models (LLMs) as one unified translation engine. By introducing translation from STM into the triplet in-context demonstrations, SCALE unlocks refinement and pivoting ability of LLM, thus mitigating language bias of LLM and parallel data bias of STM, enhancing LLM speciality without sacrificing generality, and facilitating continual learning without expensive LLM fine-tuning. Our comprehensive experiments show that SCALE significantly outperforms both few-shot LLMs (GPT-4) and specialized models (NLLB) in challenging low-resource settings. Moreover, in Xhosa to English translation, SCALE experiences consistent improvement by a 4 BLEURT score without tuning LLM and surpasses few-shot GPT-4 by 2.5 COMET score and 3.8 BLEURT score when equipped with a compact model consisting of merely 600M parameters. SCALE could also effectively exploit the existing language bias of LLMs by using an English-centric STM as a pivot for translation between any language pairs, outperforming few-shot GPT-4 by an average of 6 COMET points across eight translation directions. Furthermore we provide an in-depth analysis of SCALE's robustness, translation characteristics, and latency costs, providing solid foundation for future studies exploring the potential synergy between LLMs and more specialized, task-specific models. ","[{'version': 'v1', 'created': 'Fri, 29 Sep 2023 08:46:38 GMT'}]",2023-10-02,"[['Cheng', 'Xin', ''], ['Wang', 'Xun', ''], ['Ge', 'Tao', ''], ['Chen', 'Si-Qing', ''], ['Wei', 'Furu', ''], ['Zhao', 'Dongyan', ''], ['Yan', 'Rui', '']]",0,1,2023-09-29,1,7,2,2,1,1,e6fd045dc7fdb79dcef2f71e38c0edeb1b11ee0d,263310813.0,https://www.semanticscholar.org/paper/e6fd045dc7fdb79dcef2f71e38c0edeb1b11ee0d,arXiv.org,2023.0,52.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2193630544', 'name': 'Xin Cheng'}, {'authorId': '2193104542', 'name': 'Xun Wang'}, {'authorId': '2547492', 'name': 'Tao Ge'}, {'authorId': '2111638099', 'name': 'Si-Qing Chen'}, {'authorId': '2249539478', 'name': 'Furu Wei'}, {'authorId': '2253232138', 'name': 'Dongyan Zhao'}, {'authorId': '2249533146', 'name': 'Rui Yan'}]","['Peking University', 'Microsoft', 'Renmin University of China']","['China', 'United States']",2023-09
2309.17382,Zhihan Liu,"Zhihan Liu, Hao Hu, Shenao Zhang, Hongyi Guo, Shuqi Ke, Boyi Liu,
  Zhaoran Wang","Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency",,,,,cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment, e.g., through an internal mechanism of reasoning. To this end, we propose a principled framework with provable regret guarantees to orchestrate reasoning and acting, which we call ``reason for future, act for now"" (\texttt{RAFA}). Specifically, we design a prompt template for reasoning that learns from the memory buffer and plans a future trajectory over a long horizon (``reason for future""). At each step, the LLM agent takes the initial action of the planned trajectory (``act for now""), stores the collected feedback in the memory buffer, and reinvokes the reasoning routine to replan the future trajectory from the new state.   The key idea is to cast reasoning in LLMs as learning and planning in Bayesian adaptive Markov decision processes (MDPs). Correspondingly, we prompt LLMs to form an updated posterior of the unknown environment from the memory buffer (learning) and generate an optimal trajectory for multiple future steps that maximizes a value function (planning). The learning and planning subroutines are performed in an ""in-context"" manner to emulate the actor-critic update for MDPs. Our theoretical analysis proves that the novel combination of long-term reasoning and short-term acting achieves a $\sqrt{T}$ regret. In particular, the regret bound highlights an intriguing interplay between the prior knowledge obtained through pretraining and the uncertainty reduction achieved by reasoning and acting. Our empirical validation shows that it outperforms various existing frameworks and achieves nearly perfect scores on a few benchmarks. ","[{'version': 'v1', 'created': 'Fri, 29 Sep 2023 16:36:39 GMT'}]",2023-10-02,"[['Liu', 'Zhihan', ''], ['Hu', 'Hao', ''], ['Zhang', 'Shenao', ''], ['Guo', 'Hongyi', ''], ['Ke', 'Shuqi', ''], ['Liu', 'Boyi', ''], ['Wang', 'Zhaoran', '']]",0,0,2023-09-29,1,7,2,0,0,0,d3ca116177369bf6fbe27de64506a2f401aca996,263310943.0,https://www.semanticscholar.org/paper/d3ca116177369bf6fbe27de64506a2f401aca996,arXiv.org,2023.0,87.0,1.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2249539089', 'name': 'Zhihan Liu'}, {'authorId': '2249545983', 'name': 'Hao Hu'}, {'authorId': '2145522248', 'name': 'Shenao Zhang'}, {'authorId': '2249533404', 'name': 'Hongyi Guo'}, {'authorId': '2249534528', 'name': 'Shuqi Ke'}, {'authorId': '2156641199', 'name': 'Boyi Liu'}, {'authorId': '50218397', 'name': 'Zhaoran Wang'}]","['Tsinghua University', 'Northwestern University', 'Chinese University of Hong Kong']","['China', 'United States']",2023-09
2309.17452,Zhibin Gou,"Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie
  Huang, Nan Duan, Weizhu Chen",ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving,First two authors equal contribution,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath-70B by 22% absolute. ToRA-Code-34B is also the first open-source model that achieves an accuracy exceeding 50% on MATH, which significantly outperforms GPT-4's CoT result, and is competitive with GPT-4 solving problems with programs. Additionally, we conduct a comprehensive analysis of the benefits and remaining challenges of tool interaction for mathematical reasoning, providing valuable insights for future research. ","[{'version': 'v1', 'created': 'Fri, 29 Sep 2023 17:59:38 GMT'}, {'version': 'v2', 'created': 'Wed, 4 Oct 2023 08:13:03 GMT'}]",2023-10-05,"[['Gou', 'Zhibin', ''], ['Shao', 'Zhihong', ''], ['Gong', 'Yeyun', ''], ['Shen', 'Yelong', ''], ['Yang', 'Yujiu', ''], ['Huang', 'Minlie', ''], ['Duan', 'Nan', ''], ['Chen', 'Weizhu', '']]",0,1,2023-09-29,2,8,2,1,0,1,765225535151b5908d99a96a509729795c2eb840,263310365.0,https://www.semanticscholar.org/paper/765225535151b5908d99a96a509729795c2eb840,arXiv.org,2023.0,64.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '1797090', 'name': 'Zhibin Gou'}, {'authorId': '144485528', 'name': 'Zhihong Shao'}, {'authorId': '2254121650', 'name': 'Yeyun Gong'}, {'authorId': '2237948786', 'name': 'Yelong Shen'}, {'authorId': '2253837985', 'name': 'Yujiu Yang'}, {'authorId': '2249709440', 'name': 'Minlie Huang'}, {'authorId': '46429989', 'name': 'Nan Duan'}, {'authorId': '2249538838', 'name': 'Weizhu Chen'}]","['Tsinghua University', 'Microsoft']","['China', 'United States']",2023-09
2310.00074,Hangfeng He,"Hangfeng He, Hongming Zhang, Dan Roth",SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  To comprehensively assess the capacity of current models for complex reasoning, it is crucial to assess their step-by-step reasoning in a scalable manner. Established reference-based evaluation metrics rely on human-annotated reasoning chains to assess the model-derived chains. However, such ``gold-standard'' human-written reasoning chains may not be unique and their acquisition is often labor-intensive. Existing reference-free reasoning metrics eliminate the need for human-crafted reasoning chains as references, but they typically require fine-tuning on datasets with human-derived reasoning chains, which complicates the process and raises concerns regarding generalizability across diverse datasets. To address these challenges, we harness GPT-4 to automatically evaluate reasoning chain quality, obviating the need for human-crafted references. Leveraging the Socratic method, we devise tailored prompts to enhance reference-free reasoning evaluation, which we term SocREval (Socratic method for Reasoning Evaluation). Empirical results from four human annotated datasets reveal that SocREval significantly improves GPT-4's performance, surpassing existing reference-free and reference-based reasoning evaluation metrics. Beyond its demonstrated efficacy, our proposed framework, large language models (LLMs) with the Socratic method, proves to be both cost-efficient and robust to prompt writing and example selection, as substantiated by our in-depth analysis. ","[{'version': 'v1', 'created': 'Fri, 29 Sep 2023 18:25:46 GMT'}]",2023-10-03,"[['He', 'Hangfeng', ''], ['Zhang', 'Hongming', ''], ['Roth', 'Dan', '']]",0,1,2023-09-29,1,3,2,1,0,1,12dc76b5f04fb1fc34f94f26632d193305a97e7e,263334142.0,https://www.semanticscholar.org/paper/12dc76b5f04fb1fc34f94f26632d193305a97e7e,arXiv.org,2023.0,45.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2256964816', 'name': 'Hangfeng He'}, {'authorId': '2111112132', 'name': 'Hongming Zhang'}, {'authorId': '2249759427', 'name': 'Dan Roth'}]","['University of Rochester', 'University of Pennsylvania', 'Tencent']","['China', 'United States']",2023-09
2310.00297,Jianhao Yan,"Jianhao Yan, Jin Xu, Chiyu Song, Chenming Wu, Yafu Li, Yue Zhang",Understanding In-Context Learning from Repetitions,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  This paper explores the elusive mechanism underpinning in-context learning in Large Language Models (LLMs). Our work provides a novel perspective by examining in-context learning via the lens of surface repetitions. We quantitatively investigate the role of surface features in text generation, and empirically establish the existence of \emph{token co-occurrence reinforcement}, a principle that strengthens the relationship between two tokens based on their contextual co-occurrences. By investigating the dual impacts of these features, our research illuminates the internal workings of in-context learning and expounds on the reasons for its failures. This paper provides an essential contribution to the understanding of in-context learning and its potential limitations, providing a fresh perspective on this exciting capability. ","[{'version': 'v1', 'created': 'Sat, 30 Sep 2023 08:13:49 GMT'}]",2023-10-03,"[['Yan', 'Jianhao', ''], ['Xu', 'Jin', ''], ['Song', 'Chiyu', ''], ['Wu', 'Chenming', ''], ['Li', 'Yafu', ''], ['Zhang', 'Yue', '']]",0,0,2023-09-30,1,6,1,0,0,0,a865d04c266fd2b3ea820b5741b7420779db9f73,263334398.0,https://www.semanticscholar.org/paper/a865d04c266fd2b3ea820b5741b7420779db9f73,arXiv.org,2023.0,45.0,0.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2249842908', 'name': 'Jianhao Yan'}, {'authorId': '2252968561', 'name': 'Jin Xu'}, {'authorId': '2254053209', 'name': 'Chiyu Song'}, {'authorId': '2250291793', 'name': 'Chenming Wu'}, {'authorId': '2110450452', 'name': 'Yafu Li'}, {'authorId': '2249762135', 'name': 'Yue Zhang'}]","['Westlake University', 'Baidu', 'Zhejiang University', 'Tsinghua University', 'Institute for Advanced Study']","['China', 'United States']",2023-09
2310.00492,Xuansheng Wu,"Xuansheng Wu, Wenlin Yao, Jianshu Chen, Xiaoman Pan, Xiaoyang Wang,
  Ninghao Liu, Dong Yu",From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning,"28 pages, 13 figures, 12 tables",,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Large Language Models (LLMs) have achieved remarkable success, demonstrating powerful instruction-following capabilities across diverse tasks. Instruction fine-tuning is critical in enabling LLMs to align with user intentions and effectively follow instructions. In this work, we investigate how instruction fine-tuning modifies pre-trained models, focusing on two perspectives: instruction recognition and knowledge evolution. To study the behavior shift of LLMs, we employ a suite of local and global explanation methods, including a gradient-based approach for input-output attribution and techniques for interpreting patterns and concepts in self-attention and feed-forward layers. Our findings reveal three significant impacts of instruction fine-tuning: 1) It empowers LLMs to better recognize the instruction parts from user prompts, thereby facilitating high-quality response generation and addressing the ``lost-in-the-middle'' issue observed in pre-trained models; 2) It aligns the knowledge stored in feed-forward layers with user-oriented tasks, exhibiting minimal shifts across linguistic levels. 3) It facilitates the learning of word-word relations with instruction verbs through the self-attention mechanism, particularly in the lower and middle layers, indicating enhanced recognition of instruction words. These insights contribute to a deeper understanding of the behavior shifts in LLMs after instruction fine-tuning and lay the groundwork for future research aimed at interpreting and optimizing LLMs for various applications. We will release our code and data soon. ","[{'version': 'v1', 'created': 'Sat, 30 Sep 2023 21:16:05 GMT'}]",2023-10-03,"[['Wu', 'Xuansheng', ''], ['Yao', 'Wenlin', ''], ['Chen', 'Jianshu', ''], ['Pan', 'Xiaoman', ''], ['Wang', 'Xiaoyang', ''], ['Liu', 'Ninghao', ''], ['Yu', 'Dong', '']]",0,0,2023-09-30,1,7,3,0,0,0,04b880e1e32f37b3796d41a47d013fa07095ae32,263334329.0,https://www.semanticscholar.org/paper/04b880e1e32f37b3796d41a47d013fa07095ae32,arXiv.org,2023.0,66.0,2.0,0.0,True,['Computer Science'],"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","[{'authorId': '2145346360', 'name': 'Xuansheng Wu'}, {'authorId': '2087264100', 'name': 'Wenlin Yao'}, {'authorId': '2108276402', 'name': 'Jianshu Chen'}, {'authorId': '2243367575', 'name': 'Xiaoman Pan'}, {'authorId': '2250363276', 'name': 'Xiaoyang Wang'}, {'authorId': '2256183798', 'name': 'Ninghao Liu'}, {'authorId': '2256336899', 'name': 'Dong Yu'}]","['University of Georgia', 'Tencent']","['China', 'United States']",2023-09
